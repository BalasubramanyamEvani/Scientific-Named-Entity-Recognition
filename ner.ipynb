{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TW-vrTprVey8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_37qPluUVey9"
      },
      "outputs": [],
      "source": [
        "def read_train_set(train_set_file_path):\n",
        "  with open(train_set_file_path, \"r\") as fd:\n",
        "    raw_text = fd.read().strip()\n",
        "    raw_docs = re.split(r\"\\n\\t?\\n\", raw_text)\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for doc in raw_docs:\n",
        "      tokens = []\n",
        "      tags = []\n",
        "      for line in doc.split(\"\\n\"):\n",
        "        token, tag = line.split(\"\\t\")\n",
        "        tokens.append(token)\n",
        "        tags.append(tag)\n",
        "      token_docs.append(tokens)\n",
        "      tag_docs.append(tags)\n",
        "    return token_docs, tag_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_texts, full_tags = read_train_set(os.path.join(os.getcwd(), \"data\", \"conll\", \"full_ner_dataset.conll\"))\n",
        "\n",
        "partial_texts, partial_tags = read_train_set(os.path.join(os.getcwd(), \"data\", \"conll\", \"ner_dataset_one_left_out.conll\"))"
      ],
      "metadata": {
        "id": "eKtsnl8KgMvj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n4ErzmX5Vey-"
      },
      "outputs": [],
      "source": [
        "partial_train_texts, partial_val_texts, partial_train_tags, partial_val_tags = train_test_split(partial_texts, partial_tags, test_size=0.2)\n",
        "\n",
        "own_test_txt_path = os.path.join(os.getcwd(), \"data\", \"own_test_set\", \"XLNet.txt\")\n",
        "actual_test_txt_path = os.path.join(os.getcwd(), \"data\", \"final_test_set\", \"anlp-sciner-test.txt\")\n",
        "\n",
        "own_test_paragraphs = None\n",
        "actual_test_paragraphs = None\n",
        "\n",
        "with open(own_test_txt_path, \"r\") as fd:\n",
        "  content = fd.read()\n",
        "  own_test_paragraphs = content.split(\"\\n\")\n",
        "\n",
        "with open(actual_test_txt_path, \"r\") as fd:\n",
        "  content = fd.read()\n",
        "  actual_test_paragraphs = content.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7oFpmaZgVey-"
      },
      "outputs": [],
      "source": [
        "unique_tags = set(tag for doc in full_tags for tag in doc)\n",
        "tag2id = { tag: id for id, tag in enumerate(unique_tags) }\n",
        "id2tag = { id: tag for tag, id in tag2id.items() }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJZnWTd8Vey-"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\", do_lower_case=False)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "partial_train_encodings = tokenizer(partial_train_texts, is_split_into_words=True, truncation=True, padding=True, max_length=512)\n",
        "partial_val_encodings = tokenizer(partial_val_texts, is_split_into_words=True, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "full_train_encodings = tokenizer(full_texts, is_split_into_words=True, truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "lmJgMjHXVey_"
      },
      "outputs": [],
      "source": [
        "def align_labels(tags, encodings):\n",
        "  labels = []\n",
        "  for i, label in enumerate(tags):\n",
        "    word_ids = encodings.word_ids(batch_index=i) \n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None:\n",
        "          label_ids.append(-100)\n",
        "      elif word_idx != previous_word_idx:\n",
        "          label_ids.append(tag2id[label[word_idx]])\n",
        "      else:\n",
        "          label_ids.append(-100)\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "partial_train_labels = align_labels(partial_train_tags, partial_train_encodings)\n",
        "partial_val_labels = align_labels(partial_val_tags, partial_val_encodings)\n",
        "\n",
        "full_train_labels = align_labels(full_tags, full_train_encodings)"
      ],
      "metadata": {
        "id": "7Pf1alQFkxIJ"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  logits, labels = eval_preds\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "  true_labels = [[id2tag[l] for l in label if l != -100] for label in labels]\n",
        "  true_predictions = [\n",
        "      [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "      for prediction, label in zip(predictions, labels)\n",
        "  ]\n",
        "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "  return {\n",
        "      \"precision\": all_metrics[\"overall_precision\"],\n",
        "      \"recall\": all_metrics[\"overall_recall\"],\n",
        "      \"f1\": all_metrics[\"overall_f1\"],\n",
        "      \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "  }"
      ],
      "metadata": {
        "id": "E9xe8K54XHmV"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "AHsmpQcVVey_"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ga4T8K_Vey_"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_cased\", num_labels=len(unique_tags), ignore_mismatched_sizes=True)\n",
        "# model = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=len(unique_tags), ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "MY7uORCXVezA"
      },
      "outputs": [],
      "source": [
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "partial_train_dataset = NERDataset(partial_train_encodings, partial_train_labels)\n",
        "partial_val_dataset = NERDataset(partial_val_encodings, partial_val_labels)\n",
        "\n",
        "full_train_dataset = NERDataset(full_train_encodings, full_train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ALIuVgIT27fn"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cP3dlb6VezA"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=partial_train_dataset,\n",
        "    eval_dataset=partial_val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBtTJ0hfVezA"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zlXq9bFVezB"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('./partial_saved_model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"./partial_saved_model\""
      ],
      "metadata": {
        "id": "oeBs4qSWYuMd"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test_set(paragraphs):\n",
        "  test_set_result = []\n",
        "\n",
        "  for i, paragraph in enumerate(paragraphs):\n",
        "    encoded_paragraph = tokenizer.encode(paragraph, truncation=True, max_length=512)\n",
        "    input_ids = torch.tensor(encoded_paragraph).unsqueeze(0)\n",
        "    word_ids = tokenizer(paragraph.split(\" \"), return_tensors=\"pt\", is_split_into_words=True).word_ids()\n",
        "\n",
        "    if all(id is None for id in word_ids):\n",
        "        continue\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      input_ids = input_ids.to(\"cuda\")\n",
        "      outputs = model(input_ids)\n",
        "\n",
        "    predictions = outputs[0].argmax(axis=-1)[0][1:-1]\n",
        "    paragraph_words = paragraph.split(\" \")\n",
        "\n",
        "    for j, word in enumerate(paragraph_words):\n",
        "      if j not in word_ids:\n",
        "        test_set_result.append(f\"{word}\\tO\\n\")\n",
        "      else:\n",
        "        first_subword_index = word_ids.index(j) - 1\n",
        "        if first_subword_index >= len(predictions):\n",
        "          test_set_result.append(f\"{word}\\tO\\n\")\n",
        "        else:\n",
        "          first_subword_tag = id2tag[predictions[first_subword_index].item()]\n",
        "          test_set_result.append(f\"{word}\\t{first_subword_tag}\\n\")\n",
        "\n",
        "    test_set_result.append(\"\\n\")\n",
        "  \n",
        "  return test_set_result"
      ],
      "metadata": {
        "id": "JhGWdybQEwZ_"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "own_test_set_result = process_test_set(own_test_paragraphs)"
      ],
      "metadata": {
        "id": "rXVA_HbWoxgd"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = \"\".join(own_test_set_result)\n",
        "output_s_conll_file_path = os.path.join(os.getcwd(), f\"XLNet-result.conll\")\n",
        "with open(output_s_conll_file_path, 'w') as fd:\n",
        "  fd.write(content)"
      ],
      "metadata": {
        "id": "SVtnZB3mrggE"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "TGuQE2evBD8d"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=full_train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "Vr8q3GAa9kBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MdNOg7fyBLbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('./full_saved_model')\n",
        "model_checkpoint = \"./full_saved_model\""
      ],
      "metadata": {
        "id": "YxJp7AQZ9oM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_test_set_result = process_test_set(actual_test_paragraphs)"
      ],
      "metadata": {
        "id": "tGxb4jm99ohg"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = \"\".join(actual_test_set_result)\n",
        "output_s_conll_file_path = os.path.join(os.getcwd(), f\"sciner-mysys.conll\")\n",
        "with open(output_s_conll_file_path, 'w') as fd:\n",
        "  fd.write(content)"
      ],
      "metadata": {
        "id": "hBfEUGNBpK_W"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9GiyfdQDpuT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}