{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW-vrTprVey8"
      },
      "outputs": [],
      "source": [
        "# Importing necessary packages for performing NER on scientific text related to NLP\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_37qPluUVey9"
      },
      "outputs": [],
      "source": [
        "def read_train_set(train_set_file_path):\n",
        "  \"\"\"\n",
        "  Given a path to the train set \"conll\" file, the function reads the \"conll\" file.\n",
        "  The reading is done as follows:\n",
        "  \n",
        "  1. Each paragraph (raw_doc) is extracted by splitting on double new lines\n",
        "  2. Each raw_doc is then traversed \n",
        "  3. For each raw_doc we split on new line and and then on tab to extract\n",
        "     the respecitve token and tag pair \n",
        "\n",
        "  Args:\n",
        "      train_set_file_path (str): train set conll file path\n",
        "\n",
        "  Returns:\n",
        "      tuple(List[str], List[str]): Tuple where the first value are the tokens and the second\n",
        "      value is the respective tag \n",
        "  \"\"\"\n",
        "  with open(train_set_file_path, \"r\") as fd:\n",
        "    raw_text = fd.read().strip()\n",
        "    raw_docs = re.split(r\"\\n\\t?\\n\", raw_text)\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for doc in raw_docs:\n",
        "      tokens = []\n",
        "      tags = []\n",
        "      for line in doc.split(\"\\n\"):\n",
        "        token, tag = line.split(\"\\t\") \n",
        "        tokens.append(token)\n",
        "        tags.append(tag)\n",
        "      token_docs.append(tokens)\n",
        "      tag_docs.append(tags)\n",
        "    return token_docs, tag_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKtsnl8KgMvj"
      },
      "outputs": [],
      "source": [
        "# Reading the full ner dataset created\n",
        "full_texts, full_tags = read_train_set(os.path.join(os.getcwd(), \"data\", \"conll\", \"full_ner_dataset.conll\"))\n",
        "\n",
        "# Reading the partial ner dataset which doesn't contain one file - XLNet.conll (which is used a test set)\n",
        "partial_texts, partial_tags = read_train_set(os.path.join(os.getcwd(), \"data\", \"conll\", \"ner_dataset_one_left_out.conll\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4ErzmX5Vey-"
      },
      "outputs": [],
      "source": [
        "# The partial train dataset is then split into train and validation splits -> 80% - 20%\n",
        "partial_train_texts, partial_val_texts, partial_train_tags, partial_val_tags = train_test_split(partial_texts, partial_tags, test_size=0.2)\n",
        "\n",
        "# Reading  our own test set (xlnet) and the actual test set (sciner)\n",
        "own_test_txt_path = os.path.join(os.getcwd(), \"data\", \"own_test_set\", \"XLNet.txt\")\n",
        "actual_test_txt_path = os.path.join(os.getcwd(), \"data\", \"final_test_set\", \"anlp-sciner-test.txt\")\n",
        "\n",
        "# for each test set, we obtain the respective paragraphs by splitting on newline\n",
        "own_test_paragraphs = None\n",
        "actual_test_paragraphs = None\n",
        "\n",
        "with open(own_test_txt_path, \"r\") as fd:\n",
        "  content = fd.read()\n",
        "  own_test_paragraphs = content.split(\"\\n\")\n",
        "\n",
        "with open(actual_test_txt_path, \"r\") as fd:\n",
        "  content = fd.read()\n",
        "  actual_test_paragraphs = content.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oFpmaZgVey-"
      },
      "outputs": [],
      "source": [
        "# Unique tags that our present in out full train dataset\n",
        "unique_tags = set(tag for doc in full_tags for tag in doc)\n",
        "\n",
        "# tag2id dictionary which will be useful during training\n",
        "tag2id = { tag: id for id, tag in enumerate(unique_tags) }\n",
        "\n",
        "# id2tag dictionary which will be useful to decode predicted named entities\n",
        "id2tag = { id: tag for tag, id in tag2id.items() }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJZnWTd8Vey-"
      },
      "outputs": [],
      "source": [
        "# The three models we tested, their repective tokenizers are intialized here\n",
        "# 1. BERT BASE CASED\n",
        "# 2. SCIBERT CASED\n",
        "# 3. SCIBERT UNCASED\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\", do_lower_case=False)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "# Getting the partial train, validation and full train dataset encodings\n",
        "partial_train_encodings = tokenizer(partial_train_texts, is_split_into_words=True, truncation=True, padding=True, max_length=512)\n",
        "partial_val_encodings = tokenizer(partial_val_texts, is_split_into_words=True, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "full_train_encodings = tokenizer(full_texts, is_split_into_words=True, truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmJgMjHXVey_"
      },
      "outputs": [],
      "source": [
        "def align_labels(tags, encodings):\n",
        "  \"\"\"\n",
        "  The toknizers used previously added the special tokens used by the model and also\n",
        "  certain words which were not part of it's dictionary was tokenized into two subwords.\n",
        "  This introduces a mismatch between our inputs and the labels. \n",
        "\n",
        "  To resolve this here first rule we will ll apply is that special tokens get a label of -100\n",
        "  which will be ignored during loss calculation. Then, each token gets the same label as the token \n",
        "  that started the word its inside, since they are part of the same entity\n",
        "\n",
        "  Args:\n",
        "      tags (List[str]): tags generated previously\n",
        "      encodings (List): encodings generated by the tokenizer\n",
        "  \n",
        "  Returns:\n",
        "      List[List]: aligned labels \n",
        "  \"\"\"\n",
        "  labels = []\n",
        "  for i, label in enumerate(tags):\n",
        "    word_ids = encodings.word_ids(batch_index=i) \n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "      if word_idx is None:\n",
        "          label_ids.append(-100)\n",
        "      elif word_idx != previous_word_idx:\n",
        "          label_ids.append(tag2id[label[word_idx]])\n",
        "      else:\n",
        "          label_ids.append(-100)\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pf1alQFkxIJ"
      },
      "outputs": [],
      "source": [
        "# aligning labels for partial train, validation sets and full train set\n",
        "partial_train_labels = align_labels(partial_train_tags, partial_train_encodings)\n",
        "partial_val_labels = align_labels(partial_val_tags, partial_val_encodings)\n",
        "\n",
        "full_train_labels = align_labels(full_tags, full_train_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9xe8K54XHmV"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    Definig metrics to evaluation our predictions on wich is done using\n",
        "    the Seqeval framework.\n",
        "\n",
        "    Ref: https://vkhangpham.medium.com/build-a-custom-ner-pipeline-with-hugging-face-a84d09e03d88\n",
        "\n",
        "    Args:\n",
        "        eval_preds: Predictions done on evaluation set\n",
        "\n",
        "    Returns:\n",
        "        dict: metrics dictionary which contains - precision, recall, f1 and accuracy\n",
        "    \"\"\"\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    true_labels = [[id2tag[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHsmpQcVVey_"
      },
      "outputs": [],
      "source": [
        "# Defining the data collator to feed into the Trainer API from HuggingFace\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ga4T8K_Vey_"
      },
      "outputs": [],
      "source": [
        "# Defining the three models which were experimented uppon\n",
        "# 1. BERT BASE CASED\n",
        "# 2. SCIBERT CASED\n",
        "# 3. SCIBERT UNCASED\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(unique_tags), ignore_mismatched_sizes=True)\n",
        "# model = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_cased\", num_labels=len(unique_tags), ignore_mismatched_sizes=True)\n",
        "# model = AutoModelForTokenClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels=len(unique_tags), ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY7uORCXVezA"
      },
      "outputs": [],
      "source": [
        "class NERDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Torch Dataset created for NER training process\n",
        "    \"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Initializing the partial train and validation datasets and full train dataset for\n",
        "# final model training\n",
        "\n",
        "partial_train_dataset = NERDataset(partial_train_encodings, partial_train_labels)\n",
        "partial_val_dataset = NERDataset(partial_val_encodings, partial_val_labels)\n",
        "\n",
        "full_train_dataset = NERDataset(full_train_encodings, full_train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALIuVgIT27fn"
      },
      "outputs": [],
      "source": [
        "# In case we face Cuda OOM issues \n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cP3dlb6VezA"
      },
      "outputs": [],
      "source": [
        "# Definign Training Arguments for partial dataset training and validation\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "# Initializing a Trainer instance using the above defined arguments\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=partial_train_dataset,\n",
        "    eval_dataset=partial_val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBtTJ0hfVezA"
      },
      "outputs": [],
      "source": [
        "# Calling train and evaluate - to train the model and see the performance\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zlXq9bFVezB"
      },
      "outputs": [],
      "source": [
        "# save the partial model\n",
        "\n",
        "trainer.save_model('./partial_saved_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeBs4qSWYuMd"
      },
      "outputs": [],
      "source": [
        "# load the partial model\n",
        "\n",
        "model_checkpoint = \"./partial_saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhGWdybQEwZ_"
      },
      "outputs": [],
      "source": [
        "def process_test_set(paragraphs):\n",
        "  \"\"\"\n",
        "  The function proceses the paragraphs generated from the test and returns back the final\n",
        "  conll content as string which needs to be written on disk (and submitted)\n",
        "\n",
        "  Essentially this function implements one way to get the entites for the original words. This\n",
        "  is achieved by find the index of the first subword of the original word and used that subword \n",
        "  index to get the predicted tag/entity value \n",
        "\n",
        "  Args:\n",
        "      paragraphs (List): list of paragrpahs\n",
        "\n",
        "  Returns:\n",
        "      str: conll string content which needs to be written onto a .conll file \n",
        "  \"\"\"\n",
        "  # result list to store the conll content\n",
        "  test_set_result = []\n",
        "\n",
        "  # iterating over each paragraph\n",
        "  for i, paragraph in enumerate(paragraphs):\n",
        "    # encoding the paragraph\n",
        "    encoded_paragraph = tokenizer.encode(paragraph, truncation=True, max_length=512)\n",
        "    input_ids = torch.tensor(encoded_paragraph).unsqueeze(0)\n",
        "    # fetching the word_ids\n",
        "    word_ids = tokenizer(paragraph.split(\" \"), return_tensors=\"pt\", is_split_into_words=True).word_ids()\n",
        "\n",
        "    # this condition happens due to extra new line at the end sometimes\n",
        "    if all(id is None for id in word_ids):\n",
        "        continue\n",
        "    \n",
        "    # inference to fetch model logits\n",
        "    with torch.no_grad():\n",
        "      input_ids = input_ids.to(\"cuda\")\n",
        "      outputs = model(input_ids)\n",
        "\n",
        "    # extracting the predictions by taking an argmax on logits\n",
        "    predictions = outputs[0].argmax(axis=-1)[0][1:-1]\n",
        "\n",
        "    # extract words in the test set paragraph\n",
        "    paragraph_words = paragraph.split(\" \")\n",
        "\n",
        "    # this loop essentially maps the words extracted before to it's respective tag/entity\n",
        "    for j, word in enumerate(paragraph_words):\n",
        "      \n",
        "      # due to tokenization misalignment word might not be in word_ids\n",
        "      if j not in word_ids:\n",
        "        test_set_result.append(f\"{word}\\tO\\n\")\n",
        "      else:\n",
        "        first_subword_index = word_ids.index(j) - 1\n",
        "        # O token probably special tokens\n",
        "        if first_subword_index >= len(predictions):\n",
        "          test_set_result.append(f\"{word}\\tO\\n\")\n",
        "        else:\n",
        "          first_subword_tag = id2tag[predictions[first_subword_index].item()]\n",
        "          test_set_result.append(f\"{word}\\t{first_subword_tag}\\n\")\n",
        "\n",
        "    test_set_result.append(\"\\n\")\n",
        "  \n",
        "  return test_set_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# processing our generated test set - xlnet\n",
        "\n",
        "own_test_set_result = process_test_set(own_test_paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVtnZB3mrggE"
      },
      "outputs": [],
      "source": [
        "# Write the predictions onto a file system as a conll file\n",
        "\n",
        "content = \"\".join(own_test_set_result)\n",
        "output_s_conll_file_path = os.path.join(os.getcwd(), f\"XLNet-result.conll\")\n",
        "with open(output_s_conll_file_path, 'w') as fd:\n",
        "  fd.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGuQE2evBD8d"
      },
      "outputs": [],
      "source": [
        "# to prevent Cuda OOM Issues\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr8q3GAa9kBF"
      },
      "outputs": [],
      "source": [
        "# Definign Training Arguments for full dataset training\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "# Initializing a Trainer instance using the above defined arguments\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=full_train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdNOg7fyBLbO"
      },
      "outputs": [],
      "source": [
        "# Call the train method of Trainer to train the model\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxJp7AQZ9oM0"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "# All three models are present with us and we can share it if requested\n",
        "\n",
        "trainer.save_model('./full_saved_model')\n",
        "model_checkpoint = \"./full_saved_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGxb4jm99ohg"
      },
      "outputs": [],
      "source": [
        "# processing our generated test set - sciner\n",
        "\n",
        "actual_test_set_result = process_test_set(actual_test_paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBfEUGNBpK_W"
      },
      "outputs": [],
      "source": [
        "# Write the predictions onto a file system as a conll file\n",
        "\n",
        "content = \"\".join(actual_test_set_result)\n",
        "output_s_conll_file_path = os.path.join(os.getcwd(), f\"sciner-mysys.conll\")\n",
        "with open(output_s_conll_file_path, 'w') as fd:\n",
        "  fd.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TCAOW-CmW24"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
