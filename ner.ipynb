{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import scipdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing directories for Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./data/bevani_data/pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url: str, title: str, storage_path=\"./pdfs\"):\n",
    "    response = requests.get(url, stream=True)\n",
    "    file_path = os.path.join(storage_path, f\"{title}.pdf\")\n",
    "    with open(file_path, 'wb') as fd:\n",
    "        fd.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pdf_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Survey on Model Compression for Natural Lang...</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07105v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Noisy Text Data: Achilles' Heel of popular tra...</td>\n",
       "      <td>http://arxiv.org/pdf/2110.03353v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improving the robustness and accuracy of biome...</td>\n",
       "      <td>http://arxiv.org/pdf/2111.08529v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automated essay scoring using efficient transf...</td>\n",
       "      <td>http://arxiv.org/pdf/2102.13136v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annotating the Tweebank Corpus on Named Entity...</td>\n",
       "      <td>http://arxiv.org/pdf/2201.07281v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A Survey on Model Compression for Natural Lang...   \n",
       "1  Noisy Text Data: Achilles' Heel of popular tra...   \n",
       "2  Improving the robustness and accuracy of biome...   \n",
       "3  Automated essay scoring using efficient transf...   \n",
       "4  Annotating the Tweebank Corpus on Named Entity...   \n",
       "\n",
       "                             pdf_url  \n",
       "0  http://arxiv.org/pdf/2202.07105v1  \n",
       "1  http://arxiv.org/pdf/2110.03353v1  \n",
       "2  http://arxiv.org/pdf/2111.08529v1  \n",
       "3  http://arxiv.org/pdf/2102.13136v1  \n",
       "4  http://arxiv.org/pdf/2201.07281v2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"./ner_task_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.loc[:, [\"title\", \"pdf_url\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    download_pdf(url=row[\"pdf_url\"], title=row[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Data Prepation - Parsing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_regex():\n",
    "    regex = r\"https?:\\/\\/[^\\s]+\"\n",
    "    return regex\n",
    "\n",
    "def remove_reference_num_regex():\n",
    "    regex = r\"\\[\\d+\\]\"\n",
    "    return regex\n",
    "\n",
    "def parse_and_clean_pdf(file):\n",
    "    file_content = scipdf.parse_pdf_to_dict(file)\n",
    "    res = []\n",
    "    res.append(file_content[\"abstract\"])\n",
    "    for section in file_content[\"sections\"]:\n",
    "        text = re.sub(remove_url_regex(), \"\", section[\"text\"])\n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        text = re.sub(remove_reference_num_regex(), \"\", text)\n",
    "        res.append(text)\n",
    "    res = \" \".join(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def process_pdf_data(directory):\n",
    "    pdf_text_path = os.path.join(directory, \"pdf_text\")\n",
    "    def write_content(title, content):\n",
    "        txt_file_path = os.path.join(pdf_text_path, f\"{title}.txt\")\n",
    "        with open(txt_file_path, 'w') as fd:\n",
    "            fd.write(content)\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files = [file for file in files if file.endswith(\".pdf\")]\n",
    "    files = sorted(files)\n",
    "    for file in files:\n",
    "        content = parse_and_clean_pdf(os.path.join(directory, file))\n",
    "        filename, _ = os.path.splitext(os.path.basename(file))\n",
    "        write_content(filename, content)\n",
    "        \n",
    "process_pdf_data(\"./data/bevani_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
