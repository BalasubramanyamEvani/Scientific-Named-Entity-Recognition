We show that BERT (Devlin et al., 2018) is a Markov random field language model. This formulation gives way to a natural procedure to sample sentences from BERT. We generate from BERT and find that it can produce highquality, fluent generations. Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.
BERT (Devlin et al., 2018) is a recently released sequence model used to achieve state-of-art results on a wide range of natural language understanding tasks, including constituency parsing (Kitaev and Klein, 2018) and machine translation (Lample and Conneau, 2019). Early work probing BERT's linguistic capabilities has found it surprisingly robust (Goldberg, 2019). BERT is trained on a masked language modeling objective. Unlike a traditional language modeling objective of predicting the next word in a sequence given the history, masked language modeling predicts a word given its left and right context. Because the model expects context from both directions, it is not immediately obvious how BERT can be used as a traditional language model (i.e., to evaluate the probability of a text sequence) or how to sample from it. We attempt to answer these questions by showing that BERT is a combination of a Markov random field language model (MRF-LM, Jernite et al., 2015;Mikolov et al., 2013) with pseudo loglikelihood (Besag, 1977) training. This formulation automatically leads to a sampling procedure based on Gibbs sampling.
Let X = (x 1 , . . . , x T ) be a sequence of random variables x i , each of which is categorical in that it can take one of M items from a vocabulary V = {v 1 , . . . , v M }. These random variables form a fully-connected graph with undirected edges, indicating that each variable x i is dependent on all the other variables. Joint Distribution To define a Markov random field (MRF), we start by defining a potential over cliques. Among all possible cliques, we only consider the clique corresponding to the full graph. All other cliques will be assigned a potential of 1 (i.e. exp(0)). The potential for this full-graph clique decomposes into a sum of T log-potential terms: φ(X) = T t=1 φ t (X) = exp T t=1 log φ t (X) , where we use X to denote the fully-connected graph created from the original sequence. Each log-potential φ t (X) is defined as log φ t (X) =      1h(x t ) f θ (X \t ), if [MASK] / ∈ X 1:t−1 ∪ X t+1:T 0, otherwise,(1) where f θ (X \t ) ∈ R M , 1h(x t ) is a one-hot vector with index x t set to 1, and X \t = (x 1 , . . . , x t−1 , [MASK] , x t+1 , . . . , x T ) From this log-potential, we can define a probability of a given sequence X as p θ (X) = 1 Z(θ) T t=1 φ t (X),(2) arXiv:1902.04094v2 [cs.CL] 9 Apr 2019 where Z(θ) = X T t=1 φ t (X ), for all X . This normalization constant is unfortunately impractical to compute exactly, rendering exact maximum log-likelihood intractable. Conditional Distribution Given a fixed X \t , the conditional probability of x t is derived to be p(x t |X \t ) = 1 Z(X \t ) exp(1h(x t ) f θ (X \t )),(3) where Z(X \t ) = M m=1 exp(1h(m) f θ (X \t )). This derivation follows from the peculiar formulation of the log-potential in Eq. (1). It is relatively straightforward to compute, as it is simply softmax normalization over M terms (Bridle, 1990). (Stochastic) Pseudo Log-Likelihood Learning One way to avoid the issue of intractability in computing the normalization constant Z(θ) above 1 is to resort to an approximate learning strategy. BERT uses pseudo log-likelihood learning, where the pseudo log-likelihood is defined as: PLL(θ; D) = 1 |D| X∈D |X| t=1 log p(x t |X \t ), . (4 ) where D is a set of training examples. We maximize the predictability of each token in a sequence given all the other tokens, instead of the joint probability of the entire sequence. It is still expensive to compute the pseudo loglikelihood in Eq. (4) for even one example, especially when f θ is not linear. This is because we must compute |X| forward passes of f θ for each sequence, when |X| can be long and f θ be computationally heavy. Instead we could stochastically estimate it by 1 |X| |X| t=1 log p(x t |X \t ) =E t∼U ({1,...,|X|}) log p(x t |X \t ) ≈ 1 K K k=1 log p(xt k |X \t k ), wheret k ∼ U({1, . . . , |X|}. Let us refer to this as stochastic pseudo log-likelihood learning. In Reality The stochastic pseudo log-likelihood learning above states that we "mask out" one token in a sequence at a time and let f θ predict it based on all the other "observed" tokens in the sequence. Devlin et al. (2018) however proposed to "mask out" multiple tokens at a time and predict all of them given both all "observed" and "masked out" tokens in the sequence. This brings the original BERT closer to a denoising autoencoder , which could still be considered as training a Markov random field with (approximate) score matching (Vincent, 2011).
The discussion so far implies that BERT is a Markov random field language model (MRF-LM) and that it learns a distribution over sentences (of some given length). This framing suggests that we can use BERT not only as parameter initialization for finetuning but as a generative model of sentences to either score a sentence or sample a sentence. Ranking Let us fix the length T . Then, we can use BERT to rank a set of sentences. We cannot compute the exact probabilities of these sentences, but we can compute their unnormalized log-probabilities according to Eq. (2): T t=1 log φ t (X). These unnormalized probabilities can be used to find the most likely sentence within the set or to sort the sentences according to their probabilities. Sampling Sampling from a Markov random field is less trivial than is from a directed graphical model which naturally admits ancestral sampling. One of the most widely used approaches the nearest regional centre is alemanno , with another connection to potenza and maradona , and the nearest railway station is in bergamo , where the line terminates on its northern end for all of thirty seconds , she was n't going to speak . maybe this time , she 'd actually agree to go . thirty seconds later , she 'd been speaking to him in her head every ' let him get away , mrs . nightingale . you could do it again . ' ' he -' ' no , please . i have to touch him . and when you do , you run . " oh , i 'm sure they would be of a good service , " she assured me . " how are things going in the morning ? is your husband well ? " " yes , very well he also " turned the tale [ of ] the marriage into a book " as he wanted it to " be elegiac " . both sagas contain stories of both couple and their wedding night ; " i know . " she paused ." did he touch you ? " " no . " " ah . " " oh , no , " i said , confused , not sure why " i had a bad dream . " " about an alien ship ? who was it ? " i check the text message that 's been only partially restored yet, the one that says love . i watched him through the glass , wondering if he was going to attempt to break in on our meeting . but he did n't seem to even bother to knock when he entered the room . i was n't replaced chris hall ( st . louis area manager ) . june 9 : mike howard ( syndicated " good morning " , replaced steve koval , replaced dan nickolas , and replaced phil smith ) ; " how long has it been since you have made yourself an offer like that ? " asked planner . " oh " was the reply . planner had heard of some of his other business associates who had is Markov-chain Monte-Carlo (MCMC) sampling (Neal, 1993;Swendsen and Wang, 1986;Salakhutdinov, 2009;Desjardins et al., 2010;Cho et al., 2010). In this report, we only consider Gibbs sampling which fits naturally with (stochastic) pseudo log-likelihood learning. In Gibbs sampling, we start with a random initial state X 0 , which we initialize to be an all-mask sequence, i.e., ([MASK] , . . . , [MASK]), though we could with a sentence consisting of randomly sampled words or by retrieving a sentence from data. At each iteration i, we sample the position t i uniformly at random from {1, . . . , T } and mask out the selected location, i.e., x i t i = [MASK], resulting in X i \t i . We now compute p(x t i |X i \t i ) according to Eq. (3), samplex t i from it 2 , and construct the next sequence by X i+1 = (x i 1 , . . . , x i t i −1 ,x t i , x i t i +1 , . . . , x i T ). We repeat this procedure many times, preferably with thinning. 3 Because Gibbs sampling, as well as any MCMC sampler with a local proposal distribution, tends to get stuck in a mode of the distribution, we advise running multiple chains of Gibbs sampling or using different sentence initializations.
The undirectedness of the MRF-LM and the bidirectional nature of BERT do not naturally admit sequential sampling, but given that the dominant approach to text generation is left-to-right, we experiment with generating from BERT in such a manner. As with our non-sequential sampling scheme, we can begin with a seed sentence of either all masks or a random sentence. Whereas previously we sampled a position t ∈ {1, . . . , T } to mask out and generate for at each time step, in the sequential setting, at each time step t, we mask out x t t , generate a word for that position, and substitute it into the sequence. After T timesteps, we have a sampled a token at each position, at which we point we can terminate or repeat the process from the current sentence.
Our experiments demonstrate the potential of using BERT as a standalone language model rather than as a parameter initializer for transfer learning (Devlin et al., 2018;Lample and Conneau, 2019;Nogueira and Cho, 2019). We show that sentences sampled from BERT are well-formed and are assigned high probabilities by an off-theshelf language model. We take pretrained BERT models trained on a mix of Toronto Book Corpus (TBC, Zhu et al., 2015) and Wikipedia provided by Devlin et al. (2018) and its PyTorch implementation 4 provided by HuggingFace. We experiment with both the base and large BERT configuations.
We consider several evaluation metrics to estimate the quality and diversity of the generations.  Quality To automatically measure the quality of the generations, we follow Yu et al. (2017) by computing BLEU (Papineni et al., 2002) between the generations and the original data distributions to measure how similar the generations are. We use a random sample of 5000 sentences from the test set of WikiText-103 (WT103, Merity et al., 2016) and a random sample of 5000 sentences from TBC as references. We also use the perplexity of a trained language model evaluated on the generations as a rough proxy for fluency. Specifically, we use the Gated Convolutional Language Model (Dauphin et al., 2016) pretrained on WikiText-103 5 . Diversity To measure the diversity of each model's generations, we compute self-BLEU (Zhu et al., 2018): for each generated sentence, we compute BLEU treating the rest of the sentences as references, and average across sentences. Self-BLEU measures how similar each generated sentence is to the other generations; high self-BLEU indicates that the model has low sample diversity. We also evaluate the percentage of n-grams that are unique, when compared to the original data distribution and within the corpus of generations. We note that this metric is somewhat in opposition to BLEU between generations and data, as fewer unique n-grams implies higher BLEU. Methodology We use the non-sequential sampling scheme with sampling from the top k = 100 most frequent words at each time step, as empirically this led to the most coherent generations. We show generations from the sequential sampler in Table 4 (Dauphin et al., 2016). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets. Transformer (Radford et al., 2018, GPT), which was trained on TBC and has approximately the same number of parameters as the base configuration of BERT. For BERT, we pad each input with special symbols [CLS] and [SEP]. For GPT, we start with a start of sentence token and generate left to right. For all models, we generate 1000 uncased sequences of length 40. Finally, as a trivial baseline, we sample 1000 sentences from TBC and the training split of WT103 and compute all automatic metrics against these samples.
We present sample generations, quality results, and diversity results respectively in Tables 1, 2, 3. We find that, compared to GPT, the BERT generations are of worse quality, but are more diverse. Surprisingly, the outside language model, which was trained on Wikipedia, is less perplexed by the GPT generations than the BERT generations, even though GPT was only trained on romance novels and BERT was trained on romance novels and Wikipedia. On actual data from TBC, the outside language model is about as perplexed as on the BERT generations, which suggests that domain shift is an issue in using a trained language model for evaluating generations and that the GPT generations might have collapsed to fairly generic and simple sentences. This observation is further bolstered by the fact that the GPT generations have a higher corpus-BLEU with TBC than TBC has with itself. The perplexity on BERT samples is not absurdly high, and in reading the samples, we find that many are fairly coherent. The corpus-BLEU between BERT models and the datasets is low, particularly with WT103. We find that BERT generations are more diverse than GPT generations. GPT has high n-gram overlap (smaller percent of unique n-grams) with TBC, but surprisingly also with WikiText-103, despite being trained on different data. Furthermore, GPT generations have greater n-gram overlap with these datasets than these datasets have with themselves, further suggesting that GPT is relying significantly on generic sentences. BERT has lower n-gram overlap with both corpora, with similar degrees of n-gram overlap as the samples of the data. For a more rigorous evaluation of generation quality, we collect human judgments on sentence fluency for 100 samples from BERT large, BERT base, and GPT using a four point Likert scale. For each sample we ask three annotators to rate the sentence on its fluency and take the average of the three judgments as the sentence's fluency score. We present a histogram of the results in Figure 1. For BERT large, BERT base, and GPT we respectively get mean scores over the samples of 2.37 (σ = 0.83), 2.65 (σ = 0.65), and 2.80 (σ = 0.51). All means are within a standard deviation of each other. BERT base and GPT have similar unimodal distributions with BERT base having a slightly more non-fluent samples. BERT large has a bimodal distribution.
We show that BERT is a Markov random field language model. Formulating BERT in this way gives rise to a practical algorithm for generating from BERT based on Gibbs sampling that does not require any additional parameters or training. We verify in experiments that the algorithm produces diverse and fairly fluent generations. The power of this framework is in allowing the principled application of Gibbs sampling, and potentially other MCMC algorithms, for generating from BERT. Future work might explore these improved sampling methods, especially those that do not need to run the model over the entire sequence each iteration and that more robustly handle variablelength sequences. To facilitate such investigation, we release our code on GitHub at https: //github.com/nyu-dl/bert-gen and a demo as a Colab notebook at https:// colab.research.google.com/drive/ 1MxKZGtQ9SSBjTK5ArsZ5LKhkztzg52RV.
We thank Ilya Kulikov and Nikita Nangia for their help, as well as reviewers for insightful comments. AW is supported by an NSF Fellowship. KC is partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from Pattern Recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure).