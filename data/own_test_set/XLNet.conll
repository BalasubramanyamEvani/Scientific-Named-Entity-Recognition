With	O
the	O
capability	O
of	O
modeling	O
bidirectional	O
contexts,	O
denoising	O
autoencoding	O
based	O
pretraining	O
like	O
BERT	B-MethodName
achieves	O
better	O
performance	O
than	O
pretraining	O
approaches	O
based	O
on	O
autoregressive	O
language	O
modeling.	O
However,	O
relying	O
on	O
corrupting	O
the	O
input	O
with	O
masks,	O
BERT	B-MethodName
neglects	O
dependency	O
between	O
the	O
masked	O
positions	O
and	O
suffers	O
from	O
a	O
pretrain-finetune	O
discrepancy.	O
In	O
light	O
of	O
these	O
pros	O
and	O
cons,	O
we	O
propose	O
XLNet,	B-MethodName
a	O
generalized	O
autoregressive	O
pretraining	O
method	O
that	O
(1)	O
enables	O
learning	O
bidirectional	O
contexts	O
by	O
maximizing	O
the	O
expected	O
likelihood	O
over	O
all	O
permutations	O
of	O
the	O
factorization	O
order	O
and	O
(2)	O
overcomes	O
the	O
limitations	O
of	O
BERT	B-MethodName
thanks	O
to	O
its	O
autoregressive	O
formulation.	O
Furthermore,	O
XLNet	B-MethodName
integrates	O
ideas	O
from	O
Transformer-XL,	O
the	O
state-of-the-art	O
autoregressive	O
model,	O
into	O
pretraining.	O
Empirically,	O
under	O
comparable	O
experiment	O
settings,	O
XLNet	B-MethodName
outperforms	O
BERT	B-MethodName
on	O
20	O
tasks,	O
often	O
by	O
a	O
large	O
margin,	O
including	O
question	B-TaskName
answering,	I-TaskName
natural	B-TaskName
language	I-TaskName
inference,	I-TaskName
sentiment	B-TaskName
analysis,	I-TaskName
and	O
document	B-TaskName
ranking.	I-TaskName
1	O
.	O

Unsupervised	O
representation	O
learning	O
has	O
been	O
highly	O
successful	O
in	O
the	O
domain	O
of	O
natural	O
language	O
processing	O
.	O
Typically,	O
these	O
methods	O
first	O
pretrain	O
neural	O
networks	O
on	O
large-scale	O
unlabeled	O
text	O
corpora,	O
and	O
then	O
finetune	O
the	O
models	O
or	O
representations	O
on	O
downstream	O
tasks.	O
Under	O
this	O
shared	O
high-level	O
idea,	O
different	O
unsupervised	O
pretraining	O
objectives	O
have	O
been	O
explored	O
in	O
literature.	O
Among	O
them,	O
autoregressive	B-TaskName
(AR)	B-TaskName
language	B-TaskName
modeling	I-TaskName
and	O
autoencoding	B-TaskName
(AE)	B-TaskName
have	O
been	O
the	O
two	O
most	O
successful	O
pretraining	O
objectives.	O
AR	B-TaskName
language	I-TaskName
modeling	I-TaskName
seeks	O
to	O
estimate	O
the	O
probability	O
distribution	O
of	O
a	O
text	O
corpus	O
with	O
an	O
autoregressive	O
model	O
.	O
Specifically,	O
given	O
a	O
text	O
sequence	O
x	O
=	O
(x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
T	O
),	O
AR	B-TaskName
language	I-TaskName
modeling	I-TaskName
factorizes	O
the	O
likelihood	O
into	O
a	O
forward	O
product	O
p(x)	O
=	O
T	O
t=1	O
p(x	O
t	O
|	O
x	O
<t	O
)	O
or	O
a	O
backward	O
one	O
p(x)	O
=	O
1	O
t=T	O
p(x	O
t	O
|	O
x	O
>t	O
).	O
A	O
parametric	O
model	O
(e.g.	O
a	O
neural	O
network)	O
is	O
trained	O
to	O
model	O
each	O
conditional	O
distribution.	O
Since	O
an	O
AR	O
language	O
model	O
is	O
only	O
trained	O
to	O
encode	O
a	O
uni-directional	O
context	O
(either	O
forward	O
or	O
backward),	O
it	O
is	O
not	O
effective	O
at	O
modeling	O
deep	O
bidirectional	O
contexts.	O
On	O
the	O
contrary,	O
downstream	O
language	O
understanding	O
tasks	O
often	O
require	O
bidirectional	O
context	O
information.	O
This	O
results	O
in	O
a	O
gap	O
between	O
AR	O
language	O
modeling	O
and	O
effective	O
pretraining.	O
In	O
comparison,	O
AE	O
based	O
pretraining	O
does	O
not	O
perform	O
explicit	O
density	O
estimation	O
but	O
instead	O
aims	O
to	O
reconstruct	O
the	O
original	O
data	O
from	O
corrupted	O
input.	O
A	O
notable	O
example	O
is	O
BERT	B-MethodName
,	O
which	O
has	O
been	O
the	O
state-of-the-art	O
pretraining	O
approach.	O
Given	O
the	O
input	O
token	O
sequence,	O
a	O
certain	O
portion	O
of	O
tokens	O
are	O
replaced	O
by	O
a	O
special	O
symbol	O
[MASK],	O
and	O
the	O
model	O
is	O
trained	O
to	O
recover	O
the	O
original	O
tokens	O
from	O
the	O
corrupted	O
version.	O
Since	O
density	O
estimation	O
is	O
not	O
part	O
of	O
the	O
objective,	O
BERT	B-MethodName
is	O
allowed	O
to	O
utilize	O
bidirectional	O
contexts	O
for	O
reconstruction.	O
As	O
an	O
immediate	O
benefit,	O
this	O
closes	O
the	O
aforementioned	O
bidirectional	O
information	O
gap	O
in	O
AR	O
language	O
modeling,	O
leading	O
to	O
improved	O
performance.	O
However,	O
the	O
artificial	O
symbols	O
like	O
[MASK]	O
used	O
by	O
BERT	B-MethodName
during	O
pretraining	O
are	O
absent	O
from	O
real	O
data	O
at	O
finetuning	O
time,	O
resulting	O
in	O
a	O
pretrain-finetune	O
discrepancy.	O
Moreover,	O
since	O
the	O
predicted	O
tokens	O
are	O
masked	O
in	O
the	O
input,	O
BERT	B-MethodName
is	O
not	O
able	O
to	O
model	O
the	O
joint	O
probability	O
using	O
the	O
product	O
rule	O
as	O
in	O
AR	O
language	O
modeling.	O
In	O
other	O
words,	O
BERT	B-MethodName
assumes	O
the	O
predicted	O
tokens	O
are	O
independent	O
of	O
each	O
other	O
given	O
the	O
unmasked	O
tokens,	O
which	O
is	O
oversimplified	O
as	O
high-order,	O
long-range	O
dependency	O
is	O
prevalent	O
in	O
natural	O
language	O
.	O
Faced	O
with	O
the	O
pros	O
and	O
cons	O
of	O
existing	O
language	O
pretraining	O
objectives,	O
in	O
this	O
work,	O
we	O
propose	O
XLNet,	B-MethodName
a	O
generalized	O
autoregressive	O
method	O
that	O
leverages	O
the	O
best	O
of	O
both	O
AR	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
AE	B-TaskName
while	O
avoiding	O
their	O
limitations.	O
•	O
Firstly,	O
instead	O
of	O
using	O
a	O
fixed	O
forward	O
or	O
backward	O
factorization	O
order	O
as	O
in	O
conventional	O
AR	O
models,	O
XLNet	B-MethodName
maximizes	O
the	O
expected	O
log	O
likelihood	O
of	O
a	O
sequence	O
w.r.t.	O
all	O
possible	O
permutations	O
of	O
the	O
factorization	O
order.	O
Thanks	O
to	O
the	O
permutation	O
operation,	O
the	O
context	O
for	O
each	O
position	O
can	O
consist	O
of	O
tokens	O
from	O
both	O
left	O
and	O
right.	O
In	O
expectation,	O
each	O
position	O
learns	O
to	O
utilize	O
contextual	O
information	O
from	O
all	O
positions,	O
i.e.,	O
capturing	O
bidirectional	O
context.	O
•	O
Secondly,	O
as	O
a	O
generalized	O
AR	O
language	O
model,	O
XLNet	B-MethodName
does	O
not	O
rely	O
on	O
data	O
corruption.	O
Hence,	O
XLNet	B-MethodName
does	O
not	O
suffer	O
from	O
the	O
pretrain-finetune	O
discrepancy	O
that	O
BERT	B-MethodName
is	O
subject	O
to.	O
Meanwhile,	O
the	O
autoregressive	O
objective	O
also	O
provides	O
a	O
natural	O
way	O
to	O
use	O
the	O
product	O
rule	O
for	O
factorizing	O
the	O
joint	O
probability	O
of	O
the	O
predicted	O
tokens,	O
eliminating	O
the	O
independence	O
assumption	O
made	O
in	O
BERT.	B-MethodName
In	O
addition	O
to	O
a	O
novel	O
pretraining	O
objective,	O
XLNet	B-MethodName
improves	O
architectural	O
designs	O
for	O
pretraining.	O
•	O
Inspired	O
by	O
the	O
latest	O
advancements	O
in	O
AR	O
language	O
modeling,	O
XLNet	B-MethodName
integrates	O
the	O
segment	O
recurrence	O
mechanism	O
and	O
relative	O
encoding	O
scheme	O
of	O
Transformer-XL	O
into	O
pretraining,	O
which	O
empirically	O
improves	O
the	O
performance	O
especially	O
for	O
tasks	O
involving	O
a	O
longer	O
text	O
sequence.	O
•	O
Naively	O
applying	O
a	O
Transformer(-XL)	O
architecture	O
to	O
permutation-based	O
language	O
modeling	O
does	O
not	O
work	O
because	O
the	O
factorization	O
order	O
is	O
arbitrary	O
and	O
the	O
target	O
is	O
ambiguous.	O
As	O
a	O
solution,	O
we	O
propose	O
to	O
reparameterize	O
the	O
Transformer(-XL)	O
network	O
to	O
remove	O
the	O
ambiguity.	O
Empirically,	O
under	O
comparable	O
experiment	O
setting,	O
XLNet	B-MethodName
consistently	O
outperforms	O
BERT	B-MethodName
on	O
a	O
wide	O
spectrum	O
of	O
problems	O
including	O
GLUE	B-DatasetName
language	B-TaskName
understanding	I-TaskName
tasks,	O
reading	B-TaskName
comprehension	I-TaskName
tasks	O
like	O
SQuAD	B-DatasetName
and	O
RACE,	B-DatasetName
text	B-TaskName
classification	I-TaskName
tasks	O
such	O
as	O
Yelp	B-DatasetName
and	O
IMDB,	B-DatasetName
and	O
the	O
ClueWeb09-B	B-DatasetName
document	B-TaskName
ranking	I-TaskName
task.	O

The	O
idea	O
of	O
permutation-based	O
AR	B-TaskName
modeling	I-TaskName
has	O
been	O
explored	O
in	O
,	O
but	O
there	O
are	O
several	O
key	O
differences.	O
Firstly,	O
previous	O
models	O
aim	O
to	O
improve	O
density	O
estimation	O
by	O
baking	O
an	O
"orderless"	O
inductive	O
bias	O
into	O
the	O
model	O
while	O
XLNet	B-MethodName
is	O
motivated	O
by	O
enabling	O
AR	O
language	O
models	O
to	O
learn	O
bidirectional	O
contexts.	O
Technically,	O
to	O
construct	O
a	O
valid	O
target-aware	O
prediction	O
distribution,	O
XLNet	B-MethodName
incorporates	O
the	O
target	O
position	O
into	O
the	O
hidden	O
state	O
via	O
two-stream	O
attention	O
while	O
previous	O
permutation-based	O
AR	O
models	O
relied	O
on	O
implicit	O
position	O
awareness	O
inherent	O
to	O
their	O
MLP	O
architectures.	O
Finally,	O
for	O
both	O
orderless	O
NADE	O
and	O
XLNet,	B-MethodName
we	O
would	O
like	O
to	O
emphasize	O
that	O
"orderless"	O
does	O
not	O
mean	O
that	O
the	O
input	O
sequence	O
can	O
be	O
randomly	O
permuted	O
but	O
that	O
the	O
model	O
allows	O
for	O
different	O
factorization	O
orders	O
of	O
the	O
distribution.	O
Another	O
related	O
idea	O
is	O
to	O
perform	O
autoregressive	O
denoising	O
in	O
the	O
context	O
of	O
text	O
generation	O
,	O
which	O
only	O
considers	O
a	O
fixed	O
order	O
though.	O
2	O
Proposed	O
Method	O

In	O
this	O
section,	O
we	O
first	O
review	O
and	O
compare	O
the	O
conventional	O
AR	O
language	O
modeling	O
and	O
BERT	B-MethodName
for	O
language	O
pretraining.	O
Given	O
a	O
text	O
sequence	O
x	O
=	O
[x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
T	O
],	O
AR	O
language	O
modeling	O
performs	O
pretraining	O
by	O
maximizing	O
the	O
likelihood	O
under	O
the	O
forward	O
autoregressive	O
factorization:	O
max	O
θ	O
log	O
p	O
θ	O
(x)	O
=	O
T	O
t=1	O
log	O
p	O
θ	O
(x	O
t	O
|	O
x	O
<t	O
)	O
=	O
T	O
t=1	O
log	O
exp	O
h	O
θ	O
(x	O
1:t−1	O
)	O
e(x	O
t	O
)	O
x	O
exp	O
(h	O
θ	O
(x	O
1:t−1	O
)	O
e(x	O
))	O
,(1)	O
where	O
h	O
θ	O
(x	O
1:t−1	O
)	O
is	O
a	O
context	O
representation	O
produced	O
by	O
neural	O
models,	O
such	O
as	O
RNNs	O
or	O
Transformers,	O
and	O
e(x)	O
denotes	O
the	O
embedding	O
of	O
x.	O
In	O
comparison,	O
BERT	B-MethodName
is	O
based	O
on	O
denoising	O
auto-encoding.	O
Specifically,	O
for	O
a	O
text	O
sequence	O
x,	O
BERT	B-MethodName
first	O
constructs	O
a	O
corrupted	O
versionx	O
by	O
randomly	O
setting	O
a	O
portion	O
(e.g.	O
15%)	O
of	O
tokens	O
in	O
x	O
to	O
a	O
special	O
symbol	O
[MASK].	O
Let	O
the	O
masked	O
tokens	O
bex.	O
The	O
training	O
objective	O
is	O
to	O
reconstructx	O
fromx:	O
max	O
θ	O
log	O
p	O
θ	O
(x	O
|x)	O
≈	O
T	O
t=1	O
m	O
t	O
log	O
p	O
θ	O
(x	O
t	O
|x)	O
=	O
T	O
t=1	O
m	O
t	O
log	O
exp	O
H	O
θ	O
(x)	O
t	O
e(x	O
t	O
)	O
x	O
exp	O
H	O
θ	O
(x)	O
t	O
e(x	O
)	O
,(2)	O
where	O
m	O
t	O
=	O
1	O
indicates	O
x	O
t	O
is	O
masked,	O
and	O
H	O
θ	O
is	O
a	O
Transformer	O
that	O
maps	O
a	O
length-T	O
text	O
sequence	O
x	O
into	O
a	O
sequence	O
of	O
hidden	O
vectors	O
H	O
θ	O
(x)	O
=	O
[H	O
θ	O
(x)	O
1	O
,	O
H	O
θ	O
(x)	O
2	O
,	O
•	O
•	O
•	O
,	O
H	O
θ	O
(x)	O
T	O
].	O
The	O
pros	O
and	O
cons	O
of	O
the	O
two	O
pretraining	O
objectives	O
are	O
compared	O
in	O
the	O
following	O
aspects:	O
•	O
Independence	O
Assumption:	O
As	O
emphasized	O
by	O
the	O
≈	O
sign	O
in	O
Eq.	O
(2),	O
BERT	B-MethodName
factorizes	O
the	O
joint	O
conditional	O
probability	O
p(x	O
|x)	O
based	O
on	O
an	O
independence	O
assumption	O
that	O
all	O
masked	O
tokensx	O
are	O
separately	O
reconstructed.	O
In	O
comparison,	O
the	O
AR	O
language	O
modeling	O
objective	O
(1)	O
factorizes	O
p	O
θ	O
(x)	O
using	O
the	O
product	O
rule	O
that	O
holds	O
universally	O
without	O
such	O
an	O
independence	O
assumption.	O
•	O
Input	O
noise:	O
The	O
input	O
to	O
BERT	B-MethodName
contains	O
artificial	O
symbols	O
like	O
[MASK]	O
that	O
never	O
occur	O
in	O
downstream	O
tasks,	O
which	O
creates	O
a	O
pretrain-finetune	O
discrepancy.	O
Replacing	O
[MASK]	O
with	O
original	O
tokens	O
as	O
in	O
does	O
not	O
solve	O
the	O
problem	O
because	O
original	O
tokens	O
can	O
be	O
only	O
used	O
with	O
a	O
small	O
probability	O
-otherwise	O
Eq.	O
(2)	O
will	O
be	O
trivial	O
to	O
optimize.	O
In	O
comparison,	O
AR	O
language	O
modeling	O
does	O
not	O
rely	O
on	O
any	O
input	O
corruption	O
and	O
does	O
not	O
suffer	O
from	O
this	O
issue.	O
•	O
Context	O
dependency:	O
The	O
AR	O
representation	O
h	O
θ	O
(x	O
1:t−1	O
)	O
is	O
only	O
conditioned	O
on	O
the	O
tokens	O
up	O
to	O
position	O
t	O
(i.e.	O
tokens	O
to	O
the	O
left),	O
while	O
the	O
BERT	B-MethodName
representation	O
H	O
θ	O
(x)	O
t	O
has	O
access	O
to	O
the	O
contextual	O
information	O
on	O
both	O
sides.	O
As	O
a	O
result,	O
the	O
BERT	B-MethodName
objective	O
allows	O
the	O
model	O
to	O
be	O
pretrained	O
to	O
better	O
capture	O
bidirectional	O
context.	O
