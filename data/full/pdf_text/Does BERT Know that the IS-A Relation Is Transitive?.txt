The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models.
One such question is: can a black-box model make logically consistent predictions for transitive relations?
Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context.
However, to what extent BERT captures the transitive nature of some lexical relations is unclear.
From a probing perspective, we examine WordNet word senses and the IS-A relation, which is a transitive relation.
That is, for senses A, B, and C, A is-a B and B is-a C entail
A is-a C. We aim to quantify how much BERT agrees with the transitive property of IS-A relations, via a minimalist probing setting.
Our investigation reveals that BERT's predictions do not fully obey the transitivity property of the IS-A relation.
1    The IS-A relation denotes a subclass relation.
If A is-a B, then the concept A is a subclass of the concept B, or A is subsumed by B. The IS-A relation is frequently encoded in lexical taxonomies.
The IS-A relation has great significance since it empowers generalization, and generalization is at the core of machine inference for text understanding.
The IS-A hierarchy is inherently transitive, i.e., for three concepts (or word senses) A, B, and C, A is-a B and B is-a C entail
A is-a C. For example, knowing that humanoid is a type of automaton, and automaton is a type of artifact, then by transitivity, the relation humanoid is an artifact also holds.
The concept of transitivity is easy to comprehend by humans.
However, deep learning models, including pre-trained language models such as BERT (Devlin et al., 2019), are known to lack 1
The source code and dataset of this paper are available at  probe-bert-transitivity.
some human-level generalization capacities in text understanding, or it may show some capacities for making correct predictions but for the wrong reasons, including being insensitive to negation and exploiting only surface features (Kassner and Schutze, 2020;Ettinger, 2020), lacking understanding of perceptual properties (Forbes et al., 2019;Weir et
al., 2020), and surface form competition (Holtzman et al., 2021).
Despite the issues raised above, previous work has shown that BERT's layers align with the NLP pipeline, and representations in the different layers of BERT are found to capture different levels of textual understanding, from syntactic (e.g., partof-speech tagging) to semantic (e.g., semantic role labeling) as the layers go from the lower to higher layers (Tenney et al., 2019a,b).
Recent studies also suggest that BERT can capture lexical relation clues from words in contexts (Vulić et al., 2020;Misra et
al., 2020).
Researchers begin to recognize BERT as an open knowledge source and query BERT for information (Petroni et al., 2019).
Moreover, BERT, even without fine-tuning on downstream tasks, possesses a fair ability to produce contextualized embeddings that cluster to word senses (Wiedemann et al., 2019;Haber and Poesio, 2020;Mickus et al., 2020;Loureiro et al., 2021).
These findings suggest that BERT has some understanding of the building blocks of language.
Following these findings, since an IS-A taxonomy can be built on top of explicit word senses, do contextualized embeddings learned from BERT for word senses (in particular contexts) respect the properties of the IS-A taxonomy, specifically transitivity?
That is, does BERT make logically consistent predictions that enforce the transitivity constraint of the IS-A relation?
In this paper, we introduce a minimalist probing method to investigate whether BERT knows that the IS-A relation is transitive.
We first quantify how well BERT predicts the IS-A relation.
Next, we measure the extent to which BERT enforces the transitivity constraint.
That is, given that BERT predicts A is-a B and B is-a C, does it then predict A is-a C?
In our work, we make use of WordNet (Fellbaum, 1998) and propose a method to sample word sense pairs with contexts from WordNet example sentences to build a probing dataset.
We use a nearest neighbor classifier for probing, which does not require any parameter tuning.
Our findings indicate that BERT can predict IS-A relations with an accuracy score of 72.6%.
However, when BERT predicts A is-a B and B is-a C, it only predicts A is-a C 82.4% of the time.
This suggests that simply treating BERT as is as a knowledge base (Petroni et al., 2019) is not completely satisfactory, and additional work needs to be done to incorporate the transitivity constraint in natural language inference when using BERT.
A key weakness of deep learning models is that they are black-box models and do not offer explainable and interpretable predictions.
This has led to a large body of research regarding their interpretability (Linardatos et al., 2021).
The pre-trained language model BERT has been extensively analyzed since its release.
In particular, feature-based probes have been proposed to show how a particular layer, head, or neuron of BERT works on a downstream NLP task.
Usually with a small set of additional parameters, a probe is trained in a supervised manner using feature representations from the pre-trained BERT, e.g., contextualized embeddings, to solve a particular task (Wu et al., 2020).
Attention and structural probes have been invented to investigate different aspects of BERT and linguistic properties (Lin et al., 2019;Jawahar et
al., 2019;Clark et
al., 2019;Hewitt and Manning, 2019;Manning et
al., 2020;Tenney et al., 2019a,b;Pruksachatkun et al., 2020).
Latent ontology of contextual embeddings has also been investigated via cluster analysis (Michael et al., 2020).
On probing contextualized representations for lexico-semantic relations, previous studies have investigated BERT for lexical relation classification via a neural network probe on type-level embeddings (Vulić et al., 2020).
Our work differs from prior work by our goal to explicitly investigate how much BERT understands the IS-A relation and more importantly, obeys the transitivity constraint.
That is, we aim to determine if and how often BERT makes logically consistent predictions for the IS-A relation.
Moreover, we focus on investigating sense-based IS-A relation, which is associated with explicit word senses in their contexts, so contextualized embeddings can be clearly mapped to word senses.
We probe if and how well BERT can predict the IS-A relation and its transitivity.
Task Definition For a dataset of interest, we denote it as D = {[(u 1 , v 1 ), y 1 ], • • • ,
[(u n , v n ), y n ]}.
(u 1:n , v 1:n )
[(u 1 , v 1 ) • • • , (u n , v n )] are representations for pairs of word senses and y 1:n
= (y 1 , • • • , y n ) are labels for the IS-A relation classification task, where 1 denotes the positive IS-A relation and 0 otherwise.
In our probing task, we quantify the extent to which (u 1:n , v 1:n ) encode relations
y 1:n .
To probe BERT, we use the contextualized embedding (i.e., BERT's final hidden state output) of a word in a given context as the representation for the sense associated with the word's meaning in that context.
In this work, we focus on the BERT-base model.
Given a target word w t and its context c, BERT produces a final hidden state output as the contextualized embedding o t for the target word w t .
If w t is tokenized into subwords, we take the average over all subwords to be the contextualized embedding.
WordNet (Fellbaum, 1998) is a rich lexical database of word senses connected via the IS-A relation with example sentences for sense usages, making it a natural resource for probing.
A 1-hop IS-A relation is illustrated in Figure 1.
By transitivity, an n-hop IS-A relation is formed from a chain of n parent-child IS-A links.
We focus on noun pairs in this work as nouns make up 70% of all senses in WordNet, and the path lengths for nouns are often longer than for verbs.
We propose a path-based sampling method to generate pairs from WordNet, as follows:
1. Let L = { s | s ∈ S and hypo(s)
= ∅ } denote all leaf senses from WordNet, where S represents the set of all senses in WordNet, and hypo(s) denotes the set of hyponym (children) senses of sense s. 2.
For IS-A, sample a leaf sense N uniformly at random from L, connect N to the root R, which gives a path p (N → R).
For not IS-A, similarly sample two leaf senses N 1 , N 2 randomly from L and obtain two paths p 1 (N 1 → R) and p 2 (N 2 → R).
A, B, C from p and ensure that example sentences exist for senses A, B, C.
This results in the 3-tuple (A, B, C) and three positive examples (A, B), (B, C), (A, C).
For not IS-A, randomly sample A ′ and B ′ from p 1 and p 2 respectively, ensuring that example sentences exist for senses A ′ and B ′ .
If A ′ is not on the path of B ′ → R and vice versa, then we obtain a negative example (A ′ , B ′ ); else return to step 2.
Since our goal is to determine what BERT as a pretrained language model knows about transitivity, we use a simple nearest neighbor (1-nn) classifier without further fine-tuning of BERT's parameters.
We also adopt a 1-nn classifier instead of a more complex classifier so that we are measuring what BERT knows and not what is learned by a subsequent complex classification model.
Our 1-nn probing classifier works by finding the closest example in the training set for a test example, and using the closest training example's label as the prediction.
Euclidean distance is used as the distance metric for our 1-nn probing classifier.
We represent each example, which is a pair of senses, by the concatenation of the contextualized embeddings of the pair.
For a pair of target words (w 1 , w 2 ) and their respective contextualized embeddings (o 1 , o 2 ), r(o 1 , o 2 ) denotes the relation embedding of the pair: r(o 1 , o 2 ) =
[o 1 ; o 2 ] (1) Let r and r ′ denote two examples, and let m denote the dimension of the relation embeddings.
The Euclidean metric d(r, r ′ ) is computed as follows: d(r, r ′ )
= m i=1 (r i − r ′ i ) 2 (2) Model and Hyperparameters For our BERT model, we use the basic bert-base-uncased model 2 , which has 12 layers with a hidden dimension of 768.
For 1-nn, we adopt the scikit-learn (Pedregosa et al., 2011) (A, C)] in the test set, the average numbers of hops for (A, B) and (B, C) are 1.5 and 2.1 respectively.
This difference is due to the fact that the senses with at least an example sentence are not evenly distributed along a path for nouns in WordNet.
On average, only 46% of senses on a sampled path have example sentences, out of which 72% of the senses in the bottom half (i.e., the half closer to the leaves) of the path are associated with example sentences, whereas only 17% of the top half have example sentences.
Therefore, when a sense C is sampled from the top half of the path, it is likely to be further away from sense B. Evaluation Metric We adopt accuracy as our evaluation metric, which measures the percentage of test examples correctly predicted by the probing classifier.
All accuracy scores are computed using the scikit-learn package.
The accuracy scores for the test set are shown in Table 1.
The overall accuracy score for all pairs of both IS-A and not IS-A classes is 72.6%, suggesting that BERT correctly predicts IS-A relations to some extent.
We also provide a breakdown of the accuracy scores according to different number of IS-A hops.
The scores indicate that BERT predicts IS-A relations with higher accuracy for smaller number of hops (1-4) than for larger number of hops (5-6), although the prediction accuracy does not drop by a large amount when the number of hops increases, and the accuracy does not vary too much within 1-4 hops.
We quantify BERT's prediction ability for transitivity by measuring how often BERT makes logically consistent predictions for IS-A relations.
Specifically, suppose word senses (A, B, C) form the following transitive IS-A relations: A is-a B is-a C. We measure how often BERT correctly predicts the IS-A relation (A, C) given that it correctly predicts (A, B) and (B, C).
Table 2 shows the accuracy scores for the 666 transitive 3-tuples.
In the table, p(AB) denotes the percentage of cor-rectly predicted (A, B) in the 666 (A, B) pairs.
Similar definitions apply to p(BC) and p(AC).
p(AC|AB, BC) denotes the percentage of correctly predicted (A, C), given that (A, B) and (B, C) are correctly predicted.
The conditional probability in Table 2 indicates that when BERT predicts that A is-a B and B is-a C, it correctly predicts that A is-a C 82.4% of the time.
That A is-a C is not always predicted correctly (given that BERT correctly predicts A is-a B and B is-a C) suggests that BERT lacks the ability to make logically consistent predictions.
In this paper, we have investigated how much BERT agrees with the transitivity constraint of the IS-A relation, via a minimalist probing setting.
Our findings indicate that although BERT can predict IS-A relations to some extent, it does not always make logically consistent predictions.
Allowing BERT and more generally neural network models to enforce the transitivity constraint of the IS-A relation would be a worthy future research goal.
Besides the IS-A relation, there are other transitivity relations like after, before, larger than, smaller than, etc.
It would also be interesting to investigate to what extent BERT also enforces or fails to enforce these other transitivity relations in future work.
reviewers for their helpful comments, and Wei Lu for his constructive suggestions and feedback on this work.
The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore ( We thank the anonymous