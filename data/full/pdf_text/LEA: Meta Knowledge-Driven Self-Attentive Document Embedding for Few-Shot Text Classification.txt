In recent years, NLP has advanced greatly along with the proliferation of pre-trained language models. The pre-trained language models are also properly adapted to downstream tasks when there is sufficient labeled data. However, in real-world applications, we often encounter the deficiency of labeled data. When only given a few instances for a new task, extracting task-aware features from a pre-trained language model regardless of the adaptation is a promising alternative. In the study, we propose a novel embedding transfer method, called LEA, for leveraging pre-trained language models with even only few-shot instances. LEA derives meta-level attention aspects using our new meta-learning framework. We evaluate our method on five text classification benchmark datasets. The results show that the novel method robustly provides the competitive performance compared to recent few-shot learning methods.
A deficiency of supervised data is often experienced in real-world NLP applications. Few-shot learning aims to yield an AI-driven NLP model capable of recognizing unseen tasks using a few labeled data. Meanwhile, fine-tuning pre-trained models (PTMs) (Howard and Ruder, 2018;Devlin et al., 2019;Lan et al., 2019; has been the most successful approach in recent years of NLP. Unfortunately, it is still challenging to utilize PTMs  in few-shot learning. To address this subtle problem , we propose a meta-knowledge driven self-attentive embedding transfer method, called LEA (LEarningto-Attend), based on a novel meta-learning framework, through which meta-level attention aspects are derived by encoding how to attend for given tasks. LEA is an efficient and practical method that facilitates the utilization of large-sized PTMs in few-shot learning. There are the two common transfer learning paradigms in NLP: feature-based transfer (Cer et al., 2018) and fine-tuning (Houlsby et al., 2019). Our approach belongs to the feature-based transfer. LEA includes two key ideas: (1) construction of a meta-level attention aspects dictionary and (2) inference of the task-specific attention aspects upon the arrival of a new task. The former is a process by which useful meta-level attention aspects across tasks are derived based on a particular PTM via our meta-learning framework. The latter refers to as a task-adaption process, where a subset of taskspecific attention aspects is inferred by determining the top-k most relevant attention aspects from the meta-level attention aspects dictionary. While LEA can be applied to a wide variety of downstream tasks, we demonstrate LEA on few-shot text classification problems in the paper.
Few-shot text classification: In (Geng et al., 2019), INDUCTION is proposed to build classwise embedding to represent each class using a particular dynamic routing algorithm coalesced with meta-learning. In (Bao et al., 2019), DS is introduced to keep track of underlying word distributions across all available classes and to specify important lexical features for new classes. Meta-learning: As a metric learning-based method, (Snell et al., 2017) suggested a deep neural network, called a prototype network (PROTO), through which class representations are composed using a learning similarity metric for members of the same class. In (Sung et al., 2018), similar to PROTO, a deep neural network, called a relation network, is proposed to learn a non-linear distance metric rather than the Euclidean distance. In addition, LEO (Rusu et al., 2018) learns a lowdimensional latent embedding of the model parameters such that the classifiers are generated from the latent space into which the tasks are mapped. Frog-GNN (Xu and Xiang, 2021) focuses on all querysupport pairs and proposes a multi-perspective aggregation based graph neural network to explicitly reflect intra-class similarity and inter-class dissimilarity.

Few-shot text classification is a task in which a classifier must be adapted to accommodate new classes using only a few labeled examples. In the literature, this is called a C-way K-shot problem in which K-labeled examples are given for each of the C number of classes. In a meta-learning setting, tasks are divided into a meta-training set (S tr ), meta-validation set (S val ), and meta-test set (S test ) as disjoint sets of classes.
Our proposed meta training strategy follows the overall procedure of optimization-based metalearning (Finn et al., 2017). For a parametric model f θ , MAML seeks to find task-specific parameters θ i for any new task τ i sampled from a particular distribution of tasks. For a particular task τ i ∼ p(τ ), the task data D τ i consist of D tr τ i and D val τ i during the meta-training phase. MAML alternates between two update processes during meta-training: (1) task-adaptation and (2) meta-optimization. Task adaptation (or inner update): Each task learner updates its own parameters through a gradient descent using the loss evaluated based on its own training data D tr τ i with the initial parameter θ m given by the outer meta-optimization process. The task-adaptation process is formulated as in Equation 1. θ ′ τ i ← θ m − α ▽ θm L τ i f θm , D tr τ i ,(1) Meta-optimization (or outer update): The metalearner updates its parameters through a gradient descent using the loss evaluated by D val τ i with respect to the task-specific parameters θ ′ τ i . The metaoptimization process is formulated as in Equation 2: θ m ← θ m − β ▽ θm τ i ∼p(τ ) L τ i f θ ′ τ i , D val τ i ,(2) where L τ i denotes a loss function for a task τ i , and the inner and outer updates are applied through their own standard gradient descent with fixed learning rates α and β, respectively, which are given as hyperparameters. In the meta-testing phase, the meta-learner provides the initial parameters for task-specific model learners. Subsequently, each task learner is individually tailored to find the optimal parameters θ ′ τ i by applying the above task adaptation process. In this meta-testing, the dataset of task τ i is given as D τ i = D tr τ i , D te τ i .
We conducted all experiments with BERT (Devlin et al., 2019) and RoBERTa  as the underlying PTMs in the study. Given a text input, a dummy token (CLS) is added to the beginning of the input, and another token (SEP) is added to the end of a sentence. The PTMs end up with providing the corresponding embedding vectors (i.e., denoted as [CLS] and [SEP]) for the artificial tokens as well as embeddings for original tokens for the input text. For downstream classification tasks, the special embedding vector [CLS] is typically used to make a prediction as the representative of an text instance. In this study, the [CLS] vector plays an important role in probing the distinctive properties for an incoming task. In manufacturing a task-specific embedding, we especially utilize the token-level output embeddings of the individual tokens of the jth text instance under a particular task τ i , which we denote as H τ i j = [h τ i j,1 , . . . , h τ i j,L ]. Likewise, the corresponding [CLS] embedding is denoted as c τ i j .
The overall architecture of LEA is shown in Figure 1. It represents our meta learning framework for the task-specific feature extraction. It is trained in an end-to-end manner using our proposed metalearning strategy. The meta training alternates two for number of tasks in batch do
Sample task instance τ i ∼ S tr
Decide top-k weights g τ i using c τ i 8: Generate τ i -attention aspects W τ i A using g τ i 9: Generate document embeddings (E tr τ i , E val τ i ) using H τ i 10: Initialize θ ′ τ i = θm 11: for number of adaptation steps do
Compute Task-Adaptation loss L tr τ i f θ ′ τ i , E tr τ i
Perform gradient step w.r.t. θ ′ τ i 14: θ ′ τ i ← θ ′ τ i − α ▽ θ ′ τ i L tr τ i f θ ′ τ i , E tr τ i
end for 16: Compute Meta-Optimization loss L val τ i (f θ ′ τ i ) 17: end for
Perform gradient step w.r.t ϕ 19: ϕ ← ϕ − β ▽ ϕ τ i L val τ i f θ ′ τ i , E val τ i + λ • Ω 20: end while processes: (1) deriving all valid meta-attention aspects across tasks (namely, meta-optimization), and (2) choosing a task-specific subset from all the meta-attention aspects for each task (called, task adaptation). The high-level operation is described in Algorithm 1.
In this study, the meta-level knowledge dictionary maintains all attention aspects derived across tasks τ i ∼ p(τ ). The concept was inspired by (Lin et al., 2017). The meta-attention aspects in the dictionary are established throughout the meta-optimization process during which it seeks to learn how to attend according to the distribution of tasks. Herein, we define a matrix W A ∈ R A N ×u as the meta-level attention aspect dictionary. In addition, A N and u are the total number of attention aspects and dimension of the attention aspect, respectively.
When a novel task τ i is given, its related attention aspects, denoted by W τ i A , are selectively obtained by assigning the corresponding weights to members of the meta-level attention aspects W A in the task-adaptation process. Here, W τ i A ∈ R k×u indicates the selected k attention aspects of the task τ i . Note that k and K are different in that the former is the number of topmost relevant attention aspects, whereas the latter, indicates as K-shot, refers to the number of samples in few-shot learning. To do so, we assess the relevance of the task among the meta-level attention aspects W A . First, each task is fed into an encoding process, which is formulated as follows: e τ i n = 1 N K 2 K kn=1 N m=1 K km=1 f θr f θe (c τ i kn ), f θe (c τ i km ) ,(3) where e τ i n is the representative embedding for the particular class n under a given task τ i , f θr indicates the relation network (Sung et al., 2018), and f θe is an encoder network that transforms the delegate embedding [CLS] (denoted as c τ i j for the case of the jth text instance of a specific task τ i ) of a text instance in PTMs (Devlin et al., 2019;Lan et al., 2019;. As a result, the class embedding e τ i n is enforced to encode the pairwise relationship with other classes. Using the aforementioned class embedding, we attempt to selectively (i.e., top-k) collect taskspecific attention aspects for a given task by employing a gating mechanism (Shazeer et al., 2017). The gating output vector is calculated through the following formulation: g τ i n = softmax (G(e τ i n ; W g , W n , k)) ,(4) where g τ i n is the gating output vector whose number of dimensions must be the same as the size of the meta-attention-aspects dictionary. The gating process G produces a sparse output vector by being parameterized with {W g ∈ R A N ×A N , W n ∈ R A N ×A N , k}, where the remaining values except for the k elements are forced to become zeros, and the top-k weights are finally generated through a softmax function. As a result, we can extract the top-k task-specific attention aspects for the task τ i . This is formulated as follows: W τ i A = ((W A ) T g τ i n ) T . (5)
Here, we perform the self-attentive feature extraction using the aforementioned top-k task-specific attention aspects for a task. We then apply it into the generation of document embeddings for text  classification. For a text input, we utilize the corresponding embedding vectors for the individual tokens, which are denoted as H τ i j = [h τ i j,1 , . . . , h τ i j,L ] for the jth text example of the task τ i . This is formulated as follows: E τ i j = W τ i A H τ i j ,(6) where E τ i j ∈ R k×L is the self-attentive document embedding of the jth input of the task τ i , and H τ i j ∈ R u×L is a set of token embedding vectors for the jth instance with L tokens in the task τ i . For the text classification, we sum E τ i j columnwise and then feed it into a fully connected neural network (denoted as F C θ ′ τ i ) with the parameters θ ′ τ i , which are optimized in the task-adaptation step to make the final predictions.
As noted in Algorithm 1, LEA alternates the following two update steps: (1) task adaptation (or inner-update) and (2) meta-optimization (or outerupdate). The former proceeds as follows: θ ′ τ i ← θ ′ τ i − α ▽ θ ′ τ i L tr τ i f θ ′ τ i , E tr τ i ,(7) where θ ′ τ i indicates the task model parameters, and L tr τ i is the classification loss by relying on E tr τ i derived from D tr τ i . During the meta-optimization step, the groups of parameters {W A , W g , W n , θ m , θ e , θ r , θ a } are trained in the outer loop with D val τ i . This is formulated as follows: where, Ω, as a regularization, includes the term that encourages all attention aspects to have equal importance (Shazeer et al., 2017), and λ is its associated coefficient as usual. ϕ ← ϕ−β ▽ ϕ τ i L val τ i f θ ′ τ i , E val τ i , )+λ•Ω (8)
We evaluated LEA on five text datasets -20 Newsgroup (Lang, 1995), Huffpost headline (Misra and Grover, 2021), Reuters-21578(Lewis., 1997), RCV-1 (Lewis et al., 2004), and Amazon product reviews (He and McAuley, 2016) -and compared it with current state-of-art methods. We conducted two different experiments: 5-way 1-shot and 5-way 5shot all over datasets. The details of the datasets are introduced in Appendix A.1.
In this experiment, we evaluate and compare LEA with six state-of-art methods as follows: Here, MAML (Finn et al., 2017) denotes the representative optimization-based meta-learning algorithm, PROTO (Snell et al., 2017) indicates the prototype network, LEO (Rusu et al., 2018) denotes the meta-learning algorithm using latent embedding optimization, INDUCTION indicates the induction network (Geng et al., 2019), DS (Bao et al., 2019) denotes few-shot text classification algorithm using the underlying word distributions, and Frog-GNN (Xu and Xiang, 2021) denotes the multi-perspective aggregation based graph neural network.
We performed all experiments on a frozen BERT BASE (Devlin et al., 2019) as a represen- For the comparison with Frog-GNN, we referred to the reported results from (Xu and Xiang, 2021). We additionally applied LEA on RoBERTa BASE  and fastText (Bojanowski et al., 2017) to verify the applicability of LEA. All performance scores are reported as the average for three repetitions. As shown in Table 1 and 2, LEA exhibits the competitive performance in both the 5-way 1-shot and 5-way 5-shot, compared to the state-of-the-arts for all the datasets. Namely, the results demonstrate that LEA quickly recognizes how to attend for new tasks using the established meta-attention aspects and provides a robust performance in few-shot text classification problems.
We also investigate the impact of the number (i.e., k) of task-specific attention aspects. This specific study was conducted on the same frozen BERT BASE as the underlying PTM with the 5-way 5-shot experiment for the all datasets. We fixed the size of the meta attention aspects dictionary to 150 and measured the performances by gradually scaling the k up to 1, 10, 20, 30, 50, 75, 150. As shown in Figure 2, all the datasets exhibit their best performance when setting the top-k attention aspects to 20. This empirical result indicates that each task derives its optimal document embedding by referring only to the most relevant subset rather than exploiting all meta-level attention aspects.
In addition, we plots the task-specific document embeddings and observe the relationships among classes on 20 Newsgroups dataset. To qualitatively characterize the task-specific document embedding space, we split 20 Newsgroup into seven top-level domains, that is, 'atheism', 'computer', 'for-sale', 'recreation', 'science', 'religion', and 'talk' and projected them via t-SNE as shown in Figure 3a. Figure 3b shows the relationships between the 'recreation' domain composed of four classes and the rest on the space. Figure 3c shows the relationships between the four classes of the 'science' domain and the others on the space. These plots demonstrate that LEA produces a structured task-specific embedding space after our task-adaptation step.
We hypothesized that a type of task-specific selfattentive mechanism might improve few-shot learning performance, especially when it is prohibitive to fine-tune a large-sized PTM. We have attempted to design a novel embedding transfer method for deriving a meta-level attention aspects dictionary to enable a new task to simply borrow the most relevant attention aspects from the dictionary. As a result, we proposed a novel meta-learning framework for the learning-to-attend and showed that LEA is an effective method that facilitates the utilization of large-sized PTMs in few-shot learning.

We introduce the datasets and the split (i.e., train/val/test) which had been maintained in our experiments. 20 Newsgroups is a collection of discourses in newsgroup posts for 20 topics (Lang, 1995). Huffpost Headlines is a collection of news headlines published in the Huffington Post from 2012 to 2018 (Misra and Grover, 2021). It is composed of 41 topics. Reuters-21578 is composed of documents that appeared on the Reuters newswire in 1987 (Lewis., 1997). In addition, we use the ApteMod version and discard documents with more than one label to avoid ambiguity, and thus 31 classes remain. RCV-1 is a set of newswire stories published by Reuters journalists from 1996 to 1997 (Lewis et al., 2004) and comprises 71 topic classes. Amazon data is a real-world dataset collected from Amazon.com as a set of customer reviews from 24 types of product categories (He and McAuley, 2016). Our goal is to match reviews to their own corresponding product categories. To train and evaluate the models, we divided each of the aforementioned datasets into a metatraining set (S tr ), meta-validation set (S val ), and meta-test set (S test ) as disjoint sets of classes within the experimental setting. In this work, we used the same split of classes as in (Bao et al., 2019) for the Huffpost headline (Misra and Grover, 2021), Reuters-21578(Lewis., 1997), and RCV-1 (Lewis et al., 2004) datasets. Hence, the Huffpost headline is divided into 20, 5, and 16 disjoint classes for meta-training, validation, and test sets. In terms of Reuters-21678, 15, 5, and 11 disjoint classes are used for meta-training/validation/test sets and 37, 10, and 24 disjoint classes for RCV-1. In Amazon product data, we split the data using rules in (Bao et al., 2019), and its training and  validation sets are used for meta-training set. As a result, Amazon product data is divided into 15, and 9 disjoint classes for meta-training and test sets and meta-validation set is not used in Amazon product data. For the 20 Newsgroup dataset, we randomly selected 20 topic classes, and the metatraining set, meta-validation set, and meta-test set contained 10, 5, and 5 disjoint classes, respectively. We summarize the above information in Table 4.
We share the breakdown of LEA's implementation. In the encoding process of our experiments, the 768-dimensional [CLS] vector, which is of the same size of the output of the pre-trained BERTbase-uncased, is linearly transformed through f θe into a 300-dimensional vector. The relation network, f θr is composed of two-layers neural network with ReLU activation and input size is two times of encoder outputs and the size of output is the number of meta-attention-aspects, i.e., 150. The gating network, W g is linear transformation and its size is the number of meta-attention-aspects. For each task classifier, that is, f θ ′ τ i , it is designed as single-layer fully connected neural network. We set the size to 150 for the meta-attention-aspects dictionary, and importantly fixed the number of top-k attention aspects to 20. Table 3 summarizes the above model parameters.
In our work, we train all experiments on a single NVIDIA A100 32G GPU. During the meta-training process, we sampled four tasks with 15 queries from S tr , and it leads to performing task adaptation four times per each meta-optimization update and early stop when the validation loss fails to improve for 20 steps. In validation and test process, we sampled 30 tasks with 15 queries from S val and S test , and only performed task adaptation using K-shots. After that, the performance of the adapted task model is obtained using queries. We used the Adam optimizer with learning rates of 0.1 and 0.001 in the inner and outer updates, that is, α and β in , respectively. In addition, the coefficient λ of the regularization term was set as 0.0001. We summarize the hyperparameters in Table 5.
Herein, we visualize the heatmaps in some cases to investigate how to assign attention weights to text. Figure 4b demonstrates a termination of stock sale pact, and Figure 4a shows a company growth in terms of consumer products. These were extracted under the Corporate and Industrial topic in the RCV-1 dataset and some seminal words such as "agreement", "contractual" and "receivership" are highlighted to appear in the topic. Figure 4c shows that the Turkish market was closed related to the Market topic, and its relevant words such as "Turkish," "markets," and "closed" are highly attended as expected. Figure 4d talks about the authority of platinum and gold coins under the Economics topic, and the words "US," "Mint," "authority," "gold," "platinum," and "coin" are hence highlighted. As shown in these cases, LEA properly captures important words under a certain topic and assigns attention weights to a given text.