-DOCSTART- -X- O
Acknowledgements -X- _ O
We -X- _ O
are -X- _ O
grateful -X- _ O
to -X- _ O
Nal -X- _ O
Kalchbrenner -X- _ O
and -X- _ O
Stephan -X- _ O
Gouws -X- _ O
for -X- _ O
their -X- _ O
fruitful -X- _ O
comments, -X- _ O
corrections -X- _ O
and -X- _ O
inspiration. -X- _ O

In -X- _ O
this -X- _ O
work, -X- _ O
we -X- _ O
presented -X- _ O
the -X- _ O
Transformer, -X- _ B-MethodName
the -X- _ O
first -X- _ O
sequence -X- _ O
transduction -X- _ O
model -X- _ O
based -X- _ O
entirely -X- _ O
on -X- _ O
attention, -X- _ O
replacing -X- _ O
the -X- _ O
recurrent -X- _ O
layers -X- _ O
most -X- _ O
commonly -X- _ O
used -X- _ O
in -X- _ O
encoder-decoder -X- _ O
architectures -X- _ O
with -X- _ O
multi-headed -X- _ O
self-attention. -X- _ O
For -X- _ O
translation -X- _ O
tasks, -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
can -X- _ O
be -X- _ O
trained -X- _ O
significantly -X- _ O
faster -X- _ O
than -X- _ O
architectures -X- _ O
based -X- _ O
on -X- _ O
recurrent -X- _ O
or -X- _ O
convolutional -X- _ O
layers. -X- _ O
On -X- _ O
both -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
English-to-German -X- _ I-DatasetName
and -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
English-to-French -X- _ I-DatasetName
translation -X- _ O
tasks, -X- _ O
we -X- _ O
achieve -X- _ O
a -X- _ O
new -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art. -X- _ O
In -X- _ O
the -X- _ O
former -X- _ O
task -X- _ O
our -X- _ O
best -X- _ O
model -X- _ O
outperforms -X- _ O
even -X- _ O
all -X- _ O
previously -X- _ O
reported -X- _ O
ensembles. -X- _ O
We -X- _ O
are -X- _ O
excited -X- _ O
about -X- _ O
the -X- _ O
future -X- _ O
of -X- _ O
attention-based -X- _ O
models -X- _ O
and -X- _ O
plan -X- _ O
to -X- _ O
apply -X- _ O
them -X- _ O
to -X- _ O
other -X- _ O
tasks. -X- _ O
We -X- _ O
plan -X- _ O
to -X- _ O
extend -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
to -X- _ O
problems -X- _ O
involving -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
modalities -X- _ O
other -X- _ O
than -X- _ O
text -X- _ O
and -X- _ O
to -X- _ O
investigate -X- _ O
local, -X- _ O
restricted -X- _ O
attention -X- _ O
mechanisms -X- _ O
to -X- _ O
efficiently -X- _ O
handle -X- _ O
large -X- _ O
inputs -X- _ O
and -X- _ O
outputs -X- _ O
such -X- _ O
as -X- _ O
images, -X- _ O
audio -X- _ O
and -X- _ O
video. -X- _ O
Making -X- _ O
generation -X- _ O
less -X- _ O
sequential -X- _ O
is -X- _ O
another -X- _ O
research -X- _ O
goals -X- _ O
of -X- _ O
ours. -X- _ O
The -X- _ O
code -X- _ O
we -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
and -X- _ O
evaluate -X- _ O
our -X- _ O
models -X- _ O
is -X- _ O
available -X- _ O
at -X- _ O
tensorflow/tensor2tensor. -X- _ O

To -X- _ O
evaluate -X- _ O
if -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
can -X- _ O
generalize -X- _ O
to -X- _ O
other -X- _ O
tasks -X- _ O
we -X- _ O
performed -X- _ O
experiments -X- _ O
on -X- _ O
English -X- _ B-TaskName
constituency -X- _ I-TaskName
parsing. -X- _ I-TaskName
This -X- _ O
task -X- _ O
presents -X- _ O
specific -X- _ O
challenges: -X- _ O
the -X- _ O
output -X- _ O
is -X- _ O
subject -X- _ O
to -X- _ O
strong -X- _ O
structural -X- _ O
constraints -X- _ O
and -X- _ O
is -X- _ O
significantly -X- _ O
longer -X- _ O
than -X- _ O
the -X- _ O
input. -X- _ O
Furthermore, -X- _ O
RNN -X- _ O
sequence-to-sequence -X- _ O
models -X- _ O
have -X- _ O
not -X- _ O
been -X- _ O
able -X- _ O
to -X- _ O
attain -X- _ O
state-of-the-art -X- _ O
results -X- _ O
in -X- _ O
small-data -X- _ O
regimes -X- _ O
. -X- _ O
We -X- _ O
trained -X- _ O
a -X- _ O
4-layer -X- _ O
transformer -X- _ O
with -X- _ O
d -X- _ O
model -X- _ O
= -X- _ O
1024 -X- _ O
on -X- _ O
the -X- _ O
Wall -X- _ B-DatasetName
Street -X- _ I-DatasetName
Journal -X- _ I-DatasetName
(WSJ) -X- _ I-DatasetName
portion -X- _ O
of -X- _ O
the -X- _ O
Penn -X- _ B-DatasetName
Treebank -X- _ I-DatasetName
, -X- _ O
about -X- _ O
40K -X- _ O
training -X- _ O
sentences. -X- _ O
We -X- _ O
also -X- _ O
trained -X- _ O
it -X- _ O
in -X- _ O
a -X- _ O
semi-supervised -X- _ O
setting, -X- _ O
using -X- _ O
the -X- _ O
larger -X- _ O
high-confidence -X- _ O
and -X- _ O
BerkleyParser -X- _ B-DatasetName
corpora -X- _ I-DatasetName
from -X- _ O
with -X- _ O
approximately -X- _ O
17M -X- _ O
sentences -X- _ O
. -X- _ O
We -X- _ O
used -X- _ O
a -X- _ O
vocabulary -X- _ B-HyperparameterName
of -X- _ O
16K -X- _ B-HyperparameterValue
tokens -X- _ I-HyperparameterValue
for -X- _ O
the -X- _ O
WSJ -X- _ O
only -X- _ O
setting -X- _ O
and -X- _ O
a -X- _ O
vocabulary -X- _ B-HyperparameterName
of -X- _ O
32K -X- _ B-HyperparameterValue
tokens -X- _ I-HyperparameterValue
for -X- _ O
the -X- _ O
semi-supervised -X- _ O
setting. -X- _ O
We -X- _ O
performed -X- _ O
only -X- _ O
a -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
experiments -X- _ O
to -X- _ O
select -X- _ O
the -X- _ O
dropout, -X- _ B-HyperparameterName
both -X- _ O
attention -X- _ O
and -X- _ O
residual -X- _ O
(section -X- _ O
5.4), -X- _ O
learning -X- _ B-HyperparameterName
rates -X- _ I-HyperparameterName
and -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
on -X- _ O
the -X- _ O
Section -X- _ O
22 -X- _ O
development -X- _ O
set, -X- _ O
all -X- _ O
other -X- _ O
parameters -X- _ O
remained -X- _ O
unchanged -X- _ O
from -X- _ O
the -X- _ O
English-to-German -X- _ O
base -X- _ O
translation -X- _ O
model. -X- _ O
During -X- _ O
inference, -X- _ O
we -X- _ O
increased -X- _ O
the -X- _ O
maximum -X- _ O
output -X- _ O
length -X- _ O
to -X- _ O
input -X- _ O
length -X- _ O
+ -X- _ O
300. -X- _ O
We -X- _ O
used -X- _ O
a -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
21 -X- _ B-HyperparameterValue
and -X- _ O
α -X- _ B-HyperparameterName
= -X- _ O
0.3 -X- _ B-HyperparameterValue
for -X- _ O
both -X- _ O
WSJ -X- _ O
only -X- _ O
and -X- _ O
the -X- _ O
semi-supervised -X- _ O
setting. -X- _ O
Our -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
4 -X- _ O
show -X- _ O
that -X- _ O
despite -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
task-specific -X- _ O
tuning -X- _ O
our -X- _ O
model -X- _ O
performs -X- _ O
surprisingly -X- _ O
well, -X- _ O
yielding -X- _ O
better -X- _ O
results -X- _ O
than -X- _ O
all -X- _ O
previously -X- _ O
reported -X- _ O
models -X- _ O
with -X- _ O
the -X- _ O
exception -X- _ O
of -X- _ O
the -X- _ O
Recurrent -X- _ O
Neural -X- _ O
Network -X- _ O
Grammar -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
RNN -X- _ O
sequence-to-sequence -X- _ O
models -X- _ O
, -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
Berkeley-Parser -X- _ B-MethodName
even -X- _ O
when -X- _ O
training -X- _ O
only -X- _ O
on -X- _ O
the -X- _ O
WSJ -X- _ B-DatasetName
training -X- _ O
set -X- _ O
of -X- _ O
40K -X- _ O
sentences. -X- _ O

To -X- _ O
evaluate -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
different -X- _ O
components -X- _ O
of -X- _ O
the -X- _ O
Transformer, -X- _ B-MethodName
we -X- _ O
varied -X- _ O
our -X- _ O
base -X- _ O
model -X- _ O
in -X- _ O
different -X- _ O
ways, -X- _ O
measuring -X- _ O
the -X- _ O
change -X- _ O
in -X- _ O
performance -X- _ O
on -X- _ O
English-to-German -X- _ B-TaskName
translation -X- _ I-TaskName
on -X- _ O
the -X- _ O
development -X- _ O
set, -X- _ O
newstest2013. -X- _ B-DatasetName
We -X- _ O
used -X- _ O
beam -X- _ B-MethodName
search -X- _ I-MethodName
as -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
section, -X- _ O
but -X- _ O
no -X- _ O
checkpoint -X- _ O
averaging. -X- _ O
We -X- _ O
present -X- _ O
these -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
3. -X- _ O
In -X- _ O
Table -X- _ O
3 -X- _ O
rows -X- _ O
(A), -X- _ O
we -X- _ O
vary -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
attention -X- _ O
heads -X- _ O
and -X- _ O
the -X- _ O
attention -X- _ O
key -X- _ O
and -X- _ O
value -X- _ O
dimensions, -X- _ O
keeping -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
computation -X- _ O
constant, -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
Section -X- _ O
3.2.2. -X- _ O
While -X- _ O
single-head -X- _ O
attention -X- _ O
is -X- _ O
0.9 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
worse -X- _ O
than -X- _ O
the -X- _ O
best -X- _ O
setting, -X- _ O
quality -X- _ O
also -X- _ O
drops -X- _ O
off -X- _ O
with -X- _ O
too -X- _ O
many -X- _ O
heads. -X- _ O
In -X- _ O
Table -X- _ O
3 -X- _ O
rows -X- _ O
(B), -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
reducing -X- _ O
the -X- _ O
attention -X- _ O
key -X- _ O
size -X- _ O
d -X- _ O
k -X- _ O
hurts -X- _ O
model -X- _ O
quality. -X- _ O
This -X- _ O
suggests -X- _ O
that -X- _ O
determining -X- _ O
compatibility -X- _ O
is -X- _ O
not -X- _ O
easy -X- _ O
and -X- _ O
that -X- _ O
a -X- _ O
more -X- _ O
sophisticated -X- _ O
compatibility -X- _ O
function -X- _ O
than -X- _ O
dot -X- _ O
product -X- _ O
may -X- _ O
be -X- _ O
beneficial. -X- _ O
We -X- _ O
further -X- _ O
observe -X- _ O
in -X- _ O
rows -X- _ O
(C) -X- _ O
and -X- _ O
(D) -X- _ O
that, -X- _ O
as -X- _ O
expected, -X- _ O
bigger -X- _ O
models -X- _ O
are -X- _ O
better, -X- _ O
and -X- _ O
dropout -X- _ O
is -X- _ O
very -X- _ O
helpful -X- _ O
in -X- _ O
avoiding -X- _ O
over-fitting. -X- _ O
In -X- _ O
row -X- _ O
(E) -X- _ O
we -X- _ O
replace -X- _ O
our -X- _ O
sinusoidal -X- _ O
positional -X- _ O
encoding -X- _ O
with -X- _ O
learned -X- _ O
positional -X- _ O
embeddings -X- _ O
, -X- _ O
and -X- _ O
observe -X- _ O
nearly -X- _ O
identical -X- _ O
results -X- _ O
to -X- _ O
the -X- _ O
base -X- _ O
model. -X- _ O

On -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
English-to-German -X- _ I-DatasetName
translation -X- _ O
task, -X- _ O
the -X- _ O
big -X- _ O
transformer -X- _ B-MethodName
model -X- _ O
(Transformer -X- _ B-MethodName
(big) -X- _ I-MethodName
in -X- _ O
Table -X- _ O
2) -X- _ O
outperforms -X- _ O
the -X- _ O
best -X- _ O
previously -X- _ O
reported -X- _ O
models -X- _ O
(including -X- _ O
ensembles) -X- _ O
by -X- _ O
more -X- _ O
than -X- _ O
2.0 -X- _ B-MetricValue
BLEU, -X- _ B-MetricName
establishing -X- _ O
a -X- _ O
new -X- _ O
state-of-the-art -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
of -X- _ O
28.4. -X- _ B-MetricValue
The -X- _ O
configuration -X- _ O
of -X- _ O
this -X- _ O
model -X- _ O
is -X- _ O
listed -X- _ O
in -X- _ O
the -X- _ O
bottom -X- _ O
line -X- _ O
of -X- _ O
Table -X- _ O
3. -X- _ O
Training -X- _ O
took -X- _ O
3.5 -X- _ O
days -X- _ O
on -X- _ O
8 -X- _ O
P100 -X- _ O
GPUs. -X- _ O
Even -X- _ O
our -X- _ O
base -X- _ O
model -X- _ O
surpasses -X- _ O
all -X- _ O
previously -X- _ O
published -X- _ O
models -X- _ O
and -X- _ O
ensembles, -X- _ O
at -X- _ O
a -X- _ O
fraction -X- _ O
of -X- _ O
the -X- _ O
training -X- _ O
cost -X- _ O
of -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
competitive -X- _ O
models. -X- _ O
On -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
English-to-French -X- _ I-DatasetName
translation -X- _ O
task, -X- _ O
our -X- _ O
big -X- _ O
model -X- _ O
achieves -X- _ O
a -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
of -X- _ O
41.0, -X- _ B-MetricValue
outperforming -X- _ O
all -X- _ O
of -X- _ O
the -X- _ O
previously -X- _ O
published -X- _ O
single -X- _ O
models, -X- _ O
at -X- _ O
less -X- _ O
than -X- _ O
1/4 -X- _ O
the -X- _ O
training -X- _ O
cost -X- _ O
of -X- _ O
the -X- _ O
previous -X- _ O
state-of-the-art -X- _ O
model. -X- _ O
The -X- _ O
Transformer -X- _ O
(big) -X- _ O
model -X- _ O
trained -X- _ O
for -X- _ O
English-to-French -X- _ O
used -X- _ O
dropout -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
P -X- _ O
drop -X- _ B-HyperparameterName
= -X- _ O
0.1, -X- _ B-HyperparameterValue
instead -X- _ O
of -X- _ O
0.3. -X- _ B-HyperparameterValue
For -X- _ O
the -X- _ O
base -X- _ O
models, -X- _ O
we -X- _ O
used -X- _ O
a -X- _ O
single -X- _ O
model -X- _ O
obtained -X- _ O
by -X- _ O
averaging -X- _ O
the -X- _ O
last -X- _ O
5 -X- _ O
checkpoints, -X- _ O
which -X- _ O
were -X- _ O
written -X- _ O
at -X- _ O
10-minute -X- _ O
intervals. -X- _ O
For -X- _ O
the -X- _ O
big -X- _ O
models, -X- _ O
we -X- _ O
averaged -X- _ O
the -X- _ O
last -X- _ O
20 -X- _ O
checkpoints. -X- _ O
We -X- _ O
used -X- _ O
beam -X- _ B-MethodName
search -X- _ I-MethodName
with -X- _ O
a -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
4 -X- _ B-HyperparameterValue
and -X- _ O
length -X- _ B-HyperparameterName
penalty -X- _ I-HyperparameterName
α -X- _ B-HyperparameterValue
= -X- _ I-HyperparameterValue
0.6 -X- _ I-HyperparameterValue
. -X- _ O
These -X- _ O
hyperparameters -X- _ O
were -X- _ O
chosen -X- _ O
after -X- _ O
experimentation -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
set. -X- _ O
We -X- _ O
set -X- _ O
the -X- _ O
maximum -X- _ O
output -X- _ O
length -X- _ O
during -X- _ O
inference -X- _ O
to -X- _ O
input -X- _ O
length -X- _ O
+ -X- _ O
50, -X- _ O
but -X- _ O
terminate -X- _ O
early -X- _ O
when -X- _ O
possible -X- _ O
. -X- _ O

We -X- _ O
employ -X- _ O
three -X- _ O
types -X- _ O
of -X- _ O
regularization -X- _ O
during -X- _ O
training: -X- _ O
Residual -X- _ O
Dropout -X- _ B-HyperparameterName
We -X- _ O
apply -X- _ O
dropout -X- _ B-HyperparameterName
to -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
each -X- _ O
sub-layer, -X- _ O
before -X- _ O
it -X- _ O
is -X- _ O
added -X- _ O
to -X- _ O
the -X- _ O
sub-layer -X- _ O
input -X- _ O
and -X- _ O
normalized. -X- _ O
In -X- _ O
addition, -X- _ O
we -X- _ O
apply -X- _ O
dropout -X- _ B-HyperparameterName
to -X- _ O
the -X- _ O
sums -X- _ O
of -X- _ O
the -X- _ O
embeddings -X- _ O
and -X- _ O
the -X- _ O
positional -X- _ O
encodings -X- _ O
in -X- _ O
both -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
stacks. -X- _ O
For -X- _ O
the -X- _ O
base -X- _ O
model, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
rate -X- _ O
of -X- _ O
P -X- _ O
drop -X- _ B-HyperparameterName
= -X- _ O
0.1. -X- _ B-HyperparameterValue
Label -X- _ B-HyperparameterName
Smoothing -X- _ I-HyperparameterName
During -X- _ O
training, -X- _ O
we -X- _ O
employed -X- _ O
label -X- _ O
smoothing -X- _ O
of -X- _ O
value -X- _ O
ls -X- _ B-HyperparameterName
= -X- _ O
0.1 -X- _ B-HyperparameterValue
. -X- _ O
This -X- _ O
hurts -X- _ O
perplexity, -X- _ B-MetricName
as -X- _ O
the -X- _ O
model -X- _ O
learns -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
unsure, -X- _ O
but -X- _ O
improves -X- _ O
accuracy -X- _ B-MetricName
and -X- _ O
BLEU -X- _ B-MetricName
score. -X- _ O
6 -X- _ B-MetricValue
Results -X- _ O

We -X- _ O
used -X- _ O
the -X- _ O
Adam -X- _ B-HyperparameterValue
optimizer -X- _ B-HyperparameterName
with -X- _ O
β -X- _ O
1 -X- _ O
= -X- _ O
0.9, -X- _ O
β -X- _ O
2 -X- _ O
= -X- _ O
0.98 -X- _ O
and -X- _ O
= -X- _ O
10 -X- _ O
−9 -X- _ O
. -X- _ O
We -X- _ O
varied -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
over -X- _ O
the -X- _ O
course -X- _ O
of -X- _ O
training, -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
formula: -X- _ O
lrate -X- _ O
= -X- _ O
d -X- _ O
−0.5 -X- _ O
model -X- _ O
• -X- _ O
min(step_num -X- _ O
−0.5 -X- _ O
, -X- _ O
step_num -X- _ O
• -X- _ O
warmup_steps -X- _ O
−1.5 -X- _ O
)(3) -X- _ O
This -X- _ O
corresponds -X- _ O
to -X- _ O
increasing -X- _ O
the -X- _ O
learning -X- _ O
rate -X- _ O
linearly -X- _ O
for -X- _ O
the -X- _ O
first -X- _ O
warmup_steps -X- _ O
training -X- _ O
steps, -X- _ O
and -X- _ O
decreasing -X- _ O
it -X- _ O
thereafter -X- _ O
proportionally -X- _ O
to -X- _ O
the -X- _ O
inverse -X- _ O
square -X- _ O
root -X- _ O
of -X- _ O
the -X- _ O
step -X- _ O
number. -X- _ O
We -X- _ O
used -X- _ O
warmup_steps -X- _ B-HyperparameterName
= -X- _ O
4000. -X- _ B-HyperparameterValue

We -X- _ O
trained -X- _ O
our -X- _ O
models -X- _ O
on -X- _ O
one -X- _ O
machine -X- _ O
with -X- _ O
8 -X- _ O
NVIDIA -X- _ O
P100 -X- _ O
GPUs. -X- _ O
For -X- _ O
our -X- _ O
base -X- _ O
models -X- _ O
using -X- _ O
the -X- _ O
hyperparameters -X- _ O
described -X- _ O
throughout -X- _ O
the -X- _ O
paper, -X- _ O
each -X- _ O
training -X- _ O
step -X- _ O
took -X- _ O
about -X- _ O
0.4 -X- _ O
seconds. -X- _ O
We -X- _ O
trained -X- _ O
the -X- _ O
base -X- _ O
models -X- _ O
for -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
100,000 -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
or -X- _ O
12 -X- _ O
hours. -X- _ O
For -X- _ O
our -X- _ O
big -X- _ O
models,(described -X- _ O
on -X- _ O
the -X- _ O
bottom -X- _ O
line -X- _ O
of -X- _ O
table -X- _ O
3), -X- _ O
step -X- _ O
time -X- _ O
was -X- _ O
1.0 -X- _ O
seconds. -X- _ O
The -X- _ O
big -X- _ O
models -X- _ O
were -X- _ O
trained -X- _ O
for -X- _ O
300,000 -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
(3.5 -X- _ O
days). -X- _ O

We -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
standard -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
English-German -X- _ I-DatasetName
dataset -X- _ O
consisting -X- _ O
of -X- _ O
about -X- _ O
4.5 -X- _ O
million -X- _ O
sentence -X- _ O
pairs. -X- _ O
Sentences -X- _ O
were -X- _ O
encoded -X- _ O
using -X- _ O
byte-pair -X- _ O
encoding -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
a -X- _ O
shared -X- _ O
sourcetarget -X- _ O
vocabulary -X- _ O
of -X- _ O
about -X- _ O
37000 -X- _ O
tokens. -X- _ O
For -X- _ O
English-French, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
significantly -X- _ O
larger -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
English-French -X- _ I-DatasetName
dataset -X- _ O
consisting -X- _ O
of -X- _ O
36M -X- _ O
sentences -X- _ O
and -X- _ O
split -X- _ O
tokens -X- _ O
into -X- _ O
a -X- _ O
32000 -X- _ O
word-piece -X- _ O
vocabulary -X- _ O
. -X- _ O
Sentence -X- _ O
pairs -X- _ O
were -X- _ O
batched -X- _ O
together -X- _ O
by -X- _ O
approximate -X- _ O
sequence -X- _ O
length. -X- _ O
Each -X- _ O
training -X- _ O
batch -X- _ O
contained -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
sentence -X- _ O
pairs -X- _ O
containing -X- _ O
approximately -X- _ O
25000 -X- _ O
source -X- _ O
tokens -X- _ O
and -X- _ O
25000 -X- _ O
target -X- _ O
tokens. -X- _ O

This -X- _ O
section -X- _ O
describes -X- _ O
the -X- _ O
training -X- _ O
regime -X- _ O
for -X- _ O
our -X- _ O
models. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
we -X- _ O
compare -X- _ O
various -X- _ O
aspects -X- _ O
of -X- _ O
self-attention -X- _ O
layers -X- _ O
to -X- _ O
the -X- _ O
recurrent -X- _ O
and -X- _ O
convolutional -X- _ O
layers -X- _ O
commonly -X- _ O
used -X- _ O
for -X- _ O
mapping -X- _ O
one -X- _ O
variable-length -X- _ O
sequence -X- _ O
of -X- _ O
symbol -X- _ O
representations -X- _ O
(x -X- _ O
1 -X- _ O
, -X- _ O
..., -X- _ O
x -X- _ O
n -X- _ O
) -X- _ O
to -X- _ O
another -X- _ O
sequence -X- _ O
of -X- _ O
equal -X- _ O
length -X- _ O
(z -X- _ O
1 -X- _ O
, -X- _ O
..., -X- _ O
z -X- _ O
n -X- _ O
), -X- _ O
with -X- _ O
x -X- _ O
i -X- _ O
, -X- _ O
z -X- _ O
i -X- _ O
∈ -X- _ O
R -X- _ O
d -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
a -X- _ O
hidden -X- _ O
layer -X- _ O
in -X- _ O
a -X- _ O
typical -X- _ O
sequence -X- _ O
transduction -X- _ O
encoder -X- _ O
or -X- _ O
decoder. -X- _ O
Motivating -X- _ O
our -X- _ O
use -X- _ O
of -X- _ O
self-attention -X- _ O
we -X- _ O
consider -X- _ O
three -X- _ O
desiderata. -X- _ O
One -X- _ O
is -X- _ O
the -X- _ O
total -X- _ O
computational -X- _ O
complexity -X- _ O
per -X- _ O
layer. -X- _ O
Another -X- _ O
is -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
computation -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
parallelized, -X- _ O
as -X- _ O
measured -X- _ O
by -X- _ O
the -X- _ O
minimum -X- _ O
number -X- _ O
of -X- _ O
sequential -X- _ O
operations -X- _ O
required. -X- _ O
The -X- _ O
third -X- _ O
is -X- _ O
the -X- _ O
path -X- _ O
length -X- _ O
between -X- _ O
long-range -X- _ O
dependencies -X- _ O
in -X- _ O
the -X- _ O
network. -X- _ O
Learning -X- _ O
long-range -X- _ O
dependencies -X- _ O
is -X- _ O
a -X- _ O
key -X- _ O
challenge -X- _ O
in -X- _ O
many -X- _ O
sequence -X- _ O
transduction -X- _ O
tasks. -X- _ O
One -X- _ O
key -X- _ O
factor -X- _ O
affecting -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
learn -X- _ O
such -X- _ O
dependencies -X- _ O
is -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
paths -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
signals -X- _ O
have -X- _ O
to -X- _ O
traverse -X- _ O
in -X- _ O
the -X- _ O
network. -X- _ O
The -X- _ O
shorter -X- _ O
these -X- _ O
paths -X- _ O
between -X- _ O
any -X- _ O
combination -X- _ O
of -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
sequences, -X- _ O
the -X- _ O
easier -X- _ O
it -X- _ O
is -X- _ O
to -X- _ O
learn -X- _ O
long-range -X- _ O
dependencies -X- _ O
. -X- _ O
Hence -X- _ O
we -X- _ O
also -X- _ O
compare -X- _ O
the -X- _ O
maximum -X- _ O
path -X- _ O
length -X- _ O
between -X- _ O
any -X- _ O
two -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
positions -X- _ O
in -X- _ O
networks -X- _ O
composed -X- _ O
of -X- _ O
the -X- _ O
different -X- _ O
layer -X- _ O
types. -X- _ O
As -X- _ O
noted -X- _ O
in -X- _ O
Table -X- _ O
1, -X- _ O
a -X- _ O
self-attention -X- _ O
layer -X- _ O
connects -X- _ O
all -X- _ O
positions -X- _ O
with -X- _ O
a -X- _ O
constant -X- _ O
number -X- _ O
of -X- _ O
sequentially -X- _ O
executed -X- _ O
operations, -X- _ O
whereas -X- _ O
a -X- _ O
recurrent -X- _ O
layer -X- _ O
requires -X- _ O
O(n) -X- _ O
sequential -X- _ O
operations. -X- _ O
In -X- _ O
terms -X- _ O
of -X- _ O
computational -X- _ O
complexity, -X- _ O
self-attention -X- _ O
layers -X- _ O
are -X- _ O
faster -X- _ O
than -X- _ O
recurrent -X- _ O
layers -X- _ O
when -X- _ O
the -X- _ O
sequence -X- _ O
length -X- _ O
n -X- _ O
is -X- _ O
smaller -X- _ O
than -X- _ O
the -X- _ O
representation -X- _ O
dimensionality -X- _ O
d, -X- _ O
which -X- _ O
is -X- _ O
most -X- _ O
often -X- _ O
the -X- _ O
case -X- _ O
with -X- _ O
sentence -X- _ O
representations -X- _ O
used -X- _ O
by -X- _ O
state-of-the-art -X- _ O
models -X- _ O
in -X- _ O
machine -X- _ O
translations, -X- _ O
such -X- _ O
as -X- _ O
word-piece -X- _ O
and -X- _ O
byte-pair -X- _ O
representations. -X- _ O
To -X- _ O
improve -X- _ O
computational -X- _ O
performance -X- _ O
for -X- _ O
tasks -X- _ O
involving -X- _ O
very -X- _ O
long -X- _ O
sequences, -X- _ O
self-attention -X- _ O
could -X- _ O
be -X- _ O
restricted -X- _ O
to -X- _ O
considering -X- _ O
only -X- _ O
a -X- _ O
neighborhood -X- _ O
of -X- _ O
size -X- _ O
r -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
centered -X- _ O
around -X- _ O
the -X- _ O
respective -X- _ O
output -X- _ O
position. -X- _ O
This -X- _ O
would -X- _ O
increase -X- _ O
the -X- _ O
maximum -X- _ O
path -X- _ O
length -X- _ O
to -X- _ O
O(n/r). -X- _ O
We -X- _ O
plan -X- _ O
to -X- _ O
investigate -X- _ O
this -X- _ O
approach -X- _ O
further -X- _ O
in -X- _ O
future -X- _ O
work. -X- _ O
A -X- _ O
single -X- _ O
convolutional -X- _ O
layer -X- _ O
with -X- _ O
kernel -X- _ O
width -X- _ O
k -X- _ O
< -X- _ O
n -X- _ O
does -X- _ O
not -X- _ O
connect -X- _ O
all -X- _ O
pairs -X- _ O
of -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
positions. -X- _ O
Doing -X- _ O
so -X- _ O
requires -X- _ O
a -X- _ O
stack -X- _ O
of -X- _ O
O(n/k) -X- _ O
convolutional -X- _ O
layers -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
contiguous -X- _ O
kernels, -X- _ O
or -X- _ O
O(log -X- _ O
k -X- _ O
(n)) -X- _ O
in -X- _ O
the -X- _ O
case -X- _ O
of -X- _ O
dilated -X- _ O
convolutions -X- _ O
, -X- _ O
increasing -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
longest -X- _ O
paths -X- _ O
between -X- _ O
any -X- _ O
two -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
network. -X- _ O
Convolutional -X- _ O
layers -X- _ O
are -X- _ O
generally -X- _ O
more -X- _ O
expensive -X- _ O
than -X- _ O
recurrent -X- _ O
layers, -X- _ O
by -X- _ O
a -X- _ O
factor -X- _ O
of -X- _ O
k. -X- _ O
Separable -X- _ O
convolutions -X- _ O
, -X- _ O
however, -X- _ O
decrease -X- _ O
the -X- _ O
complexity -X- _ O
considerably, -X- _ O
to -X- _ O
O(k -X- _ O
• -X- _ O
n -X- _ O
• -X- _ O
d -X- _ O
+ -X- _ O
n -X- _ O
• -X- _ O
d -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O
Even -X- _ O
with -X- _ O
k -X- _ O
= -X- _ O
n, -X- _ O
however, -X- _ O
the -X- _ O
complexity -X- _ O
of -X- _ O
a -X- _ O
separable -X- _ O
convolution -X- _ O
is -X- _ O
equal -X- _ O
to -X- _ O
the -X- _ O
combination -X- _ O
of -X- _ O
a -X- _ O
self-attention -X- _ O
layer -X- _ O
and -X- _ O
a -X- _ O
point-wise -X- _ O
feed-forward -X- _ O
layer, -X- _ O
the -X- _ O
approach -X- _ O
we -X- _ O
take -X- _ O
in -X- _ O
our -X- _ O
model. -X- _ O
As -X- _ O
side -X- _ O
benefit, -X- _ O
self-attention -X- _ O
could -X- _ O
yield -X- _ O
more -X- _ O
interpretable -X- _ O
models. -X- _ O
We -X- _ O
inspect -X- _ O
attention -X- _ O
distributions -X- _ O
from -X- _ O
our -X- _ O
models -X- _ O
and -X- _ O
present -X- _ O
and -X- _ O
discuss -X- _ O
examples -X- _ O
in -X- _ O
the -X- _ O
appendix. -X- _ O
Not -X- _ O
only -X- _ O
do -X- _ O
individual -X- _ O
attention -X- _ O
heads -X- _ O
clearly -X- _ O
learn -X- _ O
to -X- _ O
perform -X- _ O
different -X- _ O
tasks, -X- _ O
many -X- _ O
appear -X- _ O
to -X- _ O
exhibit -X- _ O
behavior -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
syntactic -X- _ O
and -X- _ O
semantic -X- _ O
structure -X- _ O
of -X- _ O
the -X- _ O
sentences. -X- _ O

Since -X- _ O
our -X- _ O
model -X- _ O
contains -X- _ O
no -X- _ O
recurrence -X- _ O
and -X- _ O
no -X- _ O
convolution, -X- _ O
in -X- _ O
order -X- _ O
for -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
make -X- _ O
use -X- _ O
of -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
sequence, -X- _ O
we -X- _ O
must -X- _ O
inject -X- _ O
some -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
relative -X- _ O
or -X- _ O
absolute -X- _ O
position -X- _ O
of -X- _ O
the -X- _ O
O(n -X- _ O
2 -X- _ O
• -X- _ O
d) -X- _ O
O(1) -X- _ O
O(1) -X- _ O
Recurrent -X- _ O
O(n -X- _ O
• -X- _ O
d -X- _ O
2 -X- _ O
) -X- _ O
O(n) -X- _ O
O(n) -X- _ O
Convolutional -X- _ O
O(k -X- _ O
• -X- _ O
n -X- _ O
• -X- _ O
d -X- _ O
2 -X- _ O
) -X- _ O
O(1) -X- _ O
O(log -X- _ O
k -X- _ O
(n)) -X- _ O
Self-Attention -X- _ O
(restricted) -X- _ O
O(r -X- _ O
• -X- _ O
n -X- _ O
• -X- _ O
d) -X- _ O
O(1) -X- _ O
O(n/r) -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
sequence. -X- _ O
To -X- _ O
this -X- _ O
end, -X- _ O
we -X- _ O
add -X- _ O
"positional -X- _ O
encodings" -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
embeddings -X- _ O
at -X- _ O
the -X- _ O
bottoms -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
stacks. -X- _ O
The -X- _ O
positional -X- _ O
encodings -X- _ O
have -X- _ O
the -X- _ O
same -X- _ O
dimension -X- _ O
d -X- _ O
model -X- _ O
as -X- _ O
the -X- _ O
embeddings, -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
two -X- _ O
can -X- _ O
be -X- _ O
summed. -X- _ O
There -X- _ O
are -X- _ O
many -X- _ O
choices -X- _ O
of -X- _ O
positional -X- _ O
encodings, -X- _ O
learned -X- _ O
and -X- _ O
fixed -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
work, -X- _ O
we -X- _ O
use -X- _ O
sine -X- _ O
and -X- _ O
cosine -X- _ O
functions -X- _ O
of -X- _ O
different -X- _ O
frequencies: -X- _ O
P -X- _ O
E -X- _ O
(pos,2i) -X- _ O
= -X- _ O
sin(pos/10000 -X- _ O
2i/dmodel -X- _ O
) -X- _ O
P -X- _ O
E -X- _ O
(pos,2i+1) -X- _ O
= -X- _ O
cos(pos/10000 -X- _ O
2i/dmodel -X- _ O
) -X- _ O
where -X- _ O
pos -X- _ O
is -X- _ O
the -X- _ O
position -X- _ O
and -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
dimension. -X- _ O
That -X- _ O
is, -X- _ O
each -X- _ O
dimension -X- _ O
of -X- _ O
the -X- _ O
positional -X- _ O
encoding -X- _ O
corresponds -X- _ O
to -X- _ O
a -X- _ O
sinusoid. -X- _ O
The -X- _ O
wavelengths -X- _ O
form -X- _ O
a -X- _ O
geometric -X- _ O
progression -X- _ O
from -X- _ O
2π -X- _ O
to -X- _ O
10000 -X- _ O
• -X- _ O
2π. -X- _ O
We -X- _ O
chose -X- _ O
this -X- _ O
function -X- _ O
because -X- _ O
we -X- _ O
hypothesized -X- _ O
it -X- _ O
would -X- _ O
allow -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
easily -X- _ O
learn -X- _ O
to -X- _ O
attend -X- _ O
by -X- _ O
relative -X- _ O
positions, -X- _ O
since -X- _ O
for -X- _ O
any -X- _ O
fixed -X- _ O
offset -X- _ O
k, -X- _ O
P -X- _ O
E -X- _ O
pos+k -X- _ O
can -X- _ O
be -X- _ O
represented -X- _ O
as -X- _ O
a -X- _ O
linear -X- _ O
function -X- _ O
of -X- _ O
P -X- _ O
E -X- _ O
pos -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
experimented -X- _ O
with -X- _ O
using -X- _ O
learned -X- _ O
positional -X- _ O
embeddings -X- _ O
instead, -X- _ O
and -X- _ O
found -X- _ O
that -X- _ O
the -X- _ O
two -X- _ O
versions -X- _ O
produced -X- _ O
nearly -X- _ O
identical -X- _ O
results -X- _ O
(see -X- _ O
Table -X- _ O
3 -X- _ O
row -X- _ O
(E)). -X- _ O
We -X- _ O
chose -X- _ O
the -X- _ O
sinusoidal -X- _ O
version -X- _ O
because -X- _ O
it -X- _ O
may -X- _ O
allow -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
extrapolate -X- _ O
to -X- _ O
sequence -X- _ O
lengths -X- _ O
longer -X- _ O
than -X- _ O
the -X- _ O
ones -X- _ O
encountered -X- _ O
during -X- _ O
training. -X- _ O

Similarly -X- _ O
to -X- _ O
other -X- _ O
sequence -X- _ O
transduction -X- _ O
models, -X- _ O
we -X- _ O
use -X- _ O
learned -X- _ O
embeddings -X- _ O
to -X- _ O
convert -X- _ O
the -X- _ O
input -X- _ O
tokens -X- _ O
and -X- _ O
output -X- _ O
tokens -X- _ O
to -X- _ O
vectors -X- _ O
of -X- _ O
dimension -X- _ O
d -X- _ O
model -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
use -X- _ O
the -X- _ O
usual -X- _ O
learned -X- _ O
linear -X- _ O
transformation -X- _ O
and -X- _ O
softmax -X- _ O
function -X- _ O
to -X- _ O
convert -X- _ O
the -X- _ O
decoder -X- _ O
output -X- _ O
to -X- _ O
predicted -X- _ O
next-token -X- _ O
probabilities. -X- _ O
In -X- _ O
our -X- _ O
model, -X- _ O
we -X- _ O
share -X- _ O
the -X- _ O
same -X- _ O
weight -X- _ O
matrix -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
embedding -X- _ O
layers -X- _ O
and -X- _ O
the -X- _ O
pre-softmax -X- _ O
linear -X- _ O
transformation, -X- _ O
similar -X- _ O
to -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
embedding -X- _ O
layers, -X- _ O
we -X- _ O
multiply -X- _ O
those -X- _ O
weights -X- _ O
by -X- _ O
√ -X- _ O
d -X- _ O
model -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
attention -X- _ O
sub-layers, -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
layers -X- _ O
in -X- _ O
our -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
contains -X- _ O
a -X- _ O
fully -X- _ O
connected -X- _ O
feed-forward -X- _ O
network, -X- _ O
which -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
each -X- _ O
position -X- _ O
separately -X- _ O
and -X- _ O
identically. -X- _ O
This -X- _ O
consists -X- _ O
of -X- _ O
two -X- _ O
linear -X- _ O
transformations -X- _ O
with -X- _ O
a -X- _ O
ReLU -X- _ B-HyperparameterValue
activation -X- _ B-HyperparameterName
in -X- _ O
between. -X- _ O
FFN(x) -X- _ O
= -X- _ O
max(0, -X- _ O
xW -X- _ O
1 -X- _ O
+ -X- _ O
b -X- _ O
1 -X- _ O
)W -X- _ O
2 -X- _ O
+ -X- _ O
b -X- _ O
2 -X- _ O
(2) -X- _ O
While -X- _ O
the -X- _ O
linear -X- _ O
transformations -X- _ O
are -X- _ O
the -X- _ O
same -X- _ O
across -X- _ O
different -X- _ O
positions, -X- _ O
they -X- _ O
use -X- _ O
different -X- _ O
parameters -X- _ O
from -X- _ O
layer -X- _ O
to -X- _ O
layer. -X- _ O
Another -X- _ O
way -X- _ O
of -X- _ O
describing -X- _ O
this -X- _ O
is -X- _ O
as -X- _ O
two -X- _ O
convolutions -X- _ O
with -X- _ O
kernel -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
1. -X- _ B-HyperparameterValue
The -X- _ O
dimensionality -X- _ O
of -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
is -X- _ O
d -X- _ O
model -X- _ O
= -X- _ O
512, -X- _ O
and -X- _ O
the -X- _ O
inner-layer -X- _ O
has -X- _ O
dimensionality -X- _ O
d -X- _ O
f -X- _ O
f -X- _ O
= -X- _ O
2048. -X- _ O

The -X- _ O
Transformer -X- _ B-MethodName
uses -X- _ O
multi-head -X- _ O
attention -X- _ O
in -X- _ O
three -X- _ O
different -X- _ O
ways: -X- _ O
• -X- _ O
In -X- _ O
"encoder-decoder -X- _ O
attention" -X- _ O
layers, -X- _ O
the -X- _ O
queries -X- _ O
come -X- _ O
from -X- _ O
the -X- _ O
previous -X- _ O
decoder -X- _ O
layer, -X- _ O
and -X- _ O
the -X- _ O
memory -X- _ O
keys -X- _ O
and -X- _ O
values -X- _ O
come -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
encoder. -X- _ O
This -X- _ O
allows -X- _ O
every -X- _ O
position -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
attend -X- _ O
over -X- _ O
all -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
sequence. -X- _ O
This -X- _ O
mimics -X- _ O
the -X- _ O
typical -X- _ O
encoder-decoder -X- _ O
attention -X- _ O
mechanisms -X- _ O
in -X- _ O
sequence-to-sequence -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
. -X- _ O
• -X- _ O
The -X- _ O
encoder -X- _ O
contains -X- _ O
self-attention -X- _ O
layers. -X- _ O
In -X- _ O
a -X- _ O
self-attention -X- _ O
layer -X- _ O
all -X- _ O
of -X- _ O
the -X- _ O
keys, -X- _ O
values -X- _ O
and -X- _ O
queries -X- _ O
come -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
place, -X- _ O
in -X- _ O
this -X- _ O
case, -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
previous -X- _ O
layer -X- _ O
in -X- _ O
the -X- _ O
encoder. -X- _ O
Each -X- _ O
position -X- _ O
in -X- _ O
the -X- _ O
encoder -X- _ O
can -X- _ O
attend -X- _ O
to -X- _ O
all -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
previous -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
encoder. -X- _ O
• -X- _ O
Similarly, -X- _ O
self-attention -X- _ O
layers -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
allow -X- _ O
each -X- _ O
position -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
attend -X- _ O
to -X- _ O
all -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
up -X- _ O
to -X- _ O
and -X- _ O
including -X- _ O
that -X- _ O
position. -X- _ O
We -X- _ O
need -X- _ O
to -X- _ O
prevent -X- _ O
leftward -X- _ O
information -X- _ O
flow -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
to -X- _ O
preserve -X- _ O
the -X- _ O
auto-regressive -X- _ O
property. -X- _ O
We -X- _ O
implement -X- _ O
this -X- _ O
inside -X- _ O
of -X- _ O
scaled -X- _ O
dot-product -X- _ O
attention -X- _ O
by -X- _ O
masking -X- _ O
out -X- _ O
(setting -X- _ O
to -X- _ O
−∞) -X- _ O
all -X- _ O
values -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
of -X- _ O
the -X- _ O
softmax -X- _ O
which -X- _ O
correspond -X- _ O
to -X- _ O
illegal -X- _ O
connections. -X- _ O
See -X- _ O
Figure -X- _ O
2. -X- _ O

Instead -X- _ O
of -X- _ O
performing -X- _ O
a -X- _ O
single -X- _ O
attention -X- _ O
function -X- _ O
with -X- _ O
d -X- _ O
model -X- _ O
-dimensional -X- _ O
keys, -X- _ O
values -X- _ O
and -X- _ O
queries, -X- _ O
we -X- _ O
found -X- _ O
it -X- _ O
beneficial -X- _ O
to -X- _ O
linearly -X- _ O
project -X- _ O
the -X- _ O
queries, -X- _ O
keys -X- _ O
and -X- _ O
values -X- _ O
h -X- _ O
times -X- _ O
with -X- _ O
different, -X- _ O
learned -X- _ O
linear -X- _ O
projections -X- _ O
to -X- _ O
d -X- _ O
k -X- _ O
, -X- _ O
d -X- _ O
k -X- _ O
and -X- _ O
d -X- _ O
v -X- _ O
dimensions, -X- _ O
respectively. -X- _ O
On -X- _ O
each -X- _ O
of -X- _ O
these -X- _ O
projected -X- _ O
versions -X- _ O
of -X- _ O
queries, -X- _ O
keys -X- _ O
and -X- _ O
values -X- _ O
we -X- _ O
then -X- _ O
perform -X- _ O
the -X- _ O
attention -X- _ O
function -X- _ O
in -X- _ O
parallel, -X- _ O
yielding -X- _ O
d -X- _ O
v -X- _ O
-dimensional -X- _ O
output -X- _ O
values. -X- _ O
These -X- _ O
are -X- _ O
concatenated -X- _ O
and -X- _ O
once -X- _ O
again -X- _ O
projected, -X- _ O
resulting -X- _ O
in -X- _ O
the -X- _ O
final -X- _ O
values, -X- _ O
as -X- _ O
depicted -X- _ O
in -X- _ O
Figure -X- _ O
2. -X- _ O
Multi-head -X- _ O
attention -X- _ O
allows -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
jointly -X- _ O
attend -X- _ O
to -X- _ O
information -X- _ O
from -X- _ O
different -X- _ O
representation -X- _ O
subspaces -X- _ O
at -X- _ O
different -X- _ O
positions. -X- _ O
With -X- _ O
a -X- _ O
single -X- _ O
attention -X- _ O
head, -X- _ O
averaging -X- _ O
inhibits -X- _ O
this. -X- _ O
MultiHead(Q, -X- _ O
K, -X- _ O
V -X- _ O
) -X- _ O
= -X- _ O
Concat(head -X- _ O
1 -X- _ O
, -X- _ O
..., -X- _ O
head -X- _ O
h -X- _ O
)W -X- _ O
O -X- _ O
where -X- _ O
head -X- _ O
i -X- _ O
= -X- _ O
Attention(QW -X- _ O
Q -X- _ O
i -X- _ O
, -X- _ O
KW -X- _ O
K -X- _ O
i -X- _ O
, -X- _ O
V -X- _ O
W -X- _ O
V -X- _ O
i -X- _ O
) -X- _ O
Where -X- _ O
the -X- _ O
projections -X- _ O
are -X- _ O
parameter -X- _ O
matrices -X- _ O
W -X- _ O
Q -X- _ O
i -X- _ O
∈ -X- _ O
R -X- _ O
dmodel×d -X- _ O
k -X- _ O
, -X- _ O
W -X- _ O
K -X- _ O
i -X- _ O
∈ -X- _ O
R -X- _ O
dmodel×d -X- _ O
k -X- _ O
, -X- _ O
W -X- _ O
V -X- _ O
i -X- _ O
∈ -X- _ O
R -X- _ O
dmodel×dv -X- _ O
and -X- _ O
W -X- _ O
O -X- _ O
∈ -X- _ O
R -X- _ O
hdv×dmodel -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
work -X- _ O
we -X- _ O
employ -X- _ O
h -X- _ O
= -X- _ O
8 -X- _ O
parallel -X- _ O
attention -X- _ O
layers, -X- _ O
or -X- _ O
heads. -X- _ O
For -X- _ O
each -X- _ O
of -X- _ O
these -X- _ O
we -X- _ O
use -X- _ O
d -X- _ O
k -X- _ O
= -X- _ O
d -X- _ O
v -X- _ O
= -X- _ O
d -X- _ O
model -X- _ O
/h -X- _ O
= -X- _ O
64. -X- _ O
Due -X- _ O
to -X- _ O
the -X- _ O
reduced -X- _ O
dimension -X- _ O
of -X- _ O
each -X- _ O
head, -X- _ O
the -X- _ O
total -X- _ O
computational -X- _ O
cost -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
that -X- _ O
of -X- _ O
single-head -X- _ O
attention -X- _ O
with -X- _ O
full -X- _ O
dimensionality. -X- _ O

We -X- _ O
call -X- _ O
our -X- _ O
particular -X- _ O
attention -X- _ O
"Scaled -X- _ O
Dot-Product -X- _ O
Attention" -X- _ O
(Figure -X- _ O
2). -X- _ O
The -X- _ O
input -X- _ O
consists -X- _ O
of -X- _ O
queries -X- _ O
and -X- _ O
keys -X- _ O
of -X- _ O
dimension -X- _ O
d -X- _ O
k -X- _ O
, -X- _ O
and -X- _ O
values -X- _ O
of -X- _ O
dimension -X- _ O
d -X- _ O
v -X- _ O
. -X- _ O
We -X- _ O
compute -X- _ O
the -X- _ O
dot -X- _ O
products -X- _ O
of -X- _ O
the -X- _ O
query -X- _ O
with -X- _ O
all -X- _ O
keys, -X- _ O
divide -X- _ O
each -X- _ O
by -X- _ O
√ -X- _ O
d -X- _ O
k -X- _ O
, -X- _ O
and -X- _ O
apply -X- _ O
a -X- _ O
softmax -X- _ O
function -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
weights -X- _ O
on -X- _ O
the -X- _ O
values. -X- _ O
In -X- _ O
practice, -X- _ O
we -X- _ O
compute -X- _ O
the -X- _ O
attention -X- _ O
function -X- _ O
on -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
queries -X- _ O
simultaneously, -X- _ O
packed -X- _ O
together -X- _ O
into -X- _ O
a -X- _ O
matrix -X- _ O
Q. -X- _ O
The -X- _ O
keys -X- _ O
and -X- _ O
values -X- _ O
are -X- _ O
also -X- _ O
packed -X- _ O
together -X- _ O
into -X- _ O
matrices -X- _ O
K -X- _ O
and -X- _ O
V -X- _ O
. -X- _ O
We -X- _ O
compute -X- _ O
the -X- _ O
matrix -X- _ O
of -X- _ O
outputs -X- _ O
as: -X- _ O
Attention(Q, -X- _ O
K, -X- _ O
V -X- _ O
) -X- _ O
= -X- _ O
softmax( -X- _ O
QK -X- _ O
T -X- _ O
√ -X- _ O
d -X- _ O
k -X- _ O
)V -X- _ O
(1) -X- _ O
The -X- _ O
two -X- _ O
most -X- _ O
commonly -X- _ O
used -X- _ O
attention -X- _ O
functions -X- _ O
are -X- _ O
additive -X- _ O
attention -X- _ O
, -X- _ O
and -X- _ O
dot-product -X- _ O
(multiplicative) -X- _ O
attention. -X- _ O
Dot-product -X- _ O
attention -X- _ O
is -X- _ O
identical -X- _ O
to -X- _ O
our -X- _ O
algorithm, -X- _ O
except -X- _ O
for -X- _ O
the -X- _ O
scaling -X- _ O
factor -X- _ O
of -X- _ O
1 -X- _ O
√ -X- _ O
d -X- _ O
k -X- _ O
. -X- _ O
Additive -X- _ O
attention -X- _ O
computes -X- _ O
the -X- _ O
compatibility -X- _ O
function -X- _ O
using -X- _ O
a -X- _ O
feed-forward -X- _ O
network -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
hidden -X- _ O
layer. -X- _ O
While -X- _ O
the -X- _ O
two -X- _ O
are -X- _ O
similar -X- _ O
in -X- _ O
theoretical -X- _ O
complexity, -X- _ O
dot-product -X- _ O
attention -X- _ O
is -X- _ O
much -X- _ O
faster -X- _ O
and -X- _ O
more -X- _ O
space-efficient -X- _ O
in -X- _ O
practice, -X- _ O
since -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
implemented -X- _ O
using -X- _ O
highly -X- _ O
optimized -X- _ O
matrix -X- _ O
multiplication -X- _ O
code. -X- _ O
While -X- _ O
for -X- _ O
small -X- _ O
values -X- _ O
of -X- _ O
d -X- _ O
k -X- _ O
the -X- _ O
two -X- _ O
mechanisms -X- _ O
perform -X- _ O
similarly, -X- _ O
additive -X- _ O
attention -X- _ O
outperforms -X- _ O
dot -X- _ O
product -X- _ O
attention -X- _ O
without -X- _ O
scaling -X- _ O
for -X- _ O
larger -X- _ O
values -X- _ O
of -X- _ O
d -X- _ O
k -X- _ O
. -X- _ O
We -X- _ O
suspect -X- _ O
that -X- _ O
for -X- _ O
large -X- _ O
values -X- _ O
of -X- _ O
d -X- _ O
k -X- _ O
, -X- _ O
the -X- _ O
dot -X- _ O
products -X- _ O
grow -X- _ O
large -X- _ O
in -X- _ O
magnitude, -X- _ O
pushing -X- _ O
the -X- _ O
softmax -X- _ O
function -X- _ O
into -X- _ O
regions -X- _ O
where -X- _ O
it -X- _ O
has -X- _ O
extremely -X- _ O
small -X- _ O
gradients -X- _ O
4 -X- _ O
. -X- _ O
To -X- _ O
counteract -X- _ O
this -X- _ O
effect, -X- _ O
we -X- _ O
scale -X- _ O
the -X- _ O
dot -X- _ O
products -X- _ O
by -X- _ O
1 -X- _ O
√ -X- _ O
d -X- _ O
k -X- _ O
. -X- _ O

An -X- _ O
attention -X- _ O
function -X- _ O
can -X- _ O
be -X- _ O
described -X- _ O
as -X- _ O
mapping -X- _ O
a -X- _ O
query -X- _ O
and -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
key-value -X- _ O
pairs -X- _ O
to -X- _ O
an -X- _ O
output, -X- _ O
where -X- _ O
the -X- _ O
query, -X- _ O
keys, -X- _ O
values, -X- _ O
and -X- _ O
output -X- _ O
are -X- _ O
all -X- _ O
vectors. -X- _ O
The -X- _ O
output -X- _ O
is -X- _ O
computed -X- _ O
as -X- _ O
a -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
values, -X- _ O
where -X- _ O
the -X- _ O
weight -X- _ O
assigned -X- _ O
to -X- _ O
each -X- _ O
value -X- _ O
is -X- _ O
computed -X- _ O
by -X- _ O
a -X- _ O
compatibility -X- _ O
function -X- _ O
of -X- _ O
the -X- _ O
query -X- _ O
with -X- _ O
the -X- _ O
corresponding -X- _ O
key. -X- _ O

Encoder: -X- _ O
The -X- _ O
encoder -X- _ O
is -X- _ O
composed -X- _ O
of -X- _ O
a -X- _ O
stack -X- _ O
of -X- _ O
N -X- _ O
= -X- _ O
6 -X- _ O
identical -X- _ O
layers. -X- _ O
Each -X- _ O
layer -X- _ O
has -X- _ O
two -X- _ O
sub-layers. -X- _ O
The -X- _ O
first -X- _ O
is -X- _ O
a -X- _ O
multi-head -X- _ O
self-attention -X- _ O
mechanism, -X- _ O
and -X- _ O
the -X- _ O
second -X- _ O
is -X- _ O
a -X- _ O
simple, -X- _ O
positionwise -X- _ O
fully -X- _ O
connected -X- _ O
feed-forward -X- _ O
network. -X- _ O
We -X- _ O
employ -X- _ O
a -X- _ O
residual -X- _ O
connection -X- _ O
around -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
sub-layers, -X- _ O
followed -X- _ O
by -X- _ O
layer -X- _ O
normalization -X- _ O
. -X- _ O
That -X- _ O
is, -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
each -X- _ O
sub-layer -X- _ O
is -X- _ O
LayerNorm(x -X- _ O
+ -X- _ O
Sublayer(x)), -X- _ O
where -X- _ O
Sublayer(x) -X- _ O
is -X- _ O
the -X- _ O
function -X- _ O
implemented -X- _ O
by -X- _ O
the -X- _ O
sub-layer -X- _ O
itself. -X- _ O
To -X- _ O
facilitate -X- _ O
these -X- _ O
residual -X- _ O
connections, -X- _ O
all -X- _ O
sub-layers -X- _ O
in -X- _ O
the -X- _ O
model, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
embedding -X- _ O
layers, -X- _ O
produce -X- _ O
outputs -X- _ O
of -X- _ O
dimension -X- _ O
d -X- _ O
model -X- _ O
= -X- _ O
512. -X- _ O
Decoder: -X- _ O
The -X- _ O
decoder -X- _ O
is -X- _ O
also -X- _ O
composed -X- _ O
of -X- _ O
a -X- _ O
stack -X- _ O
of -X- _ O
N -X- _ O
= -X- _ O
6 -X- _ O
identical -X- _ O
layers. -X- _ O
In -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
two -X- _ O
sub-layers -X- _ O
in -X- _ O
each -X- _ O
encoder -X- _ O
layer, -X- _ O
the -X- _ O
decoder -X- _ O
inserts -X- _ O
a -X- _ O
third -X- _ O
sub-layer, -X- _ O
which -X- _ O
performs -X- _ O
multi-head -X- _ O
attention -X- _ O
over -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
stack. -X- _ O
Similar -X- _ O
to -X- _ O
the -X- _ O
encoder, -X- _ O
we -X- _ O
employ -X- _ O
residual -X- _ O
connections -X- _ O
around -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
sub-layers, -X- _ O
followed -X- _ O
by -X- _ O
layer -X- _ O
normalization. -X- _ O
We -X- _ O
also -X- _ O
modify -X- _ O
the -X- _ O
self-attention -X- _ O
sub-layer -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
stack -X- _ O
to -X- _ O
prevent -X- _ O
positions -X- _ O
from -X- _ O
attending -X- _ O
to -X- _ O
subsequent -X- _ O
positions. -X- _ O
This -X- _ O
masking, -X- _ O
combined -X- _ O
with -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
output -X- _ O
embeddings -X- _ O
are -X- _ O
offset -X- _ O
by -X- _ O
one -X- _ O
position, -X- _ O
ensures -X- _ O
that -X- _ O
the -X- _ O
predictions -X- _ O
for -X- _ O
position -X- _ O
i -X- _ O
can -X- _ O
depend -X- _ O
only -X- _ O
on -X- _ O
the -X- _ O
known -X- _ O
outputs -X- _ O
at -X- _ O
positions -X- _ O
less -X- _ O
than -X- _ O
i. -X- _ O

Most -X- _ O
competitive -X- _ O
neural -X- _ O
sequence -X- _ O
transduction -X- _ O
models -X- _ O
have -X- _ O
an -X- _ O
encoder-decoder -X- _ O
structure -X- _ O
. -X- _ O
Here, -X- _ O
the -X- _ O
encoder -X- _ O
maps -X- _ O
an -X- _ O
input -X- _ O
sequence -X- _ O
of -X- _ O
symbol -X- _ O
representations -X- _ O
(x -X- _ O
1 -X- _ O
, -X- _ O
..., -X- _ O
x -X- _ O
n -X- _ O
) -X- _ O
to -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
continuous -X- _ O
representations -X- _ O
z -X- _ O
= -X- _ O
(z -X- _ O
1 -X- _ O
, -X- _ O
..., -X- _ O
z -X- _ O
n -X- _ O
). -X- _ O
Given -X- _ O
z, -X- _ O
the -X- _ O
decoder -X- _ O
then -X- _ O
generates -X- _ O
an -X- _ O
output -X- _ O
sequence -X- _ O
(y -X- _ O
1 -X- _ O
, -X- _ O
..., -X- _ O
y -X- _ O
m -X- _ O
) -X- _ O
of -X- _ O
symbols -X- _ O
one -X- _ O
element -X- _ O
at -X- _ O
a -X- _ O
time. -X- _ O
At -X- _ O
each -X- _ O
step -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
auto-regressive -X- _ O
, -X- _ O
consuming -X- _ O
the -X- _ O
previously -X- _ O
generated -X- _ O
symbols -X- _ O
as -X- _ O
additional -X- _ O
input -X- _ O
when -X- _ O
generating -X- _ O
the -X- _ O
next. -X- _ O
The -X- _ O
Transformer -X- _ B-MethodName
follows -X- _ O
this -X- _ O
overall -X- _ O
architecture -X- _ O
using -X- _ O
stacked -X- _ O
self-attention -X- _ O
and -X- _ O
point-wise, -X- _ O
fully -X- _ O
connected -X- _ O
layers -X- _ O
for -X- _ O
both -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder, -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
halves -X- _ O
of -X- _ O
Figure -X- _ O
1, -X- _ O
respectively. -X- _ O
Figure -X- _ O
1: -X- _ O
The -X- _ O
Transformer -X- _ B-MethodName
-model -X- _ O
architecture. -X- _ O

The -X- _ O
goal -X- _ O
of -X- _ O
reducing -X- _ O
sequential -X- _ O
computation -X- _ O
also -X- _ O
forms -X- _ O
the -X- _ O
foundation -X- _ O
of -X- _ O
the -X- _ O
Extended -X- _ O
Neural -X- _ O
GPU -X- _ O
, -X- _ O
ByteNet -X- _ B-MethodName
and -X- _ O
ConvS2S -X- _ B-MethodName
, -X- _ O
all -X- _ O
of -X- _ O
which -X- _ O
use -X- _ O
convolutional -X- _ O
neural -X- _ O
networks -X- _ O
as -X- _ O
basic -X- _ O
building -X- _ O
block, -X- _ O
computing -X- _ O
hidden -X- _ O
representations -X- _ O
in -X- _ O
parallel -X- _ O
for -X- _ O
all -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
positions. -X- _ O
In -X- _ O
these -X- _ O
models, -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
operations -X- _ O
required -X- _ O
to -X- _ O
relate -X- _ O
signals -X- _ O
from -X- _ O
two -X- _ O
arbitrary -X- _ O
input -X- _ O
or -X- _ O
output -X- _ O
positions -X- _ O
grows -X- _ O
in -X- _ O
the -X- _ O
distance -X- _ O
between -X- _ O
positions, -X- _ O
linearly -X- _ O
for -X- _ O
ConvS2S -X- _ B-MethodName
and -X- _ O
logarithmically -X- _ O
for -X- _ O
ByteNet. -X- _ B-MethodName
This -X- _ O
makes -X- _ O
it -X- _ O
more -X- _ O
difficult -X- _ O
to -X- _ O
learn -X- _ O
dependencies -X- _ O
between -X- _ O
distant -X- _ O
positions -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
this -X- _ O
is -X- _ O
reduced -X- _ O
to -X- _ O
a -X- _ O
constant -X- _ O
number -X- _ O
of -X- _ O
operations, -X- _ O
albeit -X- _ O
at -X- _ O
the -X- _ O
cost -X- _ O
of -X- _ O
reduced -X- _ O
effective -X- _ O
resolution -X- _ O
due -X- _ O
to -X- _ O
averaging -X- _ O
attention-weighted -X- _ O
positions, -X- _ O
an -X- _ O
effect -X- _ O
we -X- _ O
counteract -X- _ O
with -X- _ O
Multi-Head -X- _ O
Attention -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
section -X- _ O
3.2. -X- _ O
Self-attention, -X- _ O
sometimes -X- _ O
called -X- _ O
intra-attention -X- _ O
is -X- _ O
an -X- _ O
attention -X- _ O
mechanism -X- _ O
relating -X- _ O
different -X- _ O
positions -X- _ O
of -X- _ O
a -X- _ O
single -X- _ O
sequence -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
compute -X- _ O
a -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
sequence. -X- _ O
Self-attention -X- _ O
has -X- _ O
been -X- _ O
used -X- _ O
successfully -X- _ O
in -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
tasks -X- _ O
including -X- _ O
reading -X- _ O
comprehension, -X- _ O
abstractive -X- _ O
summarization, -X- _ O
textual -X- _ O
entailment -X- _ O
and -X- _ O
learning -X- _ O
task-independent -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O
End-to-end -X- _ O
memory -X- _ O
networks -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
recurrent -X- _ O
attention -X- _ O
mechanism -X- _ O
instead -X- _ O
of -X- _ O
sequencealigned -X- _ O
recurrence -X- _ O
and -X- _ O
have -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
perform -X- _ O
well -X- _ O
on -X- _ O
simple-language -X- _ B-TaskName
question -X- _ I-TaskName
answering -X- _ I-TaskName
and -X- _ O
language -X- _ B-TaskName
modeling -X- _ I-TaskName
tasks -X- _ O
. -X- _ O
To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge, -X- _ O
however, -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
is -X- _ O
the -X- _ O
first -X- _ O
transduction -X- _ O
model -X- _ O
relying -X- _ O
entirely -X- _ O
on -X- _ O
self-attention -X- _ O
to -X- _ O
compute -X- _ O
representations -X- _ O
of -X- _ O
its -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
without -X- _ O
using -X- _ O
sequencealigned -X- _ O
RNNs -X- _ O
or -X- _ O
convolution. -X- _ O
In -X- _ O
the -X- _ O
following -X- _ O
sections, -X- _ O
we -X- _ O
will -X- _ O
describe -X- _ O
the -X- _ O
Transformer, -X- _ B-MethodName
motivate -X- _ O
self-attention -X- _ O
and -X- _ O
discuss -X- _ O
its -X- _ O
advantages -X- _ O
over -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
and -X- _ O
. -X- _ O

Recurrent -X- _ O
neural -X- _ O
networks, -X- _ O
long -X- _ O
short-term -X- _ O
memory -X- _ O
and -X- _ O
gated -X- _ O
recurrent -X- _ O
neural -X- _ O
networks -X- _ O
in -X- _ O
particular, -X- _ O
have -X- _ O
been -X- _ O
firmly -X- _ O
established -X- _ O
as -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
approaches -X- _ O
in -X- _ O
sequence -X- _ O
modeling -X- _ O
and -X- _ O
transduction -X- _ O
problems -X- _ O
such -X- _ O
as -X- _ B-TaskName
language -X- _ I-TaskName
modeling -X- _ I-TaskName
and -X- _ O
machine -X- _ B-TaskName
translation -X- _ I-TaskName
. -X- _ O
Numerous -X- _ O
efforts -X- _ O
have -X- _ O
since -X- _ O
continued -X- _ O
to -X- _ O
push -X- _ O
the -X- _ O
boundaries -X- _ O
of -X- _ O
recurrent -X- _ O
language -X- _ O
models -X- _ O
and -X- _ O
encoder-decoder -X- _ O
architectures -X- _ O
. -X- _ O
Recurrent -X- _ O
models -X- _ O
typically -X- _ O
factor -X- _ O
computation -X- _ O
along -X- _ O
the -X- _ O
symbol -X- _ O
positions -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
sequences. -X- _ O
Aligning -X- _ O
the -X- _ O
positions -X- _ O
to -X- _ O
steps -X- _ O
in -X- _ O
computation -X- _ O
time, -X- _ O
they -X- _ O
generate -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
hidden -X- _ O
states -X- _ O
h -X- _ O
t -X- _ O
, -X- _ O
as -X- _ O
a -X- _ O
function -X- _ O
of -X- _ O
the -X- _ O
previous -X- _ O
hidden -X- _ O
state -X- _ O
h -X- _ O
t−1 -X- _ O
and -X- _ O
the -X- _ O
input -X- _ O
for -X- _ O
position -X- _ O
t. -X- _ O
This -X- _ O
inherently -X- _ O
sequential -X- _ O
nature -X- _ O
precludes -X- _ O
parallelization -X- _ O
within -X- _ O
training -X- _ O
examples, -X- _ O
which -X- _ O
becomes -X- _ O
critical -X- _ O
at -X- _ O
longer -X- _ O
sequence -X- _ O
lengths, -X- _ O
as -X- _ O
memory -X- _ O
constraints -X- _ O
limit -X- _ O
batching -X- _ O
across -X- _ O
examples. -X- _ O
Recent -X- _ O
work -X- _ O
has -X- _ O
achieved -X- _ O
significant -X- _ O
improvements -X- _ O
in -X- _ O
computational -X- _ O
efficiency -X- _ O
through -X- _ O
factorization -X- _ O
tricks -X- _ O
and -X- _ O
conditional -X- _ O
computation -X- _ O
, -X- _ O
while -X- _ O
also -X- _ O
improving -X- _ O
model -X- _ O
performance -X- _ O
in -X- _ O
case -X- _ O
of -X- _ O
the -X- _ O
latter. -X- _ O
The -X- _ O
fundamental -X- _ O
constraint -X- _ O
of -X- _ O
sequential -X- _ O
computation, -X- _ O
however, -X- _ O
remains. -X- _ O
Attention -X- _ O
mechanisms -X- _ O
have -X- _ O
become -X- _ O
an -X- _ O
integral -X- _ O
part -X- _ O
of -X- _ O
compelling -X- _ O
sequence -X- _ O
modeling -X- _ O
and -X- _ O
transduction -X- _ O
models -X- _ O
in -X- _ O
various -X- _ O
tasks, -X- _ O
allowing -X- _ O
modeling -X- _ O
of -X- _ O
dependencies -X- _ O
without -X- _ O
regard -X- _ O
to -X- _ O
their -X- _ O
distance -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
or -X- _ O
output -X- _ O
sequences -X- _ O
. -X- _ O
In -X- _ O
all -X- _ O
but -X- _ O
a -X- _ O
few -X- _ O
cases -X- _ O
, -X- _ O
however, -X- _ O
such -X- _ O
attention -X- _ O
mechanisms -X- _ O
are -X- _ O
used -X- _ O
in -X- _ O
conjunction -X- _ O
with -X- _ O
a -X- _ O
recurrent -X- _ O
network. -X- _ O
In -X- _ O
this -X- _ O
work -X- _ O
we -X- _ O
propose -X- _ O
the -X- _ O
Transformer, -X- _ B-MethodName
a -X- _ O
model -X- _ O
architecture -X- _ O
eschewing -X- _ O
recurrence -X- _ O
and -X- _ O
instead -X- _ O
relying -X- _ O
entirely -X- _ O
on -X- _ O
an -X- _ O
attention -X- _ O
mechanism -X- _ O
to -X- _ O
draw -X- _ O
global -X- _ O
dependencies -X- _ O
between -X- _ O
input -X- _ O
and -X- _ O
output. -X- _ O
The -X- _ O
Transformer -X- _ B-MethodName
allows -X- _ O
for -X- _ O
significantly -X- _ O
more -X- _ O
parallelization -X- _ O
and -X- _ O
can -X- _ O
reach -X- _ O
a -X- _ O
new -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
in -X- _ O
translation -X- _ O
quality -X- _ O
after -X- _ O
being -X- _ O
trained -X- _ O
for -X- _ O
as -X- _ O
little -X- _ O
as -X- _ O
twelve -X- _ O
hours -X- _ O
on -X- _ O
eight -X- _ O
P100 -X- _ O
GPUs. -X- _ O

The -X- _ O
dominant -X- _ O
sequence -X- _ O
transduction -X- _ O
models -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
complex -X- _ O
recurrent -X- _ O
or -X- _ O
convolutional -X- _ O
neural -X- _ O
networks -X- _ O
that -X- _ O
include -X- _ O
an -X- _ O
encoder -X- _ O
and -X- _ O
a -X- _ O
decoder. -X- _ O
The -X- _ O
best -X- _ O
performing -X- _ O
models -X- _ O
also -X- _ O
connect -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
through -X- _ O
an -X- _ O
attention -X- _ O
mechanism. -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
new -X- _ O
simple -X- _ O
network -X- _ O
architecture, -X- _ O
the -X- _ O
Transformer, -X- _ B-MethodName
based -X- _ O
solely -X- _ O
on -X- _ O
attention -X- _ O
mechanisms, -X- _ O
dispensing -X- _ O
with -X- _ O
recurrence -X- _ O
and -X- _ O
convolutions -X- _ O
entirely. -X- _ O
Experiments -X- _ O
on -X- _ O
two -X- _ O
machine -X- _ B-TaskName
translation -X- _ I-TaskName
tasks -X- _ O
show -X- _ O
these -X- _ O
models -X- _ O
to -X- _ O
be -X- _ O
superior -X- _ O
in -X- _ O
quality -X- _ O
while -X- _ O
being -X- _ O
more -X- _ O
parallelizable -X- _ O
and -X- _ O
requiring -X- _ O
significantly -X- _ O
less -X- _ O
time -X- _ O
to -X- _ O
train. -X- _ O
Our -X- _ O
model -X- _ O
achieves -X- _ O
28.4 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
on -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
Englishto-German -X- _ I-DatasetName
translation -X- _ O
task, -X- _ O
improving -X- _ O
over -X- _ O
the -X- _ O
existing -X- _ O
best -X- _ O
results, -X- _ O
including -X- _ O
ensembles, -X- _ O
by -X- _ O
over -X- _ O
2 -X- _ B-MetricValue
BLEU. -X- _ B-MetricName
On -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
2014 -X- _ I-DatasetName
English-to-French -X- _ I-DatasetName
translation -X- _ O
task, -X- _ O
our -X- _ O
model -X- _ O
establishes -X- _ O
a -X- _ O
new -X- _ O
single-model -X- _ O
state-of-the-art -X- _ O
BLEU -X- _ B-MetricName
score -X- _ O
of -X- _ O
41.8 -X- _ B-MetricValue
after -X- _ O
training -X- _ O
for -X- _ O
3.5 -X- _ O
days -X- _ O
on -X- _ O
eight -X- _ O
GPUs, -X- _ O
a -X- _ O
small -X- _ O
fraction -X- _ O
of -X- _ O
the -X- _ O
training -X- _ O
costs -X- _ O
of -X- _ O
the -X- _ O
best -X- _ O
models -X- _ O
from -X- _ O
the -X- _ O
literature. -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
generalizes -X- _ O
well -X- _ O
to -X- _ O
other -X- _ O
tasks -X- _ O
by -X- _ O
applying -X- _ O
it -X- _ O
successfully -X- _ O
to -X- _ O
English -X- _ B-TaskName
constituency -X- _ I-TaskName
parsing -X- _ I-TaskName
both -X- _ O
with -X- _ O
large -X- _ O
and -X- _ O
limited -X- _ O
training -X- _ O
data. -X- _ O
* -X- _ O
Equal -X- _ O
contribution. -X- _ O
Listing -X- _ O
order -X- _ O
is -X- _ O
random. -X- _ O
Jakob -X- _ O
proposed -X- _ O
replacing -X- _ O
RNNs -X- _ O
with -X- _ O
self-attention -X- _ O
and -X- _ O
started -X- _ O
the -X- _ O
effort -X- _ O
to -X- _ O
evaluate -X- _ O
this -X- _ O
idea. -X- _ O
Ashish, -X- _ O
with -X- _ O
Illia, -X- _ O
designed -X- _ O
and -X- _ O
implemented -X- _ O
the -X- _ O
first -X- _ O
Transformer -X- _ B-MethodName
models -X- _ O
and -X- _ O
has -X- _ O
been -X- _ O
crucially -X- _ O
involved -X- _ O
in -X- _ O
every -X- _ O
aspect -X- _ O
of -X- _ O
this -X- _ O
work. -X- _ O
Noam -X- _ O
proposed -X- _ O
scaled -X- _ O
dot-product -X- _ O
attention, -X- _ O
multi-head -X- _ O
attention -X- _ O
and -X- _ O
the -X- _ O
parameter-free -X- _ O
position -X- _ O
representation -X- _ O
and -X- _ O
became -X- _ O
the -X- _ O
other -X- _ O
person -X- _ O
involved -X- _ O
in -X- _ O
nearly -X- _ O
every -X- _ O
detail. -X- _ O
Niki -X- _ O
designed, -X- _ O
implemented, -X- _ O
tuned -X- _ O
and -X- _ O
evaluated -X- _ O
countless -X- _ O
model -X- _ O
variants -X- _ O
in -X- _ O
our -X- _ O
original -X- _ O
codebase -X- _ O
and -X- _ O
tensor2tensor. -X- _ O
Llion -X- _ O
also -X- _ O
experimented -X- _ O
with -X- _ O
novel -X- _ O
model -X- _ O
variants, -X- _ O
was -X- _ O
responsible -X- _ O
for -X- _ O
our -X- _ O
initial -X- _ O
codebase, -X- _ O
and -X- _ O
efficient -X- _ O
inference -X- _ O
and -X- _ O
visualizations. -X- _ O
Lukasz -X- _ O
and -X- _ O
Aidan -X- _ O
spent -X- _ O
countless -X- _ O
long -X- _ O
days -X- _ O
designing -X- _ O
various -X- _ O
parts -X- _ O
of -X- _ O
and -X- _ O
implementing -X- _ O
tensor2tensor, -X- _ O
replacing -X- _ O
our -X- _ O
earlier -X- _ O
codebase, -X- _ O
greatly -X- _ O
improving -X- _ O
results -X- _ O
and -X- _ O
massively -X- _ O
accelerating -X- _ O
our -X- _ O
research. -X- _ O
† -X- _ O
Work -X- _ O
performed -X- _ O
while -X- _ O
at -X- _ O
Google -X- _ O
Brain. -X- _ O
‡ -X- _ O
Work -X- _ O
performed -X- _ O
while -X- _ O
at -X- _ O
Google -X- _ O
Research. -X- _ O

