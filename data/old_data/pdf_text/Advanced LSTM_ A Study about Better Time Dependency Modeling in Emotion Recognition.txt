Long short-term memory (LSTM) is normally used in recurrent neural network (RNN) as basic recurrent unit. However, conventional LSTM assumes that the state at current time step depends on previous time step. This assumption constraints the time dependency modeling capability. In this study, we propose a new variation of LSTM, advanced LSTM (A-LSTM), for better temporal context modeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The A-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based weighted pooling RNN can also complement the state-of-the-art emotion classification framework. This shows the advantage of A-LSTM.
Recurrent neural network is recently used as a dynamic model for sequential input. Long short-term memory (LSTM) is usually adopted as basic units in RNN because it is able to solve the gradients vanishing and exploding problems in RNN training . It uses memory cell and gates to control whether information will be memorized, output or forgotten. The LSTM takes two inputs, output from lower layer and output from previous time step in current layer. This configuration implies an assumption that the current state depends on the state of previous time step. This assumption of time dependency may constraint the modeling capability of RNN. In this paper, we propose a new variation of LSTM, advanced LSTM (A-LSTM), to address this issue. In A-LSTM, current state depends on multiple states of different time steps. This releases the constrains in conventional LSTM and provides better time dependency modeling capability. This paper presents our early study on A-LSTM. We explore the modeling capability of A-LSTM in the application of voice-based emotion recognition. Recognizing emotion based on audio in real world will improve the user experience of voice-based artificial intelligent (AI) product, like Siri, Alexa. The input voice to system in real application may contains long silence (or pause) or non-speech voice filler, the conventional low level statistics feature like Interspeech 2010 paralinguistic challenge feature set (IS10) or GeMAPs , may be failed. Weighted pooling based on attention mechanism is an appealing solution for these cases , which relies on RNN. We built an attention based weighted pooling framework with multi-task learning for emotion recognition in this study. When we apply A-LSTM in this framework, it gains 5.5% relative improvement compared with conventional LSTM. The remaining of the paper is organized as following structure. Section 2 reviews previous work. Section 3 introduces the IEMOCAP corpus which is used in this paper. Besides, the acoustic feature extraction is also described. Section 4 describes the details of the proposed approach. Section 5 describes the experiments and analysis of results. Section 6 concludes the work and leads to the future direction of the work.  shows that temporal information is beneficial for emotion identification.  shows that the performance of the neural network will be improved when higher layers can see more time steps from lower layer. These works rely on DNN rather than RNN. They do not discuss the timing sequence modeling.  proposed solutions to having alternative connections between layers in DNN. These solutions are different from the conventional connections within network.  modify the LSTM architecture relying on residual or highway connection. However, the modifications in these papers are focusing on connecting the memory cells between lower and higher layers. They do not modify the connection within the same layer.  modifies the output hidden value to higher layer by a weighted summation.  follows similar idea. It uses weighted pooling of the hidden values of multiple historic time steps at each time steps which improves the information richness to higher layer. This is equivalent to allow higher layer see more time steps. But they do not modify the memory cell which means the time dependency is not changed.  shows that the combination of near time steps may not improve the system a lot. The combination should contain a long term range. Besides, they do not combine the multiple states at each step, which is different from  . But in these papers, the regression of valence and arousal values are normally set as auxiliary tasks, which is hard to obtain.

We apply A-LSTM in the application of categorical emotion classification. We used IEMOCAP  corpus in this study which has 5 sections and 10 actors in total. In each section, there were two actors (one male and one female) involved in scripted or spontaneous scenarios to perform specific emotions. The utterances were segmented and with one categorical label, which is among angry, fear, excited, neutral, disgust, surprised, sad, happy, frustrated, other and XXX. XXX was the case that the annotators were not able to have agreement on the label. The corpus has 10039 utterances with average duration of 4.5 s per utterance (12.55 hr in total). The distribution of emotion classes is not balanced. In this study, we select 4 classes, neutral, happy, angry and sad. The total number of utterances used is 4490. The corpus has video and audio channels. We only used audios in this study. The audio was collected by high quality microphones (Schoeps CMIT 5U) at the sample rate of 48 kHz. We downsampled them to 16 kHz and extract a 36D acoustic feature. The acoustic feature includes 13D MFCCs, zero crossing rate (ZCR), energy, entropy of energy, spectral centroid, spectral spread, spectral entropy, spectral flux, spectral rolloff, 12D chroma vector, chroma deviation, harmonic ratio and pitch. The extraction was performed within a 25 ms window whose shifting step size was 10 ms (100 fps). The acoustic feature sequence was z-normalized within each utterance.

Attention based weighted pooling RNN is a data-driven framework to learn utterance representation from data, which can be suitable for practical application . It relies on the attention mechanism  to learn the weight of each time step. The weighted summation is then computed as the representation of the whole utterance. Multi-task learning incorporates several aspects of knowledge into training, therefore it can learn better representation. The system diagram is shown in Figure 1. The diagram has two parts, trunk and branch (two dashed boxes in the diagram). The branch is the part for different tasks, which includes emotion, speaker and gender classifications in this study. The trunk is the shared part of all tasks. The attention based weighted pooling is computed as Equation 1, where h T is the hidden value output from the LSTM layer at time T , and A T is a scalar number representing the corresponding weight at time T . A T is computed in a softmax fashion following Equation 2, where W is a parameter need be learned. exp(W • h T ) represents the potential energy at time T . This is similar to attention mechanism. If the frame at time T has high potential energy, its weight will be high and therefore gain high "attention"; if the potential energy is low, the weight and "attention" will also be low. By this way, the model can learn to assign weights to different time steps from data. If weights at all time steps are same, the weighted pooling is equal to arithmetic mean. W eightedP ooling = tn T =t1 A T × h T (1) A T = exp(W • h T ) tn T =t1 exp(W • h T )(2) In this study, we define that trunk part has two hidden layers. The first layer is fully connected layer which has 256 RELU neurons. The second one is a bidirectional LSTM (BLSTM) layer with 128 neurons. The hidden values go to weighted pooling layer after the LSTM layer. In the branch part, each task has one hidden fully connected layer with 256 RELU neurons and one softmax layer performing classification.
Conventional LSTM tasks take the output from lower layer and previous time step as input and feed value to higher layer. The gating mechanism is used to control information flow by point-wise multiplication (denoted as operation in the following contents). There is a cell to memorize information within the unit. The diagram is shown in Figure 2.  The cell is updated as Equation 3, where f t and i t are the forgetting and inputting gates at time t. C t is new candidate cell values. It is computed as Equation 4, where tanh is the activation function, W C is a set of weights to be learned, b C is the bias, and [h t−1 , x t ] is the concatenation of the values from previous time step (h value) and lower layer (x value). h value at time t is computed by Equation 5, where o t is outputting gate. It can be seen that the states at time t depends on the states at time t − 1, because C t is computed from h t−1 and C t−1 . The computation about controlling gates are omitted for simplification. C t = f t C t−1 + i t C t (3) C t = tanh(W C • [h t−1 , x t ] + b C ) (4) h t = o t tanh(C t )(5) The A-LSTM is different from the conventional one. It releases the assumption that time t state depends on time t − 1 state. It use weighted summation of multiple states at different time steps to compute cell (C value) and hidden value (h value). The diagram is shown in Figure 3. In A-LSTM, Equation 3 is modified to Equation 6, and Equation 4 is modified to Equation 7. C is computed follow-ing Equation 8, where T is the set of selected time steps to be combined. In Figure 3, the selected time steps is t − 2, t − 1 and t for time t + 1. T is therefore denoted as a set of {3,2,1}. In the remaining contents, we follow the same naming convention to show our configuration of A-LSTM. W C T is a scalar number as corresponding weight at a specific time step. It is learned from Equation 9. Candidate value of h at time t is computed following Equation 10. It is same as Equation 5 except the cell value now is updated to C . After h t is obtained, the computation of h is computed following Equation 11 and 12. The equations are similar to C computation. In Equation 9and 12, W is shared, which is the parameter to be learned from data. In this study, C and h are computed every max(T ) steps rather than every step. For example, in the case of Figure 3, they are computed every 3 steps. A-LSTM is able to allow more flexible time dependency modeling capability. It makes the cell to recall far back historic records. Recalling every once in a while will be like the human learning mechanism, which makes learning better. Therefore the cell memory can memorize information better compared with conventional LSTM. C t = f t C t−1 + i t C t (6 ) C t = tanh(W C • [h t−1 , x t ] + b C )(7) C = T W C T × C T (8 ) W C T = exp(W • C T ) T exp(W • C T )(9) h t = o t tanh(C t ) (10) h = T W h T × h T (11) W h T = exp(W • h T ) T exp(W • h T )(12)
We evaluate our proposed A-LSTM on selected utterances from IEMOCAP corpus, which belonged to neutral, happy, angry and sad classes. We run two sets of experiments. In the first one, we compared different types of LSTMs. All the systems were based on weighted pooling RNN framework. In the second one we compared RNN framework with a deep neural network (DNN) framework, which represents current state-of-the-art system on IEMOCAP. Multi-task learning was applied during all the systems. The weights for emotion, speaker and gender classification were 1, 0.3, 0.6 respectively. We randomly selected 1 male and 1 female as testing subjects. The data from other subjects were used as training data. 10% of the training data was used as validation data to check whether we need early stopping. The early stopping criteria was that in continuous 3 epochs, the accuracy on the validation data was lower than the highest accuracy. Macro average F-score (MAF) (also named as unweighted average F-score) macro average precision (MAP) (also named as unweighted average precision) and accuracy were used as performance metrics. The metrics were computed with the open source tool, Scikit-learn . Since the classes were imbalanced, we mainly rely on the MAF for performance evaluation.
We built up two baseline systems for comparison under the RNN framework. The first one used conventional LSTM. The second one used recurrent unit that was similar to the A-LSTM structure except that W Ct and W ht were fixed. They were determined to same values which made the combination equivalent to arithmetic mean of the states at selected time steps. We therefore name it as "mean LSTM". The proposed framework used A-LSTM as recurrent unit. The parameter details of the neural network has been described in Section 4.1. Dropout was used in all the layers in the network except the attention based weighted pooling layer and the parameter of W in Equation 9 and 12. The dropout rate was 0.5. The set of T for A-LSTM used in this experiment was {5, 3, 1}. The time steps were selected every 2 time points. It was observed in pilot experiment that the training would be difficult when too many times steps in T , we therefore fixed to 3 selected time steps. Adam  was used as optimizer. The batch size for all systems was 32. The performance of the two baseline systems and the proposed systems are listed in Table 1. Comparing A-LSTM and conventional LSTM shows that the A-LSTM is able to outperform the conventional LSTM by 5.5% in terms of MAF. Since the weighted pooling layer can see the hidden values from all time steps, this improvement is not from the benefit of seeing more time steps in higher layer. It leveraged the advantage of the flexible time dependency modeling capability of A-LSTM. This is especially useful in emotion recognition, because emotion is usually shown a state within a range of time steps rather than at a time step instantly. In this study, we have 256 neurons in the BLSTM (each direction has 128 neurons), so we only need add 256 parameters, which is the W size, to achieve this improve. This cost can be ignored compared with about 600 k parameters of network. The results also show that there is no improvement when we fixed the weights. Comparing mean LSTM and A-LSTM implies that learnable weights are better. Learning weights as a framework of data-driven assignment allows the model to make the assignment according to different situations. It is better because time dependency may vary at different time steps.
We also built a DNN with multi-task learning for comparison. The network has two parts, shared part and separate part. The former part is shared by all the tasks, which has 2 fully connected layers with 4096 RELU neurons per layer. The later part has 3 separate sub-networks respectively for 3 tasks. Each sub-network has 1 fully connected layers with 2048 RELU neurons. On top of that, there is a softmax layer for classification. The batch size was 32 and dropout rate was 0.5. The optimizer was stochastic gradients descending (SGD). We used IS10 feature set extracted with openSMILE  as input because it was suitable for the three tasks. IS10 was z-normalized based on the mean and variance from training part. We also used the tool of Focal  to fuse the results from these two frameworks. The results of the experiment are shown in Table 2. It is shown that the RNN framework is about 23.2 % worse than DNN framework. There are two reasons here. First, we have very limited data, which is only about 3200 training utterances. This amount may not train RNN framework sufficiently, especially training RNN is more difficult than DNN. Second, all the utterances were well segmented in IEMOCAP. It may not have long silence and pause as the situation in real world. The fusion result shows combining the two frameworks is better than either single one. It indicates that RNN framework can complement the DNN even with few training data. Besides, there are about 58 M parameters in DNN which is about 100 times as the one in RNN which means that RNN will have low hardware requirement when it is employed.
