The attention mechanism is a key component of the neural revolution in Natural Language Processing (NLP).
As the size of attention-based models has been scaling with the available computational resources, a number of pruning techniques have been developed to detect and to exploit sparseness in such models in order to make them more efficient.
The majority of such efforts have focused on looking for attention patterns and then hard-coding them to achieve sparseness, or pruning the weights of the attention mechanisms based on statistical information from the training data.
Here, we marry these two lines of research by proposing Attention Pruning (AP): a novel pruning framework that collects observations about the attention patterns in a fixed dataset and then induces a global sparseness mask for the model.
This can save 90% of the attention computation for language modelling and about 50% for machine translation and for solving GLUE tasks, while maintaining the quality of the results.
Moreover, using our method, we discovered important distinctions between self-and crossattention patterns, which could guide future NLP research in attention-based modelling.
Our framework can in principle speed up any model that uses attention mechanism, thus helping develop better models for existing or for new NLP applications.
Our implementation is available at https:// github.com/irugina/AP.
Given enough computational power, the scalability of the attention mechanism (Bahdanau et al., 2015;Hermann et al., 2015;Vaswani et al., 2017) will allow for building ever larger Natural Language Processing (NLP) models with billions of parameters (Shoeybi et al., 2019;Raffel et al., 2020;Brown et al., 2020).
While impressive, these advances also pose a responsibility to the Natural Language Processing (NLP) community to interpret the behavior of the hundreds of attention heads in a single model, and potentially to reduce the number of computations.
Responding to this challenge, previous work has taken pioneering steps to discover and to explain the sparseness in the attention patters (Vig and Belinkov, 2019;Clark et al., 2019;Kovaleva et al., 2019).
Here, we argue that as the number of heads grows in the range of thousands, automatic measures would be needed to discover and to impose sparseness to such models.
In this paper we introduce a simple taskagnostic data-informed pruning method for attention mechanisms:
Attention Pruning.
We train Transformer-based models and we analyze the global observed attention patterns, averaged over all input sequences in the train set, in order to identify and to remove weak connections between input tokens.
Following Frankle and Carbin (2019), we then retrain these models, enforcing sparseness through masking, and we demonstrate that attention mechanisms incorporate extraneous connections between the input tokens: we obtain comparable performance while using sparse attention patterns for NLP tasks such as language and sequence-tosequence (seq2seq) modelling, as well as prediction on GLUE tasks.
Figure 1 summarizes the impact of using our pruning method on standard NLP tasks.
These global sparseness patterns could help improve both interpretability and inference-time computational efficiency for widely-used attention models.
Our contributions are as follows: •
We introduce a novel method for pruning attention patterns, by pruning only the computational graph of the model and not the weights of the attention mechanism, and we demonstrate theoretical and empirical computational gains.
Our method is data-informed and global, and it can prune the attention without losing much information, while staying close to the original results.
Our pruning method is application-agnostic, which allows us to study the impact of the pruning mechanism on attention models holistically, i.e., by including controlled experiments on language and sequence-tosequence modelling, as well as for prediction on GLUE tasks.
In our seq2seq experiments, we study the impact of attention pruning on three types of attention mechanisms, including self-attention in the encoder, in the decoder, as well as encoder-decoder attention, and we discover important distinctions between these three cases.
The rest of the paper is organized as follows.
In Section 2 we discuss related work.
In Section 3 we present our method of attention pruning.
In Section 4 we present our experiments on language modelling.
In Section 5 we present our results on machine translation.
In Section 6 we discuss empirically the compatibility of our method with alternative techniques.
In Section 7 we present our results on GLUE with BERT.
In Section 8 we conduct useful ablation and out-of-domain studies.
In Section 9 we discuss theoretically and empirically efficient implementation of our method.
In Section 10 we conclude and discuss future work.
There are several fruitful directions for research focused on improving the computational efficiency and the interpretability of the attention mechanism.
Sparseness plays a central role in all of them, as simple attention mechanisms inherently scale quadratically with sequence length and assign non-zero correlation between any two input tokens.
One line of research is that of reducing computational complexity of attention using insights provided by empirical observations of patterns.
introduced two sparse matrix factorizations that reduce the computational com- Kitaev et al. (2020) created a sparse attention mechanism with O(N log N ) computational complexity, which is achieved by using local sensitivity hashing to cluster tokens that should attend to each other and then only computing attention within tokens from the same chunks.
More directly related to our work is that of Beltagy et al. (2020) and Zaheer et al. (2020) who looked directly at sparsifying the attention patterns rather than at the underlying matrix factorization, and reduced the computational complexity of attention from O(N 2 ) to O(N ) using GPU kernels (Gray et al., 2017).
A key difference between these approaches and ours is that we do not impose any a priori restrictions on the type of attention patterns we can generate.
plexity from O(N 2 ) to O(N √ N ).
Another successful approach has been to adapt low-rank matrix approximation methods to simplify attention computations.
Xiong et al. (2021) adapted Nyström's method (Baker, 1979).
leveraged the Johnson-Lindenstrauss lemma to introduce projections and to improve complexity.
In contrast to this line of work our contribution is easier to implement because (i) we do not require architectural modifications and we only change a few lines of code in practice, and (ii) we do not require optimizing the numerical stability of any mathematical methods and we introduce very few and simple hyperparameters.
Correia et al. (2019) and Peters et al. (2019) explored directly incorporating sparseness into Transformer models through the choice of activa-tion functions and introduced the α-entmax functions.
This encompasses both softmax for α = 1 and sparsemax (or projections onto the probability simplex) for α = 2.
For any α > 1 αentmax is sparse.
Peters et al. (2019) provided an efficient implementation and experimented with α = 1.5.
We leverage global attention masks, rather than creating a sparse attention pattern for each key-value pair, and we manage both to provide quantifiable speed guarantees and to achieve higher sparseness in practice.
There has been a lot of research on understanding over-parameterization and on developing methods that make BERT models faster and more lightweight (Ganesh et al., 2020).
Previous work has found that different attention heads encode similar patterns and hence these heads are not always all necessary Michel et al. (2019); Kovaleva et al. (2019); Voita et al. (2019), and thus good performance can be achieved by removing entire attention heads at test time.
pruned entire Transformer layers at a time and again obtained good performance while removing a large percentage of the model's parameters.
Our pruning method takes a more finegrained approach and prunes individual connections rather than entire heads or layers, and thus it could be used in conjunction with the abovementioned methods.
We first review attention mechanisms as used in Transformer-based architectures, and then we present our novel pruning procedure.
Attention mechanisms are used to learn connections (correlations) between two sequences of lengths N and M , respectively.
Transformer models use the scaled dot-product attention introduced in (Vaswani et al., 2017), which takes as input three matrices: a matrix Q ∈ R M ×d k composed of query vectors q ∈ R d k , a matrix K ∈
R N ×d
k composed of key vectors k ∈ R d k , and a third matrix V ∈ R N ×dv , which groups value vectors v ∈ R dv .
The scaled dot-product outputs a transformation of the sequence of length M governed by the values associated with other sequence's tokens, as well as the relative strength of the connection between any two tokens from the two sequences:
A = softmax QK √ d k V
The general sequence-to-sequence Transformer model uses three types of attention layers: two self-attention mechanisms for the encoder and for the decoder, respectively, as well as a third encoder-decoder attention, which we will call cross-attention.
Additionally, each attention layer is a multi-headed attention mechanisms that concatenates several instances of the dot-product attention described above.
Tailor attention to the dataset.
The attention mechanism (Vaswani et al., 2017) creates a matrix with N M connections where N is the length of a source sequence, i.e., its number of tokens, and M is the length of the target sequence.
In practice, in order to compute high-level representations of sources and targets, a portion of these connections is ignored by pre-trained models, which could be observed with visualization tools on pretrained Transformer-based models (Vig and Belinkov, 2019;Clark et al., 2019;Kovaleva et al., 2019).
More specifically, there are entries in the attention matrix whose values are close to zero with high probability, and thus there is no correlation between the corresponding tokens.
We would like to prune such connections, thus reducing the noise coming from such entries.
For each possible tuple (type, l, m), we calculate the corresponding average attention matrix A (type,l,m) from the corresponding attention matrices A (type,l,m) i generated by source-target pairs in the training set as follows: A (type,l,m) = 1 n n i=1 A (type,l,m) i ,(1) where type is either a self-attention encoder, a self-attention decoder or an encoder-decoder, l ∈ {1, . . .
L} is the layer in the Transformer, m ∈ {1, . . .
H} is the index of the head within the Transformer layer, and n is the size of the training set.
The summation is element-wise.
We introduce a single additional hyperparameter p: the percentage of entries we want to prune in each multi-head attention mechanism.
We would prune an entry in the attention matrix if almost all of the source-target pairs in the training dataset are below a threshold τ (type,l) , given by the p percentile of A (type,l,1) , . . .
, (type,l,H) .
A In other words, the global percentage p defines a layer-dependent pruning threshold τ (type,l) , which corresponds to the p percentile of all attention entries within that layer, across all heads.
The pruning mask M p , which depends on p, is computed as follows: (type,l,m)
< τ (type,l) .
1 M (type,l,m) p = (−∞) •
A (2) We use the Iverson bracket notation, where the comparison operation yields a matrix, whose entries are 1 if the comparison is satisfied, and 0 otherwise (using broadcasting for the scalars).
The mask modifies the calculation of the attention matrix as follows: A (t) i,p = softmax Q (t) i K (t) i + M (t) p √
d k V (t) i , where t = (type, l, m), Q with the zero matrix.
Also, note that our pruning mechanism only reduces the computational graph, and does not affect the number of parameters of the model.
(type,l,m)
R N ×d
k , K (type,l,m)
R M ×d k , V (type,l,m)
i ∈ R N ×d , An interpretation of inducing the mask M p onto our computational graph is that we constrain the inductive bias of the attention mechanism by adding the inductive bias of whether a pair of source-target positions correlate, which we learn from the trainig dataset by inspecting the outputs of a pre-trained model.
Observing whether a pair of positions correlate from in the training dataset would allow us to infer a positional inductive bias from the dataset itself that we can incorporate during training.
Therefore, we propose the following algorithm.
Attention Pruning (AP).
Choose a model F with attention matrices in its computational graph and a percentage p for pruning.
Then follow the steps below: 1.
Train the model F , initialized with weights θ, on the training set
D = {(x i , y i )} i=1,...,n via validation on the D valid split to obtain optimized weights θ
and then compute the accuracy (or any other desired evaluation measure) 2.
For each possible tuple (type, l, m), calculate the average A (type,l,m) from the attention matrices  (Frankle and Carbin, 2019).
However, except from sharing Step 1, our AP method diverges significantly from other methods inspired by the Lottery Ticket Hypothesis, such as (Yu et al., 2020), as we study sparseness not in the parameters of the neural network, but in the attention patterns of the model on a fixed dataset.
Such a distinction is important, because while the sparseness of the weight matrices is usually not interpretable, the sparseness of the attention patterns is, as it has been observed in the literature (Kovaleva et al., 2019;Raganato et al.,
2020;Beltagy et al., 2020).
Moreover, here our focus is not on pruning entire heads, but rather on measuring the sparseness in attention patterns in a more fine-grained fashion (Steps 2 and 3), and then on re-purposing sparse heads by retraining (Step 4).
A (type,l,m) i ≡ A (type,l,m)
i (x i , y i ; θ * ) for i = 1, . . .
, Given our method and the above motivation we now ask the following: (1) Can AP reveal a global sparseness for attention patterns?
(2) Can that sparseness be deployed for efficient inference?
(3) Can AP yield an insight into the sparseness properties of different attention types?
We answer positively all these questions in the following experimental sections.
We first test pruning attention matrices on the WikiText-103 (Merity et al., 2017)   Figure 2:
Transformer-XL pruning masks (binary valued) averaged over all layers and attention heads for p ∈ {30%, 90%}.
AP prunes entries in the left half (attention to past sequences) more aggressively than the conventional self-attention entries in the right half.
Note that the right half also has an auto-regressive mask.
architecture (Dai et al., 2019), which adds recurrence to Transformer models by caching selfattention hidden states between text segments.
We can see in Table 1 and Figure 1(a) that we can prune over 80-90% of the attention entries and still maintain good performance on language modelling tasks.
Figure 2 shows Transformer XL's prune masks, averaged over all its layers and heads, when using p = 30% or p = 90%.
We speculate that attention pruning performs so well here because it enables Transformer-XL to pay attention to long sequences only when the distant past is actually relevant.
We further test our method on encoder-decoder Transformer models using various translation tasks and the implementation by Ott et al. (2019).
In all experiments, we use half-precision floating point (FP16).We run two sets of experiments on the WMT17 English-German (en-de) and the IWSLT14 German-English (de-en) machine translation tasks following the default base Transformer architectures from Ott et al. (2019) (transformer_iwslt_de_en and transformer_wmt_en_de, respectively), and we evaluate on the best model, measured by the highest validation-split BLEU score in the case of IWSLT14 de-en, and by the lowest validation loss in the case of WMT17 en-de.
We prune all three types of attentionself-attention-encoder, self-attention-decoder and encoder-decoder-, and we run five experiments per translation task, for p ∈ {20, 40, 50, 60, 80}, for both IWSLT14 de-en and WMT17 en-de.
Table 2 summarizes the pruned models performance for the two translation tasks.
We can see that AP performs significantly worse here than it did for the language modeling task above.
This suggest that a finer-grained analysis on pruning specific attention types, with which we proceed below.
We now look at whether the three types of attention mechanism require specialized pruning.
In order to isolate the effects of AP, we first prune only one of the three types of attention mechanisms.
The results are summarized in Tables 3 and 4.
For both datasets, we clearly see the same trend: the Transformer models are more sensitive to pruning cross-attention patterns than to removing self-attention, which is in line with what was reported in previous work (You et al., 2020).
Figures 3 and 5 represent the average attention patterns observed on the IWSLT14 de-en dataset for cross-attention and the encoder's self-attention mechanism, respectively.
A direct comparison between the two suggests that cross-attention mechanisms are more brittle, which is in part because they exhibit variable context windows.
This makes sense since in this case the queries and the keys are generated from different sequences.
We prune the two types of self-attention patterns (self-attention-encoder and self-attentiondecoder) for p ∈ {20, 40, 50, 60, 80}.
The results are shown in Table 5.
In agreement with 5.1, we find that we can prune large percentage of self-attention connections while maintaining good BLEU scores.
The average attention patterns among all encoder selfattention heads for IWSLT14 are shown in Figure 5.
We can see that they are indeed sharper than in the case of cross-attention, and we show that a fixed window encodes the relevant contextual information for processing an input token.
Moreover, we observe that numerous attention heads encode very similar patterns, in agreement with (Voita et al., 2019).
The results for WMT17 en-de (see Figure 1(b), Table 5) are particularly encouraging.
We can prune over 60% of the self-attention entries, while losing less than 1 BLEU point absolute.
This suggests that AP is particularly robust when used with large datasets, possibly because we use summary statistics.
One disadvantage of our pruning method is that we use a single pruning percentage p for all attention Figure 4: Encoder self-attention pattern created on IWSLT14 de-en using the train dataset by averaging all layers and heads for all data points in train.
The 1.5entmax activation yields sharper attention patterns.
Figure 5: Encoder self-attention pattern created for IWSLT14 de-en on the train dataset by averaging all layers and heads for all data points in train.
We can see that the self-attention patterns maintain a constant context window for all token sequence positions.
patterns within a Transformer layer.
Translation models are sequence-to-sequence models from a source fragment of length M to a target fragment of length N , where M and N are not invariant across a dataset.
Because of this, we either leave extraneous noisy connection between tokens in shorter sequences or we lose important information when modeling longer sequences.
This problem would be ameliorated if we masked attention mechanisms according to patterns sharper than those produced by the softmax activation function.
Therefore, we turn to the αentmax activation, which was used in (Correia Table 6: Experiments on IWSLT14 de-en using 1.5entmax.
We can see that pruning self-attention mechanisms maintains good performance at higher sparseness percentages than those induced by 1.5-entmax alone.
This is in agreement with the trend observed in Section 5.1, where we saw that pruning cross-attention patterns yields a sharp drop in performance.
et al., 2019) and (Peters et al., 2019), and which encourages sparseness in the attention patterns.
We repeated all IWSLT14 de-en experiments from Section 5 using α = 1.5.
Figure 4 shows the average observed attention pattern among the encoder self-attention heads in a model that uses 1.5entmax as its normalizing activation function.
We can see that they are indeed sharper than the analogous ones, which we presented in Figure 5.
Table 6 demonstrates how attention pruning performs in conjunction with 1.5-entmax on the IWSLT14 de-en translation task.
We would like to note that the behavior in 5.1 regarding crossattention's lack of robustness to pruning is even more pronounced now.
In order to analyze the sparseness of BERT on GLUE tasks (Wang et al., 2019), we study the vanilla BERT architecture bert-base-cased, and we apply our AP method on top of it.
We train on the following GLUE tasks: MNLI (Williams et al., 2018), QQP (Chen et al., 2018), STS-B (Cer et al., 2017), SST-2 (Socher et al., 2013), RTE (Bentivogli et al., 2009), MRPC (Dolan and Brockett, 2005), QNLI , CoLA (Warstadt et al., 2019).
For each task, we fine-tune the BERT model on a single GPU for three epochs with a learning rate of 2e-5 and a batch size of 32.
The maximum length of the input sequences is 128.
Since the datasets are small, we run each of our experiments with five different random seeds in order to be able to capture the mean and the standard deviation of the results.
In Table 7 and Figure 1(c), we present the re- sults of our experiments.
Generally, we observe that we can perform attention pruning on BERT heads, while maintaining the performance.
For example, AP with p = 20 loses only 1.00 points absolute on average compared to the baseline p = 0 model.
Another important observation is that, for some GLUE tasks, we can prune even more than 50% of the attention weights while maintaining competitive scores, e.g., for MNLI, QNLI, QQP, SST-2, and SST-B. For the remaining tasks -RTE, MRPC and CoLA-, we maintain reasonable performance by pruning a sizable fraction of the attention weights.
We explore the importance of using attention masks tailored to specific datasets by comparing against two other scenarios: (i) prune random entries in attention patterns, and (ii) prune using an attention mask learned on a different dataset.
We prune self-attention mechanisms in encoder-decoder models trained with softmax on the IWSLT14 translation tasks for p ∈ {20, 40, 50, 60, 80} using either random or out-of-distribution attention masks, and we compare to the baseline results above.
For the out-of-distribution experiments, we generate attention masks on IWSLT14 en-de.
Tables 8 and  9 show the results.
We note that data-informed masks outperform random pruning by a large margin, especially as the percentage p increases.
However, using attention masks gathered for the other translation direction does not meaningfully influence our results.
We speculate that this is because we only look at self-attention patterns, which in the case of translation are very sharp and exhibit a constant context window.
We perform out-of-domain experiments by training GLUE tasks with AP using attention patterns from all GLUE tasks.
In Figure 6, we show the relative difference in accuracy for our experiments on four GLUE datasets.
Note that the columns for SST-2 and CoLA show significantly negative relative accuracy, which means that the attention patterns from these datasets are not useful for the majority of the GLUE tasks.
This is in line with the nature of the datasets: SST-2 and CoLA are two tasks in GLUE that do not involve pairs of sentences.
Particularly detrimental to the performance is using CoLA masks on the STS-B dataset (black entry in Figure 6).
Our experiment in the figure is for p = 40, but we observed a similar pattern for a variety of pruning percentages.
Towards Efficient Hardware Implementation of Attention Pruning
In order to quantify the computational advantage we obtain from pruning, we estimate the number of Multiply-Accumulate Operations, or MACs for short (Randell, 1971), executed by a simple attention mechanism during the forward pass of the model.
Let the input be a tensor x ∈ R B×N ×d , where B is the batch size, N is the number of tokens in a sequence, and d is the number of embedding dimensions.
Let us also suppose that we have h attention heads.
In order to process the input x, we need to follow this sequence of operations: (i) Compute the key, query, and value matrices;   This is particularly helpful for large N d, suggesting that AP might be computationally beneficial in applications with long input sequences such as summarization or question answering.
We demonstrate empirical gains using blocksparse GPU kernels.
Gray et al. (2017) first implemented efficient sparse matrix multiplication operations and applied them to both dense and convolutional layers.
extended this work to attention mechanisms with certain predetermined sparseness patterns.
Tillet et al. (2019)
introduced Triton, a language and compiler used to generate optimized GPU code that allows for higher design flexibility than CUDA.
We use the DeepSpeed 2 implementation of sparse attention, which requires efficient sampled dense-dense and sparse-dense multiplications as well as softmax operations.
Since in order to obtain performance gains we need to use sparseness patterns structured around blocks, as a proof-of-concept, we turn our attention to question answering applications and apply AP to the Stanford Question Answering Dataset (SQuAD)  and use a blocksize of 16.
We choose SQuAD for hardware benchmarking as the sequences are longer (up to 384 tokens) than those in the GLUE benchmark.
We fine-tune BERT on a single GeForce GTX  1080 GPU for two epochs with a learning rate of 3e-5 and a batch size of 4.
The length of the sequences is capped at 384 tokens.
At test time, we use a batch size of 256.
The results in Table 10 suggest that we can empirically reduce both inference time and GPU memory consumption using AP and sparse kernels.
At the same time, we do not compromise performance noticeably: CUDA and Triton kernels for p = 0 yield 81.02 Exact and 88.63 F1 scores, while AP with Triton for p = 90 yields 79.62 Exact and 87.32 F1 scores.
We would like to underscore the 27% reduction in memory, in particular, as memory limitations usually prevent utilization of attention-based models when hardware is constrained.
Thus AP is especially promising in this context.
To the best of our knowledge, we are the first to use sparse attention kernels for unstructured attention pruning.
While, as a proof of concept, we obtained 9.4% speedup and 27% reduction in memory footprint, further work lays ahead in unleashing the full potential of sparse kernels for attention pruning based on data-driven global sparseness.
In particular, the theory from the beginning of this section suggests that tasks with longer sequences would benefit more from our contribution for hardware efficiency.
Therefore, while attention pruning has intrinsic benefit for analysing attention-based models, here we have already demonstrated initial empirical efficiency gains on popular hardware.
We motivated Attention Pruning as a novel method for pruning attention by leveraging on datainformed sparseness.
By performing controlled experiments on a broad range of tasks (from language modelling through machine translation to prediction on GLUE tasks), we demonstrated that we can prune most computations using pre-computed attention patterns while maintaining comparable performance, and sometimes even achieving improvements.
We further applied our AP method on seq2seq tasks, which allowed us to study attention patterns between self-and crossattention, and as a result we discovered important distinctions between these two types of attention.
In future work, we plan to evaluate our method for other models, other NLP tasks, and on datasets of various sizes.
We also plan to implement Attention Pruning more efficiently for existing hardware.
We conjecture that "co-design" approaches for efficient sparse kernels (Gray et al., 2017) and their successful utilization  would be helpful for making Attention Pruning more scalable.
Therefore, we release our code to the research community, to encourage further codesign efforts for attention pruning.
We would like to thank Renbin Liu, Yonatan Belinkov, Peter Lu and Samuel Kim for fruitful conversations.
The authors acknowledge the MIT Su-perCloud and Lincoln Laboratory Supercomputing Center (Reuther et al., 2018)