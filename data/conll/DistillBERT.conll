-DOCSTART- -X- O
Task-specific -X- _ O
distillation -X- _ O
Most -X- _ O
of -X- _ O
the -X- _ O
prior -X- _ O
works -X- _ O
focus -X- _ O
on -X- _ O
building -X- _ O
task-specific -X- _ O
distillation -X- _ O
setups. -X- _ O
Tang -X- _ O
et -X- _ O
al. -X- _ O
transfer -X- _ O
fine-tune -X- _ O
classification -X- _ O
model -X- _ O
BERT -X- _ B-MethodName
to -X- _ O
an -X- _ O
LSTM-based -X- _ B-MethodName
classifier. -X- _ O
Chatterjee -X- _ O
distill -X- _ B-MethodName
BERT -X- _ I-MethodName
model -X- _ O
fine-tuned -X- _ O
on -X- _ O
SQuAD -X- _ B-DatasetName
in -X- _ O
a -X- _ O
smaller -X- _ O
Transformer -X- _ B-MethodName
model -X- _ O
previously -X- _ O
initialized -X- _ O
from -X- _ O
BERT. -X- _ B-MethodName
In -X- _ O
the -X- _ O
present -X- _ O
work, -X- _ O
we -X- _ O
found -X- _ O
it -X- _ O
beneficial -X- _ O
to -X- _ O
use -X- _ O
a -X- _ O
general-purpose -X- _ O
pre-training -X- _ O
distillation -X- _ O
rather -X- _ O
than -X- _ O
a -X- _ O
task-specific -X- _ O
distillation. -X- _ O
Turc -X- _ O
et -X- _ O
al. -X- _ O
use -X- _ O
the -X- _ O
original -X- _ O
pretraining -X- _ O
objective -X- _ O
to -X- _ O
train -X- _ O
smaller -X- _ O
student, -X- _ O
then -X- _ O
fine-tuned -X- _ O
via -X- _ O
distillation. -X- _ B-MethodName
As -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
ablation -X- _ O
study, -X- _ O
we -X- _ O
found -X- _ O
it -X- _ O
beneficial -X- _ O
to -X- _ O
leverage -X- _ O
the -X- _ O
teacher's -X- _ O
knowledge -X- _ O
to -X- _ O
pre-train -X- _ O
with -X- _ O
additional -X- _ O
distillation -X- _ O
signal. -X- _ O
Multi-distillation -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
combine -X- _ O
the -X- _ O
knowledge -X- _ O
of -X- _ O
an -X- _ O
ensemble -X- _ O
of -X- _ O
teachers -X- _ O
using -X- _ O
multi-task -X- _ O
learning -X- _ O
to -X- _ O
regularize -X- _ O
the -X- _ O
distillation. -X- _ O
The -X- _ O
authors -X- _ O
apply -X- _ O
Multi-Task -X- _ O
Knowledge -X- _ O
Distillation -X- _ O
to -X- _ O
learn -X- _ O
a -X- _ O
compact -X- _ O
question -X- _ O
answering -X- _ O
model -X- _ O
from -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
large -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
models. -X- _ O
An -X- _ O
application -X- _ O
of -X- _ O
multi-distillation -X- _ O
is -X- _ O
multi-linguality: -X- _ B-TaskName
Tsai -X- _ O
et -X- _ O
al. -X- _ O
adopts -X- _ O
a -X- _ O
similar -X- _ O
approach -X- _ O
to -X- _ O
us -X- _ O
by -X- _ O
pre-training -X- _ O
a -X- _ O
multilingual -X- _ O
model -X- _ O
from -X- _ O
scratch -X- _ O
solely -X- _ O
through -X- _ O
distillation. -X- _ O
However, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
the -X- _ O
ablation -X- _ O
study, -X- _ O
leveraging -X- _ O
the -X- _ O
teacher's -X- _ O
knowledge -X- _ O
with -X- _ O
initialization -X- _ O
and -X- _ O
additional -X- _ O
losses -X- _ O
leads -X- _ O
to -X- _ O
substantial -X- _ O
gains. -X- _ O
Other -X- _ O
compression -X- _ O
techniques -X- _ O
have -X- _ O
been -X- _ O
studied -X- _ O
to -X- _ O
compress -X- _ O
large -X- _ O
models. -X- _ O
Recent -X- _ O
developments -X- _ O
in -X- _ O
weights -X- _ O
pruning -X- _ O
reveal -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
remove -X- _ O
some -X- _ O
heads -X- _ O
in -X- _ O
the -X- _ O
self-attention -X- _ O
at -X- _ O
test -X- _ O
time -X- _ O
without -X- _ O
significantly -X- _ O
degrading -X- _ O
the -X- _ O
performance -X- _ O
Michel -X- _ O
et -X- _ O
al. -X- _ O
. -X- _ O
Some -X- _ O
layers -X- _ O
can -X- _ O
be -X- _ O
reduced -X- _ O
to -X- _ O
one -X- _ O
head. -X- _ O
A -X- _ O
separate -X- _ O
line -X- _ O
of -X- _ O
study -X- _ O
leverages -X- _ O
quantization -X- _ O
to -X- _ O
derive -X- _ O
smaller -X- _ O
models -X- _ O
(Gupta -X- _ O
et -X- _ O
al. -X- _ O
). -X- _ O
Pruning -X- _ O
and -X- _ O
quantization -X- _ O
are -X- _ O
orthogonal -X- _ O
to -X- _ O
the -X- _ O
present -X- _ O
work. -X- _ O

We -X- _ O
introduced -X- _ O
DistilBERT, -X- _ B-MethodName
a -X- _ O
general-purpose -X- _ O
pre-trained -X- _ O
version -X- _ O
of -X- _ O
BERT, -X- _ B-MethodName
40% -X- _ B-MetricValue
smaller, -X- _ O
60% -X- _ B-MetricValue
faster, -X- _ O
that -X- _ O
retains -X- _ O
97% -X- _ B-MetricValue
of -X- _ O
the -X- _ O
language -X- _ O
understanding -X- _ O
capabilities. -X- _ O
We -X- _ O
showed -X- _ O
that -X- _ O
a -X- _ O
general-purpose -X- _ O
language -X- _ O
model -X- _ O
can -X- _ O
be -X- _ O
successfully -X- _ O
trained -X- _ O
with -X- _ O
distillation -X- _ O
and -X- _ O
analyzed -X- _ O
the -X- _ O
various -X- _ O
components -X- _ O
with -X- _ O
an -X- _ O
ablation -X- _ O
study. -X- _ O
We -X- _ O
further -X- _ O
demonstrated -X- _ O
that -X- _ O
DistilBERT -X- _ B-MethodName
is -X- _ O
a -X- _ O
compelling -X- _ O
option -X- _ O
for -X- _ O
edge -X- _ O
applications. -X- _ O

In -X- _ O
this -X- _ O
section, -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
influence -X- _ O
of -X- _ O
various -X- _ O
components -X- _ O
of -X- _ O
the -X- _ O
triple -X- _ O
loss -X- _ O
and -X- _ O
the -X- _ O
student -X- _ O
initialization -X- _ O
on -X- _ O
the -X- _ O
performances -X- _ O
of -X- _ O
the -X- _ O
distilled -X- _ O
model. -X- _ O
We -X- _ O
report -X- _ O
the -X- _ O
macro-score -X- _ O
on -X- _ O
GLUE. -X- _ B-DatasetName
Table -X- _ O
4 -X- _ O
presents -X- _ O
the -X- _ O
deltas -X- _ O
with -X- _ O
the -X- _ O
full -X- _ O
triple -X- _ O
loss: -X- _ O
removing -X- _ O
the -X- _ O
Masked -X- _ B-MetricName
Language -X- _ I-MetricName
Modeling -X- _ I-MetricName
loss -X- _ I-MetricName
has -X- _ O
little -X- _ O
impact -X- _ O
while -X- _ O
the -X- _ O
two -X- _ O
distillation -X- _ O
losses -X- _ O
account -X- _ O
for -X- _ O
a -X- _ O
large -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
performance. -X- _ O

To -X- _ O
further -X- _ O
investigate -X- _ O
the -X- _ O
speed-up/size -X- _ O
trade-off -X- _ O
of -X- _ O
DistilBERT, -X- _ B-MethodName
we -X- _ O
compare -X- _ O
(in -X- _ O
Table -X- _ O
3) -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
of -X- _ O
each -X- _ O
model -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
inference -X- _ O
time -X- _ O
needed -X- _ O
to -X- _ O
do -X- _ O
a -X- _ O
full -X- _ O
pass -X- _ O
on -X- _ O
the -X- _ O
STS-B -X- _ B-DatasetName
development -X- _ O
set -X- _ O
on -X- _ O
CPU -X- _ O
(Intel -X- _ O
Xeon -X- _ O
E5-2690 -X- _ O
v3 -X- _ O
Haswell -X- _ O
@2.9GHz) -X- _ O
using -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
1. -X- _ B-HyperparameterValue
DistilBERT -X- _ B-MethodName
has -X- _ O
40% -X- _ B-MetricValue
fewer -X- _ O
parameters -X- _ O
than -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
is -X- _ O
60% -X- _ B-MetricValue
faster -X- _ O
than -X- _ O
BERT. -X- _ B-MethodName
On -X- _ O
device -X- _ O
computation -X- _ O
We -X- _ O
studied -X- _ O
whether -X- _ O
DistilBERT -X- _ B-MethodName
could -X- _ O
be -X- _ O
used -X- _ O
for -X- _ O
on-the-edge -X- _ O
applications -X- _ O
by -X- _ O
building -X- _ O
a -X- _ O
mobile -X- _ O
application -X- _ O
for -X- _ O
question -X- _ O
answering. -X- _ O
We -X- _ O
compare -X- _ O
the -X- _ O
average -X- _ O
inference -X- _ O
time -X- _ O
on -X- _ O
a -X- _ O
recent -X- _ O
smartphone -X- _ O
(iPhone -X- _ O
7 -X- _ O
Plus) -X- _ O
against -X- _ O
our -X- _ O
previously -X- _ O
trained -X- _ O
question -X- _ O
answering -X- _ O
model -X- _ O
based -X- _ O
on -X- _ O
BERT-base. -X- _ B-MethodName
Excluding -X- _ O
the -X- _ O
tokenization -X- _ O
step, -X- _ O
DistilBERT -X- _ B-MethodName
is -X- _ O
71% -X- _ B-MetricValue
faster -X- _ O
than -X- _ O
BERT, -X- _ B-MethodName
and -X- _ O
the -X- _ O
whole -X- _ O
model -X- _ O
weighs -X- _ O
207 -X- _ O
MB -X- _ O
(which -X- _ O
could -X- _ O
be -X- _ O
further -X- _ O
reduced -X- _ O
with -X- _ O
quantization). -X- _ O
Our -X- _ O
code -X- _ O
is -X- _ O
available -X- _ O
5 -X- _ O
. -X- _ O

Downstream -X- _ O
tasks -X- _ O
We -X- _ O
further -X- _ O
study -X- _ O
the -X- _ O
performances -X- _ O
of -X- _ O
DistilBERT -X- _ B-MethodName
on -X- _ O
several -X- _ O
downstream -X- _ O
tasks -X- _ O
under -X- _ O
efficient -X- _ O
inference -X- _ O
constraints: -X- _ O
a -X- _ O
classification -X- _ O
task -X- _ O
(IMDb -X- _ O
sentiment -X- _ O
classification -X- _ O
- -X- _ O
Maas -X- _ O
et -X- _ O
al. -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
question -X- _ O
answering -X- _ O
task -X- _ O
(SQuAD -X- _ B-DatasetName
v1.1 -X- _ O
-Rajpurkar -X- _ O
et -X- _ O
al. -X- _ O
). -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
2, -X- _ O
DistilBERT -X- _ B-MethodName
is -X- _ O
only -X- _ O
0.6% -X- _ B-MetricValue
point -X- _ O
behind -X- _ O
BERT -X- _ B-MethodName
in -X- _ O
test -X- _ O
accuracy -X- _ B-MetricName
on -X- _ O
the -X- _ O
IMDb -X- _ B-DatasetName
benchmark -X- _ O
while -X- _ O
being -X- _ O
40% -X- _ B-MetricValue
smaller. -X- _ O
On -X- _ O
SQuAD, -X- _ B-DatasetName
DistilBERT -X- _ B-MethodName
is -X- _ O
within -X- _ O
3.9 -X- _ B-MetricValue
points -X- _ I-MetricValue
of -X- _ O
the -X- _ O
full -X- _ O
BERT. -X- _ B-MethodName
We -X- _ O
also -X- _ O
studied -X- _ O
whether -X- _ O
we -X- _ O
could -X- _ O
add -X- _ O
another -X- _ O
step -X- _ O
of -X- _ O
distillation -X- _ B-MethodName
during -X- _ O
the -X- _ O
adaptation -X- _ O
phase -X- _ O
by -X- _ O
fine-tuning -X- _ O
DistilBERT -X- _ B-MethodName
on -X- _ O
SQuAD -X- _ B-DatasetName
using -X- _ O
a -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
previously -X- _ O
fine-tuned -X- _ O
on -X- _ O
SQuAD -X- _ B-DatasetName
as -X- _ O
a -X- _ O
∅ -X- _ O
-L -X- _ O
cos -X- _ O
-L -X- _ O
mlm -X- _ O
-2.96 -X- _ O
L -X- _ O
ce -X- _ O
-∅ -X- _ O
-L -X- _ O
mlm -X- _ O
-1.46 -X- _ O
L -X- _ O
ce -X- _ O
-L -X- _ O
cos -X- _ O
-∅ -X- _ O
-0. -X- _ O
31 -X- _ O
Triple -X- _ B-MetricName
loss -X- _ I-MetricName
+ -X- _ O
random -X- _ O
weights -X- _ O
initialization -X- _ O
-3.69 -X- _ O
teacher -X- _ O
for -X- _ O
an -X- _ O
additional -X- _ O
term -X- _ O
in -X- _ O
the -X- _ O
loss -X- _ O
(knowledge -X- _ O
distillation). -X- _ O
In -X- _ O
this -X- _ O
setting, -X- _ O
there -X- _ O
are -X- _ O
thus -X- _ O
two -X- _ O
successive -X- _ O
steps -X- _ O
of -X- _ O
distillation, -X- _ B-MethodName
one -X- _ O
during -X- _ O
the -X- _ O
pre-training -X- _ O
phase -X- _ O
and -X- _ O
one -X- _ O
during -X- _ O
the -X- _ O
adaptation -X- _ O
phase. -X- _ O
In -X- _ O
this -X- _ O
case, -X- _ O
we -X- _ O
were -X- _ O
able -X- _ O
to -X- _ O
reach -X- _ O
interesting -X- _ O
performances -X- _ O
given -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
model: -X- _ O
79.8 -X- _ O
F1 -X- _ O
and -X- _ O
70.4 -X- _ O
EM, -X- _ O
i.e. -X- _ O
within -X- _ O
3 -X- _ O
points -X- _ O
of -X- _ O
the -X- _ O
full -X- _ O
model. -X- _ O

General -X- _ O
Language -X- _ O
Understanding -X- _ O
We -X- _ O
assess -X- _ O
the -X- _ O
language -X- _ O
understanding -X- _ O
and -X- _ O
generalization -X- _ O
capabilities -X- _ O
of -X- _ B-MethodName
DistilBERT -X- _ I-MethodName
on -X- _ O
the -X- _ O
General -X- _ B-DatasetName
Language -X- _ I-DatasetName
Understanding -X- _ I-DatasetName
Evaluation -X- _ I-DatasetName
(GLUE) -X- _ I-DatasetName
benchmark -X- _ I-DatasetName
[Wang -X- _ O
et -X- _ O
al., -X- _ O
2018], -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
9 -X- _ O
datasets -X- _ O
for -X- _ O
evaluating -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
systems. -X- _ O
We -X- _ O
report -X- _ O
scores -X- _ O
on -X- _ O
the -X- _ O
development -X- _ O
sets -X- _ O
for -X- _ O
each -X- _ O
task -X- _ O
by -X- _ O
fine-tuning -X- _ O
DistilBERT -X- _ B-MethodName
without -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
ensembling -X- _ O
or -X- _ O
multi-tasking -X- _ O
scheme -X- _ O
for -X- _ O
fine-tuning -X- _ O
(which -X- _ O
are -X- _ O
mostly -X- _ O
orthogonal -X- _ O
to -X- _ O
the -X- _ O
present -X- _ O
work). -X- _ O
We -X- _ O
compare -X- _ O
the -X- _ O
results -X- _ O
to -X- _ O
the -X- _ O
baseline -X- _ O
provided -X- _ O
by -X- _ O
the -X- _ O
authors -X- _ O
of -X- _ O
GLUE: -X- _ B-DatasetName
an -X- _ O
ELMo -X- _ B-MethodName
(Peters -X- _ O
et -X- _ O
al. -X- _ O
) -X- _ O
encoder -X- _ O
followed -X- _ O
by -X- _ O
two -X- _ O
BiLSTMs. -X- _ B-MethodName
4 -X- _ O
The -X- _ O
results -X- _ O
on -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
9 -X- _ O
tasks -X- _ O
are -X- _ O
showed -X- _ O
on -X- _ O
Table -X- _ O
1 -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
macro-score -X- _ O
(average -X- _ O
of -X- _ O
individual -X- _ O
scores). -X- _ O
Among -X- _ O
the -X- _ O
9 -X- _ O
tasks, -X- _ O
DistilBERT -X- _ B-MethodName
is -X- _ O
always -X- _ O
on -X- _ O
par -X- _ O
or -X- _ O
improving -X- _ O
over -X- _ O
the -X- _ O
ELMo -X- _ B-MethodName
baseline -X- _ O
(up -X- _ O
to -X- _ O
19 -X- _ B-MetricValue
points -X- _ I-MetricValue
of -X- _ O
accuracy -X- _ B-MetricName
on -X- _ O
STS-B). -X- _ B-DatasetName
DistilBERT -X- _ B-MethodName
also -X- _ O
compares -X- _ O
surprisingly -X- _ O
well -X- _ O
to -X- _ O
BERT, -X- _ B-MethodName
retaining -X- _ O
97% -X- _ B-MetricValue
of -X- _ O
the -X- _ O
performance -X- _ B-MetricName
with -X- _ O
40% -X- _ B-MetricValue
fewer -X- _ O
parameters. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
previously -X- _ O
described -X- _ O
optimization -X- _ O
and -X- _ O
architectural -X- _ O
choices, -X- _ O
an -X- _ O
important -X- _ O
element -X- _ O
in -X- _ O
our -X- _ O
training -X- _ O
procedure -X- _ O
is -X- _ O
to -X- _ O
find -X- _ O
the -X- _ O
right -X- _ O
initialization -X- _ O
for -X- _ O
the -X- _ O
sub-network -X- _ O
to -X- _ O
converge. -X- _ O
Taking -X- _ O
advantage -X- _ O
of -X- _ O
the -X- _ O
common -X- _ O
dimensionality -X- _ O
between -X- _ O
teacher -X- _ O
and -X- _ O
student -X- _ O
networks, -X- _ O
we -X- _ O
initialize -X- _ O
the -X- _ O
student -X- _ O
from -X- _ O
the -X- _ O
teacher -X- _ O
by -X- _ O
taking -X- _ O
one -X- _ O
layer -X- _ O
out -X- _ O
of -X- _ O
two. -X- _ O

Knowledge -X- _ B-MethodName
distillation -X- _ I-MethodName
[Bucila -X- _ O
et -X- _ O
al., -X- _ O
2006, -X- _ O
Hinton -X- _ O
et -X- _ O
al., -X- _ O
2015] -X- _ O
is -X- _ O
a -X- _ O
compression -X- _ O
technique -X- _ O
in -X- _ O
which -X- _ O
a -X- _ O
compact -X- _ O
model -X- _ O
-the -X- _ O
student -X- _ O
-is -X- _ O
trained -X- _ O
to -X- _ O
reproduce -X- _ O
the -X- _ O
behaviour -X- _ O
of -X- _ O
a -X- _ O
larger -X- _ O
model -X- _ O
-the -X- _ O
teacheror -X- _ O
an -X- _ O
ensemble -X- _ O
of -X- _ O
models. -X- _ O
In -X- _ O
supervised -X- _ O
learning, -X- _ O
a -X- _ O
classification -X- _ O
model -X- _ O
is -X- _ O
generally -X- _ O
trained -X- _ O
to -X- _ O
predict -X- _ O
an -X- _ O
instance -X- _ O
class -X- _ O
by -X- _ O
maximizing -X- _ O
the -X- _ O
estimated -X- _ O
probability -X- _ O
of -X- _ O
gold -X- _ O
labels. -X- _ O
A -X- _ O
standard -X- _ O
training -X- _ O
objective -X- _ O
thus -X- _ O
involves -X- _ O
minimizing -X- _ O
the -X- _ O
cross-entropy -X- _ O
between -X- _ O
the -X- _ O
model's -X- _ O
predicted -X- _ O
distribution -X- _ O
and -X- _ O
the -X- _ O
one-hot -X- _ O
empirical -X- _ O
distribution -X- _ O
of -X- _ O
training -X- _ O
labels. -X- _ O
A -X- _ O
model -X- _ O
performing -X- _ O
well -X- _ O
on -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
will -X- _ O
predict -X- _ O
an -X- _ O
output -X- _ O
distribution -X- _ O
with -X- _ O
high -X- _ O
probability -X- _ O
on -X- _ O
the -X- _ O
correct -X- _ O
class -X- _ O
and -X- _ O
with -X- _ O
near-zero -X- _ O
probabilities -X- _ O
on -X- _ O
other -X- _ O
classes. -X- _ O
But -X- _ O
some -X- _ O
of -X- _ O
these -X- _ O
"near-zero" -X- _ O
probabilities -X- _ O
are -X- _ O
larger -X- _ O
than -X- _ O
others -X- _ O
and -X- _ O
reflect, -X- _ O
in -X- _ O
part, -X- _ O
the -X- _ O
generalization -X- _ O
capabilities -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
how -X- _ O
well -X- _ O
it -X- _ O
will -X- _ O
perform -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
3 -X- _ O
. -X- _ O
Training -X- _ O
loss -X- _ O
The -X- _ O
student -X- _ O
is -X- _ O
trained -X- _ O
with -X- _ O
a -X- _ O
distillation -X- _ B-MetricName
loss -X- _ I-MetricName
over -X- _ O
the -X- _ O
soft -X- _ O
target -X- _ O
probabilities -X- _ O
of -X- _ O
the -X- _ O
teacher: -X- _ O
L -X- _ O
ce -X- _ O
= -X- _ O
i -X- _ O
t -X- _ O
i -X- _ O
* -X- _ O
log(s -X- _ O
i -X- _ O
) -X- _ O
where -X- _ O
t -X- _ O
i -X- _ O
(resp. -X- _ O
s -X- _ O
i -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
probability -X- _ O
estimated -X- _ O
by -X- _ O
the -X- _ O
teacher -X- _ O
(resp. -X- _ O
the -X- _ O
student). -X- _ O
This -X- _ O
objective -X- _ O
results -X- _ O
in -X- _ O
a -X- _ O
rich -X- _ O
training -X- _ O
signal -X- _ O
by -X- _ O
leveraging -X- _ O
the -X- _ O
full -X- _ O
teacher -X- _ O
distribution. -X- _ O
Following -X- _ O
Hinton -X- _ O
et -X- _ O
al. -X- _ O
we -X- _ O
used -X- _ O
a -X- _ O
softmax-temperature: -X- _ B-HyperparameterName
p -X- _ O
i -X- _ O
= -X- _ O
exp(zi/T -X- _ O
) -X- _ O
j -X- _ O
exp(zj -X- _ O
/T -X- _ O
) -X- _ O
where -X- _ O
T -X- _ B-HyperparameterName
controls -X- _ O
the -X- _ O
smoothness -X- _ O
of -X- _ O
the -X- _ O
output -X- _ O
distribution -X- _ O
and -X- _ O
z -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
model -X- _ O
score -X- _ O
for -X- _ O
the -X- _ O
class -X- _ O
i. -X- _ O
The -X- _ O
same -X- _ O
temperature -X- _ O
T -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
student -X- _ O
and -X- _ O
the -X- _ O
teacher -X- _ O
at -X- _ O
training -X- _ O
time, -X- _ O
while -X- _ O
at -X- _ O
inference, -X- _ O
T -X- _ B-HyperparameterName
is -X- _ O
set -X- _ O
to -X- _ O
1 -X- _ B-HyperparameterValue
to -X- _ O
recover -X- _ O
a -X- _ O
standard -X- _ O
softmax. -X- _ O
The -X- _ O
final -X- _ O
training -X- _ O
objective -X- _ O
is -X- _ O
a -X- _ O
linear -X- _ O
combination -X- _ O
of -X- _ O
the -X- _ O
distillation -X- _ B-MetricName
loss -X- _ I-MetricName
L -X- _ I-MetricName
ce -X- _ O
with -X- _ O
the -X- _ O
supervised -X- _ O
training -X- _ O
loss, -X- _ O
in -X- _ O
our -X- _ O
case -X- _ O
the -X- _ O
masked -X- _ B-MetricName
language -X- _ I-MetricName
modeling -X- _ I-MetricName
loss -X- _ I-MetricName
L -X- _ O
mlm -X- _ O
[Devlin -X- _ O
et -X- _ O
al., -X- _ O
2018]. -X- _ O
We -X- _ O
found -X- _ O
it -X- _ O
beneficial -X- _ O
to -X- _ O
add -X- _ O
a -X- _ O
cosine -X- _ B-MetricName
embedding -X- _ I-MetricName
loss -X- _ I-MetricName
(L -X- _ I-MetricName
cos -X- _ I-MetricName
) -X- _ I-MetricName
which -X- _ O
will -X- _ O
tend -X- _ O
to -X- _ O
align -X- _ O
the -X- _ O
directions -X- _ O
of -X- _ O
the -X- _ O
student -X- _ O
and -X- _ O
teacher -X- _ O
hidden -X- _ O
states -X- _ O
vectors. -X- _ O
3 -X- _ O
DistilBERT: -X- _ B-MethodName
a -X- _ O
distilled -X- _ O
version -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
Student -X- _ O
architecture -X- _ O
In -X- _ O
the -X- _ O
present -X- _ O
work, -X- _ O
the -X- _ O
student -X- _ O
-DistilBERT -X- _ B-MethodName
-has -X- _ O
the -X- _ O
same -X- _ O
general -X- _ O
architecture -X- _ O
as -X- _ O
BERT. -X- _ B-MethodName
The -X- _ O
token-type -X- _ O
embeddings -X- _ O
and -X- _ O
the -X- _ O
pooler -X- _ O
are -X- _ O
removed -X- _ O
while -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
layers -X- _ O
is -X- _ O
reduced -X- _ O
by -X- _ O
a -X- _ O
factor -X- _ O
of -X- _ O
2. -X- _ O
Most -X- _ O
of -X- _ O
the -X- _ O
operations -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
architecture -X- _ O
(linear -X- _ O
layer -X- _ O
and -X- _ O
layer -X- _ O
normalisation) -X- _ O
are -X- _ O
highly -X- _ O
optimized -X- _ O
in -X- _ O
modern -X- _ O
linear -X- _ O
algebra -X- _ O
frameworks -X- _ O
and -X- _ O
our -X- _ O
investigations -X- _ O
showed -X- _ O
that -X- _ O
variations -X- _ O
on -X- _ O
the -X- _ O
last -X- _ O
dimension -X- _ O
of -X- _ O
the -X- _ O
tensor -X- _ O
(hidden -X- _ O
size -X- _ O
dimension) -X- _ O
have -X- _ O
a -X- _ O
smaller -X- _ O
impact -X- _ O
on -X- _ O
computation -X- _ O
efficiency -X- _ O
(for -X- _ O
a -X- _ O
fixed -X- _ O
parameters -X- _ O
budget) -X- _ O
than -X- _ O
variations -X- _ O
on -X- _ O
other -X- _ O
factors -X- _ O
like -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
layers. -X- _ O
Thus -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
reducing -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
layers. -X- _ O

The -X- _ O
last -X- _ O
two -X- _ O
years -X- _ O
have -X- _ O
seen -X- _ O
the -X- _ O
rise -X- _ O
of -X- _ O
Transfer -X- _ B-MethodName
Learning -X- _ I-MethodName
approaches -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
(NLP) -X- _ O
with -X- _ O
large-scale -X- _ O
pre-trained -X- _ O
language -X- _ O
models -X- _ O
becoming -X- _ O
a -X- _ O
basic -X- _ O
tool -X- _ O
in -X- _ O
many -X- _ O
NLP -X- _ O
tasks -X- _ O
[Devlin -X- _ O
et -X- _ O
al., -X- _ O
2018, -X- _ O
Radford -X- _ O
et -X- _ O
al., -X- _ O
2019. -X- _ O
While -X- _ O
these -X- _ O
models -X- _ O
lead -X- _ O
to -X- _ O
significant -X- _ O
improvement, -X- _ O
they -X- _ O
often -X- _ O
have -X- _ O
several -X- _ O
hundred -X- _ O
million -X- _ O
parameters -X- _ O
and -X- _ O
current -X- _ O
research -X- _ O
1 -X- _ O
on -X- _ O
pre-trained -X- _ O
models -X- _ O
indicates -X- _ O
that -X- _ O
training -X- _ O
even -X- _ O
larger -X- _ O
models -X- _ O
still -X- _ O
leads -X- _ O
to -X- _ O
better -X- _ O
performances -X- _ O
on -X- _ O
downstream -X- _ O
tasks. -X- _ O
The -X- _ O
trend -X- _ O
toward -X- _ O
bigger -X- _ O
models -X- _ O
raises -X- _ O
several -X- _ O
concerns. -X- _ O
First -X- _ O
is -X- _ O
the -X- _ O
environmental -X- _ O
cost -X- _ O
of -X- _ O
exponentially -X- _ O
scaling -X- _ O
these -X- _ O
models' -X- _ O
computational -X- _ O
requirements -X- _ O
as -X- _ O
mentioned -X- _ O
in -X- _ O
Schwartz -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
Strubell -X- _ O
et -X- _ O
al. -X- _ O
. -X- _ O
Second, -X- _ O
while -X- _ O
operating -X- _ O
these -X- _ O
models -X- _ O
on-device -X- _ O
in -X- _ O
real-time -X- _ O
has -X- _ O
the -X- _ O
potential -X- _ O
to -X- _ O
enable -X- _ O
novel -X- _ O
and -X- _ O
interesting -X- _ O
language -X- _ O
processing -X- _ O
applications, -X- _ O
the -X- _ O
growing -X- _ O
computational -X- _ O
and -X- _ O
memory -X- _ O
requirements -X- _ O
of -X- _ O
these -X- _ O
models -X- _ O
may -X- _ O
hamper -X- _ O
wide -X- _ O
adoption. -X- _ O
In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
reach -X- _ O
similar -X- _ O
performances -X- _ O
on -X- _ O
many -X- _ O
downstream-tasks -X- _ O
using -X- _ O
much -X- _ O
smaller -X- _ O
language -X- _ O
models -X- _ O
pre-trained -X- _ O
with -X- _ O
knowledge -X- _ O
distillation, -X- _ B-MethodName
resulting -X- _ O
in -X- _ O
models -X- _ O
that -X- _ O
are -X- _ O
lighter -X- _ O
and -X- _ O
faster -X- _ O
at -X- _ O
inference -X- _ O
time, -X- _ O
while -X- _ O
also -X- _ O
requiring -X- _ O
a -X- _ O
smaller -X- _ O
computational -X- _ O
training -X- _ O
budget. -X- _ O
Our -X- _ O
general-purpose -X- _ O
pre-trained -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
fine-tuned -X- _ O
with -X- _ O
good -X- _ O
performances -X- _ O
on -X- _ O
several -X- _ O
downstream -X- _ O
tasks, -X- _ O
keeping -X- _ O
the -X- _ O
flexibility -X- _ O
of -X- _ O
larger -X- _ O
models. -X- _ O
We -X- _ O
also -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
compressed -X- _ O
models -X- _ O
are -X- _ O
small -X- _ O
enough -X- _ O
to -X- _ O
run -X- _ O
on -X- _ O
the -X- _ O
edge, -X- _ O
e.g. -X- _ O
on -X- _ O
mobile -X- _ O
devices. -X- _ O
Using -X- _ O
a -X- _ O
triple -X- _ O
loss, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
a -X- _ O
40% -X- _ B-MetricValue
smaller -X- _ O
Transformer -X- _ O
(Vaswani -X- _ O
et -X- _ O
al. -X- _ O
) -X- _ O
pre-trained -X- _ O
through -X- _ O
distillation -X- _ B-MethodName
via -X- _ O
the -X- _ O
supervision -X- _ O
of -X- _ O
a -X- _ O
bigger -X- _ O
Transformer -X- _ B-MethodName
language -X- _ O
model -X- _ O
can -X- _ O
achieve -X- _ O
similar -X- _ O
performance -X- _ O
on -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
downstream -X- _ O
tasks, -X- _ O
while -X- _ O
being -X- _ O
60% -X- _ O
faster -X- _ O
at -X- _ O
inference -X- _ O
time. -X- _ O
Further -X- _ O
ablation -X- _ O
studies -X- _ O
indicate -X- _ O
that -X- _ O
all -X- _ O
the -X- _ O
components -X- _ O
of -X- _ O
the -X- _ O
triple -X- _ O
loss -X- _ O
are -X- _ O
important -X- _ O
for -X- _ O
best -X- _ O
performances. -X- _ O
We -X- _ O
have -X- _ O
made -X- _ O
the -X- _ O
trained -X- _ O
weights -X- _ O
available -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
training -X- _ O
code -X- _ O
in -X- _ O
the -X- _ O
Transformers -X- _ O
2 -X- _ O
library -X- _ O
from -X- _ O
HuggingFace -X- _ O
[Wolf -X- _ O
et -X- _ O
al., -X- _ O
2019]. -X- _ O

As -X- _ O
Transfer -X- _ B-MethodName
Learning -X- _ I-MethodName
from -X- _ O
large-scale -X- _ O
pre-trained -X- _ O
models -X- _ O
becomes -X- _ O
more -X- _ O
prevalent -X- _ O
in -X- _ O
Natural -X- _ O
Language -X- _ O
Processing -X- _ O
(NLP), -X- _ O
operating -X- _ O
these -X- _ O
large -X- _ O
models -X- _ O
in -X- _ O
on-theedge -X- _ O
and/or -X- _ O
under -X- _ O
constrained -X- _ O
computational -X- _ O
training -X- _ O
or -X- _ O
inference -X- _ O
budgets -X- _ O
remains -X- _ O
challenging. -X- _ O
In -X- _ O
this -X- _ O
work, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
method -X- _ O
to -X- _ O
pre-train -X- _ O
a -X- _ O
smaller -X- _ O
generalpurpose -X- _ O
language -X- _ O
representation -X- _ O
model, -X- _ O
called -X- _ O
DistilBERT, -X- _ B-MethodName
which -X- _ O
can -X- _ O
then -X- _ O
be -X- _ O
finetuned -X- _ O
with -X- _ O
good -X- _ O
performances -X- _ O
on -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
tasks -X- _ O
like -X- _ O
its -X- _ O
larger -X- _ O
counterparts. -X- _ O
While -X- _ O
most -X- _ O
prior -X- _ O
work -X- _ O
investigated -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
distillation -X- _ B-MethodName
for -X- _ O
building -X- _ O
task-specific -X- _ O
models, -X- _ O
we -X- _ O
leverage -X- _ O
knowledge -X- _ O
distillation -X- _ B-MethodName
during -X- _ O
the -X- _ O
pre-training -X- _ O
phase -X- _ O
and -X- _ O
show -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
a -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
by -X- _ O
40%, -X- _ B-MetricValue
while -X- _ O
retaining -X- _ O
97% -X- _ B-MetricValue
of -X- _ O
its -X- _ O
language -X- _ O
understanding -X- _ O
capabilities -X- _ O
and -X- _ O
being -X- _ O
60% -X- _ B-MetricValue
faster. -X- _ O
To -X- _ O
leverage -X- _ O
the -X- _ O
inductive -X- _ O
biases -X- _ O
learned -X- _ O
by -X- _ O
larger -X- _ O
models -X- _ O
during -X- _ O
pre-training, -X- _ O
we -X- _ O
introduce -X- _ O
a -X- _ O
triple -X- _ O
loss -X- _ O
combining -X- _ O
language -X- _ O
modeling, -X- _ O
distillation -X- _ O
and -X- _ O
cosine-distance -X- _ O
losses. -X- _ O
Our -X- _ O
smaller, -X- _ O
faster -X- _ O
and -X- _ O
lighter -X- _ O
model -X- _ O
is -X- _ O
cheaper -X- _ O
to -X- _ O
pre-train -X- _ O
and -X- _ O
we -X- _ O
demonstrate -X- _ O
its -X- _ O
capabilities -X- _ O
for -X- _ O
on-device -X- _ O
computations -X- _ O
in -X- _ O
a -X- _ O
proof-of-concept -X- _ O
experiment -X- _ O
and -X- _ O
a -X- _ O
comparative -X- _ O
on-device -X- _ O
study. -X- _ O

