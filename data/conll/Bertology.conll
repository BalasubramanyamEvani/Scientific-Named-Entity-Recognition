-DOCSTART- -X- O
We -X- _ O
thank -X- _ O
the -X- _ O
anonymous -X- _ O
reviewers -X- _ O
for -X- _ O
their -X- _ O
valuable -X- _ O
feedback. -X- _ O
This -X- _ O
work -X- _ O
is -X- _ O
funded -X- _ O
in -X- _ O
part -X- _ O
by -X- _ O
the -X- _ O
NSF -X- _ O
award -X- _ O
number -X- _ O
IIS-1844740 -X- _ O
to -X- _ O
Anna -X- _ O
Rumshisky. -X- _ O

In -X- _ O
a -X- _ O
little -X- _ O
over -X- _ O
a -X- _ O
year, -X- _ O
BERT -X- _ B-MethodName
has -X- _ O
become -X- _ O
a -X- _ O
ubiquitous -X- _ O
baseline -X- _ O
in -X- _ O
NLP -X- _ O
experiments -X- _ O
and -X- _ O
inspired -X- _ O
numerous -X- _ O
studies -X- _ O
analyzing -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
proposing -X- _ O
various -X- _ O
improvements. -X- _ O
The -X- _ O
stream -X- _ O
of -X- _ O
papers -X- _ O
seems -X- _ O
to -X- _ O
be -X- _ O
accelerating -X- _ O
rather -X- _ O
than -X- _ O
slowing -X- _ O
down, -X- _ O
and -X- _ O
we -X- _ O
hope -X- _ O
that -X- _ O
this -X- _ O
survey -X- _ O
helps -X- _ O
the -X- _ O
community -X- _ O
to -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
biggest -X- _ O
unresolved -X- _ O
questions. -X- _ O

BERTology -X- _ O
has -X- _ O
clearly -X- _ O
come -X- _ O
a -X- _ O
long -X- _ O
way, -X- _ O
but -X- _ O
it -X- _ O
is -X- _ O
fair -X- _ O
to -X- _ O
say -X- _ O
we -X- _ O
still -X- _ O
have -X- _ O
more -X- _ O
questions -X- _ O
than -X- _ O
answers -X- _ O
about -X- _ O
how -X- _ O
BERT -X- _ B-MethodName
works. -X- _ O
In -X- _ O
this -X- _ O
section, -X- _ O
we -X- _ O
list -X- _ O
what -X- _ O
we -X- _ O
believe -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
most -X- _ O
promising -X- _ O
directions -X- _ O
for -X- _ O
further -X- _ O
research. -X- _ O
Benchmarks -X- _ O
that -X- _ O
require -X- _ O
verbal -X- _ O
reasoning. -X- _ O
While -X- _ O
BERT -X- _ B-MethodName
enabled -X- _ O
breakthroughs -X- _ O
on -X- _ O
many -X- _ O
NLP -X- _ O
benchmarks, -X- _ O
a -X- _ O
growing -X- _ O
list -X- _ O
of -X- _ O
analysis -X- _ O
papers -X- _ O
are -X- _ O
showing -X- _ O
that -X- _ O
its -X- _ O
language -X- _ O
skills -X- _ O
are -X- _ O
not -X- _ O
as -X- _ O
impressive -X- _ O
as -X- _ O
it -X- _ O
seems. -X- _ O
In -X- _ O
particular, -X- _ O
it -X- _ O
was -X- _ O
shown -X- _ O
to -X- _ O
rely -X- _ O
on -X- _ O
shallow -X- _ O
heuristics -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
inference -X- _ O
Zellers -X- _ O
et -X- _ O
al., -X- _ O
2019;Jin -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
reading -X- _ O
comprehension -X- _ O
Sugawara -X- _ O
et -X- _ O
al., -X- _ O
2020;Si -X- _ O
et -X- _ O
al., -X- _ O
2019b;, -X- _ O
argument -X- _ O
reasoning -X- _ O
comprehension -X- _ O
(Niven -X- _ O
and -X- _ O
Kao, -X- _ O
2019), -X- _ O
and -X- _ O
text -X- _ O
classification -X- _ O
(Jin -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O
Such -X- _ O
heuristics -X- _ O
can -X- _ O
even -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
reconstruct -X- _ O
a -X- _ O
non-publiclyavailable -X- _ O
model -X- _ O
(Krishna -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O
As -X- _ O
with -X- _ O
any -X- _ O
optimization -X- _ O
method, -X- _ O
if -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
shortcut -X- _ O
in -X- _ O
the -X- _ O
data, -X- _ O
we -X- _ O
have -X- _ O
no -X- _ O
reason -X- _ O
to -X- _ O
expect -X- _ O
BERT -X- _ B-MethodName
to -X- _ O
not -X- _ O
learn -X- _ O
it. -X- _ O
But -X- _ O
harder -X- _ O
datasets -X- _ O
that -X- _ O
cannot -X- _ O
be -X- _ O
resolved -X- _ O
with -X- _ O
shallow -X- _ O
heuristics -X- _ O
are -X- _ O
unlikely -X- _ O
to -X- _ O
emerge -X- _ O
if -X- _ O
their -X- _ O
development -X- _ O
is -X- _ O
not -X- _ O
as -X- _ O
valued -X- _ O
as -X- _ O
modeling -X- _ O
work. -X- _ O
Benchmarks -X- _ O
for -X- _ O
the -X- _ O
full -X- _ O
range -X- _ O
of -X- _ O
linguistic -X- _ O
competence. -X- _ O
While -X- _ O
the -X- _ O
language -X- _ O
models -X- _ O
seem -X- _ O
to -X- _ O
acquire -X- _ O
a -X- _ O
great -X- _ O
deal -X- _ O
of -X- _ O
knowledge -X- _ O
about -X- _ O
language, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
currently -X- _ O
have -X- _ O
comprehensive -X- _ O
stress -X- _ O
tests -X- _ O
for -X- _ O
different -X- _ O
aspects -X- _ O
of -X- _ O
linguistic -X- _ O
knowledge. -X- _ O
A -X- _ O
step -X- _ O
in -X- _ O
this -X- _ O
direction -X- _ O
is -X- _ O
the -X- _ O
"Checklist" -X- _ O
behavioral -X- _ O
testing -X- _ O
(Ribeiro -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
the -X- _ O
best -X- _ O
paper -X- _ O
at -X- _ O
ACL -X- _ O
2020. -X- _ O
Ideally, -X- _ O
such -X- _ O
tests -X- _ O
would -X- _ O
measure -X- _ O
not -X- _ O
only -X- _ O
errors, -X- _ O
but -X- _ O
also -X- _ O
sensitivity -X- _ O
(Ettinger, -X- _ O
2019). -X- _ O
Developing -X- _ O
methods -X- _ O
to -X- _ O
"teach" -X- _ O
reasoning. -X- _ O
While -X- _ O
large -X- _ O
pre-trained -X- _ O
models -X- _ O
have -X- _ O
a -X- _ O
lot -X- _ O
of -X- _ O
knowledge, -X- _ O
they -X- _ O
often -X- _ O
fail -X- _ O
if -X- _ O
any -X- _ O
reasoning -X- _ O
needs -X- _ O
to -X- _ O
be -X- _ O
performed -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
facts -X- _ O
they -X- _ O
possess -X- _ O
(Talmor -X- _ O
et -X- _ O
al., -X- _ O
2019, -X- _ O
see -X- _ O
also -X- _ O
subsection -X- _ O
3.3). -X- _ O
For -X- _ O
instance, -X- _ O
Richardson -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
propose -X- _ O
a -X- _ O
method -X- _ O
to -X- _ O
"teach" -X- _ O
BERT -X- _ B-MethodName
quantification, -X- _ O
conditionals, -X- _ O
comparatives, -X- _ O
and -X- _ O
boolean -X- _ O
coordination. -X- _ O
Learning -X- _ O
what -X- _ O
happens -X- _ O
at -X- _ O
inference -X- _ O
time. -X- _ O
Most -X- _ O
BERT -X- _ B-MethodName
analysis -X- _ O
papers -X- _ O
focus -X- _ O
on -X- _ O
different -X- _ O
probes -X- _ O
of -X- _ O
the -X- _ O
model, -X- _ O
with -X- _ O
the -X- _ O
goal -X- _ O
to -X- _ O
find -X- _ O
what -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
"knows". -X- _ O
However, -X- _ O
probing -X- _ O
studies -X- _ O
have -X- _ O
limitations -X- _ O
(subsection -X- _ O
3.4), -X- _ O
and -X- _ O
to -X- _ O
this -X- _ O
point, -X- _ O
far -X- _ O
fewer -X- _ O
papers -X- _ O
have -X- _ O
focused -X- _ O
on -X- _ O
discovering -X- _ O
what -X- _ O
knowledge -X- _ O
actually -X- _ O
gets -X- _ O
used. -X- _ O
Several -X- _ O
promising -X- _ O
directions -X- _ O
are -X- _ O
the -X- _ O
"amnesic -X- _ O
probing" -X- _ O
(Elazar -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
identifying -X- _ O
features -X- _ O
important -X- _ O
for -X- _ O
prediction -X- _ O
for -X- _ O
a -X- _ O
given -X- _ O
task -X- _ O
(Arkhangelskaia -X- _ O
and -X- _ O
Dutta, -X- _ O
2019), -X- _ O
and -X- _ O
pruning -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
remove -X- _ O
the -X- _ O
nonimportant -X- _ O
components -X- _ O
(Voita -X- _ O
et -X- _ O
al., -X- _ O
2019b;Michel -X- _ O
et -X- _ O
al., -X- _ O
2019;Prasanna -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O

There -X- _ O
is -X- _ O
a -X- _ O
nascent -X- _ O
discussion -X- _ O
around -X- _ O
pruning -X- _ O
as -X- _ O
a -X- _ O
model -X- _ O
analysis -X- _ O
technique. -X- _ O
The -X- _ O
basic -X- _ O
idea -X- _ O
is -X- _ O
that -X- _ O
a -X- _ O
compressed -X- _ O
model -X- _ O
a -X- _ O
priori -X- _ O
consists -X- _ O
of -X- _ O
elements -X- _ O
that -X- _ O
are -X- _ O
useful -X- _ O
for -X- _ O
prediction; -X- _ O
therefore -X- _ O
by -X- _ O
finding -X- _ O
out -X- _ O
what -X- _ O
they -X- _ O
do -X- _ O
we -X- _ O
may -X- _ O
find -X- _ O
out -X- _ O
what -X- _ O
the -X- _ O
whole -X- _ O
network -X- _ O
does. -X- _ O
For -X- _ O
instance, -X- _ O
BERT -X- _ B-MethodName
has -X- _ O
heads -X- _ O
that -X- _ O
seem -X- _ O
to -X- _ O
encode -X- _ O
frame-semantic -X- _ O
relations, -X- _ O
but -X- _ O
disabling -X- _ O
them -X- _ O
might -X- _ O
not -X- _ O
hurt -X- _ O
downstream -X- _ O
task -X- _ O
performance -X- _ O
Kovaleva -X- _ O
et -X- _ O
al. -X- _ O
(2019); -X- _ O
this -X- _ O
suggests -X- _ O
that -X- _ O
this -X- _ O
knowledge -X- _ O
is -X- _ O
not -X- _ O
actually -X- _ O
used. -X- _ O
For -X- _ O
the -X- _ O
base -X- _ O
Transformer, -X- _ O
Voita -X- _ O
et -X- _ O
al. -X- _ O
(2019b) -X- _ O
identify -X- _ O
the -X- _ O
functions -X- _ O
of -X- _ O
self-attention -X- _ O
heads -X- _ O
and -X- _ O
then -X- _ O
check -X- _ O
which -X- _ O
of -X- _ O
them -X- _ O
survive -X- _ O
the -X- _ O
pruning, -X- _ O
finding -X- _ O
that -X- _ O
the -X- _ O
syntactic -X- _ O
and -X- _ O
positional -X- _ O
heads -X- _ O
are -X- _ O
the -X- _ O
last -X- _ O
ones -X- _ O
to -X- _ O
go. -X- _ O
For -X- _ O
BERT, -X- _ B-MethodName
Prasanna -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
go -X- _ O
in -X- _ O
the -X- _ O
opposite -X- _ O
direction: -X- _ O
pruning -X- _ O
on -X- _ O
the -X- _ O
basis -X- _ O
of -X- _ O
importance -X- _ O
scores, -X- _ O
and -X- _ O
interpreting -X- _ O
the -X- _ O
remaining -X- _ O
"good" -X- _ O
subnetwork. -X- _ O
With -X- _ O
respect -X- _ O
to -X- _ O
self-attention -X- _ O
heads -X- _ O
specifically, -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
seem -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
case -X- _ O
that -X- _ O
only -X- _ O
the -X- _ O
heads -X- _ O
that -X- _ O
potentially -X- _ O
encode -X- _ O
nontrivial -X- _ O
linguistic -X- _ O
patterns -X- _ O
survive -X- _ O
the -X- _ O
pruning. -X- _ O
The -X- _ O
models -X- _ O
and -X- _ O
methodology -X- _ O
in -X- _ O
these -X- _ O
studies -X- _ O
differ, -X- _ O
so -X- _ O
the -X- _ O
evidence -X- _ O
is -X- _ O
inconclusive. -X- _ O
In -X- _ O
particular, -X- _ O
Voita -X- _ O
et -X- _ O
al. -X- _ O
(2019b) -X- _ O
find -X- _ O
that -X- _ O
before -X- _ O
pruning -X- _ O
the -X- _ O
majority -X- _ O
of -X- _ O
heads -X- _ O
are -X- _ O
syntactic, -X- _ O
and -X- _ O
Prasanna -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
-that -X- _ O
the -X- _ O
majority -X- _ O
of -X- _ O
heads -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
potentially -X- _ O
non-trivial -X- _ O
attention -X- _ O
patterns. -X- _ O
An -X- _ O
important -X- _ O
limitation -X- _ O
of -X- _ O
the -X- _ O
current -X- _ O
head -X- _ O
and -X- _ O
layer -X- _ O
ablation -X- _ O
studies -X- _ O
(Michel -X- _ O
et -X- _ O
al., -X- _ O
2019;Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
is -X- _ O
that -X- _ O
they -X- _ O
inherently -X- _ O
assume -X- _ O
that -X- _ O
certain -X- _ O
knowledge -X- _ O
is -X- _ O
contained -X- _ O
in -X- _ O
heads/layers. -X- _ O
However, -X- _ O
there -X- _ O
is -X- _ O
evidence -X- _ O
of -X- _ O
more -X- _ O
diffuse -X- _ O
representations -X- _ O
spread -X- _ O
across -X- _ O
the -X- _ O
full -X- _ O
network, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
gradual -X- _ O
increase -X- _ O
in -X- _ O
accuracy -X- _ O
on -X- _ O
difficult -X- _ O
semantic -X- _ O
parsing -X- _ O
tasks -X- _ O
(Tenney -X- _ O
et -X- _ O
al., -X- _ O
2019a) -X- _ O
or -X- _ O
the -X- _ O
absence -X- _ O
of -X- _ O
heads -X- _ O
that -X- _ O
would -X- _ O
perform -X- _ O
parsing -X- _ O
"in -X- _ O
general" -X- _ O
(Clark -X- _ O
et -X- _ O
al., -X- _ O
2019;Htut -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
If -X- _ O
so, -X- _ O
ablating -X- _ O
individual -X- _ O
components -X- _ O
harms -X- _ O
the -X- _ O
weightsharing -X- _ O
mechanism. -X- _ O
Conclusions -X- _ O
from -X- _ O
component -X- _ O
ablations -X- _ O
are -X- _ O
also -X- _ O
problematic -X- _ O
if -X- _ O
the -X- _ O
same -X- _ O
information -X- _ O
is -X- _ O
duplicated -X- _ O
elsewhere -X- _ O
in -X- _ O
the -X- _ O
network. -X- _ O

Given -X- _ O
the -X- _ O
above -X- _ O
evidence -X- _ O
of -X- _ O
overparameterization, -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
come -X- _ O
as -X- _ O
a -X- _ O
surprise -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
can -X- _ O
be -X- _ O
efficiently -X- _ O
compressed -X- _ O
with -X- _ O
minimal -X- _ O
accuracy -X- _ O
loss, -X- _ O
which -X- _ O
would -X- _ O
be -X- _ O
highly -X- _ O
desirable -X- _ O
for -X- _ O
real-world -X- _ O
applications. -X- _ O
Such -X- _ O
efforts -X- _ O
to -X- _ O
date -X- _ O
are -X- _ O
summarized -X- _ O
in -X- _ O
Table -X- _ O
1. -X- _ O
The -X- _ O
main -X- _ O
approaches -X- _ O
are -X- _ O
knowledge -X- _ O
distillation, -X- _ O
quantization, -X- _ O
and -X- _ O
pruning. -X- _ O
The -X- _ O
studies -X- _ O
in -X- _ O
the -X- _ O
knowledge -X- _ O
distillation -X- _ O
framework -X- _ O
(Hinton -X- _ O
et -X- _ O
al., -X- _ O
2014) -X- _ O
use -X- _ O
a -X- _ O
smaller -X- _ O
student-network -X- _ O
trained -X- _ O
to -X- _ O
mimic -X- _ O
the -X- _ O
behavior -X- _ O
of -X- _ O
a -X- _ O
larger -X- _ O
teacher-network. -X- _ O
For -X- _ O
BERT, -X- _ B-MethodName
this -X- _ O
has -X- _ O
been -X- _ O
achieved -X- _ O
through -X- _ O
experiments -X- _ O
with -X- _ O
loss -X- _ O
functions -X- _ O
(Sanh -X- _ O
et -X- _ O
al., -X- _ O
2019b;Jiao -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
mimicking -X- _ O
the -X- _ O
activation -X- _ O
patterns -X- _ O
of -X- _ O
individual -X- _ O
portions -X- _ O
of -X- _ O
the -X- _ O
teacher -X- _ O
network -X- _ O
(Sun -X- _ O
et -X- _ O
al., -X- _ O
2019a), -X- _ O
and -X- _ O
knowledge -X- _ O
transfer -X- _ O
at -X- _ O
the -X- _ O
pre-training -X- _ O
(Turc -X- _ O
et -X- _ O
al., -X- _ O
2019;Jiao -X- _ O
et -X- _ O
al., -X- _ O
2019;Sun -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
or -X- _ O
fine-tuning -X- _ O
stage -X- _ O
(Jiao -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
McCarley -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
suggest -X- _ O
that -X- _ O
distillation -X- _ O
has -X- _ O
so -X- _ O
far -X- _ O
worked -X- _ O
better -X- _ O
for -X- _ O
GLUE -X- _ B-DatasetName
than -X- _ O
for -X- _ O
reading -X- _ O
comprehension, -X- _ O
and -X- _ O
report -X- _ O
good -X- _ O
results -X- _ O
for -X- _ O
QA -X- _ B-TaskName
from -X- _ O
a -X- _ O
combination -X- _ O
of -X- _ O
structured -X- _ O
pruning -X- _ O
and -X- _ O
task-specific -X- _ O
distillation. -X- _ O
Quantization -X- _ O
decreases -X- _ O
BERT's -X- _ B-MethodName
memory -X- _ O
footprint -X- _ O
through -X- _ O
lowering -X- _ O
the -X- _ O
precision -X- _ O
of -X- _ O
its -X- _ O
weights -X- _ O
(Shen -X- _ O
et -X- _ O
al., -X- _ O
2019;Zafrir -X- _ O
et -X- _ O
al., -X- _ O
2019 -X- _ O
this -X- _ O
strategy -X- _ O
often -X- _ O
requires -X- _ O
compatible -X- _ O
hardware. -X- _ O
As -X- _ O
discussed -X- _ O
in -X- _ O
section -X- _ O
6, -X- _ O
individual -X- _ O
selfattention -X- _ O
heads -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
layers -X- _ O
can -X- _ O
be -X- _ O
disabled -X- _ O
without -X- _ O
significant -X- _ O
drop -X- _ O
in -X- _ O
performance -X- _ O
(Michel -X- _ O
et -X- _ O
al., -X- _ O
2019;Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019;Baan -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Pruning -X- _ O
is -X- _ O
a -X- _ O
compression -X- _ O
technique -X- _ O
that -X- _ O
takes -X- _ O
advantage -X- _ O
of -X- _ O
that -X- _ O
fact, -X- _ O
typically -X- _ O
reducing -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
computation -X- _ O
via -X- _ O
zeroing -X- _ O
out -X- _ O
of -X- _ O
certain -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
large -X- _ O
model. -X- _ O
In -X- _ O
structured -X- _ O
pruning, -X- _ O
architecture -X- _ O
blocks -X- _ O
are -X- _ O
dropped, -X- _ O
as -X- _ O
in -X- _ O
LayerDrop -X- _ O
. -X- _ O
In -X- _ O
unstructured, -X- _ O
the -X- _ O
weights -X- _ O
in -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
are -X- _ O
pruned -X- _ O
irrespective -X- _ O
of -X- _ O
their -X- _ O
location, -X- _ O
as -X- _ O
in -X- _ O
magnitude -X- _ O
pruning -X- _ O
or -X- _ O
movement -X- _ O
pruning -X- _ O
(Sanh -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O
Prasanna -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
and -X- _ O
explore -X- _ O
BERT -X- _ B-MethodName
from -X- _ O
the -X- _ O
perspective -X- _ O
of -X- _ O
the -X- _ O
lottery -X- _ O
ticket -X- _ O
hypothesis -X- _ O
(Frankle -X- _ O
and -X- _ O
Carbin, -X- _ O
2019), -X- _ O
looking -X- _ O
specifically -X- _ O
at -X- _ O
the -X- _ O
"winning" -X- _ O
subnetworks -X- _ O
in -X- _ O
pre-trained -X- _ O
BERT. -X- _ B-MethodName
They -X- _ O
independently -X- _ O
find -X- _ O
that -X- _ O
such -X- _ O
subnetworks -X- _ O
do -X- _ O
exist, -X- _ O
and -X- _ O
that -X- _ O
transferability -X- _ O
between -X- _ O
subnetworks -X- _ O
for -X- _ O
different -X- _ O
tasks -X- _ O
varies. -X- _ O
If -X- _ O
the -X- _ O
ultimate -X- _ O
goal -X- _ O
of -X- _ O
training -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
compression, -X- _ O
recommend -X- _ O
training -X- _ O
larger -X- _ O
models -X- _ O
and -X- _ O
compressing -X- _ O
them -X- _ O
heavily -X- _ O
rather -X- _ O
than -X- _ O
compressing -X- _ O
smaller -X- _ O
models -X- _ O
lightly. -X- _ O
Other -X- _ O
techniques -X- _ O
include -X- _ O
decomposing -X- _ O
BERT's -X- _ B-MethodName
embedding -X- _ O
matrix -X- _ O
into -X- _ O
smaller -X- _ O
matrices -X- _ O
(Lan -X- _ O
et -X- _ O
al., -X- _ O
2020a), -X- _ O
progressive -X- _ O
module -X- _ O
replacing -X- _ O
and -X- _ O
dynamic -X- _ O
elimination -X- _ O
of -X- _ O
intermediate -X- _ O
encoder -X- _ O
outputs -X- _ O
(Goyal -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O
See -X- _ O
Ganesh -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
for -X- _ O
a -X- _ O
more -X- _ O
detailed -X- _ O
discussion -X- _ O
of -X- _ O
compression -X- _ O
methods. -X- _ O

Transformer-based -X- _ O
models -X- _ O
keep -X- _ O
growing -X- _ O
by -X- _ O
orders -X- _ O
of -X- _ O
magnitude: -X- _ O
the -X- _ O
110M -X- _ O
parameters -X- _ O
of -X- _ O
base -X- _ B-MethodName
BERT -X- _ I-MethodName
are -X- _ O
now -X- _ O
dwarfed -X- _ O
by -X- _ O
17B -X- _ O
parameters -X- _ O
of -X- _ O
Turing-NLG -X- _ O
(Microsoft, -X- _ O
2020), -X- _ O
which -X- _ O
is -X- _ O
dwarfed -X- _ O
by -X- _ O
175B -X- _ O
of -X- _ O
GPT-3 -X- _ O
(Brown -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O
This -X- _ O
trend -X- _ O
raises -X- _ O
concerns -X- _ O
about -X- _ O
computational -X- _ O
complexity -X- _ O
of -X- _ O
self-attention -X- _ O
, -X- _ O
environmental -X- _ O
issues -X- _ O
(Strubell -X- _ O
et -X- _ O
al., -X- _ O
2019;Schwartz -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
fair -X- _ O
comparison -X- _ O
of -X- _ O
architectures -X- _ O
(Aßenmacher -X- _ O
and -X- _ O
Heumann, -X- _ O
2020), -X- _ O
and -X- _ O
reproducibility. -X- _ O
Human -X- _ O
language -X- _ O
is -X- _ O
incredibly -X- _ O
complex, -X- _ O
and -X- _ O
would -X- _ O
perhaps -X- _ O
take -X- _ O
many -X- _ O
more -X- _ O
parameters -X- _ O
to -X- _ O
describe -X- _ O
fully, -X- _ O
but -X- _ O
the -X- _ O
current -X- _ O
models -X- _ O
do -X- _ O
not -X- _ O
make -X- _ O
good -X- _ O
use -X- _ O
of -X- _ O
the -X- _ O
parameters -X- _ O
they -X- _ O
already -X- _ O
have. -X- _ O
Voita -X- _ O
et -X- _ O
al. -X- _ O
(2019b) -X- _ O
showed -X- _ O
that -X- _ O
all -X- _ O
but -X- _ O
a -X- _ O
few -X- _ O
Transformer -X- _ O
heads -X- _ O
could -X- _ O
be -X- _ O
pruned -X- _ O
without -X- _ O
significant -X- _ O
losses -X- _ O
in -X- _ O
performance. -X- _ O
For -X- _ O
BERT, -X- _ B-MethodName
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
observe -X- _ O
that -X- _ O
most -X- _ O
heads -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
layer -X- _ O
show -X- _ O
similar -X- _ O
self-attention -X- _ O
patterns -X- _ O
(perhaps -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
all -X- _ O
self-attention -X- _ O
heads -X- _ O
in -X- _ O
a -X- _ O
layer -X- _ O
is -X- _ O
passed -X- _ O
through -X- _ O
the -X- _ O
same -X- _ O
MLP), -X- _ O
which -X- _ O
explains -X- _ O
why -X- _ O
Michel -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
were -X- _ O
able -X- _ O
to -X- _ O
reduce -X- _ O
most -X- _ O
layers -X- _ O
to -X- _ O
a -X- _ O
single -X- _ O
head. -X- _ O
Depending -X- _ O
on -X- _ O
the -X- _ O
task, -X- _ O
some -X- _ O
BERT -X- _ B-MethodName
heads/layers -X- _ O
are -X- _ O
not -X- _ O
only -X- _ O
redundant -X- _ O
, -X- _ O
but -X- _ O
also -X- _ O
harmful -X- _ O
to -X- _ O
the -X- _ O
downstream -X- _ O
task -X- _ O
performance. -X- _ O
Positive -X- _ O
effect -X- _ O
from -X- _ O
head -X- _ O
disabling -X- _ O
was -X- _ O
reported -X- _ O
for -X- _ O
machine -X- _ B-TaskName
translation -X- _ I-TaskName
(Michel -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
abstractive -X- _ B-TaskName
summarization -X- _ I-TaskName
(Baan -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
and -X- _ O
GLUE -X- _ B-DatasetName
tasks -X- _ O
(Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Additionally, -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
(2019a) -X- _ O
examine -X- _ O
the -X- _ O
cumulative -X- _ O
gains -X- _ O
of -X- _ O
their -X- _ O
structural -X- _ O
probing -X- _ O
classifier, -X- _ O
observing -X- _ O
that -X- _ O
in -X- _ O
5 -X- _ O
out -X- _ O
of -X- _ O
8 -X- _ O
probing -X- _ O
tasks -X- _ O
some -X- _ O
layers -X- _ O
cause -X- _ O
a -X- _ O
drop -X- _ O
in -X- _ O
scores -X- _ O
(typically -X- _ O
in -X- _ O
the -X- _ O
final -X- _ O
layers). -X- _ O
Gordon -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
find -X- _ O
that -X- _ O
30-40% -X- _ O
of -X- _ O
the -X- _ O
weights -X- _ O
can -X- _ O
be -X- _ O
pruned -X- _ O
without -X- _ O
impact -X- _ O
on -X- _ O
downstream -X- _ O
tasks. -X- _ O
In -X- _ O
general, -X- _ O
larger -X- _ O
BERT -X- _ B-MethodName
models -X- _ O
perform -X- _ O
better -X- _ O
Roberts -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
but -X- _ O
not -X- _ O
always: -X- _ O
BERT-base -X- _ B-MethodName
outperformed -X- _ O
BERT-large -X- _ B-MethodName
on -X- _ O
subject-verb -X- _ O
agreement -X- _ O
(Goldberg, -X- _ O
2019) -X- _ O
and -X- _ O
sentence -X- _ O
subject -X- _ O
detection -X- _ O
. -X- _ O
Given -X- _ O
the -X- _ O
complexity -X- _ O
of -X- _ O
language, -X- _ O
and -X- _ O
amounts -X- _ O
of -X- _ O
pretraining -X- _ O
data, -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
clear -X- _ O
why -X- _ O
BERT -X- _ B-MethodName
ends -X- _ O
up -X- _ O
with -X- _ O
redundant -X- _ O
heads -X- _ O
and -X- _ O
layers. -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
suggest -X- _ O
that -X- _ O
one -X- _ O
possible -X- _ O
reason -X- _ O
is -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
attention -X- _ O
dropouts, -X- _ O
which -X- _ O
causes -X- _ O
some -X- _ O
attention -X- _ O
weights -X- _ O
to -X- _ O
be -X- _ O
zeroed-out -X- _ O
during -X- _ O
training. -X- _ O

Pre-training -X- _ O
+ -X- _ O
fine-tuning -X- _ O
workflow -X- _ O
is -X- _ O
a -X- _ O
crucial -X- _ O
part -X- _ O
of -X- _ O
BERT. -X- _ B-MethodName
The -X- _ O
former -X- _ O
is -X- _ O
supposed -X- _ O
to -X- _ O
provide -X- _ O
task-independent -X- _ O
knowledge, -X- _ O
and -X- _ O
the -X- _ O
latter -X- _ O
would -X- _ O
presumably -X- _ O
teach -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
rely -X- _ O
more -X- _ O
on -X- _ O
the -X- _ O
representations -X- _ O
useful -X- _ O
for -X- _ O
the -X- _ O
task -X- _ O
at -X- _ O
hand. -X- _ O
Kovaleva -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
did -X- _ O
not -X- _ O
find -X- _ O
that -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
case -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
fine-tuned -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
tasks -X- _ O
5 -X- _ O
: -X- _ O
during -X- _ O
fine-tuning, -X- _ O
the -X- _ O
most -X- _ O
changes -X- _ O
for -X- _ O
3 -X- _ O
epochs -X- _ O
occurred -X- _ O
in -X- _ O
the -X- _ O
last -X- _ O
two -X- _ O
layers -X- _ O
of -X- _ O
the -X- _ O
models, -X- _ O
but -X- _ O
those -X- _ O
changes -X- _ O
caused -X- _ O
self-attention -X- _ O
to -X- _ O
focus -X- _ O
on -X- _ O
[SEP] -X- _ O
rather -X- _ O
than -X- _ O
on -X- _ O
linguistically -X- _ O
interpretable -X- _ O
patterns. -X- _ O
It -X- _ O
is -X- _ O
understandable -X- _ O
why -X- _ O
fine-tuning -X- _ O
would -X- _ O
increase -X- _ O
the -X- _ O
attention -X- _ O
to -X- _ O
[CLS], -X- _ O
but -X- _ O
not -X- _ O
[SEP]. -X- _ O
If -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
are -X- _ O
correct -X- _ O
that -X- _ O
[SEP] -X- _ O
serves -X- _ O
as -X- _ O
"noop" -X- _ O
indicator, -X- _ O
fine-tuning -X- _ O
basically -X- _ O
tells -X- _ O
BERT -X- _ B-MethodName
what -X- _ O
to -X- _ O
ignore. -X- _ O
Several -X- _ O
studies -X- _ O
explored -X- _ O
the -X- _ O
possibilities -X- _ O
of -X- _ O
improving -X- _ O
the -X- _ O
fine-tuning -X- _ O
of -X- _ O
BERT: -X- _ B-MethodName
• -X- _ O
Taking -X- _ O
more -X- _ O
layers -X- _ O
into -X- _ O
account: -X- _ O
learning -X- _ O
a -X- _ O
complementary -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
information -X- _ O
in -X- _ O
deep -X- _ O
and -X- _ O
output -X- _ O
layers -X- _ O
, -X- _ O
using -X- _ O
a -X- _ O
weighted -X- _ O
combination -X- _ O
of -X- _ O
all -X- _ O
layers -X- _ O
instead -X- _ O
of -X- _ O
the -X- _ O
final -X- _ O
one -X- _ O
(Su -X- _ O
and -X- _ O
Cheng, -X- _ O
2019;Kondratyuk -X- _ O
and -X- _ O
Straka, -X- _ O
2019), -X- _ O
and -X- _ O
layer -X- _ O
dropout -X- _ O
(Kondratyuk -X- _ O
and -X- _ O
Straka, -X- _ O
2019). -X- _ O
• -X- _ O
Two-stage -X- _ O
fine-tuning -X- _ O
introduces -X- _ O
an -X- _ O
intermediate -X- _ O
supervised -X- _ O
training -X- _ O
stage -X- _ O
between -X- _ O
pre-training -X- _ O
and -X- _ O
fine-tuning -X- _ O
Arase -X- _ O
and -X- _ O
Tsujii, -X- _ O
2019;Pruksachatkun -X- _ O
et -X- _ O
al., -X- _ O
2020;Glavaš -X- _ O
and -X- _ O
Vulić, -X- _ O
2020). -X- _ O
Ben-David -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
propose -X- _ O
a -X- _ O
pivot-based -X- _ O
variant -X- _ O
of -X- _ O
MLM -X- _ B-TaskName
to -X- _ O
fine-tune -X- _ O
BERT -X- _ B-MethodName
for -X- _ O
domain -X- _ O
adaptation. -X- _ O
• -X- _ O
Adversarial -X- _ O
token -X- _ O
perturbations -X- _ O
improve -X- _ O
robustness -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
(Zhu -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
• -X- _ O
Adversarial -X- _ O
regularization -X- _ O
in -X- _ O
combination -X- _ O
with -X- _ O
Bregman -X- _ O
Proximal -X- _ O
Point -X- _ O
Optimization -X- _ O
helps -X- _ O
alleviate -X- _ O
pre-trained -X- _ O
knowledge -X- _ O
forgetting -X- _ O
and -X- _ O
therefore -X- _ O
prevents -X- _ O
BERT -X- _ B-MethodName
from -X- _ O
overfitting -X- _ O
to -X- _ O
downstream -X- _ O
tasks -X- _ O
). -X- _ O
• -X- _ O
Mixout -X- _ O
regularization -X- _ O
improves -X- _ O
the -X- _ O
stability -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
fine-tuning -X- _ O
even -X- _ O
for -X- _ O
a -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
training -X- _ O
examples -X- _ O
. -X- _ O
With -X- _ O
large -X- _ O
models, -X- _ O
even -X- _ O
fine-tuning -X- _ O
becomes -X- _ O
expensive, -X- _ O
but -X- _ O
Houlsby -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
show -X- _ O
that -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
successfully -X- _ O
approximated -X- _ O
with -X- _ O
adapter -X- _ O
modules. -X- _ O
They -X- _ O
achieve -X- _ O
competitive -X- _ O
performance -X- _ O
on -X- _ O
26 -X- _ O
classification -X- _ O
tasks -X- _ O
at -X- _ O
a -X- _ O
fraction -X- _ O
of -X- _ O
the -X- _ O
computational -X- _ O
cost. -X- _ O
Adapters -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
were -X- _ O
also -X- _ O
used -X- _ O
for -X- _ O
multi-task -X- _ O
learning -X- _ O
(Stickland -X- _ O
and -X- _ O
Murray, -X- _ O
2019) -X- _ O
and -X- _ O
cross-lingual -X- _ O
transfer -X- _ O
(Artetxe -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
An -X- _ O
alternative -X- _ O
to -X- _ O
fine-tuning -X- _ O
is -X- _ O
extracting -X- _ O
features -X- _ O
from -X- _ O
frozen -X- _ O
representations, -X- _ O
but -X- _ O
fine-tuning -X- _ O
works -X- _ O
better -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
(Peters -X- _ O
et -X- _ O
al., -X- _ O
2019b). -X- _ O
A -X- _ O
big -X- _ O
methodological -X- _ O
challenge -X- _ O
in -X- _ O
the -X- _ O
current -X- _ O
NLP -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
reported -X- _ O
performance -X- _ O
improvements -X- _ O
of -X- _ O
new -X- _ O
models -X- _ O
may -X- _ O
well -X- _ O
be -X- _ O
within -X- _ O
variation -X- _ O
induced -X- _ O
by -X- _ O
environment -X- _ O
factors -X- _ O
(Crane, -X- _ O
2018). -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
not -X- _ O
an -X- _ O
exception. -X- _ O
Dodge -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
report -X- _ O
significant -X- _ O
variation -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
fine-tuned -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
tasks -X- _ O
due -X- _ O
to -X- _ O
both -X- _ O
weight -X- _ O
initialization -X- _ O
and -X- _ O
training -X- _ O
data -X- _ O
order. -X- _ O
They -X- _ O
also -X- _ O
propose -X- _ O
early -X- _ O
stopping -X- _ O
on -X- _ O
the -X- _ O
less-promising -X- _ O
seeds. -X- _ O
Although -X- _ O
we -X- _ O
hope -X- _ O
that -X- _ O
the -X- _ O
above -X- _ O
observations -X- _ O
may -X- _ O
be -X- _ O
useful -X- _ O
for -X- _ O
the -X- _ O
practitioners, -X- _ O
this -X- _ O
section -X- _ O
does -X- _ O
not -X- _ O
exhaust -X- _ O
the -X- _ O
current -X- _ O
research -X- _ O
on -X- _ O
fine-tuning -X- _ O
and -X- _ O
its -X- _ O
alternatives. -X- _ O
For -X- _ O
example, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
cover -X- _ O
such -X- _ O
topics -X- _ O
as -X- _ O
Siamese -X- _ O
architectures, -X- _ O
policy -X- _ O
gradient -X- _ O
training, -X- _ O
automated -X- _ O
curriculum -X- _ O
learning, -X- _ O
and -X- _ O
others. -X- _ O
6 -X- _ O
How -X- _ O
big -X- _ O
should -X- _ O
BERT -X- _ B-MethodName
be? -X- _ O

The -X- _ O
original -X- _ B-MethodName
BERT -X- _ I-MethodName
is -X- _ O
a -X- _ O
bidirectional -X- _ O
Transformer -X- _ O
pre-trained -X- _ O
on -X- _ O
two -X- _ O
tasks: -X- _ O
next -X- _ O
sentence -X- _ B-TaskName
prediction -X- _ I-TaskName
(NSP) -X- _ B-TaskName
and -X- _ O
masked -X- _ B-TaskName
language -X- _ I-TaskName
model -X- _ I-TaskName
(MLM) -X- _ B-TaskName
(section -X- _ O
2). -X- _ O
Multiple -X- _ O
studies -X- _ O
have -X- _ O
come -X- _ O
up -X- _ O
with -X- _ O
alternative -X- _ O
training -X- _ O
objectives -X- _ O
to -X- _ O
improve -X- _ O
on -X- _ O
BERT, -X- _ B-MethodName
which -X- _ O
could -X- _ O
be -X- _ O
categorized -X- _ O
as -X- _ O
follows: -X- _ O
• -X- _ O
How -X- _ O
to -X- _ O
mask. -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019;Cui -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Similarly, -X- _ O
we -X- _ O
can -X- _ O
mask -X- _ O
spans -X- _ O
rather -X- _ O
than -X- _ O
single -X- _ O
tokens -X- _ O
(Joshi -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
predicting -X- _ O
how -X- _ O
many -X- _ O
are -X- _ O
missing -X- _ O
. -X- _ O
Masking -X- _ O
phrases -X- _ O
and -X- _ O
named -X- _ O
entities -X- _ O
(Sun -X- _ O
et -X- _ O
al., -X- _ O
2019b) -X- _ O
improves -X- _ O
representation -X- _ O
of -X- _ O
structured -X- _ O
knowledge. -X- _ O
• -X- _ O
Where -X- _ O
to -X- _ O
mask. -X- _ O
Lample -X- _ O
and -X- _ O
Conneau -X- _ O
(2019) -X- _ O
use -X- _ O
arbitrary -X- _ O
text -X- _ O
streams -X- _ O
instead -X- _ O
of -X- _ O
sentence -X- _ O
pairs -X- _ O
and -X- _ O
subsample -X- _ O
frequent -X- _ O
outputs -X- _ O
similar -X- _ O
to -X- _ O
Mikolov -X- _ O
et -X- _ O
al. -X- _ O
(2013). -X- _ O
Bao -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
combine -X- _ O
the -X- _ O
standard -X- _ O
autoencoding -X- _ O
MLM -X- _ B-TaskName
with -X- _ O
partially -X- _ O
autoregressive -X- _ O
LM -X- _ O
objective -X- _ O
using -X- _ O
special -X- _ O
pseudo -X- _ O
mask -X- _ O
tokens. -X- _ O
• -X- _ O
Alternatives -X- _ O
to -X- _ O
masking. -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
experiment -X- _ O
with -X- _ O
replacing -X- _ O
and -X- _ O
dropping -X- _ O
spans, -X- _ O
explore -X- _ O
deletion, -X- _ O
infilling, -X- _ O
sentence -X- _ O
permutation -X- _ O
and -X- _ O
document -X- _ O
rotation, -X- _ O
and -X- _ O
Sun -X- _ O
et -X- _ O
al. -X- _ O
(2019c) -X- _ O
predict -X- _ O
whether -X- _ O
a -X- _ O
token -X- _ O
is -X- _ O
capitalized -X- _ O
and -X- _ O
whether -X- _ O
it -X- _ O
occurs -X- _ O
in -X- _ O
other -X- _ O
segments -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
document. -X- _ O
Yang -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
train -X- _ O
on -X- _ O
different -X- _ O
permutations -X- _ O
of -X- _ O
word -X- _ O
order -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
sequence, -X- _ O
maximizing -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
word -X- _ O
order -X- _ O
(cf. -X- _ O
the -X- _ O
n-gram -X- _ O
word -X- _ O
order -X- _ O
reconstruction -X- _ O
task -X- _ O
). -X- _ O
detect -X- _ O
tokens -X- _ O
that -X- _ O
were -X- _ O
replaced -X- _ O
by -X- _ O
a -X- _ O
generator -X- _ O
network -X- _ O
rather -X- _ O
than -X- _ O
masked. -X- _ O
• -X- _ O
NSP -X- _ B-TaskName
alternatives. -X- _ O
Removing -X- _ O
NSP -X- _ B-TaskName
does -X- _ O
not -X- _ O
hurt -X- _ O
or -X- _ O
slightly -X- _ O
improves -X- _ O
performance -X- _ O
(Liu -X- _ O
et -X- _ O
al., -X- _ O
2019b;Joshi -X- _ O
et -X- _ O
al., -X- _ O
2020;Clinchant -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
and -X- _ O
Another -X- _ O
obvious -X- _ O
source -X- _ O
of -X- _ O
improvement -X- _ O
is -X- _ O
pretraining -X- _ O
data. -X- _ O
Several -X- _ O
studies -X- _ O
explored -X- _ O
the -X- _ O
benefits -X- _ O
of -X- _ O
increasing -X- _ O
the -X- _ O
corpus -X- _ O
volume -X- _ O
(Liu -X- _ O
et -X- _ O
al., -X- _ O
2019b;Baevski -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
and -X- _ O
longer -X- _ O
training -X- _ O
(Liu -X- _ O
et -X- _ O
al., -X- _ O
2019b). -X- _ O
The -X- _ O
data -X- _ O
also -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
to -X- _ O
be -X- _ O
raw -X- _ O
text: -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
number -X- _ O
efforts -X- _ O
to -X- _ O
incorporate -X- _ O
explicit -X- _ O
linguistic -X- _ O
information, -X- _ O
both -X- _ O
syntactic -X- _ O
(Sundararaman -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
and -X- _ O
semantic -X- _ O
. -X- _ O
and -X- _ O
Kumar -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
include -X- _ O
the -X- _ O
label -X- _ O
for -X- _ O
a -X- _ O
given -X- _ O
sequence -X- _ O
from -X- _ O
an -X- _ O
annotated -X- _ O
task -X- _ O
dataset. -X- _ O
Schick -X- _ O
and -X- _ O
Schütze -X- _ O
(2020) -X- _ O
separately -X- _ O
learn -X- _ O
representations -X- _ O
for -X- _ O
rare -X- _ O
words. -X- _ O
Although -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
already -X- _ O
actively -X- _ O
used -X- _ O
as -X- _ O
a -X- _ O
source -X- _ O
of -X- _ O
world -X- _ O
knowledge -X- _ O
(see -X- _ O
subsection -X- _ O
3.3), -X- _ O
there -X- _ O
is -X- _ O
also -X- _ O
work -X- _ O
on -X- _ O
explicitly -X- _ O
supplying -X- _ O
structured -X- _ O
knowledge. -X- _ O
One -X- _ O
approach -X- _ O
is -X- _ O
entityenhanced -X- _ O
models. -X- _ O
For -X- _ O
example, -X- _ O
; -X- _ O
include -X- _ O
entity -X- _ O
em- -X- _ O
Pre-training -X- _ O
is -X- _ O
the -X- _ O
most -X- _ O
expensive -X- _ O
part -X- _ O
of -X- _ O
training -X- _ O
BERT, -X- _ B-MethodName
and -X- _ O
it -X- _ O
would -X- _ O
be -X- _ O
informative -X- _ O
to -X- _ O
know -X- _ O
how -X- _ O
much -X- _ O
benefit -X- _ O
it -X- _ O
provides. -X- _ O
On -X- _ O
some -X- _ O
tasks, -X- _ O
a -X- _ O
randomly -X- _ O
initialized -X- _ O
and -X- _ O
fine-tuned -X- _ O
BERT -X- _ B-MethodName
obtains -X- _ O
competitive -X- _ O
or -X- _ O
higher -X- _ O
results -X- _ O
than -X- _ O
the -X- _ O
pre-trained -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
the -X- _ O
task -X- _ O
classifier -X- _ O
and -X- _ O
frozen -X- _ O
weights -X- _ O
(Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
The -X- _ O
consensus -X- _ O
in -X- _ O
the -X- _ O
community -X- _ O
is -X- _ O
that -X- _ O
pre-training -X- _ O
does -X- _ O
help -X- _ O
in -X- _ O
most -X- _ O
situations, -X- _ O
but -X- _ O
the -X- _ O
degree -X- _ O
and -X- _ O
its -X- _ O
exact -X- _ O
contribution -X- _ O
requires -X- _ O
further -X- _ O
investigation. -X- _ O
Prasanna -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
found -X- _ O
that -X- _ O
most -X- _ O
weights -X- _ O
of -X- _ O
pre-trained -X- _ O
BERT -X- _ B-MethodName
are -X- _ O
useful -X- _ O
in -X- _ O
fine-tuning, -X- _ O
although -X- _ O
there -X- _ O
are -X- _ O
"better" -X- _ O
and -X- _ O
"worse" -X- _ O
subnetworks. -X- _ O
One -X- _ O
explanation -X- _ O
is -X- _ O
that -X- _ O
pre-trained -X- _ O
weights -X- _ O
help -X- _ O
the -X- _ O
fine-tuned -X- _ O
BERT -X- _ B-MethodName
find -X- _ O
wider -X- _ O
and -X- _ O
flatter -X- _ O
areas -X- _ O
with -X- _ O
smaller -X- _ O
generalization -X- _ O
error, -X- _ O
which -X- _ O
makes -X- _ O
the -X- _ O
model -X- _ O
more -X- _ O
robust -X- _ O
to -X- _ O
overfitting -X- _ O
(see -X- _ O
Figure -X- _ O
5 -X- _ O
from -X- _ O
Hao -X- _ O
et -X- _ O
al. -X- _ O
(2019)). -X- _ O
Given -X- _ O
the -X- _ O
large -X- _ O
number -X- _ O
and -X- _ O
variety -X- _ O
of -X- _ O
proposed -X- _ O
modifications, -X- _ O
one -X- _ O
would -X- _ O
wish -X- _ O
to -X- _ O
know -X- _ O
how -X- _ O
much -X- _ O
impact -X- _ O
each -X- _ O
of -X- _ O
them -X- _ O
has. -X- _ O
However, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
overall -X- _ O
trend -X- _ O
towards -X- _ O
large -X- _ O
model -X- _ O
sizes, -X- _ O
systematic -X- _ O
ablations -X- _ O
have -X- _ O
become -X- _ O
expensive. -X- _ O
Most -X- _ O
new -X- _ O
models -X- _ O
claim -X- _ O
superiority -X- _ O
on -X- _ O
standard -X- _ O
benchmarks, -X- _ O
but -X- _ O
gains -X- _ O
are -X- _ O
often -X- _ O
marginal, -X- _ O
and -X- _ O
estimates -X- _ O
of -X- _ O
model -X- _ O
stability -X- _ O
and -X- _ O
significance -X- _ O
testing -X- _ O
are -X- _ O
very -X- _ O
rare. -X- _ O

To -X- _ O
date, -X- _ O
the -X- _ O
most -X- _ O
systematic -X- _ O
study -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
architecture -X- _ O
was -X- _ O
performed -X- _ O
by -X- _ O
, -X- _ O
who -X- _ O
experimented -X- _ O
with -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers, -X- _ I-HyperparameterName
heads, -X- _ B-HyperparameterName
and -X- _ O
model -X- _ O
parameters, -X- _ O
varying -X- _ O
one -X- _ O
option -X- _ O
and -X- _ O
freezing -X- _ O
the -X- _ O
others. -X- _ O
They -X- _ O
concluded -X- _ O
that -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
heads -X- _ I-HyperparameterName
was -X- _ O
not -X- _ O
as -X- _ O
significant -X- _ O
as -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers. -X- _ I-HyperparameterName
That -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
findings -X- _ O
of -X- _ O
Voita -X- _ O
et -X- _ O
al. -X- _ O
(2019b) -X- _ O
and -X- _ O
Michel -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
(section -X- _ O
6), -X- _ O
and -X- _ O
also -X- _ O
the -X- _ O
observation -X- _ O
by -X- _ O
that -X- _ O
the -X- _ O
middle -X- _ O
layers -X- _ O
were -X- _ O
the -X- _ O
most -X- _ O
transferable. -X- _ O
Larger -X- _ O
hidden -X- _ O
representation -X- _ O
size -X- _ O
was -X- _ O
con-sistently -X- _ O
better, -X- _ O
but -X- _ O
the -X- _ O
gains -X- _ O
varied -X- _ O
by -X- _ O
setting. -X- _ O
All -X- _ O
in -X- _ O
all, -X- _ O
changes -X- _ O
in -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
heads -X- _ I-HyperparameterName
and -X- _ O
layers -X- _ B-HyperparameterName
appear -X- _ O
to -X- _ O
perform -X- _ O
different -X- _ O
functions. -X- _ O
The -X- _ O
issue -X- _ O
of -X- _ O
model -X- _ O
depth -X- _ O
must -X- _ O
be -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
information -X- _ O
flow -X- _ O
from -X- _ O
the -X- _ O
most -X- _ O
task-specific -X- _ O
layers -X- _ O
closer -X- _ O
to -X- _ O
the -X- _ O
classifier -X- _ O
, -X- _ O
to -X- _ O
the -X- _ O
initial -X- _ O
layers -X- _ O
which -X- _ O
appear -X- _ O
to -X- _ O
be -X- _ O
the -X- _ O
most -X- _ O
task-invariant -X- _ O
(Hao -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
and -X- _ O
where -X- _ O
the -X- _ O
tokens -X- _ O
resemble -X- _ O
the -X- _ O
input -X- _ O
tokens -X- _ O
the -X- _ O
most -X- _ O
(Brunner -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
(see -X- _ O
subsection -X- _ O
4.3). -X- _ O
If -X- _ O
that -X- _ O
is -X- _ O
the -X- _ O
case, -X- _ O
a -X- _ O
deeper -X- _ O
model -X- _ O
has -X- _ O
more -X- _ O
capacity -X- _ O
to -X- _ O
encode -X- _ O
information -X- _ O
that -X- _ O
is -X- _ O
not -X- _ O
task-specific. -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
head, -X- _ O
many -X- _ O
self-attention -X- _ O
heads -X- _ O
in -X- _ O
vanilla -X- _ B-MethodName
BERT -X- _ I-MethodName
seem -X- _ O
to -X- _ O
naturally -X- _ O
learn -X- _ O
the -X- _ O
same -X- _ O
patterns -X- _ O
(Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
This -X- _ O
explains -X- _ O
why -X- _ O
pruning -X- _ O
them -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
too -X- _ O
much -X- _ O
impact. -X- _ O
The -X- _ O
question -X- _ O
that -X- _ O
arises -X- _ O
from -X- _ O
this -X- _ O
is -X- _ O
how -X- _ O
far -X- _ O
we -X- _ O
could -X- _ O
get -X- _ O
with -X- _ O
intentionally -X- _ O
encouraging -X- _ O
diverse -X- _ O
self-attention -X- _ O
patterns: -X- _ O
theoretically, -X- _ O
this -X- _ O
would -X- _ O
mean -X- _ O
increasing -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
number -X- _ O
of -X- _ O
weights. -X- _ O
Raganato -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
show -X- _ O
for -X- _ O
Transformer-based -X- _ O
machine -X- _ O
translation -X- _ O
we -X- _ O
can -X- _ O
simply -X- _ O
pre-set -X- _ O
the -X- _ O
patterns -X- _ O
that -X- _ O
we -X- _ O
already -X- _ O
know -X- _ O
the -X- _ O
model -X- _ O
would -X- _ O
learn, -X- _ O
instead -X- _ O
of -X- _ O
learning -X- _ O
them -X- _ O
from -X- _ O
scratch. -X- _ O
Vanilla -X- _ B-MethodName
BERT -X- _ I-MethodName
is -X- _ O
symmetric -X- _ O
and -X- _ O
balanced -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
self-attention -X- _ O
and -X- _ O
feed-forward -X- _ O
layers, -X- _ O
but -X- _ O
it -X- _ O
may -X- _ O
not -X- _ O
have -X- _ O
to -X- _ O
be. -X- _ O
For -X- _ O
the -X- _ O
base -X- _ O
Transformer, -X- _ O
Press -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
report -X- _ O
benefits -X- _ O
from -X- _ O
more -X- _ O
selfattention -X- _ O
sublayers -X- _ O
at -X- _ O
the -X- _ O
bottom -X- _ O
and -X- _ O
more -X- _ O
feedforward -X- _ O
sublayers -X- _ O
at -X- _ O
the -X- _ O
top. -X- _ O
Gong -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
note -X- _ O
that, -X- _ O
since -X- _ O
self-attention -X- _ O
patterns -X- _ O
in -X- _ O
higher -X- _ O
and -X- _ O
lower -X- _ O
layers -X- _ O
are -X- _ O
similar, -X- _ O
the -X- _ O
model -X- _ O
training -X- _ O
can -X- _ O
be -X- _ O
done -X- _ O
in -X- _ O
a -X- _ O
recursive -X- _ O
manner, -X- _ O
where -X- _ O
the -X- _ O
shallower -X- _ O
version -X- _ O
is -X- _ O
trained -X- _ O
first -X- _ O
and -X- _ O
then -X- _ O
the -X- _ O
trained -X- _ O
parameters -X- _ O
are -X- _ O
copied -X- _ O
to -X- _ O
deeper -X- _ O
layers. -X- _ O
Such -X- _ O
a -X- _ O
"warm-start" -X- _ O
can -X- _ O
lead -X- _ O
to -X- _ O
a -X- _ O
25% -X- _ O
faster -X- _ O
training -X- _ O
without -X- _ O
sacrificing -X- _ O
performance. -X- _ O


This -X- _ O
section -X- _ O
reviews -X- _ O
the -X- _ O
proposals -X- _ O
to -X- _ O
optimize -X- _ O
the -X- _ O
training -X- _ O
and -X- _ O
architecture -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
BERT. -X- _ B-MethodName

Figure -X- _ O
4 -X- _ O
presents -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
softmax -X- _ O
classifiers -X- _ O
trained -X- _ O
to -X- _ O
perform -X- _ O
the -X- _ O
bidirectional -X- _ O
language -X- _ O
modeling -X- _ O
task, -X- _ O
given -X- _ O
just -X- _ O
the -X- _ O
CWRs -X- _ O
as -X- _ O
input. -X- _ O
We -X- _ O
notice -X- _ O
that -X- _ O
higher -X- _ O
layers -X- _ O
in -X- _ O
recurrent -X- _ O
models -X- _ O
consistently -X- _ O
achieve -X- _ O
lower -X- _ O
perplexities. -X- _ O
Inter-is -X- _ O
the -X- _ O
vector -X- _ O
ori -X- _ O
during -X- _ O
pretrain -X- _ O
ing -X- _ O
performanc -X- _ O
tations -X- _ O
that -X- _ O
are -X- _ O
ing -X- _ O
are -X- _ O
also -X- _ O
tho -X- _ O
performance -X- _ O
(F -X- _ O
alizer -X- _ O
layers -X- _ O
tr -X- _ O
and -X- _ O
task-specifi -X- _ O
These -X- _ O
result -X- _ O
layerwise -X- _ O
beha -X- _ O
moving -X- _ O
up -X- _ O
the -X- _ O
specific -X- _ O
repres -X- _ O
hold -X- _ O
for -X- _ O
transf -X- _ O
differences -X- _ O
bet -X- _ O
an -X- _ O
active -X- _ O
area -X- _ O
o -X- _ O
et -X- _ O
al. -X- _ O
of -X- _ O
hierarchical -X- _ O
sentence -X- _ O
structure, -X- _ O
as -X- _ O
detected -X- _ O
by -X- _ O
the -X- _ O
probing -X- _ O
tasks -X- _ O
of -X- _ O
predicting -X- _ O
the -X- _ O
token -X- _ O
index, -X- _ O
the -X- _ O
main -X- _ O
auxiliary -X- _ O
verb -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
subject. -X- _ O
There -X- _ O
is -X- _ O
a -X- _ O
wide -X- _ O
consensus -X- _ O
in -X- _ O
studies -X- _ O
with -X- _ O
different -X- _ O
tasks, -X- _ O
datasets -X- _ O
and -X- _ O
methodologies -X- _ O
that -X- _ O
syntactic -X- _ O
information -X- _ O
is -X- _ O
most -X- _ O
prominent -X- _ O
in -X- _ O
the -X- _ O
middle -X- _ O
layers -X- _ O
of -X- _ O
BERT. -X- _ B-MethodName
4 -X- _ O
Hewitt -X- _ O
and -X- _ O
Manning -X- _ O
(2019) -X- _ O
had -X- _ O
the -X- _ O
most -X- _ O
success -X- _ O
reconstructing -X- _ O
syntactic -X- _ O
tree -X- _ O
depth -X- _ O
from -X- _ O
the -X- _ O
middle -X- _ O
BERT -X- _ O
layers -X- _ B-HyperparameterName
(6-9 -X- _ B-HyperparameterValue
for -X- _ O
base-BERT, -X- _ B-MethodName
14-19 -X- _ B-HyperparameterValue
for -X- _ O
BERT-large). -X- _ B-MethodName
Goldberg -X- _ O
(2019) -X- _ O
reports -X- _ O
the -X- _ O
best -X- _ O
subject-verb -X- _ O
agreement -X- _ O
around -X- _ O
layers -X- _ B-HyperparameterName
8-9, -X- _ B-HyperparameterValue
and -X- _ O
the -X- _ O
performance -X- _ O
on -X- _ O
syntactic -X- _ O
probing -X- _ O
tasks -X- _ O
used -X- _ O
by -X- _ O
Jawahar -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
also -X- _ O
seems -X- _ O
to -X- _ O
peak -X- _ O
around -X- _ O
the -X- _ O
middle -X- _ O
of -X- _ O
the -X- _ O
model. -X- _ O
The -X- _ O
prominence -X- _ O
of -X- _ O
syntactic -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
middle -X- _ O
BERT -X- _ B-MethodName
layers -X- _ O
is -X- _ O
related -X- _ O
to -X- _ O
's -X- _ O
observation -X- _ O
that -X- _ O
the -X- _ O
middle -X- _ O
layers -X- _ O
of -X- _ O
Transformers -X- _ O
are -X- _ O
best-performing -X- _ O
overall -X- _ O
and -X- _ O
the -X- _ O
most -X- _ O
transferable -X- _ O
across -X- _ O
tasks -X- _ O
(see -X- _ O
Figure -X- _ O
4). -X- _ O
There -X- _ O
is -X- _ O
conflicting -X- _ O
evidence -X- _ O
about -X- _ O
syntactic -X- _ O
chunks. -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
(2019a) -X- _ O
conclude -X- _ O
that -X- _ O
"the -X- _ O
basic -X- _ O
syntactic -X- _ O
information -X- _ O
appears -X- _ O
earlier -X- _ O
in -X- _ O
the -X- _ O
network -X- _ O
while -X- _ O
high-level -X- _ O
semantic -X- _ O
features -X- _ O
appear -X- _ O
at -X- _ O
the -X- _ O
higher -X- _ O
layers", -X- _ O
drawing -X- _ O
parallels -X- _ O
between -X- _ O
this -X- _ O
order -X- _ O
and -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
components -X- _ O
in -X- _ O
a -X- _ O
typical -X- _ O
NLP -X- _ O
pipeline -X- _ O
-from -X- _ O
POS-tagging -X- _ O
to -X- _ O
dependency -X- _ O
parsing -X- _ O
to -X- _ O
semantic -X- _ O
role -X- _ O
labeling. -X- _ O
Jawahar -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
also -X- _ O
report -X- _ O
that -X- _ O
the -X- _ O
lower -X- _ O
layers -X- _ O
were -X- _ O
more -X- _ O
useful -X- _ O
for -X- _ O
chunking, -X- _ O
while -X- _ O
middle -X- _ O
layers -X- _ O
were -X- _ O
more -X- _ O
useful -X- _ O
for -X- _ O
parsing. -X- _ O
At -X- _ O
the -X- _ O
same -X- _ O
time, -X- _ O
the -X- _ O
probing -X- _ O
experiments -X- _ O
by -X- _ O
find -X- _ O
the -X- _ O
opposite: -X- _ O
both -X- _ O
POS-tagging -X- _ O
and -X- _ O
chunking -X- _ O
were -X- _ O
performed -X- _ O
best -X- _ O
at -X- _ O
the -X- _ O
middle -X- _ O
layers, -X- _ O
in -X- _ O
both -X- _ O
BERT-base -X- _ B-MethodName
and -X- _ O
BERT-large. -X- _ B-MethodName
However, -X- _ O
all -X- _ O
three -X- _ O
studies -X- _ O
use -X- _ O
different -X- _ O
suites -X- _ O
of -X- _ O
probing -X- _ O
tasks. -X- _ O
The -X- _ O
final -X- _ O
layers -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
are -X- _ O
the -X- _ O
most -X- _ O
taskspecific. -X- _ O
In -X- _ O
pre-training, -X- _ O
this -X- _ O
means -X- _ O
specificity -X- _ O
to -X- _ O
the -X- _ O
MLM -X- _ B-TaskName
task, -X- _ O
which -X- _ O
explains -X- _ O
why -X- _ O
the -X- _ O
middle -X- _ O
layers -X- _ O
are -X- _ O
more -X- _ O
transferable -X- _ O
. -X- _ O
In -X- _ O
fine-tuning, -X- _ O
it -X- _ O
explains -X- _ O
why -X- _ O
the -X- _ O
final -X- _ O
layers -X- _ O
change -X- _ O
the -X- _ O
most -X- _ O
(Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
and -X- _ O
why -X- _ O
restoring -X- _ O
the -X- _ O
weights -X- _ O
of -X- _ O
lower -X- _ O
layers -X- _ O
of -X- _ O
fine-tuned -X- _ O
BERT -X- _ B-MethodName
to -X- _ O
their -X- _ O
original -X- _ O
values -X- _ O
does -X- _ O
not -X- _ O
dramatically -X- _ O
hurt -X- _ O
the -X- _ O
model -X- _ O
performance -X- _ O
(Hao -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
(2019a) -X- _ O
suggest -X- _ O
that -X- _ O
while -X- _ O
syntactic -X- _ O
information -X- _ O
appears -X- _ O
early -X- _ O
in -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
localized, -X- _ O
semantics -X- _ O
is -X- _ O
spread -X- _ O
across -X- _ O
the -X- _ O
entire -X- _ O
model, -X- _ O
which -X- _ O
explains -X- _ O
why -X- _ O
certain -X- _ O
non-trivial -X- _ O
examples -X- _ O
get -X- _ O
solved -X- _ O
incorrectly -X- _ O
at -X- _ O
first -X- _ O
but -X- _ O
correctly -X- _ O
at -X- _ O
the -X- _ O
later -X- _ O
layers. -X- _ O
This -X- _ O
is -X- _ O
rather -X- _ O
to -X- _ O
be -X- _ O
expected: -X- _ O
semantics -X- _ O
permeates -X- _ O
all -X- _ O
language, -X- _ O
and -X- _ O
linguists -X- _ O
debate -X- _ O
whether -X- _ O
meaningless -X- _ O
structures -X- _ O
can -X- _ O
exist -X- _ O
at -X- _ O
all -X- _ O
(Goldberg, -X- _ O
2006, -X- _ O
p.166-182). -X- _ O
But -X- _ O
this -X- _ O
raises -X- _ O
the -X- _ O
question -X- _ O
of -X- _ O
what -X- _ O
stacking -X- _ O
more -X- _ O
Transformer -X- _ O
layers -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
actually -X- _ O
achieves -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
the -X- _ O
spread -X- _ O
of -X- _ O
semantic -X- _ O
knowledge, -X- _ O
and -X- _ O
whether -X- _ O
that -X- _ O
is -X- _ O
beneficial. -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
compared -X- _ O
BERT-base -X- _ B-MethodName
and -X- _ O
BERT-large, -X- _ B-MethodName
and -X- _ O
found -X- _ O
that -X- _ O
the -X- _ O
overall -X- _ O
pattern -X- _ O
of -X- _ O
cumulative -X- _ O
score -X- _ O
gains -X- _ O
is -X- _ O
the -X- _ O
same, -X- _ O
only -X- _ O
more -X- _ O
spread -X- _ O
out -X- _ O
in -X- _ O
the -X- _ O
larger -X- _ O
model. -X- _ O
Note -X- _ O
that -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
(2019a)'s -X- _ O
experiments -X- _ O
concern -X- _ O
sentence-level -X- _ O
semantic -X- _ O
relations; -X- _ O
report -X- _ O
that -X- _ O
the -X- _ O
encoding -X- _ O
of -X- _ O
ConceptNet -X- _ O
semantic -X- _ O
relations -X- _ O
is -X- _ O
the -X- _ O
worst -X- _ O
in -X- _ O
the -X- _ O
early -X- _ O
layers -X- _ O
and -X- _ O
increases -X- _ O
towards -X- _ O
the -X- _ O
top. -X- _ O
Jawahar -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
place -X- _ O
"surface -X- _ O
features -X- _ O
in -X- _ O
lower -X- _ O
layers, -X- _ O
syntactic -X- _ O
features -X- _ O
in -X- _ O
middle -X- _ O
layers -X- _ O
and -X- _ O
semantic -X- _ O
features -X- _ O
in -X- _ O
higher -X- _ O
layers", -X- _ O
but -X- _ O
their -X- _ O
conclusion -X- _ O
is -X- _ O
surprising, -X- _ O
given -X- _ O
that -X- _ O
only -X- _ O
one -X- _ O
semantic -X- _ O
task -X- _ O
in -X- _ O
this -X- _ O
study -X- _ O
actually -X- _ O
topped -X- _ O
at -X- _ O
the -X- _ O
last -X- _ O
layer, -X- _ O
and -X- _ O
three -X- _ O
others -X- _ O
peaked -X- _ O
around -X- _ O
the -X- _ O
middle -X- _ O
and -X- _ O
then -X- _ O
considerably -X- _ O
degraded -X- _ O
by -X- _ O
the -X- _ O
final -X- _ O
layers. -X- _ O

The -X- _ O
first -X- _ O
layer -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
receives -X- _ O
as -X- _ O
input -X- _ O
a -X- _ O
combination -X- _ O
of -X- _ O
token, -X- _ O
segment, -X- _ O
and -X- _ O
positional -X- _ O
embeddings. -X- _ O
It -X- _ O
stands -X- _ O
to -X- _ O
reason -X- _ O
that -X- _ O
the -X- _ O
lower -X- _ O
layers -X- _ O
have -X- _ O
the -X- _ O
most -X- _ O
information -X- _ O
about -X- _ O
linear -X- _ O
word -X- _ O
order. -X- _ O
report -X- _ O
a -X- _ O
decrease -X- _ O
in -X- _ O
the -X- _ O
knowledge -X- _ O
of -X- _ O
linear -X- _ O
word -X- _ O
order -X- _ O
around -X- _ O
layer -X- _ O
4 -X- _ O
in -X- _ O
BERT-base. -X- _ B-MethodName
This -X- _ O
is -X- _ O
accompanied -X- _ O
by -X- _ O
an -X- _ O
increased -X- _ O
knowledge -X- _ O
textualizers. -X- _ O
Furthermore, -X- _ O
the -X- _ O
ELMo-based -X- _ O
models -X- _ O
facilitate -X- _ O
a -X- _ O
controlled -X- _ O
comparison-they -X- _ O
only -X- _ O
differ -X- _ O
in -X- _ O
the -X- _ O
contextualizer -X- _ O
architecture -X- _ O
used. -X- _ O
We -X- _ O
evaluate -X- _ O
how -X- _ O
well -X- _ O
CWR -X- _ O
features -X- _ O
perform -X- _ O
the -X- _ O
pretraining -X- _ O
task-bidirectional -X- _ B-TaskName
language -X- _ I-TaskName
modeling. -X- _ I-TaskName
Specifically, -X- _ O
we -X- _ O
take -X- _ O
the -X- _ O
pretrained -X- _ O
representations -X- _ O
for -X- _ O
each -X- _ O
layer -X- _ O
and -X- _ O
relearn -X- _ O
the -X- _ O
language -X- _ O
model -X- _ O
softmax -X- _ O
classifiers -X- _ O
used -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
next -X- _ O
and -X- _ O
previous -X- _ O
token. -X- _ O
The -X- _ O
ELMo -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
Billion -X- _ O
Word -X- _ O
Benchmark, -X- _ O
so -X- _ O
we -X- _ O
retrain -X- _ O
the -X- _ O
softmax -X- _ O
classifier -X- _ O
on -X- _ O
similar -X- _ O
data -X- _ O
to -X- _ O
mitigate -X- _ O
any -X- _ O
possible -X- _ O
effects -X- _ O
from -X- _ O
domain -X- _ O
shift. -X- _ O
We -X- _ O
split -X- _ O
the -X- _ O
held-out -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
Billion -X- _ B-DatasetName
Word -X- _ I-DatasetName
Benchmark -X- _ I-DatasetName
into -X- _ O
train -X- _ B-HyperparameterName
(80%, -X- _ B-HyperparameterValue
6.2M -X- _ O
tokens) -X- _ O
and -X- _ O
evaluation -X- _ B-HyperparameterName
(20%, -X- _ B-HyperparameterValue
1.6M -X- _ O
tokens) -X- _ O
sets -X- _ O
and -X- _ O
use -X- _ O
this -X- _ O
data -X- _ O
to -X- _ O
retrain -X- _ O
and -X- _ O
evaluate -X- _ O
the -X- _ O
softmax -X- _ O
classifiers. -X- _ O
We -X- _ O
expect -X- _ O
that -X- _ O
biLM -X- _ O
perplexity -X- _ O
will -X- _ O
be -X- _ O
lower -X- _ O
when -X- _ O
training -X- _ O
the -X- _ O
softmax -X- _ O
classifiers -X- _ O
on -X- _ O
representations -X- _ O
from -X- _ O
layers -X- _ O
that -X- _ O
capture -X- _ O
more -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
pretraining -X- _ O
task. -X- _ O

More -X- _ O
recently, -X- _ O
Kobayashi -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
showed -X- _ O
that -X- _ O
the -X- _ O
norms -X- _ O
of -X- _ O
attention-weighted -X- _ O
input -X- _ O
vectors, -X- _ O
which -X- _ O
yield -X- _ O
a -X- _ O
more -X- _ O
intuitive -X- _ O
interpretation -X- _ O
of -X- _ O
self-attention, -X- _ O
reduce -X- _ O
the -X- _ O
attention -X- _ O
to -X- _ O
special -X- _ O
tokens. -X- _ O
However, -X- _ O
even -X- _ O
when -X- _ O
the -X- _ O
attention -X- _ O
weights -X- _ O
are -X- _ O
normed, -X- _ O
it -X- _ O
is -X- _ O
still -X- _ O
not -X- _ O
the -X- _ O
case -X- _ O
that -X- _ O
most -X- _ O
heads -X- _ O
that -X- _ O
do -X- _ O
the -X- _ O
"heavy -X- _ O
lifting" -X- _ O
are -X- _ O
even -X- _ O
potentially -X- _ O
interpretable -X- _ O
(Prasanna -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O
One -X- _ O
methodological -X- _ O
choice -X- _ O
in -X- _ O
in -X- _ O
many -X- _ O
studies -X- _ O
of -X- _ O
attention -X- _ O
is -X- _ O
to -X- _ O
focus -X- _ O
on -X- _ O
inter-word -X- _ O
attention -X- _ O
and -X- _ O
simply -X- _ O
exclude -X- _ O
special -X- _ O
tokens -X- _ O
(e.g. -X- _ O
and -X- _ O
Htut -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019)). -X- _ O
However, -X- _ O
if -X- _ O
attention -X- _ O
to -X- _ O
special -X- _ O
tokens -X- _ O
actually -X- _ O
matters -X- _ O
at -X- _ O
inference -X- _ O
time, -X- _ O
drawing -X- _ O
conclusions -X- _ O
purely -X- _ O
from -X- _ O
inter-word -X- _ O
attention -X- _ O
patterns -X- _ O
does -X- _ O
not -X- _ O
seem -X- _ O
warranted. -X- _ O
The -X- _ O
functions -X- _ O
of -X- _ O
special -X- _ O
tokens -X- _ O
are -X- _ O
not -X- _ O
yet -X- _ O
well -X- _ O
understood. -X- _ O
[CLS] -X- _ O
is -X- _ O
typically -X- _ O
viewed -X- _ O
as -X- _ O
an -X- _ O
aggregated -X- _ O
sentence-level -X- _ O
representation -X- _ O
(although -X- _ O
all -X- _ O
token -X- _ O
representations -X- _ O
also -X- _ O
contain -X- _ O
at -X- _ O
least -X- _ O
some -X- _ O
sentence-level -X- _ O
information, -X- _ O
as -X- _ O
discussed -X- _ O
in -X- _ O
subsection -X- _ O
4.1); -X- _ O
in -X- _ O
that -X- _ O
case, -X- _ O
we -X- _ O
may -X- _ O
not -X- _ O
see -X- _ O
e.g. -X- _ O
full -X- _ O
syntactic -X- _ O
trees -X- _ O
in -X- _ O
inter-word -X- _ O
attention -X- _ O
because -X- _ O
part -X- _ O
of -X- _ O
that -X- _ O
information -X- _ O
is -X- _ O
actually -X- _ O
packed -X- _ O
in -X- _ O
[CLS]. -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
experiment -X- _ O
with -X- _ O
encoding -X- _ O
Wikipedia -X- _ O
paragraphs -X- _ O
with -X- _ O
base -X- _ B-MethodName
BERT -X- _ I-MethodName
to -X- _ O
consider -X- _ O
specifically -X- _ O
the -X- _ O
attention -X- _ O
to -X- _ O
special -X- _ O
tokens, -X- _ O
noting -X- _ O
that -X- _ O
heads -X- _ O
in -X- _ O
early -X- _ O
layers -X- _ O
attend -X- _ O
more -X- _ O
to -X- _ O
[CLS], -X- _ O
in -X- _ O
middle -X- _ O
layers -X- _ O
to -X- _ O
[SEP], -X- _ O
and -X- _ O
in -X- _ O
final -X- _ O
layers -X- _ O
to -X- _ O
periods -X- _ O
and -X- _ O
commas. -X- _ O
They -X- _ O
hypothesize -X- _ O
that -X- _ O
its -X- _ O
function -X- _ O
might -X- _ O
be -X- _ O
one -X- _ O
of -X- _ O
"no-op", -X- _ O
a -X- _ O
signal -X- _ O
to -X- _ O
ignore -X- _ O
the -X- _ O
head -X- _ O
if -X- _ O
its -X- _ O
pattern -X- _ O
is -X- _ O
not -X- _ O
applicable -X- _ O
to -X- _ O
the -X- _ O
current -X- _ O
case. -X- _ O
As -X- _ O
a -X- _ O
result, -X- _ O
for -X- _ O
example, -X- _ O
[SEP] -X- _ O
gets -X- _ O
increased -X- _ O
attention -X- _ O
starting -X- _ O
in -X- _ O
layer -X- _ O
5, -X- _ O
but -X- _ O
its -X- _ O
importance -X- _ O
for -X- _ O
prediction -X- _ O
drops. -X- _ O
However, -X- _ O
after -X- _ O
fine-tuning -X- _ O
both -X- _ O
[SEP] -X- _ O
and -X- _ O
[CLS] -X- _ O
get -X- _ O
a -X- _ O
lot -X- _ O
of -X- _ O
attention, -X- _ O
depending -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
(Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Interestingly, -X- _ O
BERT -X- _ O
also -X- _ O
pays -X- _ O
a -X- _ O
lot -X- _ O
of -X- _ O
attention -X- _ O
to -X- _ O
punctuation, -X- _ O
which -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
explain -X- _ O
by -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
periods -X- _ O
and -X- _ O
commas -X- _ O
are -X- _ O
simply -X- _ O
almost -X- _ O
as -X- _ O
frequent -X- _ O
as -X- _ O
the -X- _ O
special -X- _ O
tokens, -X- _ O
and -X- _ O
so -X- _ O
the -X- _ O
model -X- _ O
might -X- _ O
learn -X- _ O
to -X- _ O
rely -X- _ O
on -X- _ O
them -X- _ O
for -X- _ O
the -X- _ O
same -X- _ O
reasons. -X- _ O

The -X- _ O
"heterogeneous" -X- _ O
attention -X- _ O
pattern -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
could -X- _ O
potentially -X- _ O
be -X- _ O
linguistically -X- _ O
interpretable, -X- _ O
and -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
studies -X- _ O
focused -X- _ O
on -X- _ O
identifying -X- _ O
the -X- _ O
functions -X- _ O
of -X- _ O
self-attention -X- _ O
heads. -X- _ O
In -X- _ O
particular, -X- _ O
some -X- _ O
BERT -X- _ B-MethodName
heads -X- _ O
seem -X- _ O
to -X- _ O
specialize -X- _ O
in -X- _ O
certain -X- _ O
types -X- _ O
of -X- _ O
syntactic -X- _ O
relations. -X- _ O
Htut -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
and -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
report -X- _ O
that -X- _ O
there -X- _ O
are -X- _ O
BERT -X- _ B-MethodName
heads -X- _ O
that -X- _ O
attended -X- _ O
significantly -X- _ O
more -X- _ O
than -X- _ O
a -X- _ O
random -X- _ O
baseline -X- _ O
to -X- _ O
words -X- _ O
in -X- _ O
certain -X- _ O
syntactic -X- _ O
positions. -X- _ O
The -X- _ O
datasets -X- _ O
and -X- _ O
methods -X- _ O
used -X- _ O
in -X- _ O
these -X- _ O
studies -X- _ O
differ, -X- _ O
but -X- _ O
they -X- _ O
both -X- _ O
find -X- _ O
that -X- _ O
there -X- _ O
are -X- _ O
heads -X- _ O
that -X- _ O
attend -X- _ O
to -X- _ O
words -X- _ O
in -X- _ O
obj -X- _ O
role -X- _ O
more -X- _ O
than -X- _ O
the -X- _ O
positional -X- _ O
baseline. -X- _ O
The -X- _ O
evidence -X- _ O
for -X- _ O
nsubj, -X- _ O
advmod, -X- _ O
and -X- _ O
amod -X- _ O
varies -X- _ O
between -X- _ O
these -X- _ O
two -X- _ O
studies. -X- _ O
The -X- _ O
overall -X- _ O
conclusion -X- _ O
is -X- _ O
also -X- _ O
supported -X- _ O
by -X- _ O
Voita -X- _ O
et -X- _ O
al. -X- _ O
(2019b)'s -X- _ O
study -X- _ O
of -X- _ O
the -X- _ O
base -X- _ B-MethodName
Transformer -X- _ I-MethodName
in -X- _ O
machine -X- _ B-TaskName
translation -X- _ I-TaskName
context. -X- _ O
Hoover -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
hypothesize -X- _ O
that -X- _ O
even -X- _ O
complex -X- _ O
dependencies -X- _ O
like -X- _ O
dobj -X- _ O
are -X- _ O
encoded -X- _ O
by -X- _ O
a -X- _ O
combination -X- _ O
of -X- _ O
heads -X- _ O
rather -X- _ O
than -X- _ O
a -X- _ O
single -X- _ O
head, -X- _ O
but -X- _ O
this -X- _ O
work -X- _ O
is -X- _ O
limited -X- _ O
to -X- _ O
qualitative -X- _ O
analysis. -X- _ O
Zhao -X- _ O
and -X- _ O
Bethard -X- _ O
(2020) -X- _ O
looked -X- _ O
specifically -X- _ O
for -X- _ O
the -X- _ O
heads -X- _ O
encoding -X- _ O
negation -X- _ O
scope. -X- _ O
Both -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
and -X- _ O
Htut -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
conclude -X- _ O
that -X- _ O
no -X- _ O
single -X- _ O
head -X- _ O
has -X- _ O
the -X- _ O
complete -X- _ O
syntactic -X- _ O
tree -X- _ O
information, -X- _ O
in -X- _ O
line -X- _ O
with -X- _ O
evidence -X- _ O
of -X- _ O
partial -X- _ O
knowledge -X- _ O
of -X- _ O
syntax -X- _ O
(cf. -X- _ O
subsection -X- _ O
3.1). -X- _ O
However, -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
identify -X- _ O
a -X- _ O
BERT -X- _ B-MethodName
head -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
used -X- _ O
as -X- _ O
a -X- _ O
classifier -X- _ O
to -X- _ O
perform -X- _ O
coreference -X- _ O
resolution -X- _ O
on -X- _ O
par -X- _ O
with -X- _ O
a -X- _ O
rule-based -X- _ O
system, -X- _ O
which -X- _ O
by -X- _ O
itself -X- _ O
would -X- _ O
seem -X- _ O
to -X- _ O
require -X- _ O
quite -X- _ O
a -X- _ O
lot -X- _ O
of -X- _ O
syntactic -X- _ O
knowledge. -X- _ O
present -X- _ O
evidence -X- _ O
that -X- _ O
attention -X- _ O
weights -X- _ O
are -X- _ O
weak -X- _ O
indicators -X- _ O
of -X- _ O
subjectverb -X- _ O
agreement -X- _ O
and -X- _ O
reflexive -X- _ O
anaphora. -X- _ O
Instead -X- _ O
of -X- _ O
serving -X- _ O
as -X- _ O
strong -X- _ O
pointers -X- _ O
between -X- _ O
tokens -X- _ O
that -X- _ O
should -X- _ O
be -X- _ O
related, -X- _ O
BERT's -X- _ B-MethodName
self-attention -X- _ O
weights -X- _ O
were -X- _ O
close -X- _ O
to -X- _ O
a -X- _ O
uniform -X- _ O
attention -X- _ O
baseline, -X- _ O
but -X- _ O
there -X- _ O
was -X- _ O
some -X- _ O
sensitivity -X- _ O
to -X- _ O
different -X- _ O
types -X- _ O
of -X- _ O
distractors -X- _ O
coherent -X- _ O
with -X- _ O
psycholinguistic -X- _ O
data. -X- _ O
This -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
conclusions -X- _ O
by -X- _ O
Ettinger -X- _ O
(2019). -X- _ O
To -X- _ O
our -X- _ O
knowledge, -X- _ O
morphological -X- _ O
information -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
heads -X- _ O
has -X- _ O
not -X- _ O
been -X- _ O
addressed, -X- _ O
but -X- _ O
with -X- _ O
the -X- _ O
sparse -X- _ O
attention -X- _ O
variant -X- _ O
by -X- _ O
Correia -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
in -X- _ O
the -X- _ O
base -X- _ B-MethodName
Transformer, -X- _ I-MethodName
some -X- _ O
attention -X- _ O
heads -X- _ O
appear -X- _ O
to -X- _ O
merge -X- _ O
BPE-tokenized -X- _ O
words. -X- _ O
For -X- _ O
semantic -X- _ O
relations, -X- _ O
there -X- _ O
are -X- _ O
reports -X- _ O
of -X- _ O
self-attention -X- _ O
heads -X- _ O
encoding -X- _ O
core -X- _ O
frame-semantic -X- _ O
relations -X- _ O
(Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
lexicographic -X- _ O
and -X- _ O
commonsense -X- _ O
relations -X- _ O
. -X- _ O
The -X- _ O
overall -X- _ O
popularity -X- _ O
of -X- _ O
self-attention -X- _ O
as -X- _ O
an -X- _ O
interpretability -X- _ O
mechanism -X- _ O
is -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
idea -X- _ O
that -X- _ O
"attention -X- _ O
weight -X- _ O
has -X- _ O
a -X- _ O
clear -X- _ O
meaning: -X- _ O
how -X- _ O
much -X- _ O
a -X- _ O
particular -X- _ O
word -X- _ O
will -X- _ O
be -X- _ O
weighted -X- _ O
when -X- _ O
computing -X- _ O
the -X- _ O
next -X- _ O
representation -X- _ O
for -X- _ O
the -X- _ O
current -X- _ O
word" -X- _ O
(Clark -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
This -X- _ O
view -X- _ O
is -X- _ O
currently -X- _ O
debated -X- _ O
(Jain -X- _ O
and -X- _ O
Wallace, -X- _ O
2019;Serrano -X- _ O
and -X- _ O
Smith, -X- _ O
2019;Wiegreffe -X- _ O
and -X- _ O
Pinter, -X- _ O
2019;Brunner -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
and -X- _ O
in -X- _ O
a -X- _ O
multi-layer -X- _ O
model -X- _ O
where -X- _ O
attention -X- _ O
is -X- _ O
followed -X- _ O
by -X- _ O
non-linear -X- _ O
transformations, -X- _ O
the -X- _ O
patterns -X- _ O
in -X- _ O
individual -X- _ O
heads -X- _ O
do -X- _ O
not -X- _ O
provide -X- _ O
a -X- _ O
full -X- _ O
picture. -X- _ O
Also, -X- _ O
while -X- _ O
many -X- _ O
current -X- _ O
papers -X- _ O
are -X- _ O
accompanied -X- _ O
by -X- _ O
attention -X- _ O
visualizations, -X- _ O
and -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
growing -X- _ O
number -X- _ O
of -X- _ O
visualization -X- _ O
tools -X- _ O
(Vig, -X- _ O
2019;Hoover -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
the -X- _ O
visualization -X- _ O
is -X- _ O
typically -X- _ O
limited -X- _ O
to -X- _ O
qualitative -X- _ O
analysis -X- _ O
(often -X- _ O
with -X- _ O
cherry-picked -X- _ O
examples) -X- _ O
(Belinkov -X- _ O
and -X- _ O
Glass, -X- _ O
2019), -X- _ O
and -X- _ O
should -X- _ O
not -X- _ O
be -X- _ O
interpreted -X- _ O
as -X- _ O
definitive -X- _ O
evidence. -X- _ O
Kovaleva -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
show -X- _ O
that -X- _ O
most -X- _ O
selfattention -X- _ O
heads -X- _ O
do -X- _ O
not -X- _ O
directly -X- _ O
encode -X- _ O
any -X- _ O
nontrivial -X- _ O
linguistic -X- _ O
information, -X- _ O
at -X- _ O
least -X- _ O
when -X- _ O
finetuned -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
(Wang -X- _ O
et -X- _ O
al., -X- _ O
2018), -X- _ O
since -X- _ O
only -X- _ O
less -X- _ O
than -X- _ O
50% -X- _ O
of -X- _ O
heads -X- _ O
exhibit -X- _ O
the -X- _ O
"heterogeneous" -X- _ O
pattern. -X- _ O
Much -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
produced -X- _ O
the -X- _ O
vertical -X- _ O
pattern -X- _ O
(attention -X- _ O
to -X- _ O
[CLS], -X- _ O
[SEP], -X- _ O
and -X- _ O
punctuation -X- _ O
tokens), -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
observations -X- _ O
by -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019). -X- _ O
This -X- _ O
redundancy -X- _ O
is -X- _ O
likely -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
overparameterization -X- _ O
issue -X- _ O
(see -X- _ O
section -X- _ O
6). -X- _ O

Several -X- _ O
studies -X- _ O
proposed -X- _ O
classification -X- _ O
of -X- _ O
attention -X- _ O
head -X- _ O
types. -X- _ O
Raganato -X- _ O
and -X- _ O
Tiedemann -X- _ O
(2018) -X- _ O
discuss -X- _ O
attending -X- _ O
to -X- _ O
the -X- _ O
token -X- _ O
itself, -X- _ O
previous/next -X- _ O
tokens -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
end. -X- _ O
Clark -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
distinguish -X- _ O
between -X- _ O
attending -X- _ O
to -X- _ O
previous/next -X- _ O
tokens, -X- _ O
[CLS], -X- _ O
[SEP], -X- _ O
punctuation, -X- _ O
and -X- _ O
"attending -X- _ O
broadly" -X- _ O
over -X- _ O
the -X- _ O
sequence. -X- _ O
Kovaleva -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
propose -X- _ O
5 -X- _ O
patterns -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
3. -X- _ O

In -X- _ O
studies -X- _ O
of -X- _ O
BERT, -X- _ B-MethodName
the -X- _ O
term -X- _ O
"embedding" -X- _ O
refers -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
a -X- _ O
Transformer -X- _ O
layer -X- _ O
(typically, -X- _ O
the -X- _ O
final -X- _ O
one). -X- _ O
Both -X- _ O
conventional -X- _ O
static -X- _ O
embeddings -X- _ O
(Mikolov -X- _ O
et -X- _ O
al., -X- _ O
2013) -X- _ O
and -X- _ O
BERT-style -X- _ B-MethodName
embeddings -X- _ O
can -X- _ O
be -X- _ O
viewed -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
mutual -X- _ O
information -X- _ O
maximization -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
latter -X- _ O
are -X- _ O
contextualized. -X- _ O
Every -X- _ O
token -X- _ O
is -X- _ O
represented -X- _ O
by -X- _ O
a -X- _ O
vector -X- _ O
dependent -X- _ O
on -X- _ O
the -X- _ O
particular -X- _ O
context -X- _ O
of -X- _ O
occurrence, -X- _ O
and -X- _ O
contains -X- _ O
at -X- _ O
least -X- _ O
some -X- _ O
information -X- _ O
about -X- _ O
that -X- _ O
context -X- _ O
(Miaschi -X- _ O
and -X- _ O
Dell'Orletta, -X- _ O
2020). -X- _ O
Several -X- _ O
studies -X- _ O
reported -X- _ O
that -X- _ O
distilled -X- _ O
contextualized -X- _ O
embeddings -X- _ O
better -X- _ O
encode -X- _ O
lexical -X- _ O
semantic -X- _ O
information -X- _ O
(i.e. -X- _ O
they -X- _ O
are -X- _ O
better -X- _ O
at -X- _ O
traditional -X- _ O
word-level -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
word -X- _ O
similarity). -X- _ O
The -X- _ O
methods -X- _ O
to -X- _ O
distill -X- _ O
a -X- _ O
contextualized -X- _ O
representation -X- _ O
into -X- _ O
static -X- _ O
include -X- _ O
aggregating -X- _ O
the -X- _ O
information -X- _ O
across -X- _ O
multiple -X- _ O
contexts -X- _ O
(Akbik -X- _ O
et -X- _ O
al., -X- _ O
2019;Bommasani -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
encoding -X- _ O
"semantically -X- _ O
bleached" -X- _ O
sentences -X- _ O
that -X- _ O
rely -X- _ O
almost -X- _ O
exclusively -X- _ O
on -X- _ O
the -X- _ O
meaning -X- _ O
of -X- _ O
a -X- _ O
given -X- _ O
word -X- _ O
(e.g. -X- _ O
"This -X- _ O
is -X- _ O
<>") -X- _ O
(May -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
and -X- _ O
even -X- _ O
using -X- _ O
contextualized -X- _ O
embeddings -X- _ O
to -X- _ O
train -X- _ O
static -X- _ O
embeddings -X- _ O
(Wang -X- _ O
et -X- _ O
al., -X- _ O
2020d). -X- _ O
But -X- _ O
this -X- _ O
is -X- _ O
not -X- _ O
to -X- _ O
say -X- _ O
that -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
room -X- _ O
for -X- _ O
improvement. -X- _ O
Ethayarajh -X- _ O
(2019) -X- _ O
measure -X- _ O
how -X- _ O
similar -X- _ O
the -X- _ O
embeddings -X- _ O
for -X- _ O
identical -X- _ O
words -X- _ O
are -X- _ O
in -X- _ O
every -X- _ O
layer, -X- _ O
reporting -X- _ O
that -X- _ O
later -X- _ O
BERT -X- _ B-MethodName
layers -X- _ O
produce -X- _ O
more -X- _ O
context-specific -X- _ O
representations -X- _ O
3 -X- _ O
. -X- _ O
They -X- _ O
also -X- _ O
find -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
embeddings -X- _ O
occupy -X- _ O
a -X- _ O
narrow -X- _ O
cone -X- _ O
in -X- _ O
the -X- _ O
vector -X- _ O
space, -X- _ O
and -X- _ O
this -X- _ O
effect -X- _ O
increases -X- _ O
from -X- _ O
the -X- _ O
earlier -X- _ O
to -X- _ O
later -X- _ O
layers. -X- _ O
That -X- _ O
is, -X- _ O
two -X- _ O
random -X- _ O
words -X- _ O
will -X- _ O
on -X- _ O
average -X- _ O
have -X- _ O
a -X- _ O
much -X- _ O
higher -X- _ O
cosine -X- _ B-MetricName
similarity -X- _ I-MetricName
than -X- _ O
expected -X- _ O
if -X- _ O
embeddings -X- _ O
were -X- _ O
directionally -X- _ O
uniform -X- _ O
(isotropic). -X- _ O
Since -X- _ O
isotropy -X- _ O
was -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
beneficial -X- _ O
for -X- _ O
static -X- _ O
word -X- _ O
embeddings -X- _ O
(Mu -X- _ O
and -X- _ O
Viswanath, -X- _ O
2018), -X- _ O
this -X- _ O
might -X- _ O
be -X- _ O
a -X- _ O
fruitful -X- _ O
direction -X- _ O
to -X- _ O
explore -X- _ O
for -X- _ O
BERT. -X- _ B-MethodName
Since -X- _ O
BERT -X- _ B-MethodName
embeddings -X- _ O
are -X- _ O
contextualized, -X- _ O
an -X- _ O
interesting -X- _ O
question -X- _ O
is -X- _ O
to -X- _ O
what -X- _ O
extent -X- _ O
they -X- _ O
capture -X- _ O
phenomena -X- _ O
like -X- _ O
polysemy -X- _ O
and -X- _ O
homonymy. -X- _ O
There -X- _ O
is -X- _ O
indeed -X- _ O
evidence -X- _ O
that -X- _ O
BERT's -X- _ B-MethodName
contextualized -X- _ O
embeddings -X- _ O
form -X- _ O
distinct -X- _ O
clusters -X- _ O
corresponding -X- _ O
to -X- _ O
word -X- _ O
senses -X- _ O
(Wiedemann -X- _ O
et -X- _ O
al., -X- _ O
2019;Schmidt -X- _ O
and -X- _ O
Hofmann, -X- _ O
2020), -X- _ O
making -X- _ O
BERT -X- _ B-MethodName
successful -X- _ O
at -X- _ O
word -X- _ B-TaskName
sense -X- _ I-TaskName
disambiguation -X- _ I-TaskName
task. -X- _ O
However, -X- _ O
Mickus -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
note -X- _ O
that -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
word -X- _ O
depend -X- _ O
on -X- _ O
the -X- _ O
position -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
in -X- _ O
which -X- _ O
it -X- _ O
occurs, -X- _ O
likely -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
NSP -X- _ B-TaskName
objective. -X- _ O
This -X- _ O
is -X- _ O
not -X- _ O
desirable -X- _ O
from -X- _ O
the -X- _ O
linguistic -X- _ O
point -X- _ O
of -X- _ O
view, -X- _ O
and -X- _ O
could -X- _ O
be -X- _ O
a -X- _ O
promising -X- _ O
avenue -X- _ O
for -X- _ O
future -X- _ O
work. -X- _ O
The -X- _ O
above -X- _ O
discussion -X- _ O
concerns -X- _ O
token -X- _ O
embeddings, -X- _ O
but -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
typically -X- _ O
used -X- _ O
as -X- _ O
a -X- _ O
sentence -X- _ O
or -X- _ O
text -X- _ O
encoder. -X- _ O
The -X- _ O
standard -X- _ O
way -X- _ O
to -X- _ O
generate -X- _ O
sentence -X- _ O
or -X- _ O
text -X- _ O
representations -X- _ O
for -X- _ O
classification -X- _ O
is -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
[CLS] -X- _ O
token, -X- _ O
but -X- _ O
alternatives -X- _ O
are -X- _ O
also -X- _ O
being -X- _ O
discussed, -X- _ O
including -X- _ O
concatenation -X- _ O
of -X- _ O
token -X- _ O
representations -X- _ O
(Tanaka -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
normalized -X- _ O
mean -X- _ O
(Tanaka -X- _ O
et -X- _ O
al., -X- _ O
2020), -X- _ O
and -X- _ O
layer -X- _ O
activations -X- _ O
. -X- _ O
See -X- _ O
Toshniwal -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
for -X- _ O
a -X- _ O
systematic -X- _ O
comparison -X- _ O
of -X- _ O
several -X- _ O
methods -X- _ O
across -X- _ O
tasks -X- _ O
and -X- _ O
sentence -X- _ O
encoders. -X- _ O

Multiple -X- _ O
probing -X- _ O
studies -X- _ O
in -X- _ O
section -X- _ O
3 -X- _ O
and -X- _ O
section -X- _ O
4 -X- _ O
report -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
possesses -X- _ O
a -X- _ O
surprising -X- _ O
amount -X- _ O
of -X- _ O
syntactic, -X- _ O
semantic, -X- _ O
and -X- _ O
world -X- _ O
knowledge. -X- _ O
However, -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
(2019a) -X- _ O
remarks, -X- _ O
"the -X- _ O
fact -X- _ O
that -X- _ O
a -X- _ O
linguistic -X- _ O
pattern -X- _ O
is -X- _ O
not -X- _ O
observed -X- _ O
by -X- _ O
our -X- _ O
probing -X- _ O
classifier -X- _ O
does -X- _ O
not -X- _ O
guarantee -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
there, -X- _ O
and -X- _ O
the -X- _ O
observation -X- _ O
of -X- _ O
a -X- _ O
pattern -X- _ O
does -X- _ O
not -X- _ O
tell -X- _ O
us -X- _ O
how -X- _ O
it -X- _ O
is -X- _ O
used." -X- _ O
There -X- _ O
is -X- _ O
also -X- _ O
the -X- _ O
issue -X- _ O
of -X- _ O
how -X- _ O
complex -X- _ O
a -X- _ O
probe -X- _ O
should -X- _ O
be -X- _ O
allowed -X- _ O
to -X- _ O
be -X- _ O
. -X- _ O
If -X- _ O
a -X- _ O
more -X- _ O
complex -X- _ O
probe -X- _ O
recovers -X- _ O
more -X- _ O
information, -X- _ O
to -X- _ O
what -X- _ O
extent -X- _ O
are -X- _ O
we -X- _ O
still -X- _ O
relying -X- _ O
on -X- _ O
the -X- _ O
original -X- _ O
model? -X- _ O
Furthermore, -X- _ O
different -X- _ O
probing -X- _ O
methods -X- _ O
may -X- _ O
lead -X- _ O
to -X- _ O
complementary -X- _ O
or -X- _ O
even -X- _ O
contradictory -X- _ O
conclusions, -X- _ O
which -X- _ O
makes -X- _ O
a -X- _ O
single -X- _ O
test -X- _ O
(as -X- _ O
in -X- _ O
most -X- _ O
stud- -X- _ O
(Kovaleva -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
ies) -X- _ O
insufficient -X- _ O
(Warstadt -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
A -X- _ O
given -X- _ O
method -X- _ O
might -X- _ O
also -X- _ O
favor -X- _ O
one -X- _ O
model -X- _ O
over -X- _ O
another, -X- _ O
e.g., -X- _ O
RoBERTa -X- _ O
trails -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
one -X- _ O
tree -X- _ O
extraction -X- _ O
method, -X- _ O
but -X- _ O
leads -X- _ O
with -X- _ O
another -X- _ O
(Htut -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
The -X- _ O
choice -X- _ O
of -X- _ O
linguistic -X- _ O
formalism -X- _ O
also -X- _ O
matters -X- _ O
(Kuznetsov -X- _ O
and -X- _ O
Gurevych, -X- _ O
2020). -X- _ O
In -X- _ O
view -X- _ O
of -X- _ O
all -X- _ O
that, -X- _ O
the -X- _ O
alternative -X- _ O
is -X- _ O
to -X- _ O
focus -X- _ O
on -X- _ O
identifying -X- _ O
what -X- _ O
BERT -X- _ B-MethodName
actually -X- _ O
relies -X- _ O
on -X- _ O
at -X- _ O
inference -X- _ O
time. -X- _ O
This -X- _ O
direction -X- _ O
is -X- _ O
currently -X- _ O
pursued -X- _ O
both -X- _ O
at -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
architecture -X- _ O
blocks -X- _ O
(to -X- _ O
be -X- _ O
discussed -X- _ O
in -X- _ O
detail -X- _ O
in -X- _ O
subsection -X- _ O
6.3), -X- _ O
and -X- _ O
at -X- _ O
the -X- _ O
level -X- _ O
of -X- _ O
information -X- _ O
encoded -X- _ O
in -X- _ O
model -X- _ O
weights. -X- _ O
Amnesic -X- _ O
probing -X- _ O
(Elazar -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
aims -X- _ O
to -X- _ O
specifically -X- _ O
remove -X- _ O
certain -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
see -X- _ O
how -X- _ O
it -X- _ O
changes -X- _ O
performance, -X- _ O
finding, -X- _ O
for -X- _ O
example, -X- _ O
that -X- _ O
language -X- _ O
modeling -X- _ O
does -X- _ O
rely -X- _ O
on -X- _ O
part-of-speech -X- _ O
information. -X- _ O
Another -X- _ O
direction -X- _ O
is -X- _ O
information-theoretic -X- _ B-TaskName
probing. -X- _ I-TaskName
Pimentel -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
operationalize -X- _ O
probing -X- _ O
as -X- _ O
estimating -X- _ O
mutual -X- _ O
information -X- _ O
between -X- _ O
the -X- _ O
learned -X- _ O
representation -X- _ O
and -X- _ O
a -X- _ O
given -X- _ O
linguistic -X- _ O
property, -X- _ O
which -X- _ O
highlights -X- _ O
that -X- _ O
the -X- _ O
focus -X- _ O
should -X- _ O
be -X- _ O
not -X- _ O
on -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
information -X- _ O
contained -X- _ O
in -X- _ O
a -X- _ O
representation, -X- _ O
but -X- _ O
rather -X- _ O
on -X- _ O
how -X- _ O
easily -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
extracted -X- _ O
from -X- _ O
it. -X- _ O
Voita -X- _ O
and -X- _ O
Titov -X- _ O
(2020) -X- _ O
quantify -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
effort -X- _ O
needed -X- _ O
to -X- _ O
extract -X- _ O
information -X- _ O
from -X- _ O
a -X- _ O
given -X- _ O
representation -X- _ O
as -X- _ O
minimum -X- _ O
description -X- _ O
length -X- _ O
needed -X- _ O
to -X- _ O
communicate -X- _ O
both -X- _ O
the -X- _ O
probe -X- _ O
size -X- _ O
and -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
data -X- _ O
required -X- _ O
for -X- _ O
it -X- _ O
to -X- _ O
do -X- _ O
well -X- _ O
on -X- _ O
a -X- _ O
task. -X- _ O

Recently, -X- _ O
pretrained -X- _ O
high-capacity -X- _ O
language -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
ELMo -X- _ O
(Peters -X- _ O
et -X- _ O
al., -X- _ O
2018a) -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2018a) -X- _ O
have -X- _ O
become -X- _ O
increasingly -X- _ O
important -X- _ O
in -X- _ O
NLP. -X- _ O
They -X- _ O
are -X- _ O
optimised -X- _ O
to -X- _ O
either -X- _ O
predict -X- _ O
the -X- _ O
next -X- _ O
word -X- _ O
in -X- _ O
a -X- _ O
sequence -X- _ O
or -X- _ O
some -X- _ O
masked -X- _ O
word -X- _ O
anywhere -X- _ O
in -X- _ O
a -X- _ O
given -X- _ O
sequence -X- _ O
(e.g. -X- _ O
"Dante -X- _ O
was -X- _ O
born -X- _ O
in -X- _ O
[Mask] -X- _ O
in -X- _ O
the -X- _ O
year -X- _ O
1265."). -X- _ O
The -X- _ O
parameters -X- _ O
of -X- _ O
these -X- _ O
models -X- _ O
appear -X- _ O
to -X- _ O
store -X- _ O
vast -X- _ O
amounts -X- _ O
of -X- _ O
linguistic -X- _ O
knowledge -X- _ O
(Peters -X- _ O
et -X- _ O
al., -X- _ O
2018b;Goldberg, -X- _ O
2019;Tenney -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
useful -X- _ O
for -X- _ O
downstream -X- _ O
tasks. -X- _ O
This -X- _ O
knowledge -X- _ O
is -X- _ O
usually -X- _ O
accessed -X- _ O
either -X- _ O
by -X- _ O
conditioning -X- _ O
on -X- _ O
latent -X- _ O
context -X- _ O
representations -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
original -X- _ O
model -X- _ O
or -X- _ O
by -X- _ O
using -X- _ O
the -X- _ O
original -X- _ O
model -X- _ O
weights -X- _ O
to -X- _ O
initialize -X- _ O
a -X- _ O
task-specific -X- _ O
model -X- _ O
which -X- _ O
is -X- _ O
then -X- _ O
further -X- _ O
fine-tuned. -X- _ O
This -X- _ O
type -X- _ O
of -X- _ O
knowledge -X- _ O
transfer -X- _ O
is -X- _ O
crucial -X- _ O
for -X- _ O
current -X- _ O
state-of-the-art -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
tasks. -X- _ O
In -X- _ O
contrast, -X- _ O
knowledge -X- _ O
bases -X- _ O
are -X- _ O
effective -X- _ O
solutions -X- _ O
for -X- _ O
accessing -X- _ O
annotated -X- _ O
gold-standard -X- _ O
relational -X- _ O
data -X- _ O
by -X- _ O
enabling -X- _ O
queries -X- _ O
such -X- _ O
as -X- _ O
(Dante, -X- _ O
born-in, -X- _ O
X). -X- _ O
However, -X- _ O
in -X- _ O
practice -X- _ O
we -X- _ O
often -X- _ O
need -X- _ O
to -X- _ O
extract -X- _ O
relational -X- _ O
data -X- _ O
from -X- _ O
text -X- _ O
or -X- _ O
other -X- _ O
modalities -X- _ O
to -X- _ O
populate -X- _ O
these -X- _ O
knowledge -X- _ O
bases. -X- _ O
This -X- _ O
requires -X- _ O
complex -X- _ O
NLP -X- _ O
pipelines -X- _ O
involving -X- _ O
entity -X- _ O
extraction, -X- _ O
coreference -X- _ O
resolution, -X- _ O
entity -X- _ O
linking -X- _ O
and -X- _ O
relation -X- _ O
extraction -X- _ O
(Surdeanu -X- _ O
and -X- _ O
Ji, -X- _ O
2014)components -X- _ O
that -X- _ O
often -X- _ O
need -X- _ O
supervised -X- _ O
data -X- _ O
and -X- _ O
fixed -X- _ O
schemas. -X- _ O
Moreover, -X- _ O
errors -X- _ O
can -X- _ O
easily -X- _ O
propagate -X- _ O
and -X- _ O
accumulate -X- _ O
throughout -X- _ O
the -X- _ O
pipeline. -X- _ O
Instead, -X- _ O
we -X- _ O
could -X- _ O
attempt -X- _ O
to -X- _ O
query -X- _ O
neural -X- _ O
language -X- _ O
models -X- _ O
for -X- _ O
relational -X- _ O
data -X- _ O
by -X- _ O
asking -X- _ O
them -X- _ O
to -X- _ O
fill -X- _ O
in -X- _ O
masked -X- _ O
tokens -X- _ O
in -X- _ O
sequences -X- _ O
like -X- _ O
"Dante -X- _ O
was -X- _ O
born -X- _ O
arXiv:1909.01066v2 -X- _ O
[cs.CL] -X- _ O
4 -X- _ O
Sep -X- _ O
2019 -X- _ O
(Petroni -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
blanks -X- _ O
(e.g. -X- _ O
"Cats -X- _ O
like -X- _ O
to -X- _ O
chase -X- _ O
[___]"). -X- _ O
Petroni -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
showed -X- _ O
that, -X- _ O
for -X- _ O
some -X- _ O
relation -X- _ O
types, -X- _ O
vanilla -X- _ B-MethodName
BERT -X- _ I-MethodName
is -X- _ O
competitive -X- _ O
with -X- _ O
methods -X- _ O
relying -X- _ O
on -X- _ O
knowledge -X- _ O
bases -X- _ O
(Figure -X- _ O
2), -X- _ O
and -X- _ O
Roberts -X- _ O
et -X- _ O
al. -X- _ O
(2020) -X- _ O
show -X- _ O
the -X- _ O
same -X- _ O
for -X- _ O
open-domain -X- _ B-TaskName
QA -X- _ I-TaskName
using -X- _ O
T5 -X- _ B-MethodName
model -X- _ O
(Raffel -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Davison -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
suggest -X- _ O
that -X- _ O
it -X- _ O
generalizes -X- _ O
better -X- _ O
to -X- _ O
unseen -X- _ O
data. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
retrieve -X- _ O
BERT's -X- _ O
knowledge, -X- _ O
we -X- _ O
need -X- _ O
good -X- _ O
template -X- _ O
sentences, -X- _ O
and -X- _ O
there -X- _ O
is -X- _ O
work -X- _ O
on -X- _ O
their -X- _ O
automatic -X- _ O
extraction -X- _ O
and -X- _ O
augmentation -X- _ O
(Bouraoui -X- _ O
et -X- _ O
al., -X- _ O
2019;. -X- _ O
However, -X- _ O
BERT -X- _ O
cannot -X- _ O
reason -X- _ O
based -X- _ O
on -X- _ O
its -X- _ O
world -X- _ O
knowledge. -X- _ O
Forbes -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
show -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
can -X- _ O
"guess" -X- _ O
the -X- _ O
affordances -X- _ O
and -X- _ O
properties -X- _ O
of -X- _ O
many -X- _ O
objects, -X- _ O
but -X- _ O
can -X- _ O
not -X- _ O
reason -X- _ O
about -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
properties -X- _ O
and -X- _ O
affordances. -X- _ O
For -X- _ O
example, -X- _ O
it -X- _ O
"knows" -X- _ O
that -X- _ O
people -X- _ O
can -X- _ O
walk -X- _ O
into -X- _ O
houses, -X- _ O
and -X- _ O
that -X- _ O
houses -X- _ O
are -X- _ O
big, -X- _ O
but -X- _ O
it -X- _ O
cannot -X- _ O
infer -X- _ O
that -X- _ O
houses -X- _ O
are -X- _ O
bigger -X- _ O
than -X- _ O
people. -X- _ O
and -X- _ O
Richardson -X- _ O
and -X- _ O
Sabharwal -X- _ O
(2019) -X- _ O
also -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
performance -X- _ O
drops -X- _ O
with -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
necessary -X- _ O
inference -X- _ O
steps. -X- _ O
Some -X- _ O
of -X- _ O
BERT's -X- _ B-MethodName
world -X- _ O
knowledge -X- _ O
success -X- _ O
comes -X- _ O
from -X- _ O
learning -X- _ O
stereotypical -X- _ O
associations -X- _ O
(Poerner -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
e.g., -X- _ O
a -X- _ O
person -X- _ O
with -X- _ O
an -X- _ O
Italian-sounding -X- _ O
name -X- _ O
is -X- _ O
predicted -X- _ O
to -X- _ O
be -X- _ O
Italian, -X- _ O
even -X- _ O
when -X- _ O
it -X- _ O
is -X- _ O
incorrect. -X- _ O

Recent -X- _ O
progress -X- _ O
in -X- _ O
pretraining -X- _ O
language -X- _ O
models -X- _ O
on -X- _ O
large -X- _ O
textual -X- _ O
corpora -X- _ O
led -X- _ O
to -X- _ O
a -X- _ O
surge -X- _ O
of -X- _ O
improvements -X- _ O
for -X- _ O
downstream -X- _ O
NLP -X- _ O
tasks. -X- _ O
Whilst -X- _ O
learning -X- _ O
linguistic -X- _ O
knowledge, -X- _ O
these -X- _ O
models -X- _ O
may -X- _ O
also -X- _ O
be -X- _ O
storing -X- _ O
relational -X- _ O
knowledge -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
data, -X- _ O
and -X- _ O
may -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
answer -X- _ O
queries -X- _ O
structured -X- _ O
as -X- _ O
"fillin-the-blank" -X- _ O
cloze -X- _ O
statements. -X- _ O
Language -X- _ O
models -X- _ O
have -X- _ O
many -X- _ O
advantages -X- _ O
over -X- _ O
structured -X- _ O
knowledge -X- _ O
bases: -X- _ O
they -X- _ O
require -X- _ O
no -X- _ O
schema -X- _ O
engineering, -X- _ O
allow -X- _ O
practitioners -X- _ O
to -X- _ O
query -X- _ O
about -X- _ O
an -X- _ O
open -X- _ O
class -X- _ O
of -X- _ O
relations, -X- _ O
are -X- _ O
easy -X- _ O
to -X- _ O
extend -X- _ O
to -X- _ O
more -X- _ O
data, -X- _ O
and -X- _ O
require -X- _ O
no -X- _ O
human -X- _ O
supervision -X- _ O
to -X- _ O
train. -X- _ O
We -X- _ O
present -X- _ O
an -X- _ O
in-depth -X- _ O
analysis -X- _ O
of -X- _ O
the -X- _ O
relational -X- _ O
knowledge -X- _ O
already -X- _ O
present -X- _ O
(without -X- _ O
fine-tuning) -X- _ O
in -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
state-of-theart -X- _ O
pretrained -X- _ O
language -X- _ O
models. -X- _ O
We -X- _ O
find -X- _ O
that -X- _ O
(i) -X- _ O
without -X- _ O
fine-tuning, -X- _ O
BERT -X- _ B-MethodName
contains -X- _ O
relational -X- _ O
knowledge -X- _ O
competitive -X- _ O
with -X- _ O
traditional -X- _ O
NLP -X- _ O
methods -X- _ O
that -X- _ O
have -X- _ O
some -X- _ O
access -X- _ O
to -X- _ O
oracle -X- _ O
knowledge, -X- _ O
(ii) -X- _ O
BERT -X- _ B-MethodName
also -X- _ O
does -X- _ O
remarkably -X- _ O
well -X- _ O
on -X- _ O
open-domain -X- _ B-TaskName
question -X- _ I-TaskName
answering -X- _ I-TaskName
against -X- _ O
a -X- _ O
supervised -X- _ O
baseline, -X- _ O
and -X- _ O
(iii) -X- _ O
certain -X- _ O
types -X- _ O
of -X- _ O
factual -X- _ O
knowledge -X- _ O
are -X- _ O
learned -X- _ O
much -X- _ O
more -X- _ O
readily -X- _ O
than -X- _ O
others -X- _ O
by -X- _ O
standard -X- _ O
language -X- _ O
model -X- _ O
pretraining -X- _ O
approaches. -X- _ O
The -X- _ O
surprisingly -X- _ O
strong -X- _ O
ability -X- _ O
of -X- _ O
these -X- _ O
models -X- _ O
to -X- _ O
recall -X- _ O
factual -X- _ O
knowledge -X- _ O
without -X- _ O
any -X- _ O
fine-tuning -X- _ O
demonstrates -X- _ O
their -X- _ O
potential -X- _ O
as -X- _ O
unsupervised -X- _ O
open-domain -X- _ O
QA -X- _ O
systems. -X- _ O
The -X- _ O
code -X- _ O
to -X- _ O
reproduce -X- _ O
our -X- _ O
analysis -X- _ O
is -X- _ O
available -X- _ O
at -X- _ O
https: -X- _ O
//github.com/facebookresearch/LAMA. -X- _ O

The -X- _ O
bulk -X- _ O
of -X- _ O
evidence -X- _ O
about -X- _ O
commonsense -X- _ O
knowledge -X- _ O
captured -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
comes -X- _ O
from -X- _ O
practitioners -X- _ O
using -X- _ O
it -X- _ O
to -X- _ O
extract -X- _ O
such -X- _ O
knowledge. -X- _ O
One -X- _ O
direct -X- _ O
probing -X- _ O
study -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
reports -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
struggles -X- _ O
with -X- _ O
pragmatic -X- _ O
inference -X- _ O
and -X- _ O
role-based -X- _ O
event -X- _ O
knowledge -X- _ O
(Ettinger, -X- _ O
2019). -X- _ O
BERT -X- _ B-MethodName
also -X- _ O
struggles -X- _ O
with -X- _ O
abstract -X- _ O
attributes -X- _ O
of -X- _ O
objects, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
visual -X- _ O
and -X- _ O
perceptual -X- _ O
properties -X- _ O
that -X- _ O
are -X- _ O
likely -X- _ O
to -X- _ O
be -X- _ O
assumed -X- _ O
rather -X- _ O
than -X- _ O
mentioned -X- _ O
(Da -X- _ O
and -X- _ O
Kasai, -X- _ O
2019). -X- _ O
The -X- _ O
MLM -X- _ B-TaskName
component -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
easy -X- _ O
to -X- _ O
adapt -X- _ O
for -X- _ O
knowledge -X- _ O
induction -X- _ O
by -X- _ O
filling -X- _ O
in -X- _ O
the -X- _ O

To -X- _ O
date, -X- _ O
more -X- _ O
studies -X- _ O
have -X- _ O
been -X- _ O
devoted -X- _ O
to -X- _ O
BERT's -X- _ B-MethodName
knowledge -X- _ O
of -X- _ O
syntactic -X- _ O
rather -X- _ O
than -X- _ O
semantic -X- _ O
phenomena. -X- _ O
However, -X- _ O
we -X- _ O
do -X- _ O
have -X- _ O
evidence -X- _ O
from -X- _ O
an -X- _ O
MLM -X- _ B-TaskName
probing -X- _ I-TaskName
study -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
has -X- _ O
some -X- _ O
knowledge -X- _ O
of -X- _ O
semantic -X- _ O
roles -X- _ O
(Ettinger, -X- _ O
2019). -X- _ O
BERT -X- _ B-MethodName
even -X- _ O
displays -X- _ O
some -X- _ O
preference -X- _ O
for -X- _ O
the -X- _ O
incorrect -X- _ O
fillers -X- _ O
for -X- _ O
semantic -X- _ O
roles -X- _ O
that -X- _ O
are -X- _ O
semantically -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
correct -X- _ O
ones, -X- _ O
as -X- _ O
opposed -X- _ O
to -X- _ O
those -X- _ O
that -X- _ O
are -X- _ O
unrelated -X- _ O
(e.g. -X- _ O
"to -X- _ O
tip -X- _ O
a -X- _ O
chef" -X- _ O
is -X- _ O
better -X- _ O
than -X- _ O
"to -X- _ O
tip -X- _ O
a -X- _ O
robin", -X- _ O
but -X- _ O
worse -X- _ O
than -X- _ O
"to -X- _ O
tip -X- _ O
a -X- _ O
waiter"). -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
(2019b) -X- _ O
showed -X- _ O
that -X- _ O
BERT -X- _ O
encodes -X- _ O
information -X- _ O
about -X- _ O
entity -X- _ O
types, -X- _ O
relations, -X- _ O
semantic -X- _ O
roles, -X- _ O
and -X- _ O
proto-roles, -X- _ O
since -X- _ O
this -X- _ O
information -X- _ O
can -X- _ O
be -X- _ O
detected -X- _ O
with -X- _ O
probing -X- _ O
classifiers. -X- _ O
BERT -X- _ B-MethodName
struggles -X- _ O
with -X- _ O
representations -X- _ O
of -X- _ O
numbers. -X- _ O
Addition -X- _ O
and -X- _ O
number -X- _ O
decoding -X- _ O
tasks -X- _ O
showed -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
does -X- _ O
not -X- _ O
form -X- _ O
good -X- _ O
representations -X- _ O
for -X- _ O
floating -X- _ O
point -X- _ O
numbers -X- _ O
and -X- _ O
fails -X- _ O
to -X- _ O
generalize -X- _ O
away -X- _ O
from -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
(Wallace -X- _ O
et -X- _ O
al., -X- _ O
2019b). -X- _ O
A -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
problem -X- _ O
is -X- _ O
BERT's -X- _ B-MethodName
wordpiece -X- _ O
tokenization, -X- _ O
since -X- _ O
numbers -X- _ O
of -X- _ O
similar -X- _ O
values -X- _ O
can -X- _ O
be -X- _ O
divided -X- _ O
up -X- _ O
into -X- _ O
substantially -X- _ O
different -X- _ O
word -X- _ O
chunks. -X- _ O
Out-of-the-box -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
surprisingly -X- _ O
brittle -X- _ O
to -X- _ O
named -X- _ O
entity -X- _ O
replacements: -X- _ O
e.g. -X- _ O
replacing -X- _ O
names -X- _ O
in -X- _ O
the -X- _ O
coreference -X- _ O
task -X- _ O
changes -X- _ O
85% -X- _ O
of -X- _ O
predictions -X- _ O
(Balasubramanian -X- _ O
et -X- _ O
al., -X- _ O
2020). -X- _ O
This -X- _ O
suggests -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
does -X- _ O
not -X- _ O
actually -X- _ O
form -X- _ O
a -X- _ O
generic -X- _ O
idea -X- _ O
of -X- _ O
named -X- _ O
entities, -X- _ O
although -X- _ O
its -X- _ O
F1 -X- _ B-MetricName
scores -X- _ O
on -X- _ O
NER -X- _ B-TaskName
probing -X- _ I-TaskName
tasks -X- _ O
are -X- _ O
high -X- _ O
(Tenney -X- _ O
et -X- _ O
al., -X- _ O
2019a). -X- _ O
Broscheit -X- _ O
(2019) -X- _ O
find -X- _ O
that -X- _ O
fine-tuning -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
Wikipedia -X- _ O
entity -X- _ O
linking -X- _ O
"teaches" -X- _ O
it -X- _ O
additional -X- _ O
entity -X- _ O
knowledge, -X- _ O
which -X- _ O
would -X- _ O
suggest -X- _ O
that -X- _ O
it -X- _ O
did -X- _ O
not -X- _ O
absorb -X- _ O
all -X- _ O
the -X- _ O
relevant -X- _ O
entity -X- _ O
information -X- _ O
during -X- _ O
pre-training -X- _ O
on -X- _ O
Wikipedia. -X- _ O

With -X- _ O
the -X- _ O
goal -X- _ O
of -X- _ O
exploring -X- _ O
the -X- _ O
extent -X- _ O
dependency -X- _ O
relations -X- _ O
are -X- _ O
captured -X- _ O
in -X- _ O
BERT, -X- _ B-MethodName
we -X- _ O
set -X- _ O
out -X- _ O
to -X- _ O
answer -X- _ O
the -X- _ O
following -X- _ O
question: -X- _ O
Can -X- _ O
BERT -X- _ B-MethodName
outperform -X- _ O
linguistically -X- _ O
uninformed -X- _ O
baselines -X- _ O
in -X- _ O
unsupervised -X- _ O
dependency -X- _ O
parsing? -X- _ O
If -X- _ O
so, -X- _ O
to -X- _ O
what -X- _ O
extent? -X- _ O
We -X- _ O
begin -X- _ O
by -X- _ O
using -X- _ O
the -X- _ O
token-level -X- _ B-TaskName
perturbed -X- _ I-TaskName
masking -X- _ I-TaskName
technique -X- _ O
to -X- _ O
extract -X- _ O
an -X- _ O
impact -X- _ O
matrix -X- _ O
F -X- _ O
for -X- _ O
each -X- _ O
sentence. -X- _ O
We -X- _ O
then -X- _ O
utilize -X- _ O
graph-based -X- _ O
algorithms -X- _ O
to -X- _ O
induce -X- _ O
a -X- _ O
dependency -X- _ O
tree -X- _ O
from -X- _ O
F, -X- _ O
and -X- _ O
compare -X- _ O
it -X- _ O
against -X- _ O
ground-truth -X- _ O
whose -X- _ O
annotations -X- _ O
Figure -X- _ O
1: -X- _ O
Parameter-free -X- _ O
probe -X- _ O
for -X- _ O
syntactic -X- _ O
knowledge: -X- _ O
words -X- _ O
sharing -X- _ O
syntactic -X- _ O
subtrees -X- _ O
have -X- _ O
larger -X- _ O
impact -X- _ O
on -X- _ O
each -X- _ O
other -X- _ O
in -X- _ O
the -X- _ O
MLM -X- _ B-TaskName
prediction -X- _ O
parameter-free -X- _ O
approach -X- _ O
based -X- _ O
on -X- _ O
measuring -X- _ O
the -X- _ O
impact -X- _ O
that -X- _ O
one -X- _ O
word -X- _ O
has -X- _ O
on -X- _ O
predicting -X- _ O
another -X- _ O
word -X- _ O
within -X- _ O
a -X- _ O
sequence -X- _ O
in -X- _ O
the -X- _ O
MLM -X- _ B-TaskName
task -X- _ O
(Figure -X- _ O
1). -X- _ O
They -X- _ O
concluded -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
"naturally" -X- _ O
learns -X- _ O
some -X- _ O
syntactic -X- _ O
information, -X- _ O
although -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
very -X- _ O
similar -X- _ O
to -X- _ O
linguistic -X- _ O
annotated -X- _ O
resources. -X- _ O
The -X- _ O
fill-in-the-gap -X- _ O
probes -X- _ O
of -X- _ O
MLM -X- _ B-TaskName
showed -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
takes -X- _ O
subject-predicate -X- _ O
agreement -X- _ O
into -X- _ O
account -X- _ O
when -X- _ O
performing -X- _ O
the -X- _ O
cloze -X- _ B-TaskName
task -X- _ O
(Goldberg, -X- _ O
2019;van -X- _ O
Schijndel -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
even -X- _ O
for -X- _ O
meaningless -X- _ O
sentences -X- _ O
and -X- _ O
sentences -X- _ O
with -X- _ O
distractor -X- _ O
clauses -X- _ O
between -X- _ O
the -X- _ O
subject -X- _ O
and -X- _ O
the -X- _ O
verb -X- _ O
(Goldberg, -X- _ O
2019). -X- _ O
A -X- _ O
study -X- _ O
of -X- _ O
negative -X- _ O
polarity -X- _ O
items -X- _ O
(NPIs) -X- _ O
by -X- _ O
Warstadt -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
showed -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
better -X- _ O
able -X- _ O
to -X- _ O
detect -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
NPIs -X- _ O
(e.g. -X- _ O
"ever") -X- _ O
and -X- _ O
the -X- _ O
words -X- _ O
that -X- _ O
allow -X- _ O
their -X- _ O
use -X- _ O
(e.g. -X- _ O
"whether") -X- _ O
than -X- _ O
scope -X- _ O
violations. -X- _ O
The -X- _ O
above -X- _ O
claims -X- _ O
of -X- _ O
syntactic -X- _ O
knowledge -X- _ O
are -X- _ O
belied -X- _ O
by -X- _ O
the -X- _ O
evidence -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
does -X- _ O
not -X- _ O
"understand" -X- _ O
negation -X- _ O
and -X- _ O
is -X- _ O
insensitive -X- _ O
to -X- _ O
malformed -X- _ O
input. -X- _ O
In -X- _ O
particular, -X- _ O
its -X- _ O
predictions -X- _ O
were -X- _ O
not -X- _ O
altered -X- _ O
2 -X- _ O
even -X- _ O
with -X- _ O
shuffled -X- _ O
word -X- _ O
order, -X- _ O
truncated -X- _ O
sentences, -X- _ O
removed -X- _ O
subjects -X- _ O
and -X- _ O
objects -X- _ O
(Ettinger, -X- _ O
2019). -X- _ O
This -X- _ O
could -X- _ O
mean -X- _ O
that -X- _ O
either -X- _ O
BERT's -X- _ B-MethodName
syntactic -X- _ O
knowledge -X- _ O
is -X- _ O
incomplete, -X- _ O
or -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
need -X- _ O
to -X- _ O
rely -X- _ O
on -X- _ O
it -X- _ O
for -X- _ O
solving -X- _ O
its -X- _ O
tasks. -X- _ O
The -X- _ O
latter -X- _ O
seems -X- _ O
more -X- _ O
likely, -X- _ O
since -X- _ O
Glavaš -X- _ O
and -X- _ O
Vulić -X- _ O
(2020) -X- _ O
report -X- _ O
that -X- _ O
an -X- _ O
intermediate -X- _ O
fine-tuning -X- _ O
step -X- _ O
with -X- _ O
supervised -X- _ O
parsing -X- _ O
does -X- _ O
not -X- _ O
make -X- _ O
much -X- _ O
difference -X- _ O
for -X- _ O
downstream -X- _ O
task -X- _ O
performance. -X- _ O

We -X- _ O
start -X- _ O
with -X- _ O
two -X- _ O
syntactic -X- _ O
probes -X- _ O
-dependency -X- _ O
probe -X- _ O
and -X- _ O
constituency -X- _ O
probe. -X- _ O

Fundamentally, -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
a -X- _ O
stack -X- _ O
of -X- _ O
Transformer -X- _ O
encoder -X- _ O
layers -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017) -X- _ O
which -X- _ O
consist -X- _ O
of -X- _ O
multiple -X- _ O
self-attention -X- _ O
"heads". -X- _ O
For -X- _ O
every -X- _ O
input -X- _ O
token -X- _ O
in -X- _ O
a -X- _ O
sequence, -X- _ O
each -X- _ O
head -X- _ O
computes -X- _ O
key, -X- _ O
value -X- _ O
and -X- _ O
query -X- _ O
vectors, -X- _ O
used -X- _ O
to -X- _ O
create -X- _ O
a -X- _ O
weighted -X- _ O
representation. -X- _ O
The -X- _ O
outputs -X- _ O
of -X- _ O
all -X- _ O
heads -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
layer -X- _ O
are -X- _ O
combined -X- _ O
and -X- _ O
run -X- _ O
through -X- _ O
a -X- _ O
fully-connected -X- _ O
layer. -X- _ O
Each -X- _ O
layer -X- _ O
is -X- _ O
wrapped -X- _ O
with -X- _ O
a -X- _ O
skip -X- _ O
connection -X- _ O
and -X- _ O
followed -X- _ O
by -X- _ O
layer -X- _ O
normalization. -X- _ O
The -X- _ O
conventional -X- _ O
workflow -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
consists -X- _ O
of -X- _ O
two -X- _ O
stages: -X- _ O
pre-training -X- _ O
and -X- _ O
fine-tuning. -X- _ O
Pretraining -X- _ O
uses -X- _ O
two -X- _ O
self-supervised -X- _ O
tasks: -X- _ O
masked -X- _ B-TaskName
language -X- _ I-TaskName
modeling -X- _ I-TaskName
(MLM, -X- _ B-TaskName
prediction -X- _ O
of -X- _ O
randomly -X- _ O
masked -X- _ O
input -X- _ O
tokens) -X- _ O
and -X- _ O
next -X- _ B-TaskName
sentence -X- _ I-TaskName
prediction -X- _ I-TaskName
(NSP, -X- _ B-TaskName
predicting -X- _ O
if -X- _ O
two -X- _ O
input -X- _ O
sentences -X- _ O
are -X- _ O
adjacent -X- _ O
to -X- _ O
each -X- _ O
other). -X- _ O
In -X- _ O
fine-tuning -X- _ O
for -X- _ O
downstream -X- _ O
applications, -X- _ O
one -X- _ O
or -X- _ O
more -X- _ O
fully-connected -X- _ O
layers -X- _ O
are -X- _ O
typically -X- _ O
added -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
final -X- _ O
encoder -X- _ O
layer. -X- _ O
The -X- _ O
input -X- _ O
representations -X- _ O
are -X- _ O
computed -X- _ O
as -X- _ O
follows: -X- _ O
each -X- _ O
word -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
is -X- _ O
first -X- _ O
tokenized -X- _ O
into -X- _ O
wordpieces -X- _ O
(Wu -X- _ O
et -X- _ O
al., -X- _ O
2016), -X- _ O
and -X- _ O
then -X- _ O
three -X- _ O
embedding -X- _ O
layers -X- _ O
(token, -X- _ O
position, -X- _ O
and -X- _ O
segment) -X- _ O
are -X- _ O
combined -X- _ O
to -X- _ O
obtain -X- _ O
a -X- _ O
fixed-length -X- _ O
vector. -X- _ O
Special -X- _ O
token -X- _ O
[CLS] -X- _ O
is -X- _ O
used -X- _ O
for -X- _ O
classification -X- _ O
predictions, -X- _ O
and -X- _ O
[SEP] -X- _ O
separates -X- _ O
input -X- _ O
segments. -X- _ O
Google -X- _ O
1 -X- _ O
and -X- _ O
HuggingFace -X- _ O
(Wolf -X- _ O
et -X- _ O
al., -X- _ O
2020) -X- _ O
provide -X- _ O
many -X- _ O
variants -X- _ O
of -X- _ O
BERT, -X- _ B-MethodName
including -X- _ O
the -X- _ O
original -X- _ O
"base" -X- _ O
and -X- _ O
"large" -X- _ O
versions. -X- _ O
They -X- _ O
vary -X- _ O
in -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
heads, -X- _ O
layers, -X- _ O
and -X- _ O
hidden -X- _ O
state -X- _ O
size. -X- _ O
3 -X- _ O
What -X- _ O
knowledge -X- _ O
does -X- _ O
BERT -X- _ B-MethodName
have? -X- _ O
A -X- _ O
number -X- _ O
of -X- _ O
studies -X- _ O
have -X- _ O
looked -X- _ O
at -X- _ O
the -X- _ O
knowledge -X- _ O
encoded -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
weights. -X- _ O
The -X- _ O
popular -X- _ O
approaches -X- _ O
include -X- _ O
fill-in-the-gap -X- _ O
probes -X- _ O
of -X- _ O
MLM, -X- _ B-TaskName
analysis -X- _ O
of -X- _ O
self-attention -X- _ O
weights, -X- _ O
and -X- _ O
probing -X- _ O
classifiers -X- _ O
with -X- _ O
different -X- _ O
BERT -X- _ B-MethodName
representations -X- _ O
as -X- _ O
inputs. -X- _ O
3.1 -X- _ O
Syntactic -X- _ O
knowledge -X- _ O
showed -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
representations -X- _ O
are -X- _ O
hierarchical -X- _ O
rather -X- _ O
than -X- _ O
linear, -X- _ O
i.e. -X- _ O
there -X- _ O
is -X- _ O
something -X- _ O
akin -X- _ O
to -X- _ O
syntactic -X- _ O
tree -X- _ O
structure -X- _ O
in -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
word -X- _ O
order -X- _ O
information. -X- _ O
Tenney -X- _ O
et -X- _ O
al. -X- _ O
(2019b) -X- _ O
and -X- _ O
also -X- _ O
showed -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
embeddings -X- _ O
encode -X- _ O
information -X- _ O
about -X- _ O
parts -X- _ O
of -X- _ O
speech, -X- _ O
syntactic -X- _ O
chunks -X- _ O
and -X- _ O
roles. -X- _ O
Enough -X- _ O
syntactic -X- _ O
information -X- _ O
seems -X- _ O
to -X- _ O
be -X- _ O
captured -X- _ O
in -X- _ O
the -X- _ O
token -X- _ O
embeddings -X- _ O
themselves -X- _ O
to -X- _ O
recover -X- _ O
syntactic -X- _ O
trees -X- _ O
(Vilares -X- _ O
et -X- _ O
al., -X- _ O
2020;Kim -X- _ O
et -X- _ O
al., -X- _ O
2020;Rosa -X- _ O
and -X- _ O
Mareček, -X- _ O
2019), -X- _ O
although -X- _ O
probing -X- _ O
classifiers -X- _ O
could -X- _ O
not -X- _ O
recover -X- _ O
the -X- _ O
labels -X- _ O
of -X- _ O
distant -X- _ O
parent -X- _ O
nodes -X- _ O
in -X- _ O
the -X- _ O
syntactic -X- _ O
tree -X- _ O
. -X- _ O
Warstadt -X- _ O
and -X- _ O
Bowman -X- _ O
(2020) -X- _ O
report -X- _ O
evidence -X- _ O
of -X- _ O
hierarchical -X- _ O
structure -X- _ O
in -X- _ O
three -X- _ O
out -X- _ O
of -X- _ O
four -X- _ O
probing -X- _ O
tasks. -X- _ O
As -X- _ O
far -X- _ O
as -X- _ O
how -X- _ O
syntax -X- _ O
is -X- _ O
represented, -X- _ O
it -X- _ O
seems -X- _ O
that -X- _ O
syntactic -X- _ O
structure -X- _ O
is -X- _ O
not -X- _ O
directly -X- _ O
encoded -X- _ O
in -X- _ O
self-attention -X- _ O
weights. -X- _ O
Htut -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
were -X- _ O
unable -X- _ O
to -X- _ O
extract -X- _ O
full -X- _ O
parse -X- _ O
trees -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
heads -X- _ O
even -X- _ O
with -X- _ O
the -X- _ O
gold -X- _ O
annotations -X- _ O
for -X- _ O
the -X- _ O
root. -X- _ O
Jawahar -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
include -X- _ O
a -X- _ O
brief -X- _ O
illustration -X- _ O
of -X- _ O
a -X- _ O
dependency -X- _ O
tree -X- _ O
extracted -X- _ O
directly -X- _ O
from -X- _ O
self-attention -X- _ O
weights, -X- _ O
but -X- _ O
provide -X- _ O
no -X- _ O
quantitative -X- _ O
evaluation. -X- _ O
However, -X- _ O
syntactic -X- _ O
information -X- _ O
can -X- _ O
be -X- _ O
recovered -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
token -X- _ O
representations. -X- _ O
Hewitt -X- _ O
and -X- _ O
Manning -X- _ O
(2019) -X- _ O
were -X- _ O
able -X- _ O
to -X- _ O
learn -X- _ O
transformation -X- _ O
matrices -X- _ O
that -X- _ O
successfully -X- _ O
recovered -X- _ O
syntactic -X- _ O
dependencies -X- _ O
in -X- _ O
PennTreebank -X- _ O
data -X- _ O
from -X- _ O
BERT's -X- _ B-MethodName
token -X- _ O
embeddings -X- _ O
(see -X- _ O
also -X- _ O
. -X- _ O
Jawahar -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
experimented -X- _ O
with -X- _ O
transformations -X- _ O
of -X- _ O
the -X- _ O
[CLS] -X- _ O
token -X- _ O
using -X- _ O
Tensor -X- _ O
Product -X- _ O
Decomposition -X- _ O
Networks -X- _ O
(McCoy -X- _ O
et -X- _ O
al., -X- _ O
2019a), -X- _ O
concluding -X- _ O
that -X- _ O
dependency -X- _ O
trees -X- _ O
are -X- _ O
the -X- _ O
best -X- _ O
match -X- _ O
among -X- _ O
5 -X- _ O
decomposition -X- _ O
schemes -X- _ O
(although -X- _ O
the -X- _ O
reported -X- _ O
MSE -X- _ B-MetricName
differences -X- _ O
are -X- _ O
very -X- _ O
small). -X- _ O
Miaschi -X- _ O
and -X- _ O
Dell'Orletta -X- _ O
(2020) -X- _ O
performs -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
syntactic -X- _ O
probing -X- _ O
experiments -X- _ O
with -X- _ O
concatenated -X- _ O
token -X- _ O
representations -X- _ O
as -X- _ O
input. -X- _ O
Note -X- _ O
that -X- _ O
all -X- _ O
these -X- _ O
approaches -X- _ O
look -X- _ O
for -X- _ O
the -X- _ O
evidence -X- _ O
of -X- _ O
gold-standard -X- _ O
linguistic -X- _ O
structures, -X- _ O
and -X- _ O
add -X- _ O
some -X- _ O
amount -X- _ O
of -X- _ O
extra -X- _ O
knowledge -X- _ O
to -X- _ O
the -X- _ O
probe. -X- _ O
Most -X- _ O
recently, -X- _ O
proposed -X- _ O
a -X- _ O
4168 -X- _ O
3 -X- _ O
Visualization -X- _ O
with -X- _ O
Impact -X- _ O
Maps -X- _ O
Before -X- _ O
we -X- _ O
discuss -X- _ O
specific -X- _ O
syntactic -X- _ O
phenomena, -X- _ O
let -X- _ O
us -X- _ O
first -X- _ O
analyze -X- _ O
some -X- _ O
example -X- _ O
impact -X- _ O
matrices -X- _ O
derived -X- _ O
from -X- _ O
sample -X- _ O
sentences. -X- _ O
We -X- _ O
visualize -X- _ O
an -X- _ O
impact -X- _ O
matrix -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
by -X- _ O
displaying -X- _ O
a -X- _ O
heatmap. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
term -X- _ O
"impact -X- _ O
map" -X- _ O
to -X- _ O
refer -X- _ O
to -X- _ O
a -X- _ O
heatmap -X- _ O
of -X- _ O
an -X- _ O
impact -X- _ O
matrix. -X- _ O
Setup. -X- _ O
We -X- _ O
extract -X- _ O
impact -X- _ O
matrices -X- _ O
by -X- _ O
feeding -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
1,000 -X- _ O
sentences -X- _ O
from -X- _ O
the -X- _ O
English -X- _ B-DatasetName
Parallel -X- _ I-DatasetName
Universal -X- _ I-DatasetName
Dependencies -X- _ I-DatasetName
(PUD) -X- _ B-DatasetName
treebank -X- _ O
of -X- _ O
the -X- _ O
CoNLL -X- _ O
2017 -X- _ O
Shared -X- _ O
Task -X- _ O
(Zeman -X- _ O
et -X- _ O
al., -X- _ O
2017). -X- _ O
We -X- _ O
follow -X- _ O
the -X- _ O
setup -X- _ O
and -X- _ O
pre-processing -X- _ O
steps -X- _ O
employed -X- _ O
in -X- _ O
pre-training -X- _ O
BERT. -X- _ B-MethodName
An -X- _ O
example -X- _ O
impact -X- _ O
map -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1. -X- _ O
Dependency. -X- _ O
We -X- _ O
notice -X- _ O
that -X- _ O
the -X- _ O
impact -X- _ O
map -X- _ O
contains -X- _ O
many -X- _ O
stripes, -X- _ O
which -X- _ O
are -X- _ O
short -X- _ O
series -X- _ O
of -X- _ O
vertical/horizontal -X- _ O
cells, -X- _ O
typically -X- _ O
located -X- _ O
along -X- _ O
the -X- _ O
diagonal. -X- _ O
Take -X- _ O
the -X- _ O
word -X- _ O
"different" -X- _ O
as -X- _ O
an -X- _ O
example -X- _ O
(which -X- _ O
is -X- _ O
illustrated -X- _ O
by -X- _ O
the -X- _ O
second-to-last -X- _ O
column -X- _ O
in -X- _ O
the -X- _ O
impact -X- _ O
matrix). -X- _ O
We -X- _ O
observe -X- _ O
a -X- _ O
clear -X- _ O
vertical -X- _ O
stripe -X- _ O
above -X- _ O
the -X- _ O
main -X- _ O
diagonal. -X- _ O
The -X- _ O
interpretation -X- _ O
is -X- _ O
that -X- _ O
this -X- _ O
particular -X- _ O
occurrence -X- _ O
of -X- _ O
the -X- _ O
word -X- _ O
"different" -X- _ O
strongly -X- _ O
affects -X- _ O
the -X- _ O
occurrences -X- _ O
of -X- _ O
those -X- _ O
words -X- _ O
before -X- _ O
it. -X- _ O
These -X- _ O
strong -X- _ O
influences -X- _ O
are -X- _ O
shown -X- _ O
by -X- _ O
the -X- _ O
darker-colored -X- _ O
pixels -X- _ O
seen -X- _ O
in -X- _ O
the -X- _ O
second -X- _ O
last -X- _ O
column -X- _ O
of -X- _ O
the -X- _ O
impact -X- _ O
map. -X- _ O
This -X- _ O
observation -X- _ O
agrees -X- _ O
with -X- _ O
the -X- _ O
ground-truth -X- _ O
dependency -X- _ O
tree, -X- _ O
which -X- _ O
selects -X- _ O
"different" -X- _ O
as -X- _ O
the -X- _ O
head -X- _ O
of -X- _ O
all -X- _ O
remaining -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
phrase -X- _ O
"this -X- _ O
will -X- _ O
be -X- _ O
a -X- _ O
little -X- _ O
different." -X- _ O
We -X- _ O
also -X- _ O
observe -X- _ O
similar -X- _ O
patterns -X- _ O
on -X- _ O
"transitions" -X- _ O
and -X- _ O
"Hill". -X- _ O
Such -X- _ O
correlations -X- _ O
lead -X- _ O
us -X- _ O
to -X- _ O
explore -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
extracting -X- _ O
dependency -X- _ O
trees -X- _ O
from -X- _ O
the -X- _ O
matrices -X- _ O
(see -X- _ O
Section -X- _ O
4.1). -X- _ O
3 -X- _ O
Visualization -X- _ O
with -X- _ O
Impact -X- _ O
Maps -X- _ O
Before -X- _ O
we -X- _ O
discuss -X- _ O
specific -X- _ O
syntactic -X- _ O
phenomena, -X- _ O
let -X- _ O
us -X- _ O
first -X- _ O
analyze -X- _ O
some -X- _ O
example -X- _ O
impact -X- _ O
matrices -X- _ O
derived -X- _ O
from -X- _ O
sample -X- _ O
sentences. -X- _ O
We -X- _ O
visualize -X- _ O
an -X- _ O
impact -X- _ O
matrix -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
by -X- _ O
displaying -X- _ O
a -X- _ O
heatmap. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
term -X- _ O
"impact -X- _ O
map" -X- _ O
to -X- _ O
refer -X- _ O
to -X- _ O
a -X- _ O
heatmap -X- _ O
of -X- _ O
an -X- _ O
impact -X- _ O
matrix. -X- _ O
Setup. -X- _ O
We -X- _ O
extract -X- _ O
impact -X- _ O
matrices -X- _ O
by -X- _ O
feeding -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
1,000 -X- _ O
sentences -X- _ O
from -X- _ O
the -X- _ O
English -X- _ B-DatasetName
Parallel -X- _ I-DatasetName
Universal -X- _ I-DatasetName
Dependencies -X- _ I-DatasetName
(PUD) -X- _ B-DatasetName
treebank -X- _ O
of -X- _ O
the -X- _ O
CoNLL -X- _ B-TaskName
2017 -X- _ I-TaskName
Shared -X- _ I-TaskName
Task -X- _ I-TaskName
(Zeman -X- _ O
et -X- _ O
al., -X- _ O
2017). -X- _ O
We -X- _ O
follow -X- _ O
the -X- _ O
setup -X- _ O
and -X- _ O
pre-processing -X- _ O
steps -X- _ O
employed -X- _ O
in -X- _ O
pre-training -X- _ O
BERT. -X- _ B-MethodName
An -X- _ O
example -X- _ O
impact -X- _ O
map -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1. -X- _ O
Dependency. -X- _ O
We -X- _ O
notice -X- _ O
that -X- _ O
the -X- _ O
impact -X- _ O
map -X- _ O
contains -X- _ O
many -X- _ O
stripes, -X- _ O
which -X- _ O
are -X- _ O
short -X- _ O
series -X- _ O
of -X- _ O
vertical/horizontal -X- _ O
cells, -X- _ O
typically -X- _ O
located -X- _ O
along -X- _ O
the -X- _ O
diagonal. -X- _ O
Take -X- _ O
the -X- _ O
word -X- _ O
"different" -X- _ O
as -X- _ O
an -X- _ O
example -X- _ O
(which -X- _ O
is -X- _ O
illustrated -X- _ O
by -X- _ O
the -X- _ O
second-to-last -X- _ O
column -X- _ O
in -X- _ O
the -X- _ O
impact -X- _ O
matrix). -X- _ O
We -X- _ O
observe -X- _ O
a -X- _ O
clear -X- _ O
vertical -X- _ O
stripe -X- _ O
above -X- _ O
the -X- _ O
main -X- _ O
diagonal. -X- _ O
The -X- _ O
interpretation -X- _ O
is -X- _ O
that -X- _ O
this -X- _ O
particular -X- _ O
occurrence -X- _ O
of -X- _ O
the -X- _ O
word -X- _ O
"different" -X- _ O
strongly -X- _ O
affects -X- _ O
the -X- _ O
occurrences -X- _ O
of -X- _ O
those -X- _ O
words -X- _ O
before -X- _ O
it. -X- _ O
These -X- _ O
strong -X- _ O
influences -X- _ O
are -X- _ O
shown -X- _ O
by -X- _ O
the -X- _ O
darker-colored -X- _ O
pixels -X- _ O
seen -X- _ O
in -X- _ O
the -X- _ O
second -X- _ O
last -X- _ O
column -X- _ O
of -X- _ O
the -X- _ O
impact -X- _ O
map. -X- _ O
This -X- _ O
observation -X- _ O
agrees -X- _ O
with -X- _ O
the -X- _ O
ground-truth -X- _ O
dependency -X- _ O
tree, -X- _ O
which -X- _ O
selects -X- _ O
"different" -X- _ O
as -X- _ O
the -X- _ O
head -X- _ O
of -X- _ O
all -X- _ O
remaining -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
phrase -X- _ O
"this -X- _ O
will -X- _ O
be -X- _ O
a -X- _ O
little -X- _ O
different." -X- _ O
We -X- _ O
also -X- _ O
observe -X- _ O
similar -X- _ O
patterns -X- _ O
on -X- _ O
"transitions" -X- _ O
and -X- _ O
"Hill". -X- _ O
Such -X- _ O
correlations -X- _ O
lead -X- _ O
us -X- _ O
to -X- _ O
explore -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
extracting -X- _ O
dependency -X- _ O
trees -X- _ O
from -X- _ O
the -X- _ O
matrices -X- _ O
(see -X- _ O
Section -X- _ O
4.1). -X- _ O
Constituency. -X- _ O
Figure -X- _ O
2 -X- _ O
shows -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
constituency -X- _ O
tree -X- _ O
of -X- _ O
our -X- _ O
example -X- _ O
sentence -X- _ O
generated -X- _ O
by -X- _ O
Stanford -X- _ O
CoreNLP -X- _ O
(Manning -X- _ O
et -X- _ O
al., -X- _ O
2014). -X- _ O
In -X- _ O
this -X- _ O
sentence, -X- _ O
"media" -X- _ O
and -X- _ O
"on" -X- _ O
are -X- _ O
two -X- _ O
words -X- _ O
that -X- _ O
are -X- _ O
adjacent -X- _ O
to -X- _ O
"transitions". -X- _ O
From -X- _ O
the -X- _ O
tree, -X- _ O
however, -X- _ O
we -X- _ O
see -X- _ O
that -X- _ O
"media" -X- _ O
is -X- _ O
closer -X- _ O
to -X- _ O
"transitions" -X- _ O
than -X- _ O
"on" -X- _ O
is -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
syntactic -X- _ O
distance. -X- _ O
If -X- _ O
a -X- _ O
model -X- _ O
is -X- _ O
syntactically -X- _ O
uninformed, -X- _ O
we -X- _ O
would -X- _ O
expect -X- _ O
"media" -X- _ O
and -X- _ O
"on" -X- _ O
to -X- _ O
have -X- _ O
comparable -X- _ O
impacts -X- _ O
on -X- _ O
the -X- _ O
prediction -X- _ O
of -X- _ O
"transitions", -X- _ O
and -X- _ O
vice -X- _ O
versa. -X- _ O
However, -X- _ O
we -X- _ O
observe -X- _ O
a -X- _ O
far -X- _ O
greater -X- _ O
impact -X- _ O
(darker -X- _ O
color) -X- _ O
between -X- _ O
"media" -X- _ O
and -X- _ O
"transitions" -X- _ O
than -X- _ O
that -X- _ O
between -X- _ O
"on" -X- _ O
and -X- _ O
"transitions". -X- _ O
We -X- _ O
will -X- _ O
further -X- _ O
support -X- _ O
this -X- _ O
observation -X- _ O
with -X- _ O
empirical -X- _ O
experiments -X- _ O
in -X- _ O
Section -X- _ O
4.2. -X- _ O
Other -X- _ O
Structures. -X- _ O
Along -X- _ O
the -X- _ O
diagonal -X- _ O
of -X- _ O
the -X- _ O
impact -X- _ O
map, -X- _ O
we -X- _ O
see -X- _ O
that -X- _ O
words -X- _ O
are -X- _ O
grouped -X- _ O
into -X- _ O
four -X- _ O
contiguous -X- _ O
chunks -X- _ O
that -X- _ O
have -X- _ O
specific -X- _ O
intents -X- _ O
(e.g., -X- _ O
a -X- _ O
noun -X- _ O
phrase -X- _ O
-on -X- _ O
Capitol -X- _ O
Hill). -X- _ O
We -X- _ O
also -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
two -X- _ O
middle -X- _ O
chunks -X- _ O
have -X- _ O
relatively -X- _ O
strong -X- _ O
inter-chunk -X- _ O
word -X- _ O
impacts -X- _ O
and -X- _ O
thus -X- _ O
a -X- _ O
bonding -X- _ O
that -X- _ O
groups -X- _ O
them -X- _ O
together, -X- _ O
forming -X- _ O
a -X- _ O
larger -X- _ O
verb -X- _ O
phrase. -X- _ O
This -X- _ O
observation -X- _ O
suggest -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
may -X- _ O
capture -X- _ O
the -X- _ O
compositionality -X- _ O
of -X- _ O
the -X- _ O
language. -X- _ O
In -X- _ O
the -X- _ O
following -X- _ O
sections -X- _ O
we -X- _ O
quantitatively -X- _ O
evaluate -X- _ O
these -X- _ O
observations. -X- _ O

Since -X- _ O
their -X- _ O
introduction -X- _ O
in -X- _ O
2017, -X- _ O
Transformers -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017) -X- _ O
have -X- _ O
taken -X- _ O
NLP -X- _ O
by -X- _ O
storm, -X- _ O
offering -X- _ O
enhanced -X- _ O
parallelization -X- _ O
and -X- _ O
better -X- _ O
modeling -X- _ O
of -X- _ O
long-range -X- _ O
dependencies. -X- _ O
The -X- _ O
best -X- _ O
known -X- _ O
Transformer-based -X- _ O
model -X- _ O
is -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019); -X- _ O
it -X- _ O
obtained -X- _ O
state-of-the-art -X- _ O
results -X- _ O
in -X- _ O
numerous -X- _ O
benchmarks -X- _ O
and -X- _ O
is -X- _ O
still -X- _ O
a -X- _ O
must-have -X- _ O
baseline. -X- _ O
While -X- _ O
it -X- _ O
is -X- _ O
clear -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
works -X- _ O
remarkably -X- _ O
well, -X- _ O
it -X- _ O
is -X- _ O
less -X- _ O
clear -X- _ O
why, -X- _ O
which -X- _ O
limits -X- _ O
further -X- _ O
hypothesis-driven -X- _ O
improvement -X- _ O
of -X- _ O
the -X- _ O
architecture. -X- _ O
Unlike -X- _ O
CNNs, -X- _ O
the -X- _ O
Transformers -X- _ O
have -X- _ O
little -X- _ O
cognitive -X- _ O
motivation, -X- _ O
and -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
these -X- _ O
models -X- _ O
limits -X- _ O
our -X- _ O
ability -X- _ O
to -X- _ O
experiment -X- _ O
with -X- _ O
pre-training -X- _ O
and -X- _ O
perform -X- _ O
ablation -X- _ O
studies. -X- _ O
This -X- _ O
explains -X- _ O
a -X- _ O
large -X- _ O
number -X- _ O
of -X- _ O
studies -X- _ O
over -X- _ O
the -X- _ O
past -X- _ O
year -X- _ O
that -X- _ O
attempted -X- _ O
to -X- _ O
understand -X- _ O
the -X- _ O
reasons -X- _ O
behind -X- _ O
BERT's -X- _ B-MethodName
performance. -X- _ O
In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
provide -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
what -X- _ O
has -X- _ O
been -X- _ O
learned -X- _ O
to -X- _ O
date, -X- _ O
highlighting -X- _ O
the -X- _ O
questions -X- _ O
which -X- _ O
are -X- _ O
still -X- _ O
unresolved. -X- _ O
We -X- _ O
first -X- _ O
consider -X- _ O
the -X- _ O
linguistic -X- _ O
aspects -X- _ O
of -X- _ O
it, -X- _ O
i.e., -X- _ O
the -X- _ O
current -X- _ O
evidence -X- _ O
regarding -X- _ O
the -X- _ O
types -X- _ O
of -X- _ O
linguistic -X- _ O
and -X- _ O
world -X- _ O
knowledge -X- _ O
learned -X- _ O
by -X- _ O
BERT, -X- _ B-MethodName
as -X- _ O
well -X- _ O
as -X- _ O
where -X- _ O
and -X- _ O
how -X- _ O
this -X- _ O
knowledge -X- _ O
may -X- _ O
be -X- _ O
stored -X- _ O
in -X- _ O
the -X- _ O
model. -X- _ O
We -X- _ O
then -X- _ O
turn -X- _ O
to -X- _ O
the -X- _ O
technical -X- _ O
aspects -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
provide -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
the -X- _ O
current -X- _ O
proposals -X- _ O
to -X- _ O
improve -X- _ O
BERT's -X- _ B-MethodName
architecture, -X- _ O
pre-training -X- _ O
and -X- _ O
finetuning. -X- _ O
We -X- _ O
conclude -X- _ O
by -X- _ O
discussing -X- _ O
the -X- _ O
issue -X- _ O
of -X- _ O
overparameterization, -X- _ O
the -X- _ O
approaches -X- _ O
to -X- _ O
compressing -X- _ O
BERT, -X- _ B-MethodName
and -X- _ O
the -X- _ O
nascent -X- _ O
area -X- _ O
of -X- _ O
pruning -X- _ O
as -X- _ O
a -X- _ O
model -X- _ O
analysis -X- _ O
technique. -X- _ O

Transformer-based -X- _ O
models -X- _ O
have -X- _ O
pushed -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
in -X- _ O
many -X- _ O
areas -X- _ O
of -X- _ O
NLP, -X- _ O
but -X- _ O
our -X- _ O
understanding -X- _ O
of -X- _ O
what -X- _ O
is -X- _ O
behind -X- _ O
their -X- _ O
success -X- _ O
is -X- _ O
still -X- _ O
limited. -X- _ O
This -X- _ O
paper -X- _ O
is -X- _ O
the -X- _ O
first -X- _ O
survey -X- _ O
of -X- _ O
over -X- _ O
150 -X- _ O
studies -X- _ O
of -X- _ O
the -X- _ O
popular -X- _ O
BERT -X- _ B-MethodName
model. -X- _ O
We -X- _ O
review -X- _ O
the -X- _ O
current -X- _ O
state -X- _ O
of -X- _ O
knowledge -X- _ O
about -X- _ O
how -X- _ O
BERT -X- _ B-MethodName
works, -X- _ O
what -X- _ O
kind -X- _ O
of -X- _ O
information -X- _ O
it -X- _ O
learns -X- _ O
and -X- _ O
how -X- _ O
it -X- _ O
is -X- _ O
represented, -X- _ O
common -X- _ O
modifications -X- _ O
to -X- _ O
its -X- _ O
training -X- _ O
objectives -X- _ O
and -X- _ O
architecture, -X- _ O
the -X- _ O
overparameterization -X- _ O
issue -X- _ O
and -X- _ O
approaches -X- _ O
to -X- _ O
compression. -X- _ O
We -X- _ O
then -X- _ O
outline -X- _ O
directions -X- _ O
for -X- _ O
future -X- _ O
research. -X- _ O

