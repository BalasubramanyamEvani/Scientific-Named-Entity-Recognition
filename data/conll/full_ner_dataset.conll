Acknowledgements	O
We	O
are	O
grateful	O
to	O
Nal	O
Kalchbrenner	O
and	O
Stephan	O
Gouws	O
for	O
their	O
fruitful	O
comments,	O
corrections	O
and	O
inspiration.	O

In	O
this	O
work,	O
we	O
presented	O
the	O
Transformer,	B-MethodName
the	O
first	O
sequence	O
transduction	O
model	O
based	O
entirely	O
on	O
attention,	O
replacing	O
the	O
recurrent	O
layers	O
most	O
commonly	O
used	O
in	O
encoder-decoder	O
architectures	O
with	O
multi-headed	O
self-attention.	O
For	O
translation	O
tasks,	O
the	O
Transformer	B-MethodName
can	O
be	O
trained	O
significantly	O
faster	O
than	O
architectures	O
based	O
on	O
recurrent	O
or	O
convolutional	O
layers.	O
On	O
both	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
and	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
tasks,	O
we	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art.	O
In	O
the	O
former	O
task	O
our	O
best	O
model	O
outperforms	O
even	O
all	O
previously	O
reported	O
ensembles.	O
We	O
are	O
excited	O
about	O
the	O
future	O
of	O
attention-based	O
models	O
and	O
plan	O
to	O
apply	O
them	O
to	O
other	O
tasks.	O
We	O
plan	O
to	O
extend	O
the	O
Transformer	B-MethodName
to	O
problems	O
involving	O
input	O
and	O
output	O
modalities	O
other	O
than	O
text	O
and	O
to	O
investigate	O
local,	O
restricted	O
attention	O
mechanisms	O
to	O
efficiently	O
handle	O
large	O
inputs	O
and	O
outputs	O
such	O
as	O
images,	O
audio	O
and	O
video.	O
Making	O
generation	O
less	O
sequential	O
is	O
another	O
research	O
goals	O
of	O
ours.	O
The	O
code	O
we	O
used	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
is	O
available	O
at	O
tensorflow/tensor2tensor.	O

To	O
evaluate	O
if	O
the	O
Transformer	B-MethodName
can	O
generalize	O
to	O
other	O
tasks	O
we	O
performed	O
experiments	O
on	O
English	B-TaskName
constituency	I-TaskName
parsing.	I-TaskName
This	O
task	O
presents	O
specific	O
challenges:	O
the	O
output	O
is	O
subject	O
to	O
strong	O
structural	O
constraints	O
and	O
is	O
significantly	O
longer	O
than	O
the	O
input.	O
Furthermore,	O
RNN	O
sequence-to-sequence	O
models	O
have	O
not	O
been	O
able	O
to	O
attain	O
state-of-the-art	O
results	O
in	O
small-data	O
regimes	O
.	O
We	O
trained	O
a	O
4-layer	O
transformer	O
with	O
d	O
model	O
=	O
1024	O
on	O
the	O
Wall	B-DatasetName
Street	I-DatasetName
Journal	I-DatasetName
(WSJ)	I-DatasetName
portion	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
,	O
about	O
40K	O
training	O
sentences.	O
We	O
also	O
trained	O
it	O
in	O
a	O
semi-supervised	O
setting,	O
using	O
the	O
larger	O
high-confidence	O
and	O
BerkleyParser	B-DatasetName
corpora	I-DatasetName
from	O
with	O
approximately	O
17M	O
sentences	O
.	O
We	O
used	O
a	O
vocabulary	B-HyperparameterName
of	O
16K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
WSJ	O
only	O
setting	O
and	O
a	O
vocabulary	B-HyperparameterName
of	O
32K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
semi-supervised	O
setting.	O
We	O
performed	O
only	O
a	O
small	O
number	O
of	O
experiments	O
to	O
select	O
the	O
dropout,	B-HyperparameterName
both	O
attention	O
and	O
residual	O
(section	O
5.4),	O
learning	B-HyperparameterName
rates	I-HyperparameterName
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
on	O
the	O
Section	O
22	O
development	O
set,	O
all	O
other	O
parameters	O
remained	O
unchanged	O
from	O
the	O
English-to-German	O
base	O
translation	O
model.	O
During	O
inference,	O
we	O
increased	O
the	O
maximum	O
output	O
length	O
to	O
input	O
length	O
+	O
300.	O
We	O
used	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
21	B-HyperparameterValue
and	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
for	O
both	O
WSJ	O
only	O
and	O
the	O
semi-supervised	O
setting.	O
Our	O
results	O
in	O
Table	O
4	O
show	O
that	O
despite	O
the	O
lack	O
of	O
task-specific	O
tuning	O
our	O
model	O
performs	O
surprisingly	O
well,	O
yielding	O
better	O
results	O
than	O
all	O
previously	O
reported	O
models	O
with	O
the	O
exception	O
of	O
the	O
Recurrent	O
Neural	O
Network	O
Grammar	O
.	O
In	O
contrast	O
to	O
RNN	O
sequence-to-sequence	O
models	O
,	O
the	O
Transformer	B-MethodName
outperforms	O
the	O
Berkeley-Parser	B-MethodName
even	O
when	O
training	O
only	O
on	O
the	O
WSJ	B-DatasetName
training	O
set	O
of	O
40K	O
sentences.	O

To	O
evaluate	O
the	O
importance	O
of	O
different	O
components	O
of	O
the	O
Transformer,	B-MethodName
we	O
varied	O
our	O
base	O
model	O
in	O
different	O
ways,	O
measuring	O
the	O
change	O
in	O
performance	O
on	O
English-to-German	B-TaskName
translation	I-TaskName
on	O
the	O
development	O
set,	O
newstest2013.	B-DatasetName
We	O
used	O
beam	B-MethodName
search	I-MethodName
as	O
described	O
in	O
the	O
previous	O
section,	O
but	O
no	O
checkpoint	O
averaging.	O
We	O
present	O
these	O
results	O
in	O
Table	O
3.	O
In	O
Table	O
3	O
rows	O
(A),	O
we	O
vary	O
the	O
number	O
of	O
attention	O
heads	O
and	O
the	O
attention	O
key	O
and	O
value	O
dimensions,	O
keeping	O
the	O
amount	O
of	O
computation	O
constant,	O
as	O
described	O
in	O
Section	O
3.2.2.	O
While	O
single-head	O
attention	O
is	O
0.9	B-MetricValue
BLEU	B-MetricName
worse	O
than	O
the	O
best	O
setting,	O
quality	O
also	O
drops	O
off	O
with	O
too	O
many	O
heads.	O
In	O
Table	O
3	O
rows	O
(B),	O
we	O
observe	O
that	O
reducing	O
the	O
attention	O
key	O
size	O
d	O
k	O
hurts	O
model	O
quality.	O
This	O
suggests	O
that	O
determining	O
compatibility	O
is	O
not	O
easy	O
and	O
that	O
a	O
more	O
sophisticated	O
compatibility	O
function	O
than	O
dot	O
product	O
may	O
be	O
beneficial.	O
We	O
further	O
observe	O
in	O
rows	O
(C)	O
and	O
(D)	O
that,	O
as	O
expected,	O
bigger	O
models	O
are	O
better,	O
and	O
dropout	O
is	O
very	O
helpful	O
in	O
avoiding	O
over-fitting.	O
In	O
row	O
(E)	O
we	O
replace	O
our	O
sinusoidal	O
positional	O
encoding	O
with	O
learned	O
positional	O
embeddings	O
,	O
and	O
observe	O
nearly	O
identical	O
results	O
to	O
the	O
base	O
model.	O

On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
translation	O
task,	O
the	O
big	O
transformer	B-MethodName
model	O
(Transformer	B-MethodName
(big)	I-MethodName
in	O
Table	O
2)	O
outperforms	O
the	O
best	O
previously	O
reported	O
models	O
(including	O
ensembles)	O
by	O
more	O
than	O
2.0	B-MetricValue
BLEU,	B-MetricName
establishing	O
a	O
new	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
28.4.	B-MetricValue
The	O
configuration	O
of	O
this	O
model	O
is	O
listed	O
in	O
the	O
bottom	O
line	O
of	O
Table	O
3.	O
Training	O
took	O
3.5	O
days	O
on	O
8	O
P100	O
GPUs.	O
Even	O
our	O
base	O
model	O
surpasses	O
all	O
previously	O
published	O
models	O
and	O
ensembles,	O
at	O
a	O
fraction	O
of	O
the	O
training	O
cost	O
of	O
any	O
of	O
the	O
competitive	O
models.	O
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
big	O
model	O
achieves	O
a	O
BLEU	B-MetricName
score	O
of	O
41.0,	B-MetricValue
outperforming	O
all	O
of	O
the	O
previously	O
published	O
single	O
models,	O
at	O
less	O
than	O
1/4	O
the	O
training	O
cost	O
of	O
the	O
previous	O
state-of-the-art	O
model.	O
The	O
Transformer	O
(big)	O
model	O
trained	O
for	O
English-to-French	O
used	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
P	O
drop	B-HyperparameterName
=	O
0.1,	B-HyperparameterValue
instead	O
of	O
0.3.	B-HyperparameterValue
For	O
the	O
base	O
models,	O
we	O
used	O
a	O
single	O
model	O
obtained	O
by	O
averaging	O
the	O
last	O
5	O
checkpoints,	O
which	O
were	O
written	O
at	O
10-minute	O
intervals.	O
For	O
the	O
big	O
models,	O
we	O
averaged	O
the	O
last	O
20	O
checkpoints.	O
We	O
used	O
beam	B-MethodName
search	I-MethodName
with	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
4	B-HyperparameterValue
and	O
length	B-HyperparameterName
penalty	I-HyperparameterName
α	B-HyperparameterValue
=	I-HyperparameterValue
0.6	I-HyperparameterValue
.	O
These	O
hyperparameters	O
were	O
chosen	O
after	O
experimentation	O
on	O
the	O
development	O
set.	O
We	O
set	O
the	O
maximum	O
output	O
length	O
during	O
inference	O
to	O
input	O
length	O
+	O
50,	O
but	O
terminate	O
early	O
when	O
possible	O
.	O

We	O
employ	O
three	O
types	O
of	O
regularization	O
during	O
training:	O
Residual	O
Dropout	B-HyperparameterName
We	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
output	O
of	O
each	O
sub-layer,	O
before	O
it	O
is	O
added	O
to	O
the	O
sub-layer	O
input	O
and	O
normalized.	O
In	O
addition,	O
we	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
sums	O
of	O
the	O
embeddings	O
and	O
the	O
positional	O
encodings	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
For	O
the	O
base	O
model,	O
we	O
use	O
a	O
rate	O
of	O
P	O
drop	B-HyperparameterName
=	O
0.1.	B-HyperparameterValue
Label	B-HyperparameterName
Smoothing	I-HyperparameterName
During	O
training,	O
we	O
employed	O
label	O
smoothing	O
of	O
value	O
ls	B-HyperparameterName
=	O
0.1	B-HyperparameterValue
.	O
This	O
hurts	O
perplexity,	B-MetricName
as	O
the	O
model	O
learns	O
to	O
be	O
more	O
unsure,	O
but	O
improves	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
score.	O
6	B-MetricValue
Results	O

We	O
used	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
β	O
1	O
=	O
0.9,	O
β	O
2	O
=	O
0.98	O
and	O
=	O
10	O
−9	O
.	O
We	O
varied	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
over	O
the	O
course	O
of	O
training,	O
according	O
to	O
the	O
formula:	O
lrate	O
=	O
d	O
−0.5	O
model	O
•	O
min(step_num	O
−0.5	O
,	O
step_num	O
•	O
warmup_steps	O
−1.5	O
)(3)	O
This	O
corresponds	O
to	O
increasing	O
the	O
learning	O
rate	O
linearly	O
for	O
the	O
first	O
warmup_steps	O
training	O
steps,	O
and	O
decreasing	O
it	O
thereafter	O
proportionally	O
to	O
the	O
inverse	O
square	O
root	O
of	O
the	O
step	O
number.	O
We	O
used	O
warmup_steps	B-HyperparameterName
=	O
4000.	B-HyperparameterValue

We	O
trained	O
our	O
models	O
on	O
one	O
machine	O
with	O
8	O
NVIDIA	O
P100	O
GPUs.	O
For	O
our	O
base	O
models	O
using	O
the	O
hyperparameters	O
described	O
throughout	O
the	O
paper,	O
each	O
training	O
step	O
took	O
about	O
0.4	O
seconds.	O
We	O
trained	O
the	O
base	O
models	O
for	O
a	O
total	O
of	O
100,000	B-HyperparameterValue
steps	B-HyperparameterName
or	O
12	O
hours.	O
For	O
our	O
big	O
models,(described	O
on	O
the	O
bottom	O
line	O
of	O
table	O
3),	O
step	O
time	O
was	O
1.0	O
seconds.	O
The	O
big	O
models	O
were	O
trained	O
for	O
300,000	B-HyperparameterValue
steps	B-HyperparameterName
(3.5	O
days).	O

We	O
trained	O
on	O
the	O
standard	O
WMT	B-DatasetName
2014	I-DatasetName
English-German	I-DatasetName
dataset	O
consisting	O
of	O
about	O
4.5	O
million	O
sentence	O
pairs.	O
Sentences	O
were	O
encoded	O
using	O
byte-pair	O
encoding	O
,	O
which	O
has	O
a	O
shared	O
sourcetarget	O
vocabulary	O
of	O
about	O
37000	O
tokens.	O
For	O
English-French,	O
we	O
used	O
the	O
significantly	O
larger	O
WMT	B-DatasetName
2014	I-DatasetName
English-French	I-DatasetName
dataset	O
consisting	O
of	O
36M	O
sentences	O
and	O
split	O
tokens	O
into	O
a	O
32000	O
word-piece	O
vocabulary	O
.	O
Sentence	O
pairs	O
were	O
batched	O
together	O
by	O
approximate	O
sequence	O
length.	O
Each	O
training	O
batch	O
contained	O
a	O
set	O
of	O
sentence	O
pairs	O
containing	O
approximately	O
25000	O
source	O
tokens	O
and	O
25000	O
target	O
tokens.	O

This	O
section	O
describes	O
the	O
training	O
regime	O
for	O
our	O
models.	O

In	O
this	O
section	O
we	O
compare	O
various	O
aspects	O
of	O
self-attention	O
layers	O
to	O
the	O
recurrent	O
and	O
convolutional	O
layers	O
commonly	O
used	O
for	O
mapping	O
one	O
variable-length	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
another	O
sequence	O
of	O
equal	O
length	O
(z	O
1	O
,	O
...,	O
z	O
n	O
),	O
with	O
x	O
i	O
,	O
z	O
i	O
∈	O
R	O
d	O
,	O
such	O
as	O
a	O
hidden	O
layer	O
in	O
a	O
typical	O
sequence	O
transduction	O
encoder	O
or	O
decoder.	O
Motivating	O
our	O
use	O
of	O
self-attention	O
we	O
consider	O
three	O
desiderata.	O
One	O
is	O
the	O
total	O
computational	O
complexity	O
per	O
layer.	O
Another	O
is	O
the	O
amount	O
of	O
computation	O
that	O
can	O
be	O
parallelized,	O
as	O
measured	O
by	O
the	O
minimum	O
number	O
of	O
sequential	O
operations	O
required.	O
The	O
third	O
is	O
the	O
path	O
length	O
between	O
long-range	O
dependencies	O
in	O
the	O
network.	O
Learning	O
long-range	O
dependencies	O
is	O
a	O
key	O
challenge	O
in	O
many	O
sequence	O
transduction	O
tasks.	O
One	O
key	O
factor	O
affecting	O
the	O
ability	O
to	O
learn	O
such	O
dependencies	O
is	O
the	O
length	O
of	O
the	O
paths	O
forward	O
and	O
backward	O
signals	O
have	O
to	O
traverse	O
in	O
the	O
network.	O
The	O
shorter	O
these	O
paths	O
between	O
any	O
combination	O
of	O
positions	O
in	O
the	O
input	O
and	O
output	O
sequences,	O
the	O
easier	O
it	O
is	O
to	O
learn	O
long-range	O
dependencies	O
.	O
Hence	O
we	O
also	O
compare	O
the	O
maximum	O
path	O
length	O
between	O
any	O
two	O
input	O
and	O
output	O
positions	O
in	O
networks	O
composed	O
of	O
the	O
different	O
layer	O
types.	O
As	O
noted	O
in	O
Table	O
1,	O
a	O
self-attention	O
layer	O
connects	O
all	O
positions	O
with	O
a	O
constant	O
number	O
of	O
sequentially	O
executed	O
operations,	O
whereas	O
a	O
recurrent	O
layer	O
requires	O
O(n)	O
sequential	O
operations.	O
In	O
terms	O
of	O
computational	O
complexity,	O
self-attention	O
layers	O
are	O
faster	O
than	O
recurrent	O
layers	O
when	O
the	O
sequence	O
length	O
n	O
is	O
smaller	O
than	O
the	O
representation	O
dimensionality	O
d,	O
which	O
is	O
most	O
often	O
the	O
case	O
with	O
sentence	O
representations	O
used	O
by	O
state-of-the-art	O
models	O
in	O
machine	O
translations,	O
such	O
as	O
word-piece	O
and	O
byte-pair	O
representations.	O
To	O
improve	O
computational	O
performance	O
for	O
tasks	O
involving	O
very	O
long	O
sequences,	O
self-attention	O
could	O
be	O
restricted	O
to	O
considering	O
only	O
a	O
neighborhood	O
of	O
size	O
r	O
in	O
the	O
input	O
sequence	O
centered	O
around	O
the	O
respective	O
output	O
position.	O
This	O
would	O
increase	O
the	O
maximum	O
path	O
length	O
to	O
O(n/r).	O
We	O
plan	O
to	O
investigate	O
this	O
approach	O
further	O
in	O
future	O
work.	O
A	O
single	O
convolutional	O
layer	O
with	O
kernel	O
width	O
k	O
<	O
n	O
does	O
not	O
connect	O
all	O
pairs	O
of	O
input	O
and	O
output	O
positions.	O
Doing	O
so	O
requires	O
a	O
stack	O
of	O
O(n/k)	O
convolutional	O
layers	O
in	O
the	O
case	O
of	O
contiguous	O
kernels,	O
or	O
O(log	O
k	O
(n))	O
in	O
the	O
case	O
of	O
dilated	O
convolutions	O
,	O
increasing	O
the	O
length	O
of	O
the	O
longest	O
paths	O
between	O
any	O
two	O
positions	O
in	O
the	O
network.	O
Convolutional	O
layers	O
are	O
generally	O
more	O
expensive	O
than	O
recurrent	O
layers,	O
by	O
a	O
factor	O
of	O
k.	O
Separable	O
convolutions	O
,	O
however,	O
decrease	O
the	O
complexity	O
considerably,	O
to	O
O(k	O
•	O
n	O
•	O
d	O
+	O
n	O
•	O
d	O
2	O
)	O
.	O
Even	O
with	O
k	O
=	O
n,	O
however,	O
the	O
complexity	O
of	O
a	O
separable	O
convolution	O
is	O
equal	O
to	O
the	O
combination	O
of	O
a	O
self-attention	O
layer	O
and	O
a	O
point-wise	O
feed-forward	O
layer,	O
the	O
approach	O
we	O
take	O
in	O
our	O
model.	O
As	O
side	O
benefit,	O
self-attention	O
could	O
yield	O
more	O
interpretable	O
models.	O
We	O
inspect	O
attention	O
distributions	O
from	O
our	O
models	O
and	O
present	O
and	O
discuss	O
examples	O
in	O
the	O
appendix.	O
Not	O
only	O
do	O
individual	O
attention	O
heads	O
clearly	O
learn	O
to	O
perform	O
different	O
tasks,	O
many	O
appear	O
to	O
exhibit	O
behavior	O
related	O
to	O
the	O
syntactic	O
and	O
semantic	O
structure	O
of	O
the	O
sentences.	O

Since	O
our	O
model	O
contains	O
no	O
recurrence	O
and	O
no	O
convolution,	O
in	O
order	O
for	O
the	O
model	O
to	O
make	O
use	O
of	O
the	O
order	O
of	O
the	O
sequence,	O
we	O
must	O
inject	O
some	O
information	O
about	O
the	O
relative	O
or	O
absolute	O
position	O
of	O
the	O
O(n	O
2	O
•	O
d)	O
O(1)	O
O(1)	O
Recurrent	O
O(n	O
•	O
d	O
2	O
)	O
O(n)	O
O(n)	O
Convolutional	O
O(k	O
•	O
n	O
•	O
d	O
2	O
)	O
O(1)	O
O(log	O
k	O
(n))	O
Self-Attention	O
(restricted)	O
O(r	O
•	O
n	O
•	O
d)	O
O(1)	O
O(n/r)	O
tokens	O
in	O
the	O
sequence.	O
To	O
this	O
end,	O
we	O
add	O
"positional	O
encodings"	O
to	O
the	O
input	O
embeddings	O
at	O
the	O
bottoms	O
of	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
The	O
positional	O
encodings	O
have	O
the	O
same	O
dimension	O
d	O
model	O
as	O
the	O
embeddings,	O
so	O
that	O
the	O
two	O
can	O
be	O
summed.	O
There	O
are	O
many	O
choices	O
of	O
positional	O
encodings,	O
learned	O
and	O
fixed	O
.	O
In	O
this	O
work,	O
we	O
use	O
sine	O
and	O
cosine	O
functions	O
of	O
different	O
frequencies:	O
P	O
E	O
(pos,2i)	O
=	O
sin(pos/10000	O
2i/dmodel	O
)	O
P	O
E	O
(pos,2i+1)	O
=	O
cos(pos/10000	O
2i/dmodel	O
)	O
where	O
pos	O
is	O
the	O
position	O
and	O
i	O
is	O
the	O
dimension.	O
That	O
is,	O
each	O
dimension	O
of	O
the	O
positional	O
encoding	O
corresponds	O
to	O
a	O
sinusoid.	O
The	O
wavelengths	O
form	O
a	O
geometric	O
progression	O
from	O
2π	O
to	O
10000	O
•	O
2π.	O
We	O
chose	O
this	O
function	O
because	O
we	O
hypothesized	O
it	O
would	O
allow	O
the	O
model	O
to	O
easily	O
learn	O
to	O
attend	O
by	O
relative	O
positions,	O
since	O
for	O
any	O
fixed	O
offset	O
k,	O
P	O
E	O
pos+k	O
can	O
be	O
represented	O
as	O
a	O
linear	O
function	O
of	O
P	O
E	O
pos	O
.	O
We	O
also	O
experimented	O
with	O
using	O
learned	O
positional	O
embeddings	O
instead,	O
and	O
found	O
that	O
the	O
two	O
versions	O
produced	O
nearly	O
identical	O
results	O
(see	O
Table	O
3	O
row	O
(E)).	O
We	O
chose	O
the	O
sinusoidal	O
version	O
because	O
it	O
may	O
allow	O
the	O
model	O
to	O
extrapolate	O
to	O
sequence	O
lengths	O
longer	O
than	O
the	O
ones	O
encountered	O
during	O
training.	O

Similarly	O
to	O
other	O
sequence	O
transduction	O
models,	O
we	O
use	O
learned	O
embeddings	O
to	O
convert	O
the	O
input	O
tokens	O
and	O
output	O
tokens	O
to	O
vectors	O
of	O
dimension	O
d	O
model	O
.	O
We	O
also	O
use	O
the	O
usual	O
learned	O
linear	O
transformation	O
and	O
softmax	O
function	O
to	O
convert	O
the	O
decoder	O
output	O
to	O
predicted	O
next-token	O
probabilities.	O
In	O
our	O
model,	O
we	O
share	O
the	O
same	O
weight	O
matrix	O
between	O
the	O
two	O
embedding	O
layers	O
and	O
the	O
pre-softmax	O
linear	O
transformation,	O
similar	O
to	O
.	O
In	O
the	O
embedding	O
layers,	O
we	O
multiply	O
those	O
weights	O
by	O
√	O
d	O
model	O
.	O

In	O
addition	O
to	O
attention	O
sub-layers,	O
each	O
of	O
the	O
layers	O
in	O
our	O
encoder	O
and	O
decoder	O
contains	O
a	O
fully	O
connected	O
feed-forward	O
network,	O
which	O
is	O
applied	O
to	O
each	O
position	O
separately	O
and	O
identically.	O
This	O
consists	O
of	O
two	O
linear	O
transformations	O
with	O
a	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
in	O
between.	O
FFN(x)	O
=	O
max(0,	O
xW	O
1	O
+	O
b	O
1	O
)W	O
2	O
+	O
b	O
2	O
(2)	O
While	O
the	O
linear	O
transformations	O
are	O
the	O
same	O
across	O
different	O
positions,	O
they	O
use	O
different	O
parameters	O
from	O
layer	O
to	O
layer.	O
Another	O
way	O
of	O
describing	O
this	O
is	O
as	O
two	O
convolutions	O
with	O
kernel	B-HyperparameterName
size	I-HyperparameterName
1.	B-HyperparameterValue
The	O
dimensionality	O
of	O
input	O
and	O
output	O
is	O
d	O
model	O
=	O
512,	O
and	O
the	O
inner-layer	O
has	O
dimensionality	O
d	O
f	O
f	O
=	O
2048.	O

The	O
Transformer	B-MethodName
uses	O
multi-head	O
attention	O
in	O
three	O
different	O
ways:	O
•	O
In	O
"encoder-decoder	O
attention"	O
layers,	O
the	O
queries	O
come	O
from	O
the	O
previous	O
decoder	O
layer,	O
and	O
the	O
memory	O
keys	O
and	O
values	O
come	O
from	O
the	O
output	O
of	O
the	O
encoder.	O
This	O
allows	O
every	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
over	O
all	O
positions	O
in	O
the	O
input	O
sequence.	O
This	O
mimics	O
the	O
typical	O
encoder-decoder	O
attention	O
mechanisms	O
in	O
sequence-to-sequence	O
models	O
such	O
as	O
.	O
•	O
The	O
encoder	O
contains	O
self-attention	O
layers.	O
In	O
a	O
self-attention	O
layer	O
all	O
of	O
the	O
keys,	O
values	O
and	O
queries	O
come	O
from	O
the	O
same	O
place,	O
in	O
this	O
case,	O
the	O
output	O
of	O
the	O
previous	O
layer	O
in	O
the	O
encoder.	O
Each	O
position	O
in	O
the	O
encoder	O
can	O
attend	O
to	O
all	O
positions	O
in	O
the	O
previous	O
layer	O
of	O
the	O
encoder.	O
•	O
Similarly,	O
self-attention	O
layers	O
in	O
the	O
decoder	O
allow	O
each	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
to	O
all	O
positions	O
in	O
the	O
decoder	O
up	O
to	O
and	O
including	O
that	O
position.	O
We	O
need	O
to	O
prevent	O
leftward	O
information	O
flow	O
in	O
the	O
decoder	O
to	O
preserve	O
the	O
auto-regressive	O
property.	O
We	O
implement	O
this	O
inside	O
of	O
scaled	O
dot-product	O
attention	O
by	O
masking	O
out	O
(setting	O
to	O
−∞)	O
all	O
values	O
in	O
the	O
input	O
of	O
the	O
softmax	O
which	O
correspond	O
to	O
illegal	O
connections.	O
See	O
Figure	O
2.	O

Instead	O
of	O
performing	O
a	O
single	O
attention	O
function	O
with	O
d	O
model	O
-dimensional	O
keys,	O
values	O
and	O
queries,	O
we	O
found	O
it	O
beneficial	O
to	O
linearly	O
project	O
the	O
queries,	O
keys	O
and	O
values	O
h	O
times	O
with	O
different,	O
learned	O
linear	O
projections	O
to	O
d	O
k	O
,	O
d	O
k	O
and	O
d	O
v	O
dimensions,	O
respectively.	O
On	O
each	O
of	O
these	O
projected	O
versions	O
of	O
queries,	O
keys	O
and	O
values	O
we	O
then	O
perform	O
the	O
attention	O
function	O
in	O
parallel,	O
yielding	O
d	O
v	O
-dimensional	O
output	O
values.	O
These	O
are	O
concatenated	O
and	O
once	O
again	O
projected,	O
resulting	O
in	O
the	O
final	O
values,	O
as	O
depicted	O
in	O
Figure	O
2.	O
Multi-head	O
attention	O
allows	O
the	O
model	O
to	O
jointly	O
attend	O
to	O
information	O
from	O
different	O
representation	O
subspaces	O
at	O
different	O
positions.	O
With	O
a	O
single	O
attention	O
head,	O
averaging	O
inhibits	O
this.	O
MultiHead(Q,	O
K,	O
V	O
)	O
=	O
Concat(head	O
1	O
,	O
...,	O
head	O
h	O
)W	O
O	O
where	O
head	O
i	O
=	O
Attention(QW	O
Q	O
i	O
,	O
KW	O
K	O
i	O
,	O
V	O
W	O
V	O
i	O
)	O
Where	O
the	O
projections	O
are	O
parameter	O
matrices	O
W	O
Q	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
K	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
V	O
i	O
∈	O
R	O
dmodel×dv	O
and	O
W	O
O	O
∈	O
R	O
hdv×dmodel	O
.	O
In	O
this	O
work	O
we	O
employ	O
h	O
=	O
8	O
parallel	O
attention	O
layers,	O
or	O
heads.	O
For	O
each	O
of	O
these	O
we	O
use	O
d	O
k	O
=	O
d	O
v	O
=	O
d	O
model	O
/h	O
=	O
64.	O
Due	O
to	O
the	O
reduced	O
dimension	O
of	O
each	O
head,	O
the	O
total	O
computational	O
cost	O
is	O
similar	O
to	O
that	O
of	O
single-head	O
attention	O
with	O
full	O
dimensionality.	O

We	O
call	O
our	O
particular	O
attention	O
"Scaled	O
Dot-Product	O
Attention"	O
(Figure	O
2).	O
The	O
input	O
consists	O
of	O
queries	O
and	O
keys	O
of	O
dimension	O
d	O
k	O
,	O
and	O
values	O
of	O
dimension	O
d	O
v	O
.	O
We	O
compute	O
the	O
dot	O
products	O
of	O
the	O
query	O
with	O
all	O
keys,	O
divide	O
each	O
by	O
√	O
d	O
k	O
,	O
and	O
apply	O
a	O
softmax	O
function	O
to	O
obtain	O
the	O
weights	O
on	O
the	O
values.	O
In	O
practice,	O
we	O
compute	O
the	O
attention	O
function	O
on	O
a	O
set	O
of	O
queries	O
simultaneously,	O
packed	O
together	O
into	O
a	O
matrix	O
Q.	O
The	O
keys	O
and	O
values	O
are	O
also	O
packed	O
together	O
into	O
matrices	O
K	O
and	O
V	O
.	O
We	O
compute	O
the	O
matrix	O
of	O
outputs	O
as:	O
Attention(Q,	O
K,	O
V	O
)	O
=	O
softmax(	O
QK	O
T	O
√	O
d	O
k	O
)V	O
(1)	O
The	O
two	O
most	O
commonly	O
used	O
attention	O
functions	O
are	O
additive	O
attention	O
,	O
and	O
dot-product	O
(multiplicative)	O
attention.	O
Dot-product	O
attention	O
is	O
identical	O
to	O
our	O
algorithm,	O
except	O
for	O
the	O
scaling	O
factor	O
of	O
1	O
√	O
d	O
k	O
.	O
Additive	O
attention	O
computes	O
the	O
compatibility	O
function	O
using	O
a	O
feed-forward	O
network	O
with	O
a	O
single	O
hidden	O
layer.	O
While	O
the	O
two	O
are	O
similar	O
in	O
theoretical	O
complexity,	O
dot-product	O
attention	O
is	O
much	O
faster	O
and	O
more	O
space-efficient	O
in	O
practice,	O
since	O
it	O
can	O
be	O
implemented	O
using	O
highly	O
optimized	O
matrix	O
multiplication	O
code.	O
While	O
for	O
small	O
values	O
of	O
d	O
k	O
the	O
two	O
mechanisms	O
perform	O
similarly,	O
additive	O
attention	O
outperforms	O
dot	O
product	O
attention	O
without	O
scaling	O
for	O
larger	O
values	O
of	O
d	O
k	O
.	O
We	O
suspect	O
that	O
for	O
large	O
values	O
of	O
d	O
k	O
,	O
the	O
dot	O
products	O
grow	O
large	O
in	O
magnitude,	O
pushing	O
the	O
softmax	O
function	O
into	O
regions	O
where	O
it	O
has	O
extremely	O
small	O
gradients	O
4	O
.	O
To	O
counteract	O
this	O
effect,	O
we	O
scale	O
the	O
dot	O
products	O
by	O
1	O
√	O
d	O
k	O
.	O

An	O
attention	O
function	O
can	O
be	O
described	O
as	O
mapping	O
a	O
query	O
and	O
a	O
set	O
of	O
key-value	O
pairs	O
to	O
an	O
output,	O
where	O
the	O
query,	O
keys,	O
values,	O
and	O
output	O
are	O
all	O
vectors.	O
The	O
output	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
values,	O
where	O
the	O
weight	O
assigned	O
to	O
each	O
value	O
is	O
computed	O
by	O
a	O
compatibility	O
function	O
of	O
the	O
query	O
with	O
the	O
corresponding	O
key.	O

Encoder:	O
The	O
encoder	O
is	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
Each	O
layer	O
has	O
two	O
sub-layers.	O
The	O
first	O
is	O
a	O
multi-head	O
self-attention	O
mechanism,	O
and	O
the	O
second	O
is	O
a	O
simple,	O
positionwise	O
fully	O
connected	O
feed-forward	O
network.	O
We	O
employ	O
a	O
residual	O
connection	O
around	O
each	O
of	O
the	O
two	O
sub-layers,	O
followed	O
by	O
layer	O
normalization	O
.	O
That	O
is,	O
the	O
output	O
of	O
each	O
sub-layer	O
is	O
LayerNorm(x	O
+	O
Sublayer(x)),	O
where	O
Sublayer(x)	O
is	O
the	O
function	O
implemented	O
by	O
the	O
sub-layer	O
itself.	O
To	O
facilitate	O
these	O
residual	O
connections,	O
all	O
sub-layers	O
in	O
the	O
model,	O
as	O
well	O
as	O
the	O
embedding	O
layers,	O
produce	O
outputs	O
of	O
dimension	O
d	O
model	O
=	O
512.	O
Decoder:	O
The	O
decoder	O
is	O
also	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
In	O
addition	O
to	O
the	O
two	O
sub-layers	O
in	O
each	O
encoder	O
layer,	O
the	O
decoder	O
inserts	O
a	O
third	O
sub-layer,	O
which	O
performs	O
multi-head	O
attention	O
over	O
the	O
output	O
of	O
the	O
encoder	O
stack.	O
Similar	O
to	O
the	O
encoder,	O
we	O
employ	O
residual	O
connections	O
around	O
each	O
of	O
the	O
sub-layers,	O
followed	O
by	O
layer	O
normalization.	O
We	O
also	O
modify	O
the	O
self-attention	O
sub-layer	O
in	O
the	O
decoder	O
stack	O
to	O
prevent	O
positions	O
from	O
attending	O
to	O
subsequent	O
positions.	O
This	O
masking,	O
combined	O
with	O
fact	O
that	O
the	O
output	O
embeddings	O
are	O
offset	O
by	O
one	O
position,	O
ensures	O
that	O
the	O
predictions	O
for	O
position	O
i	O
can	O
depend	O
only	O
on	O
the	O
known	O
outputs	O
at	O
positions	O
less	O
than	O
i.	O

Most	O
competitive	O
neural	O
sequence	O
transduction	O
models	O
have	O
an	O
encoder-decoder	O
structure	O
.	O
Here,	O
the	O
encoder	O
maps	O
an	O
input	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
a	O
sequence	O
of	O
continuous	O
representations	O
z	O
=	O
(z	O
1	O
,	O
...,	O
z	O
n	O
).	O
Given	O
z,	O
the	O
decoder	O
then	O
generates	O
an	O
output	O
sequence	O
(y	O
1	O
,	O
...,	O
y	O
m	O
)	O
of	O
symbols	O
one	O
element	O
at	O
a	O
time.	O
At	O
each	O
step	O
the	O
model	O
is	O
auto-regressive	O
,	O
consuming	O
the	O
previously	O
generated	O
symbols	O
as	O
additional	O
input	O
when	O
generating	O
the	O
next.	O
The	O
Transformer	B-MethodName
follows	O
this	O
overall	O
architecture	O
using	O
stacked	O
self-attention	O
and	O
point-wise,	O
fully	O
connected	O
layers	O
for	O
both	O
the	O
encoder	O
and	O
decoder,	O
shown	O
in	O
the	O
left	O
and	O
right	O
halves	O
of	O
Figure	O
1,	O
respectively.	O
Figure	O
1:	O
The	O
Transformer	B-MethodName
-model	O
architecture.	O

The	O
goal	O
of	O
reducing	O
sequential	O
computation	O
also	O
forms	O
the	O
foundation	O
of	O
the	O
Extended	O
Neural	O
GPU	O
,	O
ByteNet	B-MethodName
and	O
ConvS2S	B-MethodName
,	O
all	O
of	O
which	O
use	O
convolutional	O
neural	O
networks	O
as	O
basic	O
building	O
block,	O
computing	O
hidden	O
representations	O
in	O
parallel	O
for	O
all	O
input	O
and	O
output	O
positions.	O
In	O
these	O
models,	O
the	O
number	O
of	O
operations	O
required	O
to	O
relate	O
signals	O
from	O
two	O
arbitrary	O
input	O
or	O
output	O
positions	O
grows	O
in	O
the	O
distance	O
between	O
positions,	O
linearly	O
for	O
ConvS2S	B-MethodName
and	O
logarithmically	O
for	O
ByteNet.	B-MethodName
This	O
makes	O
it	O
more	O
difficult	O
to	O
learn	O
dependencies	O
between	O
distant	O
positions	O
.	O
In	O
the	O
Transformer	B-MethodName
this	O
is	O
reduced	O
to	O
a	O
constant	O
number	O
of	O
operations,	O
albeit	O
at	O
the	O
cost	O
of	O
reduced	O
effective	O
resolution	O
due	O
to	O
averaging	O
attention-weighted	O
positions,	O
an	O
effect	O
we	O
counteract	O
with	O
Multi-Head	O
Attention	O
as	O
described	O
in	O
section	O
3.2.	O
Self-attention,	O
sometimes	O
called	O
intra-attention	O
is	O
an	O
attention	O
mechanism	O
relating	O
different	O
positions	O
of	O
a	O
single	O
sequence	O
in	O
order	O
to	O
compute	O
a	O
representation	O
of	O
the	O
sequence.	O
Self-attention	O
has	O
been	O
used	O
successfully	O
in	O
a	O
variety	O
of	O
tasks	O
including	O
reading	O
comprehension,	O
abstractive	O
summarization,	O
textual	O
entailment	O
and	O
learning	O
task-independent	O
sentence	O
representations	O
.	O
End-to-end	O
memory	O
networks	O
are	O
based	O
on	O
a	O
recurrent	O
attention	O
mechanism	O
instead	O
of	O
sequencealigned	O
recurrence	O
and	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
simple-language	B-TaskName
question	I-TaskName
answering	I-TaskName
and	O
language	B-TaskName
modeling	I-TaskName
tasks	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge,	O
however,	O
the	O
Transformer	B-MethodName
is	O
the	O
first	O
transduction	O
model	O
relying	O
entirely	O
on	O
self-attention	O
to	O
compute	O
representations	O
of	O
its	O
input	O
and	O
output	O
without	O
using	O
sequencealigned	O
RNNs	O
or	O
convolution.	O
In	O
the	O
following	O
sections,	O
we	O
will	O
describe	O
the	O
Transformer,	B-MethodName
motivate	O
self-attention	O
and	O
discuss	O
its	O
advantages	O
over	O
models	O
such	O
as	O
and	O
.	O

Recurrent	O
neural	O
networks,	O
long	O
short-term	O
memory	O
and	O
gated	O
recurrent	O
neural	O
networks	O
in	O
particular,	O
have	O
been	O
firmly	O
established	O
as	O
state	O
of	O
the	O
art	O
approaches	O
in	O
sequence	O
modeling	O
and	O
transduction	O
problems	O
such	O
as	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
Numerous	O
efforts	O
have	O
since	O
continued	O
to	O
push	O
the	O
boundaries	O
of	O
recurrent	O
language	O
models	O
and	O
encoder-decoder	O
architectures	O
.	O
Recurrent	O
models	O
typically	O
factor	O
computation	O
along	O
the	O
symbol	O
positions	O
of	O
the	O
input	O
and	O
output	O
sequences.	O
Aligning	O
the	O
positions	O
to	O
steps	O
in	O
computation	O
time,	O
they	O
generate	O
a	O
sequence	O
of	O
hidden	O
states	O
h	O
t	O
,	O
as	O
a	O
function	O
of	O
the	O
previous	O
hidden	O
state	O
h	O
t−1	O
and	O
the	O
input	O
for	O
position	O
t.	O
This	O
inherently	O
sequential	O
nature	O
precludes	O
parallelization	O
within	O
training	O
examples,	O
which	O
becomes	O
critical	O
at	O
longer	O
sequence	O
lengths,	O
as	O
memory	O
constraints	O
limit	O
batching	O
across	O
examples.	O
Recent	O
work	O
has	O
achieved	O
significant	O
improvements	O
in	O
computational	O
efficiency	O
through	O
factorization	O
tricks	O
and	O
conditional	O
computation	O
,	O
while	O
also	O
improving	O
model	O
performance	O
in	O
case	O
of	O
the	O
latter.	O
The	O
fundamental	O
constraint	O
of	O
sequential	O
computation,	O
however,	O
remains.	O
Attention	O
mechanisms	O
have	O
become	O
an	O
integral	O
part	O
of	O
compelling	O
sequence	O
modeling	O
and	O
transduction	O
models	O
in	O
various	O
tasks,	O
allowing	O
modeling	O
of	O
dependencies	O
without	O
regard	O
to	O
their	O
distance	O
in	O
the	O
input	O
or	O
output	O
sequences	O
.	O
In	O
all	O
but	O
a	O
few	O
cases	O
,	O
however,	O
such	O
attention	O
mechanisms	O
are	O
used	O
in	O
conjunction	O
with	O
a	O
recurrent	O
network.	O
In	O
this	O
work	O
we	O
propose	O
the	O
Transformer,	B-MethodName
a	O
model	O
architecture	O
eschewing	O
recurrence	O
and	O
instead	O
relying	O
entirely	O
on	O
an	O
attention	O
mechanism	O
to	O
draw	O
global	O
dependencies	O
between	O
input	O
and	O
output.	O
The	O
Transformer	B-MethodName
allows	O
for	O
significantly	O
more	O
parallelization	O
and	O
can	O
reach	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
translation	O
quality	O
after	O
being	O
trained	O
for	O
as	O
little	O
as	O
twelve	O
hours	O
on	O
eight	O
P100	O
GPUs.	O

The	O
dominant	O
sequence	O
transduction	O
models	O
are	O
based	O
on	O
complex	O
recurrent	O
or	O
convolutional	O
neural	O
networks	O
that	O
include	O
an	O
encoder	O
and	O
a	O
decoder.	O
The	O
best	O
performing	O
models	O
also	O
connect	O
the	O
encoder	O
and	O
decoder	O
through	O
an	O
attention	O
mechanism.	O
We	O
propose	O
a	O
new	O
simple	O
network	O
architecture,	O
the	O
Transformer,	B-MethodName
based	O
solely	O
on	O
attention	O
mechanisms,	O
dispensing	O
with	O
recurrence	O
and	O
convolutions	O
entirely.	O
Experiments	O
on	O
two	O
machine	B-TaskName
translation	I-TaskName
tasks	O
show	O
these	O
models	O
to	O
be	O
superior	O
in	O
quality	O
while	O
being	O
more	O
parallelizable	O
and	O
requiring	O
significantly	O
less	O
time	O
to	O
train.	O
Our	O
model	O
achieves	O
28.4	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
Englishto-German	I-DatasetName
translation	O
task,	O
improving	O
over	O
the	O
existing	O
best	O
results,	O
including	O
ensembles,	O
by	O
over	O
2	B-MetricValue
BLEU.	B-MetricName
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
model	O
establishes	O
a	O
new	O
single-model	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
41.8	B-MetricValue
after	O
training	O
for	O
3.5	O
days	O
on	O
eight	O
GPUs,	O
a	O
small	O
fraction	O
of	O
the	O
training	O
costs	O
of	O
the	O
best	O
models	O
from	O
the	O
literature.	O
We	O
show	O
that	O
the	O
Transformer	B-MethodName
generalizes	O
well	O
to	O
other	O
tasks	O
by	O
applying	O
it	O
successfully	O
to	O
English	B-TaskName
constituency	I-TaskName
parsing	I-TaskName
both	O
with	O
large	O
and	O
limited	O
training	O
data.	O
*	O
Equal	O
contribution.	O
Listing	O
order	O
is	O
random.	O
Jakob	O
proposed	O
replacing	O
RNNs	O
with	O
self-attention	O
and	O
started	O
the	O
effort	O
to	O
evaluate	O
this	O
idea.	O
Ashish,	O
with	O
Illia,	O
designed	O
and	O
implemented	O
the	O
first	O
Transformer	B-MethodName
models	O
and	O
has	O
been	O
crucially	O
involved	O
in	O
every	O
aspect	O
of	O
this	O
work.	O
Noam	O
proposed	O
scaled	O
dot-product	O
attention,	O
multi-head	O
attention	O
and	O
the	O
parameter-free	O
position	O
representation	O
and	O
became	O
the	O
other	O
person	O
involved	O
in	O
nearly	O
every	O
detail.	O
Niki	O
designed,	O
implemented,	O
tuned	O
and	O
evaluated	O
countless	O
model	O
variants	O
in	O
our	O
original	O
codebase	O
and	O
tensor2tensor.	O
Llion	O
also	O
experimented	O
with	O
novel	O
model	O
variants,	O
was	O
responsible	O
for	O
our	O
initial	O
codebase,	O
and	O
efficient	O
inference	O
and	O
visualizations.	O
Lukasz	O
and	O
Aidan	O
spent	O
countless	O
long	O
days	O
designing	O
various	O
parts	O
of	O
and	O
implementing	O
tensor2tensor,	O
replacing	O
our	O
earlier	O
codebase,	O
greatly	O
improving	O
results	O
and	O
massively	O
accelerating	O
our	O
research.	O
†	O
Work	O
performed	O
while	O
at	O
Google	O
Brain.	O
‡	O
Work	O
performed	O
while	O
at	O
Google	O
Research.	O

We	O
also	O
evaluated	O
performance	O
on	O
WMT16	B-DatasetName
Romanian-English,	I-DatasetName
augmented	O
with	O
back-translation	O
data	O
from	O
Sennrich	O
et	O
al.	O
(2016).	O
We	O
use	O
a	O
6-layer	O
transformer	O
source	O
encoder	O
to	O
map	O
Romanian	O
into	O
a	O
representation	O
that	O
BART	B-MethodName
is	O
able	O
to	O
de-noise	O
into	O
English,	O
following	O
the	O
approach	O
introduced	O
in	O
§3.4.	O
Experiment	O
results	O
are	O
presented	O
in	O
Table	O
6.	O
We	O
compare	O
our	O
results	O
against	O
a	O
baseline	O
Transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017)	O
with	O
Transformerlarge	O
settings	O
(the	O
baseline	O
row).	O
We	O
show	O
the	O
performance	O
of	O
both	O
steps	O
of	O
our	O
model	O
in	O
the	O
fixed	O
BART	B-MethodName
and	O
tuned	O
BART	B-MethodName
rows.	O
For	O
each	O
row	O
we	O
experiment	O
on	O
the	O
original	O
WMT16	B-DatasetName
Romanian-English	I-DatasetName
augmented	O
with	O
back-translation	O
data.	O
We	O
use	O
a	O
beam	B-HyperparameterName
width	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
α	B-HyperparameterValue
=	I-HyperparameterValue
1.	I-HyperparameterValue
Preliminary	O
results	O
suggested	O
that	O
our	O
approach	O
was	O
less	O
effective	O
without	O
back-translation	O
data,	O
and	O
prone	O
to	O
overfitting-future	O
work	O
should	O
explore	O
additional	O
regularization	O
techniques.	O

We	O
also	O
experiment	O
with	O
several	O
text	O
generation	O
tasks.	O
BART	B-MethodName
is	O
fine-tuned	O
as	O
a	O
standard	O
sequence-to-sequence	O
model	O
from	O
the	O
input	O
to	O
the	O
output	O
text.	O
During	O
finetuning	O
we	O
use	O
a	O
label	O
smoothed	O
cross	O
entropy	O
loss	O
(Pereyra	O
et	O
al.,	O
2017),	O
with	O
the	O
smoothing	B-HyperparameterName
parameter	I-HyperparameterName
set	O
to	O
0.1.	B-HyperparameterValue
During	O
generation,	O
we	O
set	O
beam	B-HyperparameterName
size	I-HyperparameterName
as	O
5,	B-HyperparameterValue
remove	O
duplicated	O
trigrams	O
in	O
beam	O
search,	O
and	O
tuned	O
the	O
model	O
with	O
min-len,	O
max-len,	O
length	O
penalty	O
on	O
the	O
validation	O
set	O
(Fan	O
et	O
al.,	O
2017	O
Summarization	O
To	O
provide	O
a	O
comparison	O
with	O
the	O
state-of-the-art	O
in	O
summarization,	O
we	O
present	O
results	O
on	O
two	O
summarization	O
datasets,	O
CNN/DailyMail	B-DatasetName
and	O
XSum,	B-DatasetName
which	O
have	O
distinct	O
properties.	O
Summaries	O
in	O
the	O
CNN/DailyMail	B-DatasetName
tend	O
to	O
resemble	O
source	O
sentences.	O
Extractive	O
models	O
do	O
well	O
here,	O
and	O
even	O
the	O
baseline	O
of	O
the	O
first-three	O
source	O
sentences	O
is	O
highly	O
competitive.	O
Nevertheless,	O
BART	O
outperforms	O
all	O
existing	O
work.	O
In	O
contrast,	O
XSum	B-MethodName
is	O
highly	O
abstractive,	O
and	O
extractive	O
models	O
perform	O
poorly.	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work,	O
which	O
leverages	O
BERT,	B-MethodName
by	O
roughly	O
6.0	B-MetricValue
points	I-MetricValue
on	O
all	O
ROUGE	B-MetricName
metrics-representing	O
a	O
significant	O
advance	O
in	O
performance	O
on	O
this	O
problem.	O
Qualitatively,	O
sample	O
quality	O
is	O
high	O
(see	O
§6).	O
Dialogue	O
We	O
evaluate	O
dialogue	O
response	O
generation	O
on	O
CONVAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
in	O
which	O
agents	O
must	O
generate	O
responses	O
conditioned	O
on	O
both	O
the	O
previous	O
context	O
and	O
a	O
textually-specified	O
persona.	O
BART	B-MethodName
outperforms	O
previous	O
work	O
on	O
two	O
automated	O
metrics.	O
Abstractive	B-TaskName
QA	I-TaskName
We	O
use	O
the	O
recently	O
proposed	O
ELI5	B-DatasetName
dataset	O
to	O
test	O
the	O
model's	O
ability	O
to	O
generate	O
long	O
freeform	O
answers.	O
We	O
find	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work	O
by	O
1.2	B-MetricValue
ROUGE-L,	B-MetricName
but	O
the	O
dataset	O
remains	O
a	O
challenging,	O
because	O
answers	O
are	O
only	O
weakly	O
specified	O
by	O
the	O
question.	O

Table	O
2	O
compares	O
the	O
performance	O
of	O
BART	B-MethodName
with	O
several	O
recent	O
approaches	O
on	O
the	O
well-studied	O
SQuAD	B-TaskName
and	O
GLUE	B-TaskName
tasks	O
(Warstadt	O
et	O
al.,	O
2018;Socher	O
et	O
al.,	O
2013;Dolan	O
&	O
Brockett,	O
2005;Agirre	O
et	O
al.,	O
2007;Williams	O
et	O
al.,	O
2018;Dagan	O
et	O
al.,	O
2006;Levesque	O
et	O
al.,	O
2011).	O
The	O
most	O
directly	O
comparable	O
baseline	O
is	O
RoBERTa,	B-MethodName
which	O
was	O
pre-trained	O
with	O
the	O
same	O
resources,	O
but	O
a	O
different	O
objective.	O
Overall,	O
BART	B-MethodName
performs	O
similarly,	O
with	O
only	O
small	O
differences	O
between	O
the	O
models	O
on	O
most	O
tasks.	O
suggesting	O
that	O
BART's	B-MethodName
improvements	O
on	O
generation	O
tasks	O
do	O
not	O
come	O
at	O
the	O
expense	O
of	O
classification	O
performance.	O

We	O
pre-train	O
a	O
large	O
model	O
with	O
12	O
layers	O
in	O
each	O
of	O
the	O
encoder	O
and	O
decoder,	O
and	O
a	O
hidden	O
size	O
of	O
1024.	O
Following	O
RoBERTa	B-MethodName
,	O
we	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8000,	B-HyperparameterValue
and	O
train	O
the	O
model	O
for	O
500000	B-HyperparameterValue
steps.	B-HyperparameterName
Documents	O
are	O
tokenized	O
with	O
the	O
same	O
byte-pair	O
encoding	O
as	O
GPT-2	B-MethodName
(Radford	O
et	O
al.,	O
2019).	O
Based	O
on	O
the	O
results	O
in	O
Section	O
§4,	O
we	O
use	O
a	O
combination	O
of	O
text	O
infilling	O
and	O
sentence	O
permutation.	O
We	O
mask	O
30%	B-MetricValue
of	O
tokens	O
in	O
each	O
document,	O
and	O
permute	O
all	O
sentences.	O
Although	O
sentence	O
permutation	O
only	O
shows	O
significant	O
additive	O
gains	O
on	O
the	O
CNN/DM	B-DatasetName
summarization	I-DatasetName
dataset,	O
we	O
hypothesised	O
that	O
larger	O
pre-trained	O
models	O
may	O
be	O
better	O
able	O
to	O
learn	O
from	O
this	O
task.	O
To	O
help	O
the	O
model	O
better	O
fit	O
the	O
data,	O
we	O
disabled	O
dropout	B-HyperparameterName
for	O
the	O
final	O
10%	B-MetricValue
of	O
training	O
steps.	O
We	O
use	O
the	O
same	O
pre-training	O
data	O
as	O
,	O
consisting	O
of	O
160Gb	O
of	O
news,	O
books,	O
stories,	O
and	O
web	O
text.	O

Recent	O
work	O
has	O
shown	O
that	O
downstream	O
performance	O
can	O
dramatically	O
improve	O
when	O
pre-training	O
is	O
scaled	O
to	O
large	O
batch	O
sizes	O
(Yang	O
et	O
al.,	O
2019;	O
and	O
corpora.	O
To	O
test	O
how	O
well	O
BART	B-MethodName
performs	O
in	O
this	O
regime,	O
and	O
to	O
create	O
a	O
useful	O
model	O
for	O
downstream	O
tasks,	O
we	O
trained	O
BART	B-MethodName
using	O
the	O
same	O
scale	O
as	O
the	O
RoBERTa	B-MethodName
model.	O

BART	B-MethodName
shows	O
large	O
improvements	O
on	O
summarization	O
metrics,	O
of	O
up	O
to	O
6	O
points	O
over	O
the	O
prior	O
state-of-the-art.	O
To	O
understand	O
BART's	B-MethodName
performance	O
beyond	O
automated	O
metrics,	O
we	O
analyse	O
its	O
generations	O
qualitatively.	O
Table	O
7	O
shows	O
example	O
summaries	O
generated	O
by	O
BART.	B-MethodName
Examples	O
are	O
taken	O
from	O
WikiNews	B-DatasetName
articles	I-DatasetName
published	O
after	O
the	O
creation	O
of	O
the	O
pre-training	O
corpus,	O
to	O
eliminate	O
the	O
possibility	O
of	O
the	O
events	O
described	O
being	O
present	O
in	O
the	O
model's	O
training	O
data.	O
Following	O
Narayan	O
et	O
al.	O
(2018),	O
we	O
remove	O
the	O
first	O
sentence	O
of	O
the	O
article	O
prior	O
to	O
summarizing	O
it,	O
so	O
there	O
is	O
no	O
easy	O
extractive	O
summary	O
of	O
the	O
document.	O
Unsurprisingly,	O
model	O
output	O
is	O
fluent	O
and	O
grammatical	O
English.	O
However,	O
model	O
output	O
is	O
also	O
highly	O
abstractive,	O
with	O
few	O
phrases	O
copied	O
from	O
the	O
input.	O
The	O
output	O
is	O
also	O
generally	O
factually	O
accurate,	O
and	O
integrates	O
supporting	O
evidence	O
from	O
across	O
the	O
input	O
document	O
with	O
background	O
knowledge	O
(for	O
example,	O
correctly	O
completing	O
names,	O
or	O
inferring	O
that	O
PG&E	O
operates	O
in	O
California).	O
In	O
the	O
first	O
example,	O
inferring	O
that	O
fish	O
are	O
protecting	O
reefs	O
from	O
global	O
warming	O
requires	O
non-trivial	O
inference	O
from	O
the	O
text.	O
However,	O
the	O
claim	O
that	O
the	O
work	O
was	O
published	O
in	O
Science	O
is	O
not	O
supported	O
by	O
the	O
source.	O
These	O
samples	O
demonstrate	O
that	O
the	O
BART	B-MethodName
pretraining	O
has	O
learned	O
a	O
strong	O
combination	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
and	I-TaskName
generation.	I-TaskName

Early	O
methods	O
for	O
pretraining	O
were	O
based	O
on	O
language	O
models.	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018)	O
only	O
models	O
leftward	O
context,	O
which	O
is	O
problematic	O
for	O
some	O
tasks.	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018)	O
concatenates	O
left-only	O
and	O
right-only	O
representations,	O
but	O
does	O
not	O
pre-train	O
interactions	O
between	O
these	O
features.	O
Radford	O
et	O
al.	O
(2019)	O
demonstrated	O
that	O
very	O
large	O
language	O
models	O
can	O
act	O
as	O
unsupervised	O
multitask	O
models.	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
introduced	O
masked	O
language	O
modelling,	O
which	O
allows	O
pre-training	O
to	O
learn	O
interactions	O
between	O
left	O
and	O
right	O
context	O
words.	O
Recent	O
work	O
has	O
shown	O
that	O
very	O
strong	O
performance	O
can	O
be	O
achieved	O
by	O
training	O
for	O
longer	O
,	O
by	O
tying	O
parameters	O
across	O
layers	O
(Lan	O
et	O
al.,	O
2019),	O
and	O
by	O
masking	O
spans	O
instead	O
of	O
words	O
.	O
Predictions	O
are	O
not	O
made	O
auto-regressively,	O
reducing	O
the	O
effectiveness	O
of	O
BERT	B-MethodName
for	O
generation	O
tasks.	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019)	O
fine-tunes	O
BERT	B-MethodName
with	O
an	O
ensemble	O
of	O
masks,	O
some	O
of	O
which	O
allow	O
only	O
leftward	O
context.	O
Like	O
BART,	B-MethodName
this	O
allows	O
UniLM	B-MethodName
to	O
be	O
used	O
for	O
both	O
generative	O
and	O
discriminative	O
tasks.	O
A	O
difference	O
is	O
that	O
UniLM	B-MethodName
predictions	O
are	O
conditionally	O
independent,	O
whereas	O
BART's	B-MethodName
are	O
autoregressive.	O
BART	B-MethodName
reduces	O
the	O
mismatch	O
between	O
pre-training	O
and	O
generation	O
tasks,	O
because	O
the	O
decoder	O
is	O
always	O
trained	O
on	O
uncorrupted	O
context.	O
MASS	B-MethodName
(Song	O
et	O
al.,	O
2019)	O
is	O
perhaps	O
the	O
most	O
similar	O
model	O
to	O
BART.	B-MethodName
An	O
input	O
sequence	O
where	O
a	O
contiguous	O
span	O
of	O
tokens	O
is	O
masked	O
is	O
mapped	O
to	O
a	O
sequence	O
consisting	O
of	O
the	O
missing	O
tokens.	O
MASS	B-MethodName
is	O
less	O
effective	O
for	O
discriminative	O
tasks,	O
because	O
disjoint	O
sets	O
of	O
tokens	O
are	O
fed	O
into	O
the	O
encoder	O
and	O
decoder.	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
extends	O
BERT	B-MethodName
by	O
pre-Source	O
Document	O
(abbreviated)	O

The	O
researchers	O
examined	O
three	O
types	O
of	O
coral	O
in	O
reefs	O
off	O
the	O
coast	O
of	O
Fiji	O
...	O
The	O
researchers	O
found	O
when	O
fish	O
were	O
plentiful,	O
they	O
would	O
eat	O
algae	O
and	O
seaweed	O
off	O
the	O
corals,	O
which	O
appeared	O
to	O
leave	O
them	O
more	O
resistant	O
to	O
the	O
bacterium	O
Vibrio	O
coralliilyticus,	O
a	O
bacterium	O
associated	O
with	O
bleaching.	O
The	O
researchers	O
suggested	O
the	O
algae,	O
like	O
warming	O
temperatures,	O
might	O
render	O
the	O
corals'	O
chemical	O
defenses	O
less	O
effective,	O
and	O
the	O
fish	O
were	O
protecting	O
the	O
coral	O
by	O
removing	O
the	O
algae.	O
Fisheries	O
off	O
the	O
coast	O
of	O
Fiji	O
are	O
protecting	O
coral	O
reefs	O
from	O
the	O
effects	O
of	O
global	O
warming,	O
according	O
to	O
a	O
study	O
in	O
the	O
journal	O
Science.	O
Sacoolas,	O
who	O
has	O
immunity	O
as	O
a	O
diplomat's	O
wife,	O
was	O
involved	O
in	O
a	O
traffic	O
collision	O
...	O
Prime	O
Minister	O
Johnson	O
was	O
questioned	O
about	O
the	O
case	O
while	O
speaking	O
to	O
the	O
press	O
at	O
a	O
hospital	O
in	O
Watford.	O
He	O
said,	O
"I	O
hope	O
that	O
Anne	O
Sacoolas	O
will	O
come	O
back	O
...	O
if	O
we	O
can't	O
resolve	O
it	O
then	O
of	O
course	O
I	O
will	O
be	O
raising	O
it	O
myself	O
personally	O
with	O
the	O
White	O
House."	O
Boris	O
Johnson	O
has	O
said	O
he	O
will	O
raise	O
the	O
issue	O
of	O
US	O
diplomat	O
Anne	O
Sacoolas'	O
diplomatic	O
immunity	O
with	O
the	O
White	O
House.	O
PG&E	O
stated	O
it	O
scheduled	O
the	O
blackouts	O
in	O
response	O
to	O
forecasts	O
for	O
high	O
winds	O
amid	O
dry	O
conditions.	O
The	O
aim	O
is	O
to	O
reduce	O
the	O
risk	O
of	O
wildfires.	O
Nearly	O
800	O
thousand	O
customers	O
were	O
scheduled	O
to	O
be	O
affected	O
by	O
the	O
shutoffs	O
which	O
were	O
expected	O
to	O
last	O
through	O
at	O
least	O
midday	O
tomorrow.	O
Power	O
has	O
been	O
turned	O
off	O
to	O
millions	O
of	O
customers	O
in	O
California	O
as	O
part	O
of	O
a	O
power	O
shutoff	O
plan.	O
dicting	O
masked	O
tokens	O
auto-regressively	O
in	O
a	O
permuted	O
order.	O
This	O
objective	O
allows	O
predictions	O
to	O
condition	O
on	O
both	O
left	O
and	O
right	O
context.	O
In	O
contrast,	O
the	O
BART	B-MethodName
decoder	O
works	O
left-to-right	O
during	O
pre-training,	O
matching	O
the	O
setting	O
during	O
generation.	O
Several	O
papers	O
have	O
explored	O
using	O
pre-trained	O
representations	O
to	O
improve	O
machine	B-TaskName
translation.	I-TaskName
The	O
largest	O
improvements	O
have	O
come	O
from	O
pre-training	O
on	O
both	O
source	O
and	O
target	O
languages	O
(Song	O
et	O
al.,	O
2019;Lample	O
&	O
Conneau,	O
2019),	O
but	O
this	O
requires	O
pretraining	O
on	O
all	O
languages	O
of	O
interest.	O
Other	O
work	O
has	O
shown	O
that	O
encoders	O
can	O
be	O
improved	O
using	O
pre-trained	O
representations	O
(Edunov	O
et	O
al.,	O
2019),	O
but	O
gains	O
in	O
decoders	O
are	O
more	O
limited.	O
We	O
show	O
how	O
BART	B-MethodName
can	O
be	O
used	O
to	O
improve	O
machine	O
translation	O
decoders.	O

We	O
introduced	O
BART,	B-MethodName
a	O
pre-training	O
approach	O
that	O
learns	O
to	O
map	O
corrupted	O
documents	O
to	O
the	O
original.	O
BART	B-MethodName
achieves	O
similar	O
performance	O
to	O
RoBERTa	B-MethodName
on	O
discriminative	O
tasks,	O
while	O
achieving	O
new	O
state-of-theart	O
results	O
on	O
a	O
number	O
of	O
text	B-TaskName
generation	I-TaskName
tasks.	O
Future	O
work	O
should	O
explore	O
new	O
methods	O
for	O
corrupting	O
documents	O
for	O
pre-training,	O
perhaps	O
tailoring	O
them	O
to	O
specific	O
end	O
tasks.	O

The	O
Masked	O
Language	O
Model	O
and	O
the	O
Permuted	O
Language	O
Model	O
perform	O
less	O
well	O
than	O
others	O
on	O
generation,	O
and	O
are	O
the	O
only	O
models	O
we	O
consider	O
that	O
do	O
not	O
include	O
left-to-right	O
auto-regressive	O
language	O
modelling	O
during	O
pre-training.	O
Bidirectional	O
encoders	O
are	O
crucial	O
for	O
SQuAD	B-DatasetName
As	O
noted	O
in	O
previous	O
work	O
(Devlin	O
et	O
al.,	O
2019),	O
just	O
left-to-right	O
decoder	O
performs	O
poorly	O
on	O
SQuAD,	B-DatasetName
because	O
future	O
context	O
is	O
crucial	O
in	O
classification	O
decisions.	O
However,	O
BART	B-MethodName
achieves	O
similar	O
performance	O
with	O
only	O
half	O
the	O
number	O
of	O
bidirectional	O
layers.	O
The	O
pre-training	O
objective	O
is	O
not	O
the	O
only	O
important	O
factor	O
Our	O
Permuted	O
Language	O
Model	O
performs	O
less	O
well	O
than	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019).	O
Some	O
of	O
this	O
difference	O
is	O
likely	O
due	O
to	O
not	O
including	O
other	O
architectural	O
improvements,	O
such	O
as	O
relative-position	O
embeddings	O
or	O
segment-level	O
recurrence.	O
Pure	O
language	O
models	O
perform	O
best	O
on	O
ELI5	B-DatasetName
The	O
ELI5	B-DatasetName
dataset	O
is	O
an	O
outlier,	O
with	O
much	O
higher	O
perplexities	B-MetricName
than	O
other	O
tasks,	O
and	O
is	O
the	O
only	O
generation	O
task	O
where	O
other	O
models	O
outperform	O
BART.	B-MethodName
A	O
pure	O
language	O
model	O
performs	O
best,	O
suggesting	O
that	O
BART	B-MethodName
is	O
less	O
effective	O
when	O
the	O
output	O
is	O
only	O
loosely	O
constrained	O
by	O
the	O
input.	O
BART	B-MethodName
achieves	O
the	O
most	O
consistently	O
strong	O
performance.	O
With	O
the	O
exception	O
of	O
ELI5,	B-DatasetName
BART	B-MethodName
models	O
using	O
text-infilling	O
perform	O
well	O
on	O
all	O
tasks.	O

SQuAD	B-MethodName
(Rajpurkar	O
et	O
al.,	O
2016)a	O
an	O
extractive	O
question	O
answering	O
task	O
on	O
Wikipedia	O
paragraphs.	O
Answers	O
are	O
text	O
spans	O
extracted	O
from	O
a	O
given	O
document	O
context.	O
Similar	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
use	O
concatenated	O
question	O
and	O
context	O
as	O
input	O
to	O
the	O
encoder	O
of	O
BART,	B-MethodName
and	O
additionally	O
pass	O
them	O
to	O
the	O
decoder.	O
The	O
model	O
includes	O
classifiers	O
to	O
predict	O
the	O
start	O
and	O
end	O
indices	O
of	O
each	O
token.	O
MNLI	B-DatasetName
(Williams	O
et	O
al.,	O
2017),	O
a	O
bitext	B-TaskName
classification	I-TaskName
task	I-TaskName
to	O
predict	O
whether	O
one	O
sentence	O
entails	O
another.	O
The	O
fine-tuned	O
model	O
concatenates	O
the	O
two	O
sentences	O
with	O
appended	O
an	O
EOS	O
token,	O
and	O
passes	O
them	O
to	O
both	O
the	O
BART	B-MethodName
encoder	O
and	O
decoder.	O
In	O
contrast	O
to	O
BERT,	B-MethodName
the	O
representation	O
of	O
the	O
EOS	O
token	O
is	O
used	O
to	O
classify	O
the	O
sentences	O
relations.	O
ELI5	B-DatasetName
(Fan	O
et	O
al.,	O
2019),	O
a	O
long-form	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
dataset.	O
Models	O
generate	O
answers	O
conditioned	O
on	O
the	O
concatenation	O
of	O
a	O
question	O
and	O
supporting	O
documents.	O
XSum	B-DatasetName
(Narayan	O
et	O
al.,	O
2018),	O
a	O
news	O
summarization	O
dataset	O
with	O
highly	O
abstractive	O
summaries.	O
ConvAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
a	O
dialogue	O
response	O
generation	O
task,	O
conditioned	O
on	O
context	O
and	O
a	O
persona.	O
(Hermann	O
et	O
al.,	O
2015),	O
a	O
news	O
summarization	O
dataset.	O
Summaries	O
here	O
are	O
typically	O
closely	O
related	O
to	O
source	O
sentences.	O

While	O
many	O
pre-training	O
objectives	O
have	O
been	O
proposed,	O
fair	O
comparisons	O
between	O
these	O
have	O
been	O
difficult	O
to	O
perform,	O
at	O
least	O
in	O
part	O
due	O
to	O
differences	O
in	O
training	O
data,	O
training	O
resources,	O
architectural	O
differences	O
between	O
models,	O
and	O
fine-tuning	O
procedures.	O
We	O
re-implement	O
strong	O
pre-training	O
approaches	O
recently	O
proposed	O
for	O
discriminative	B-TaskName
and	I-TaskName
generation	I-TaskName
tasks.	I-TaskName
We	O
aim,	O
as	O
much	O
as	O
possible,	O
to	O
control	O
for	O
differences	O
unrelated	O
to	O
the	O
pre-training	O
objective.	O
However,	O
we	O
do	O
make	O
minor	O
changes	O
to	O
the	O
learning	O
rate	O
and	O
usage	O
of	O
layer	O
normalisation	O
in	O
order	O
to	O
improve	O
performance	O
(tuning	O
these	O
separately	O
for	O
each	O
objective).	O
For	O
reference,	O
we	O
compare	O
our	O
implementations	O
with	O
published	O
numbers	O
from	O
BERT,	B-MethodName
which	O
was	O
also	O
trained	O
for	O
1M	B-HyperparameterValue
steps	B-HyperparameterName
on	O
a	O
combination	O
of	O
books	O
and	O
Wikipedia	O
data.	O
We	O
compare	O
the	O
following	O
approaches:	O
Language	O
Model	O
Similarly	O
to	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
we	O
train	O
a	O
left-to-right	O
Transformer	O
language	O
model.	O
This	O
model	O
is	O
equivalent	O
to	O
the	O
BART	B-MethodName
decoder,	O
without	O
cross-attention.	O
Permuted	O
Language	O
Model	O
Based	O
on	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
we	O
sample	O
1/6	O
of	O
the	O
tokens,	O
and	O
generate	O
them	O
in	O
a	O
random	O
order	O
autoregressively.	O
For	O
consistency	O
with	O
other	O
models,	O
we	O
do	O
not	O
implement	O
the	O
relative	O
positional	O
embeddings	O
or	O
attention	O
across	O
segments	O
from	O
XLNet.	B-MethodName
Masked	O
Language	O
Model	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
replace	O
15%	B-MetricValue
of	O
tokens	O
with	O
[MASK]	O
symbols,	O
and	O
train	O
the	O
model	O
to	O
independently	O
predict	O
the	O
original	O
tokens.	O
Multitask	O
Masked	O
Language	O
Model	O
As	O
in	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019),	O
we	O
train	O
a	O
Masked	O
Language	O
Model	O
with	O
additional	O
self-attention	O
masks.	O
Self	O
attention	O
masks	O
are	O
chosen	O
randomly	O
in	O
with	O
the	O
follow	O
proportions:	O
1/6	O
left-to-right,	O
1/6	O
right-to-left,	O
1/3	O
unmasked,	O
and	O
1/3	O
with	O
the	O
first	O
50%	B-MetricValue
of	O
tokens	O
unmasked	O
and	O
a	O
left-to-right	O
mask	O
for	O
the	O
remainder.	O
Masked	B-MethodName
Seq-to-Seq	I-MethodName
Inspired	I-MethodName
by	I-MethodName
MASS	I-MethodName
(Song	O
et	O
al.,	O
2019),	O
we	O
mask	O
a	O
span	O
containing	O
50%	B-MetricValue
of	O
tokens,	O
and	O
train	O
a	O
sequence	O
to	O
sequence	O
model	O
to	O
predict	O
the	O
masked	O
tokens.	O
For	O
the	O
Permuted	O
LM,	O
Masked	B-MethodName
LM	I-MethodName
and	O
Multitask	B-MethodName
Masked	I-MethodName
LM,	I-MethodName
we	O
use	O
two-stream	O
attention	O
(Yang	O
et	O
al.,	O
2019)	O
to	O
efficiently	O
compute	O
likelihoods	O
of	O
the	O
output	O
part	O
of	O
the	O
sequence	O
(using	O
a	O
diagonal	O
self-attention	O
mask	O
on	O
the	O
output	O
to	O
predict	O
words	O
left-to-right).	O
We	O
experiment	O
with	O
(1)	O
treating	O
the	O
task	O
as	O
a	O
standard	O
sequence-to-sequence	O
problem,	O
where	O
the	O
source	O
input	O
to	O
the	O
encoder	O
and	O
the	O
target	O
is	O
the	O
decoder	O
output,	O
or	O
(2)	O
adding	O
the	O
source	O
as	O
prefix	O
to	O
the	O
target	O
in	O
the	O
decoder,	O
with	O
a	O
loss	O
only	O
on	O
the	O
target	O
part	O
of	O
the	O
sequence.	O
We	O
find	O
the	O
former	O
works	O
better	O
for	O
BART	B-MethodName
models,	O
and	O
the	O
latter	O
for	O
other	O
models.	O
To	O
most	O
directly	O
compare	O
our	O
models	O
on	O
their	O
ability	O
to	O
model	O
their	O
fine-tuning	O
objective	O
(the	O
log	O
likelihood	O
of	O
the	O
human	O
text),	O
we	O
report	O
perplexity	B-MetricName
in	O
Table	O
1.	O

BART	B-MethodName
supports	O
a	O
much	O
wider	O
range	O
of	O
noising	O
schemes	O
during	O
pre-training	O
than	O
previous	O
work.	O
We	O
compare	O
a	O
range	O
of	O
options	O
using	O
base-size	O
models	O
(6	O
encoder	O
and	O
6	B-HyperparameterValue
decoder	B-HyperparameterName
layers,	I-HyperparameterName
with	O
a	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	O
768),	B-HyperparameterValue
evaluated	O
on	O
a	O
representative	O
subset	O
of	O
the	O
tasks	O
we	O
will	O
consider	O
for	O
the	O
full	O
large	O
scale	O
experiments	O
in	O
§5.	O

We	O
also	O
explore	O
using	O
BART	B-MethodName
to	O
improve	O
machine	O
translation	O
decoders	O
for	O
translating	O
into	O
English.	O
Previous	O
work	O
Edunov	O
et	O
al.	O
(2019)	O
has	O
shown	O
that	O
models	O
can	O
be	O
improved	O
by	O
incorporating	O
pre-trained	O
encoders,	O
but	O
gains	O
from	O
using	O
pre-trained	O
language	O
models	O
in	O
decoders	O
have	O
been	O
limited.	O
We	O
show	O
that	O
it	O
is	O
possible	O
to	O
use	O
the	O
entire	O
BART	B-MethodName
model	O
(both	O
encoder	O
and	O
decoder)	O
as	O
a	O
single	O
pretrained	O
decoder	O
for	O
machine	B-TaskName
translation,	I-TaskName
by	O
adding	O
a	O
new	O
set	O
of	O
encoder	O
parameters	O
that	O
are	O
learned	O
from	O
bitext	O
(see	O
Figure	O
3b).	O
More	O
precisely,	O
we	O
replace	O
BART's	B-MethodName
encoder	O
embedding	O
layer	O
with	O
a	O
new	O
randomly	O
initialized	O
encoder.	O
The	O
model	O
is	O
trained	O
end-to-end,	O
which	O
trains	O
the	O
new	O
encoder	O
to	O
map	O
foreign	O
words	O
into	O
an	O
input	O
that	O
BART	B-MethodName
can	O
de-noise	O
to	O
English.	O
The	O
new	O
encoder	O
can	O
use	O
a	O
separate	O
vocabulary	O
from	O
the	O
original	O
BART	B-MethodName
model.	O
We	O
train	O
the	O
source	O
encoder	O
in	O
two	O
steps,	O
in	O
both	O
cases	O
backpropagating	O
the	O
cross-entropy	B-MetricName
loss	O
from	O
the	O
output	O
of	O
the	O
BART	B-MethodName
model.	O
In	O
the	O
first	O
step,	O
we	O
freeze	O
most	O
of	O
BART	B-MethodName
parameters	O
and	O
only	O
update	O
the	O
randomly	O
initialized	O
source	O
encoder,	O
the	O
BART	B-MethodName
positional	O
embeddings,	O
and	O
the	O
self-attention	O
input	O
projection	O
matrix	O
of	O
BART's	B-MethodName
encoder	O
first	O
layer.	O
In	O
the	O
second	O
step,	O
we	O
train	O
all	O
model	O
parameters	O
for	O
a	O
small	O
number	O
of	O
iterations.	O

Because	O
BART	B-MethodName
has	O
an	O
autoregressive	O
decoder,	O
it	O
can	O
be	O
directly	O
fine	O
tuned	O
for	O
sequence	O
generation	O
tasks	O
such	O
as	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
and	I-TaskName
summarization.	I-TaskName
In	O
both	O
of	O
these	O
tasks,	O
information	O
is	O
copied	O
from	O
the	O
input	O
but	O
manipulated,	O
which	O
is	O
closely	O
related	O
to	O
the	O
denoising	O
pre-training	O
objective.	O
Here,	O
the	O
encoder	O
input	O
is	O
the	O
input	O
sequence,	O
and	O
the	O
decoder	O
generates	O
outputs	O
autoregressively.	O

For	O
token	B-TaskName
classification	I-TaskName
tasks,	I-TaskName
such	O
as	O
answer	O
endpoint	B-TaskName
classification	I-TaskName
for	O
SQuAD,	B-DatasetName
we	O
feed	O
the	O
complete	O
document	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
use	O
the	O
top	O
hidden	O
state	O
of	O
the	O
decoder	O
as	O
a	O
representation	O
for	O
each	O
word.	O
This	O
representation	O
is	O
used	O
to	O
classify	O
the	O
token.	O

For	O
sequence	B-TaskName
classification	I-TaskName
tasks,	O
the	O
same	O
input	O
is	O
fed	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
the	O
final	O
hidden	O
state	O
of	O
the	O
final	O
decoder	O
token	O
is	O
fed	O
into	O
new	O
multi-class	O
linear	O
classifier.	O
This	O
approach	O
is	O
related	O
to	O
the	O
CLS	O
token	O
in	O
BERT;	B-MethodName
however	O
we	O
add	O
the	O
additional	O
token	O
to	O
the	O
end	O
so	O
that	O
representation	O
for	O
the	O
token	O
in	O
the	O
decoder	O
can	O
attend	O
to	O
decoder	O
states	O
from	O
the	O
complete	O
input	O
(Figure	O
3a).	O

The	O
representations	O
produced	O
by	O
BART	B-MethodName
can	O
be	O
used	O
in	O
several	O
ways	O
for	O
downstream	O
applications.	O

BART	B-MethodName
is	O
trained	O
by	O
corrupting	O
documents	O
and	O
then	O
optimizing	O
a	O
reconstruction	B-MetricName
loss-the	I-MetricName
cross-entropy	B-MetricName
between	O
the	O
decoder's	O
output	O
and	O
the	O
original	O
document.	O
Unlike	O
existing	O
denoising	O
autoencoders,	O
which	O
are	O
tailored	O
to	O
specific	O
noising	O
schemes,	O
BART	B-MethodName
allows	O
us	O
to	O
apply	O
any	O
type	O
of	O
document	O
corruption.	O
In	O
the	O
extreme	O
case,	O
where	O
all	O
information	O
about	O
the	O
source	O
is	O
lost,	O
BART	B-MethodName
is	O
equivalent	O
to	O
a	O
language	O
model.	O
We	O
experiment	O
with	O
several	O
previously	O
proposed	O
and	O
novel	O
transformations,	O
but	O
we	O
believe	O
there	O
is	O
a	O
significant	O
potential	O
for	O
development	O
of	O
other	O
new	O
alternatives.	O
The	O
transformations	O
we	O
used	O
are	O
summarized	O
below,	O
and	O
examples	O
are	O
shown	O
in	O
Figure	O
2.	O
Token	O
Masking	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
random	O
tokens	O
are	O
sampled	O
and	O
replaced	O
with	O
[MASK]	O
elements.	O
Token	O
Deletion	O
Random	O
tokens	O
are	O
deleted	O
from	O
the	O
input.	O
In	O
contrast	O
to	O
token	O
masking,	O
the	O
model	O
must	O
decide	O
which	O
positions	O
are	O
missing	O
inputs.	O
Text	O
Infilling	O
A	O
number	O
of	O
text	O
spans	O
are	O
sampled,	O
with	O
span	O
lengths	O
drawn	O
from	O
a	O
Poisson	O
distribution	O
(λ	O
=	O
3).	O
Each	O
span	O
is	O
replaced	O
with	O
a	O
single	O
[MASK]	O
token.	O
0-length	O
spans	O
correspond	O
to	O
the	O
insertion	O
of	O
[MASK]	O
tokens.	O
Text	O
infilling	O
is	O
inspired	O
by	O
Span-BERT	B-MethodName
,	O
but	O
SpanBERT	B-MethodName
samples	O
span	O
lengths	O
from	O
a	O
different	O
(clamped	O
geometric)	O
distribution,	O
and	O
replaces	O
each	O
span	O
with	O
a	O
sequence	O
of	O
[MASK]	O
tokens	O
of	O
exactly	O
the	O
same	O
length.	O
Text	O
infilling	O
teaches	O
the	O
model	O
to	O
predict	O
how	O
many	O
tokens	O
are	O
missing	O
from	O
a	O
span.	O
Sentence	O
Permutation	O
A	O
document	O
is	O
divided	O
into	O
sentences	O
based	O
on	O
full	O
stops,	O
and	O
these	O
sentences	O
are	O
shuffled	O
in	O
a	O
random	O
order.	O
Document	O
Rotation	O
A	O
token	O
is	O
chosen	O
uniformly	O
at	O
random,	O
and	O
the	O
document	O
is	O
rotated	O
so	O
that	O
it	O
begins	O
with	O
that	O
token.	O
This	O
task	O
trains	O
the	O
model	O
to	O
identify	O
the	O
start	O
of	O
the	O
document.	O

BART	B-MethodName
uses	O
the	O
standard	O
sequence-to-sequence	O
Transformer	O
architecture	O
from	O
(Vaswani	O
et	O
al.,	O
2017),	O
except,	O
following	O
GPT,	B-MethodName
that	O
we	O
modify	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
functions	I-HyperparameterName
to	O
GeLUs	B-HyperparameterValue
(Hendrycks	O
&	O
Gimpel,	O
2016)	O
and	O
initialise	O
parameters	O
from	O
N	O
(0,	O
0.02).	O
For	O
our	O
base	O
model,	O
we	O
use	O
6	O
layers	O
in	O
the	O
encoder	O
and	O
de-coder,	O
and	O
for	O
our	O
large	O
model	O
we	O
use	O
12	O
layers	O
in	O
each.	O
The	O
architecture	O
is	O
closely	O
related	O
to	O
that	O
used	O
in	O
BERT,	B-MethodName
with	O
the	O
following	O
differences:	O
(1)	O
each	O
layer	O
of	O
the	O
decoder	O
additionally	O
performs	O
cross-attention	O
over	O
the	O
final	O
hidden	O
layer	O
of	O
the	O
encoder	O
(as	O
in	O
the	O
transformer	O
sequence-to-sequence	O
model);	O
and	O
(2)	O
BERT	B-MethodName
uses	O
an	O
additional	O
feed-forward	O
network	O
before	O
wordprediction,	O
which	O
BART	B-MethodName
does	O
not.	O
In	O
total,	O
BART	B-MethodName
contains	O
roughly	O
10%	B-MetricValue
more	O
parameters	O
than	O
the	O
equivalently	O
sized	O
BERT	B-MethodName
model.	O

BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
that	O
maps	O
a	O
corrupted	O
document	O
to	O
the	O
original	O
document	O
it	O
was	O
derived	O
from.	O
It	O
is	O
implemented	O
as	O
a	O
sequence-to-sequence	O
model	O
with	O
a	O
bidirectional	O
encoder	O
over	O
corrupted	O
text	O
and	O
a	O
left-to-right	O
autoregressive	O
decoder.	O
For	O
pre-training,	O
we	O
optimize	O
the	O
negative	B-MetricName
log	I-MetricName
likelihood	I-MetricName
of	O
the	O
original	O
document.	O

Bidirectional	O
Encoder	O
A	O
B	O
C	O
D	O
E	O
A	O
_	O
B	O
_	O
E	O
<s>	O
A	O
B	O
C	O
D(	O
c)	O
BART:	O
Inputs	O
to	O
the	O
encoder	O
need	O
not	O
be	O
aligned	O
with	O
decoder	O
outputs,	O
allowing	O
arbitary	O
noise	O
transformations.	O
Here,	O
a	O
document	O
has	O
been	O
corrupted	O
by	O
replacing	O
spans	O
of	O
text	O
with	O
mask	O
symbols.	O
The	O
corrupted	O
document	O
(left)	O
is	O
encoded	O
with	O
a	O
bidirectional	O
model,	O
and	O
then	O
the	O
likelihood	O
of	O
the	O
original	O
document	O
(right)	O
is	O
calculated	O
with	O
an	O
autoregressive	O
decoder.	O
For	O
fine-tuning,	O
an	O
uncorrupted	O
document	O
is	O
input	O
to	O
both	O
the	O
encoder	O
and	O
decoder,	O
and	O
we	O
use	O
representations	O
from	O
the	O
final	O
hidden	O
state	O
of	O
the	O
decoder.	O
Figure	O
1:	O
A	O
schematic	O
comparison	O
of	O
BART	B-MethodName
with	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018).	O
English,	O
by	O
propagation	O
through	O
BART,	B-MethodName
thereby	O
using	O
BART	B-MethodName
as	O
a	O
pre-trained	O
target-side	O
language	O
model.	O
This	O
approach	O
improves	O
performance	O
over	O
a	O
strong	O
back-translation	O
MT	O
baseline	O
by	O
1.1	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
Romanian-English	I-DatasetName
benchmark.	O
To	O
better	O
understand	O
these	O
effects,	O
we	O
also	O
report	O
an	O
ablation	O
analysis	O
that	O
replicates	O
other	O
recently	O
proposed	O
training	O
objectives.	O
This	O
study	O
allows	O
us	O
to	O
carefully	O
control	O
for	O
a	O
number	O
of	O
factors,	O
including	O
data	O
and	O
optimization	O
parameters,	O
which	O
have	O
been	O
shown	O
to	O
be	O
as	O
important	O
for	O
overall	O
performance	O
as	O
the	O
selection	O
of	O
training	O
objectives	O
.	O
We	O
find	O
that	O
BART	B-MethodName
exhibits	O
the	O
most	O
consistently	O
strong	O
performance	O
across	O
the	O
full	O
range	O
of	O
tasks	O
we	O
consider.	O

A	O
B	O
C	O
D	O
E	O
<s>	O
A	O
B	O
C	O
D	O
(b)	O
GPT:	B-MethodName
Tokens	O
are	O
predicted	O
auto-regressively,	O
meaning	O
GPT	B-MethodName
can	O
be	O
used	O
for	O
generation.	O
However	O
words	O
can	O
only	O
condition	O
on	O
leftward	O
context,	O
so	O
it	O
cannot	O
learn	O
bidirectional	O
interactions.	O

Self-supervised	O
methods	O
have	O
achieved	O
remarkable	O
success	O
in	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
(Mikolov	O
et	O
al.,	O
2013;Peters	O
et	O
al.,	O
2018;Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;.	O
The	O
most	O
successful	O
approaches	O
have	O
been	O
variants	O
of	O
masked	O
language	O
models,	O
which	O
are	O
denoising	O
autoencoders	O
that	O
are	O
trained	O
to	O
reconstruct	O
text	O
where	O
a	O
random	O
subset	O
of	O
the	O
words	O
has	O
been	O
masked	O
out.	O
Recent	O
work	O
has	O
shown	O
gains	O
by	O
improving	O
the	O
distribution	O
of	O
masked	O
tokens	O
,	O
the	O
order	O
in	O
which	O
masked	O
tokens	O
are	O
predicted	O
(Yang	O
et	O
al.,	O
2019),	O
and	O
the	O
available	O
context	O
for	O
replacing	O
masked	O
tokens	O
(Dong	O
et	O
al.,	O
2019).	O
However,	O
these	O
methods	O
typically	O
focus	O
on	O
particular	O
types	O
of	O
end	O
tasks	O
(e.g.	O
span	B-TaskName
prediction,	I-TaskName
generation,	B-TaskName
etc.),	O
limiting	O
their	O
applicability.	O
In	O
this	O
paper,	O
we	O
present	O
BART,	B-MethodName
which	O
pre-trains	O
a	O
model	O
combining	O
Bidirectional	O
and	O
Auto-Regressive	O
Transformers.	O
BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
built	O
with	O
a	O
sequence-to-sequence	O
model	O
that	O
is	O
applicable	O
to	O
a	O
very	O
wide	O
range	O
of	O
end	O
tasks.	O
Pretraining	O
has	O
two	O
stages	O
(1)	O
text	O
is	O
corrupted	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
a	O
sequence-to-sequence	O
model	O
is	O
learned	O
to	O
reconstruct	O
the	O
original	O
text.	O
BART	B-MethodName
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes	O
(see	O
Figure	O
1).	O
A	O
key	O
advantage	O
of	O
this	O
setup	O
is	O
the	O
noising	O
flexibility;	O
arbitrary	O
transformations	O
can	O
be	O
applied	O
to	O
the	O
original	O
text,	O
including	O
changing	O
its	O
length.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
arbitrary	O
length	O
spans	O
of	O
text	O
(including	O
zero	O
length)	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
This	O
approach	O
generalizes	O
the	O
original	O
word	O
masking	O
and	O
next	O
sentence	O
prediction	O
objectives	O
in	O
BERT	B-MethodName
by	O
forcing	O
the	O
model	O
to	O
reason	O
more	O
about	O
overall	O
sentence	O
length	O
and	O
make	O
longer	O
range	O
transformations	O
to	O
the	O
input.	O
BART	B-MethodName
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	O
generation	O
but	O
also	O
works	O
well	O
for	O
comprehension	O
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2018)	O
and	O
SQuAD	B-DatasetName
(Rajpurkar	O
et	O
al.,	O
2016),	O
and	O
achieves	O
new	O
state-of-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	O
dialogue,	O
question	O
answering,	O
and	O
summarization	O
tasks.	O
For	O
example,	O
it	O
improves	O
performance	O
by	O
6	B-MetricValue
ROUGE	B-MetricName
over	O
previous	O
work	O
on	O
XSum	O
(Narayan	O
et	O
al.,	O
2018).	O
BART	B-MethodName
also	O
opens	O
up	O
new	O
ways	O
of	O
thinking	O
about	O
fine	O
tuning.	O
We	O
present	O
a	O
new	O
scheme	O
for	O
machine	O
translation	O
where	O
a	O
BART	B-MethodName
model	O
is	O
stacked	O
above	O
a	O
few	O
additional	O
transformer	O
layers.	O
These	O
layers	O
are	O
trained	O
to	O
essentially	O
translate	O
the	O
foreign	O
language	O
to	O
noised	O
Bidirectional	O
Encoder	O
A	O
_	O
C	O
_	O
E	O
B	O
D	O
(a)	O
BERT:	B-MethodName
Random	O
tokens	O
are	O
replaced	O
with	O
masks,	O
and	O
the	O
document	O
is	O
encoded	O
bidirectionally.	O
Missing	O
tokens	O
are	O
predicted	O
independently,	O
so	O
BERT	B-MethodName
cannot	O
easily	O
be	O
used	O
for	O
generation.	O

We	O
present	O
BART,	B-MethodName
a	O
denoising	O
autoencoder	O
for	O
pretraining	O
sequence-to-sequence	O
models.	O
BART	B-MethodName
is	O
trained	O
by	O
(1)	O
corrupting	O
text	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
learning	O
a	O
model	O
to	O
reconstruct	O
the	O
original	O
text.	O
It	O
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
spans	O
of	O
text	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
BART	O
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	B-TaskName
generation	I-TaskName
but	O
also	O
works	O
well	O
for	O
comprehension	B-TaskName
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	O
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
and	O
SQuAD,	B-DatasetName
achieves	O
new	O
stateof-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	B-TaskName
dialogue,	I-TaskName
question	B-TaskName
answering,	I-TaskName
and	O
summarization	O
tasks,	O
with	O
gains	O
of	O
up	O
to	O
6	B-MetricValue
ROUGE.	B-MetricName
BART	B-MethodName
also	O
provides	O
a	O
1.1	B-MetricValue
BLEU	B-MetricName
increase	O
over	O
a	O
back-translation	O
system	O
for	O
machine	O
translation,	B-TaskName
with	O
only	O
target	O
language	O
pretraining.	O
We	O
also	O
report	O
ablation	O
experiments	O
that	O
replicate	O
other	O
pretraining	O
schemes	O
within	O
the	O
BART	B-MethodName
framework,	O
to	O
better	O
measure	O
which	O
factors	O
most	O
influence	O
end-task	O
performance.	O

All	O
of	O
the	O
BERT	B-MethodName
results	O
presented	O
so	O
far	O
have	O
used	O
the	O
fine-tuning	O
approach,	O
where	O
a	O
simple	O
classification	O
layer	O
is	O
added	O
to	O
the	O
pre-trained	O
model,	O
and	O
all	O
parameters	O
are	O
jointly	O
fine-tuned	O
on	O
a	O
downstream	O
task.	O
However,	O
the	O
feature-based	O
approach,	O
where	O
fixed	O
features	O
are	O
extracted	O
from	O
the	O
pretrained	O
model,	O
has	O
certain	O
advantages.	O
First,	O
not	O
all	O
tasks	O
can	O
be	O
easily	O
represented	O
by	O
a	O
Transformer	O
encoder	O
architecture,	O
and	O
therefore	O
require	O
a	O
task-specific	O
model	O
architecture	O
to	O
be	O
added.	O
Second,	O
there	O
are	O
major	O
computational	O
benefits	O
to	O
pre-compute	O
an	O
expensive	O
representation	O
of	O
the	O
training	O
data	O
once	O
and	O
then	O
run	O
many	O
experiments	O
with	O
cheaper	O
models	O
on	O
top	O
of	O
this	O
representation.	O
In	O
this	O
section,	O
we	O
compare	O
the	O
two	O
approaches	O
by	O
applying	O
BERT	B-MethodName
to	O
the	O
CoNLL-2003	B-DatasetName
Named	B-TaskName
Entity	I-TaskName
Recognition	I-TaskName
(NER)	B-TaskName
task	O
(Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder,	O
2003).	O
In	O
the	O
input	O
to	O
BERT,	B-MethodName
we	O
use	O
a	O
case-preserving	O
WordPiece	O
model,	O
and	O
we	O
include	O
the	O
maximal	O
document	O
context	O
provided	O
by	O
the	O
data.	O
Following	O
standard	O
practice,	O
we	O
formulate	O
this	O
as	O
a	O
tagging	O
task	O
but	O
do	O
not	O
use	O
a	O
CRF	O
layer	O
in	O
the	O
output.	O
We	O
use	O
the	O
representation	O
of	O
the	O
first	O
sub-token	O
as	O
the	O
input	O
to	O
the	O
token-level	O
classifier	O
over	O
the	O
NER	B-TaskName
label	O
set.	O
To	O
ablate	O
the	O
fine-tuning	O
approach,	O
we	O
apply	O
the	O
feature-based	O
approach	O
by	O
extracting	O
the	O
activations	O
from	O
one	O
or	O
more	O
layers	O
without	O
fine-tuning	O
any	O
parameters	O
of	O
BERT.	B-MethodName
These	O
contextual	O
embeddings	O
are	O
used	O
as	O
input	O
to	O
a	O
randomly	O
initialized	O
two-layer	B-HyperparameterValue
768-dimensional	B-HyperparameterValue
BiLSTM	O
before	O
the	O
classification	O
layer.	O
Results	O
are	O
presented	O
in	O
Table	O
7.	O
BERT	B-MethodName
LARGE	I-MethodName
performs	O
competitively	O
with	O
state-of-the-art	O
methods.	O
The	O
best	O
performing	O
method	O
concatenates	O
the	O
token	O
representations	O
from	O
the	O
top	O
four	O
hidden	O
layers	O
of	O
the	O
pre-trained	O
Transformer,	O
which	O
is	O
only	O
0.3	B-MetricValue
F1	B-MetricName
behind	O
fine-tuning	O
the	O
entire	O
model.	O
This	O
demonstrates	O
that	O
BERT	B-MethodName
is	O
effective	O
for	O
both	O
finetuning	O
and	O
feature-based	O
approaches.	O

To	O
generate	O
each	O
training	O
input	O
sequence,	O
we	O
sample	O
two	O
spans	O
of	O
text	O
from	O
the	O
corpus,	O
which	O
we	O
refer	O
to	O
as	O
"sentences"	O
even	O
though	O
they	O
are	O
typically	O
much	O
longer	O
than	O
single	O
sentences	O
(but	O
can	O
be	O
shorter	O
also).	O
The	O
first	O
sentence	O
receives	O
the	O
A	O
embedding	O
and	O
the	O
second	O
receives	O
the	O
B	O
embedding.	O
50%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
and	O
50%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence,	O
which	O
is	O
done	O
for	O
the	O
"next	O
sentence	O
prediction"	O
task.	O
They	O
are	O
sampled	O
such	O
that	O
the	O
combined	O
length	O
is	O
	O
512	O
tokens.	O
The	O
LM	O
masking	O
is	O
applied	O
after	O
WordPiece	O
tokenization	O
with	O
a	O
uniform	O
masking	B-HyperparameterName
rate	I-HyperparameterName
of	O
15%,	B-HyperparameterValue
and	O
no	O
special	O
consideration	O
given	O
to	O
partial	O
word	O
pieces.	O
We	O
train	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
sequences	O
(256	O
sequences	O
*	O
512	O
tokens	O
=	O
128,000	O
tokens/batch)	O
for	O
1,000,000	O
steps,	O
which	O
is	O
approximately	O
40	B-HyperparameterValue
epochs	B-HyperparameterName
over	O
the	O
3.3	O
billion	O
word	O
corpus.	O
We	O
use	O
Adam	B-HyperparameterValue
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e-4,	B-HyperparameterValue
1	O
=	O
0.9,	B-HyperparameterValue
2	O
=	O
0.999,	B-HyperparameterValue
L2	B-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01,	B-HyperparameterValue
learning	O
rate	O
warmup	O
over	O
the	O
first	O
10,000	O
steps,	O
and	O
linear	O
decay	O
of	O
the	O
learning	O
rate.	O
We	O
use	O
a	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers.	O
We	O
use	O
a	O
gelu	B-HyperparameterValue
activation	B-HyperparameterName
(Hendrycks	O
and	O
Gimpel,	O
2016)	O
rather	O
than	O
the	O
standard	O
relu,	O
following	O
OpenAI	O
GPT.	O
The	O
training	O
loss	O
is	O
the	O
sum	O
of	O
the	O
mean	O
masked	O
LM	O
likelihood	O
and	O
the	O
mean	O
next	O
sentence	O
prediction	O
likelihood.	O
Training	O
of	O
BERT	B-MethodName
BASE	I-MethodName
was	O
performed	O
on	O
4	O
Cloud	O
TPUs	O
in	O
Pod	O
configuration	O
(16	O
TPU	O
chips	O
total).	O
13	O
Training	O
of	O
BERT	B-MethodName
LARGE	I-MethodName
was	O
performed	O
on	O
16	O
Cloud	O
TPUs	O
(64	O
TPU	O
chips	O
total).	O
Each	O
pretraining	O
took	O
4	O
days	O
to	O
complete.	O
Longer	O
sequences	O
are	O
disproportionately	O
expensive	O
because	O
attention	O
is	O
quadratic	O
to	O
the	O
sequence	O
length.	O
To	O
speed	O
up	O
pretraing	O
in	O
our	O
experiments,	O
we	O
pre-train	O
the	O
model	O
with	O
sequence	B-HyperparameterName
length	I-HyperparameterName
of	O
128	B-HyperparameterValue
for	O
90%	O
of	O
the	O
steps.	O
Then,	O
we	O
train	O
the	O
rest	O
10%	O
of	O
the	O
steps	O
of	O
sequence	B-HyperparameterName
of	O
512	B-HyperparameterValue
to	O
learn	O
the	O
positional	O
embeddings.	O

Language	O
Understanding"	O
We	O
organize	O
the	O
appendix	O
into	O
three	O
sections:	O
•	O
Additional	O
implementation	O
details	O
for	O
BERT	B-MethodName
are	O
presented	O
in	O
Appendix	O
A;	O
•	O
Additional	O
details	O
for	O
our	O
experiments	O
are	O
presented	O
in	O
Appendix	O
B;	O
and	O
•	O
Additional	O
ablation	O
studies	O
are	O
presented	O
in	O
Appendix	O
C.	O
We	O
present	O
additional	O
ablation	O
studies	O
for	O
BERT	B-MethodName
including:	O

Figure	O
5	O
presents	O
MNLI	B-DatasetName
Dev	O
accuracy	O
after	O
finetuning	O
from	O
a	O
checkpoint	O
that	O
has	O
been	O
pre-trained	O
for	O
k	O
steps.	O
This	O
allows	O
us	O
to	O
answer	O
the	O
following	O
questions:	O
1	O
Note	O
that	O
the	O
purpose	O
of	O
the	O
masking	O
strategies	O
is	O
to	O
reduce	O
the	O
mismatch	O
between	O
pre-training	O
and	O
fine-tuning,	O
as	O
the	O
[MASK]	O
symbol	O
never	O
appears	O
during	O
the	O
fine-tuning	O
stage.	O
We	O
report	O
the	O
Dev	O
results	O
for	O
both	O
MNLI	O
and	O
NER.	B-TaskName
For	O
NER,	B-TaskName
we	O
report	O
both	O
fine-tuning	O
and	O
feature-based	O
approaches,	O
as	O
we	O
expect	O
the	O
mismatch	O
will	O
be	O
amplified	O
for	O
the	O
feature-based	O
approach	O
as	O
the	O
model	O
will	O
not	O
have	O
the	O
chance	O
to	O
adjust	O
the	O
representations.	O
The	O
results	O
are	O
presented	O
in	O
Table	O
8.	O
In	O
the	O
table,	O
MASK	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
the	O
[MASK]	O
symbol	O
for	O
MLM;	B-TaskName
SAME	O
means	O
that	O
we	O
keep	O
the	O
target	O
token	O
as	O
is;	O
RND	O
means	O
that	O
we	O
replace	O
the	O
target	O
token	O
with	O
another	O
random	O
token.	O
The	O
numbers	O
in	O
the	O
left	O
part	O
of	O
the	O
table	O
represent	O
the	O
probabilities	O
of	O
the	O
specific	O
strategies	O
used	O
during	O
MLM	B-TaskName
pre-training	O
(BERT	B-MethodName
uses	O
80%,	O
10%,	O
10%).	O
The	O
right	O
part	O
of	O
the	O
paper	O
represents	O
the	O
Dev	O
set	O
results.	O
For	O
the	O
feature-based	O
approach,	O
we	O
concatenate	O
the	O
last	O
4	O
layers	O
of	O
BERT	O
as	O
the	O
features,	O
which	O
was	O
shown	O
to	O
be	O
the	O
best	O
approach	O
in	O
Section	O
5.3.	O
From	O
the	O
table	O
it	O
can	O
be	O
seen	O
that	O
fine-tuning	O
is	O
surprisingly	O
robust	O
to	O
different	O
masking	O
strategies.	O
However,	O
as	O
expected,	O
using	O
only	O
the	O
MASK	O
strategy	O
was	O
problematic	O
when	O
applying	O
the	O
featurebased	O
approach	O
to	O
NER.	B-TaskName
Interestingly,	O
using	O
only	O
the	O
RND	O
strategy	O
performs	O
much	O
worse	O
than	O
our	O
strategy	O
as	O
well.	O

The	O
Semantic	B-DatasetName
Textual	I-DatasetName
Similarity	I-DatasetName
Benchmark	I-DatasetName
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
drawn	O
from	O
news	O
headlines	O
and	O
other	O
sources	O
(Cer	O
et	O
al.,	O
2017).	O
They	O
were	O
annotated	O
with	O
a	O
score	O
from	O
1	O
to	O
5	O
denoting	O
how	O
similar	O
the	O
two	O
sentences	O
are	O
in	O
terms	O
of	O
semantic	O
meaning.	O
MRPC	B-DatasetName
Microsoft	B-DatasetName
Research	I-DatasetName
Paraphrase	I-DatasetName
Corpus	I-DatasetName
consists	O
of	O
sentence	O
pairs	O
automatically	O
extracted	O
from	O
online	O
news	O
sources,	O
with	O
human	O
annotations	O
for	O
whether	O
the	O
sentences	O
in	O
the	O
pair	O
are	O
semantically	O
equivalent	O
(Dolan	O
and	O
Brockett,	O
2005).	O
RTE	B-DatasetName
Recognizing	B-DatasetName
Textual	I-DatasetName
Entailment	I-DatasetName
is	O
a	O
binary	O
entailment	O
task	O
similar	O
to	O
MNLI,	O
but	O
with	O
much	O
less	O
training	O
data	O
(Bentivogli	O
et	O
al.,	O
2009).	O
14	O
WNLI	B-DatasetName
Winograd	B-DatasetName
NLI	I-DatasetName
is	O
a	O
small	O
natural	O
language	O
inference	O
dataset	O
(Levesque	O
et	O
al.,	O
2011).	O
The	O
GLUE	B-DatasetName
webpage	O
notes	O
that	O
there	O
are	O
issues	O
with	O
the	O
construction	O
of	O
this	O
dataset,	O
15	O
and	O
every	O
trained	O
system	O
that's	O
been	O
submitted	O
to	O
GLUE	B-DatasetName
has	O
performed	O
worse	O
than	O
the	O
65.1	O
baseline	O
accuracy	O
of	O
predicting	O
the	O
majority	O
class.	O
We	O
therefore	O
exclude	O
this	O
set	O
to	O
be	O
fair	O
to	O
OpenAI	O
GPT.	B-MethodName
For	O
our	O
GLUE	B-DatasetName
submission,	O
we	O
always	O
predicted	O
the	O
ma-jority	O
class.	O

The	O
Stanford	B-DatasetName
Sentiment	I-DatasetName
Treebank	I-DatasetName
is	O
a	O
binary	O
single-sentence	O
classification	O
task	O
consisting	O
of	O
sentences	O
extracted	O
from	O
movie	O
reviews	O
with	O
human	O
annotations	O
of	O
their	O
sentiment	O
(Socher	O
et	O
al.,	O
2013).	O
CoLA	B-DatasetName
The	B-DatasetName
Corpus	I-DatasetName
of	I-DatasetName
Linguistic	I-DatasetName
Acceptability	I-DatasetName
is	O
a	O
binary	O
single-sentence	O
classification	O
task,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
an	O
English	O
sentence	O
is	O
linguistically	O
"acceptable"	O
or	O
not	O
(Warstadt	O
et	O
al.,	O
2018).	O

Tasks	O
The	O
illustration	O
of	O
fine-tuning	O
BERT	B-MethodName
on	O
different	O
tasks	O
can	O
be	O
seen	O
in	O
Figure	O
4.	O
Our	O
task-specific	O
models	O
are	O
formed	O
by	O
incorporating	O
BERT	B-MethodName
with	O
one	O
additional	O
output	O
layer,	O
so	O
a	O
minimal	O
number	O
of	O
parameters	O
need	O
to	O
be	O
learned	O
from	O
scratch.	O
Among	O
the	O
tasks,	O
MNLI	B-DatasetName
Multi-Genre	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
large-scale,	O
crowdsourced	O
entailment	O
classification	O
task	O
(Williams	O
et	O
al.,	O
2018).	O
Given	O
a	O
pair	O
of	O
sentences,	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
second	O
sentence	O
is	O
an	O
entailment,	O
contradiction,	O
or	O
neutral	O
with	O
respect	O
to	O
the	O
first	O
one.	O
QQP	B-DatasetName
Quora	B-DatasetName
Question	I-DatasetName
Pairs	I-DatasetName
is	O
a	O
binary	O
classification	O
task	O
where	O
the	O
goal	O
is	O
to	O
determine	O
if	O
two	O
questions	O
asked	O
on	O
Quora	O
are	O
semantically	O
equivalent	O
.	O
QNLI	B-DatasetName
Question	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
version	O
of	O
the	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(Rajpurkar	O
et	O
al.,	O
2016)	O
which	O
has	O
been	O
converted	O
to	O
a	O
binary	O
classification	O
task	O
(Wang	O
et	O
al.,	O
2018a)	O
...	O
...	O
...	O

OpenAI	O
GPT	B-MethodName
Here	O
we	O
studies	O
the	O
differences	O
in	O
recent	O
popular	O
representation	O
learning	O
models	O
including	O
ELMo,	B-MethodName
OpenAI	O
GPT	B-MethodName
and	O
BERT.	B-MethodName
The	O
comparisons	O
between	O
the	O
model	O
architectures	O
are	O
shown	O
visually	O
in	O
Figure	O
3.	O
Note	O
that	O
in	O
addition	O
to	O
the	O
architecture	O
differences,	O
BERT	B-MethodName
and	O
OpenAI	O
GPT	B-MethodName
are	O
finetuning	O
approaches,	O
while	O
ELMo	B-MethodName
is	O
a	O
feature-based	O
approach.	O
The	O
most	O
comparable	O
existing	O
pre-training	O
method	O
to	O
BERT	B-MethodName
is	O
OpenAI	O
GPT,	B-MethodName
which	O
trains	O
a	O
left-to-right	O
Transformer	O
LM	O
on	O
a	O
large	O
text	O
corpus.	O
In	O
fact,	O
many	O
of	O
the	O
design	O
decisions	O
in	O
BERT	B-MethodName
were	O
intentionally	O
made	O
to	O
make	O
it	O
as	O
close	O
to	O
GPT	B-MethodName
as	O
possible	O
so	O
that	O
the	O
two	O
methods	O
could	O
be	O
minimally	O
compared.	O
The	O
core	O
argument	O
of	O
this	O
work	O
is	O
that	O
the	O
bi-directionality	O
and	O
the	O
two	O
pretraining	O
tasks	O
presented	O
in	O
Section	O
3.1	O
account	O
for	O
the	O
majority	O
of	O
the	O
empirical	O
improvements,	O
but	O
we	O
do	O
note	O
that	O
there	O
are	O
several	O
other	O
differences	O
between	O
how	O
BERT	B-MethodName
and	O
GPT	B-MethodName
were	O
trained:	O
•	O
GPT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	B-DatasetName
(800M	O
words);	O
BERT	B-MethodName
is	O
trained	O
on	O
the	O
BooksCorpus	B-DatasetName
(800M	O
words)	O
and	O
Wikipedia	B-DatasetName
(2,500M	O
words).	O
•	O
GPT	B-MethodName
uses	O
a	O
sentence	O
separator	O
(	O
•	O
GPT	B-MethodName
was	O
trained	O
for	O
1M	O
steps	O
with	O
a	O
batch	O
size	O
of	O
32,000	O
words;	O
BERT	B-MethodName
was	O
trained	O
for	O
1M	O
steps	O
with	O
a	O
batch	O
size	O
of	O
128,000	O
words.	O
•	O
GPT	B-MethodName
used	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
for	O
all	O
fine-tuning	O
experiments;	O
BERT	B-MethodName
chooses	O
a	O
task-specific	O
fine-tuning	O
learning	O
rate	O
which	O
performs	O
the	O
best	O
on	O
the	O
development	O
set.	O
To	O
isolate	O
the	O
effect	O
of	O
these	O
differences,	O
we	O
perform	O
ablation	O
experiments	O
in	O
Section	O
5.1	O
which	O
demonstrate	O
that	O
the	O
majority	O
of	O
the	O
improvements	O
are	O
in	O
fact	O
coming	O
from	O
the	O
two	O
pre-training	O
tasks	O
and	O
the	O
bidirectionality	O
they	O
enable.	O

For	O
fine-tuning,	O
most	O
model	O
hyperparameters	O
are	O
the	O
same	O
as	O
in	O
pre-training,	O
with	O
the	O
exception	O
of	O
the	O
batch	B-HyperparameterName
size,	I-HyperparameterName
learning	B-HyperparameterName
rate,	I-HyperparameterName
and	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epochs.	I-HyperparameterName
The	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
was	O
always	O
kept	O
at	O
0.1.	B-HyperparameterValue
The	O
optimal	O
hyperparameter	O
values	O
are	O
task-specific,	O
but	O
we	O
found	O
the	O
following	O
range	O
of	O
possible	O
values	O
to	O
work	O
well	O
across	O
all	O
tasks:	O
•	O
Batch	B-HyperparameterName
size:	I-HyperparameterName
16,	B-HyperparameterValue
32	B-HyperparameterValue
We	O
also	O
observed	O
that	O
large	O
data	O
sets	O
(e.g.,	O
100k+	O
labeled	O
training	O
examples)	O
were	O
far	O
less	O
sensitive	O
to	O
hyperparameter	O
choice	O
than	O
small	O
data	O
sets.	O
Fine-tuning	O
is	O
typically	O
very	O
fast,	O
so	O
it	O
is	O
reasonable	O
to	O
simply	O
run	O
an	O
exhaustive	O
search	O
over	O
the	O
above	O
parameters	O
and	O
choose	O
the	O
model	O
that	O
performs	O
best	O
on	O
the	O
development	O
set.	O

Recent	O
empirical	O
improvements	O
due	O
to	O
transfer	O
learning	O
with	O
language	O
models	O
have	O
demonstrated	O
that	O
rich,	O
unsupervised	O
pre-training	O
is	O
an	O
integral	O
part	O
of	O
many	O
language	O
understanding	O
systems.	O
In	O
particular,	O
these	O
results	O
enable	O
even	O
low-resource	O
tasks	O
to	O
benefit	O
from	O
deep	O
unidirectional	O
architectures.	O
Our	O
major	O
contribution	O
is	O
further	O
generalizing	O
these	O
findings	O
to	O
deep	O
bidirectional	O
architectures,	O
allowing	O
the	O
same	O
pre-trained	O
model	O
to	O
successfully	O
tackle	O
a	O
broad	O
set	O
of	O
NLP	O
tasks.	O
Masked	B-TaskName
LM	I-TaskName
and	O
the	O
Masking	O
Procedure	O
Assuming	O
the	O
unlabeled	O
sentence	O
is	O
my	O
dog	O
is	O
hairy,	O
and	O
during	O
the	O
random	O
masking	O
procedure	O
we	O
chose	O
the	O
4-th	O
token	O
(which	O
corresponding	O
to	O
hairy),	O
our	O
masking	O
procedure	O
can	O
be	O
further	O
illustrated	O
by	O
•	O
10%	O
of	O
the	O
time:	O
Replace	O
the	O
word	O
with	O
a	O
random	O
word,	O
e.g.,	O
my	O
dog	O
is	O
hairy	O
!	O
my	O
dog	O
is	O
apple	O
•	O
10%	O
of	O
the	O
time:	O
Keep	O
the	O
word	O
unchanged,	O
e.g.,	O
my	O
dog	O
is	O
hairy	O
!	O
my	O
dog	O
is	O
hairy.	O
The	O
purpose	O
of	O
this	O
is	O
to	O
bias	O
the	O
representation	O
towards	O
the	O
actual	O
observed	O
word.	O
The	O
advantage	O
of	O
this	O
procedure	O
is	O
that	O
the	O
Transformer	O
encoder	O
does	O
not	O
know	O
which	O
words	O
it	O
will	O
be	O
asked	O
to	O
predict	O
or	O
which	O
have	O
been	O
replaced	O
by	O
random	O
words,	O
so	O
it	O
is	O
forced	O
to	O
keep	O
a	O
distributional	O
contextual	O
representation	O
of	O
every	O
input	O
token.	O
Additionally,	O
because	O
random	O
replacement	O
only	O
occurs	O
for	O
1.5%	O
of	O
all	O
tokens	O
(i.e.,	O
10%	O
of	O
15%),	O
this	O
does	O
not	O
seem	O
to	O
harm	O
the	O
model's	O
language	O
understanding	O
capability.	O
In	O
Section	O
C.2,	O
we	O
evaluate	O
the	O
impact	O
this	O
procedure.	O
Compared	O
to	O
standard	O
langauge	O
model	O
training,	O
the	O
masked	B-TaskName
LM	I-TaskName
only	O
make	O
predictions	O
on	O
15%	O
of	O
tokens	O
in	O
each	O
batch,	O
which	O
suggests	O
that	O
more	O
pre-training	O
steps	O
may	O
be	O
required	O
for	O
the	O
model	O

In	O
this	O
section,	O
we	O
explore	O
the	O
effect	O
of	O
model	B-HyperparameterName
size	I-HyperparameterName
on	O
fine-tuning	O
task	O
accuracy.	O
We	O
trained	O
a	O
number	O
of	O
BERT	B-MethodName
models	O
with	O
a	O
differing	O
number	O
of	O
layers,	O
hidden	O
units,	O
and	O
attention	O
heads,	O
while	O
otherwise	O
using	O
the	O
same	O
hyperparameters	O
and	O
training	O
procedure	O
as	O
described	O
previously.	O
Results	O
on	O
selected	O
GLUE	B-DatasetName
tasks	O
are	O
shown	O
in	O
Table	O
6.	O
In	O
this	O
table,	O
we	O
report	O
the	O
average	O
Dev	O
Set	O
accuracy	O
from	O
5	O
random	O
restarts	O
of	O
fine-tuning.	O
We	O
can	O
see	O
that	O
larger	O
models	O
lead	O
to	O
a	O
strict	O
accuracy	O
improvement	O
across	O
all	O
four	O
datasets,	O
even	O
for	O
MRPC	B-DatasetName
which	O
only	O
has	O
3,600	O
labeled	O
training	O
examples,	O
and	O
is	O
substantially	O
different	O
from	O
the	O
pre-training	O
tasks.	O
It	O
is	O
also	O
perhaps	O
surprising	O
that	O
we	O
are	O
able	O
to	O
achieve	O
such	O
significant	O
improvements	O
on	O
top	O
of	O
models	O
which	O
are	O
already	O
quite	O
large	O
relative	O
to	O
the	O
existing	O
literature.	O
For	O
example,	O
the	O
largest	O
Transformer	O
explored	O
in	O
Vaswani	O
et	O
al.	O
(2017)	O
is	O
(L=6,	B-HyperparameterName
H=1024,	B-HyperparameterName
A=16)	B-HyperparameterName
with	O
100M	O
parameters	O
for	O
the	O
encoder,	O
and	O
the	O
largest	O
Transformer	O
we	O
have	O
found	O
in	O
the	O
literature	O
is	O
(L=64,	B-HyperparameterName
H=512,	B-HyperparameterName
A=2)	B-HyperparameterName
with	O
235M	O
parameters	O
(Al-Rfou	O
et	O
al.,	O
2018).	O
By	O
contrast,	O
BERT	B-MethodName
BASE	I-MethodName
contains	O
110M	O
parameters	O
and	O
BERT	B-MethodName
LARGE	I-MethodName
contains	O
340M	O
parameters.	O
It	O
has	O
long	O
been	O
known	O
that	O
increasing	O
the	O
model	O
size	O
will	O
lead	O
to	O
continual	O
improvements	O
on	O
large-scale	O
tasks	O
such	O
as	O
machine	B-TaskName
translation	I-TaskName
and	O
language	B-TaskName
modeling,	I-TaskName
which	O
is	O
demonstrated	O
by	O
the	O
LM	O
perplexity	O
of	O
held-out	O
training	O
data	O
shown	O
in	O
Table	O
6.	O
However,	O
we	O
believe	O
that	O
this	O
is	O
the	O
first	O
work	O
to	O
demonstrate	O
convincingly	O
that	O
scaling	O
to	O
extreme	O
model	O
sizes	O
also	O
leads	O
to	O
large	O
improvements	O
on	O
very	O
small	O
scale	O
tasks,	O
provided	O
that	O
the	O
model	O
has	O
been	O
sufficiently	O
pre-trained.	O
Peters	O
et	O
al.	O
(2018b)	O
presented	O
mixed	O
results	O
on	O
the	O
downstream	O
task	O
impact	O
of	O
increasing	O
the	O
pre-trained	O
bi-LM	O
size	O
from	O
two	O
to	O
four	O
layers	O
and	O
Melamud	O
et	O
al.	O
(2016)	O
mentioned	O
in	O
passing	O
that	O
increasing	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
size	I-HyperparameterName
from	O
200	B-HyperparameterValue
to	O
600	B-HyperparameterValue
helped,	O
but	O
increasing	O
further	O
to	O
1,000	B-HyperparameterValue
did	O
not	O
bring	O
further	O
improvements.	O
Both	O
of	O
these	O
prior	O
works	O
used	O
a	O
featurebased	O
approach	O
-we	O
hypothesize	O
that	O
when	O
the	O
model	O
is	O
fine-tuned	O
directly	O
on	O
the	O
downstream	O
tasks	O
and	O
uses	O
only	O
a	O
very	O
small	O
number	O
of	O
randomly	O
initialized	O
additional	O
parameters,	O
the	O
taskspecific	O
models	O
can	O
benefit	O
from	O
the	O
larger,	O
more	O
expressive	O
pre-trained	O
representations	O
even	O
when	O
downstream	O
task	O
data	O
is	O
very	O
small.	O

A	O
left-context-only	O
model	O
which	O
is	O
trained	O
using	O
a	O
standard	O
Left-to-Right	O
(LTR)	B-MethodName
LM,	O
rather	O
than	O
an	O
MLM.	O
The	O
left-only	O
constraint	O
was	O
also	O
applied	O
at	O
fine-tuning,	O
because	O
removing	O
it	O
introduced	O
a	O
pre-train/fine-tune	O
mismatch	O
that	O
degraded	O
downstream	O
performance.	O
Additionally,	O
this	O
model	O
was	O
pre-trained	O
without	O
the	O
NSP	B-TaskName
task.	O
This	O
is	O
directly	O
comparable	O
to	O
OpenAI	O
GPT,	B-MethodName
but	O
using	O
our	O
larger	O
training	O
dataset,	O
our	O
input	O
representation,	O
and	O
our	O
fine-tuning	O
scheme.	O
We	O
first	O
examine	O
the	O
impact	O
brought	O
by	O
the	O
NSP	B-TaskName
task.	O
In	O
Table	O
5,	O
we	O
show	O
that	O
removing	O
NSP	B-TaskName
hurts	O
performance	O
significantly	O
on	O
QNLI,	O
MNLI,	O
and	O
SQuAD	O
1.1.	O
Next,	O
we	O
evaluate	O
the	O
impact	O
of	O
training	O
bidirectional	O
representations	O
by	O
comparing	O
"No	B-TaskName
NSP"	I-TaskName
to	O
"LTR	O
&	O
No	O
NSP".	O
The	O
LTR	B-MethodName
model	O
performs	O
worse	O
than	O
the	O
MLM	O
model	O
on	O
all	O
tasks,	O
with	O
large	O
drops	O
on	O
MRPC	B-MetricName
and	O
SQuAD.	B-DatasetName
For	O
SQuAD	B-DatasetName
it	O
is	O
intuitively	O
clear	O
that	O
a	O
LTR	B-MethodName
model	O
will	O
perform	O
poorly	O
at	O
token	O
predictions,	O
since	O
the	O
token-level	O
hidden	O
states	O
have	O
no	O
rightside	O
context.	O
In	O
order	O
to	O
make	O
a	O
good	O
faith	O
attempt	O
at	O
strengthening	O
the	O
LTR	B-MethodName
system,	O
we	O
added	O
a	O
randomly	O
initialized	O
BiLSTM	O
on	O
top.	O
This	O
does	O
significantly	O
improve	O
results	O
on	O
SQuAD,	B-DatasetName
but	O
the	O
results	O
are	O
still	O
far	O
worse	O
than	O
those	O
of	O
the	O
pretrained	O
bidirectional	O
models.	O
The	O
BiLSTM	O
hurts	O
performance	O
on	O
the	O
GLUE	B-DatasetName
tasks.	O
We	O
recognize	O
that	O
it	O
would	O
also	O
be	O
possible	O
to	O
train	O
separate	O
LTR	B-MethodName
and	O
RTL	O
models	O
and	O
represent	O
each	O
token	O
as	O
the	O
concatenation	O
of	O
the	O
two	O
models,	O
as	O
ELMo	O
does.	O
However:	O
(a)	O
this	O
is	O
twice	O
as	O
expensive	O
as	O
a	O
single	O
bidirectional	O
model;	O
(b)	O
this	O
is	O
non-intuitive	O
for	O
tasks	O
like	O
QA,	B-TaskName
since	O
the	O
RTL	O
model	O
would	O
not	O
be	O
able	O
to	O
condition	O
the	O
answer	O
on	O
the	O
question;	O
(c)	O
this	O
it	O
is	O
strictly	O
less	O
powerful	O
than	O
a	O
deep	O
bidirectional	O
model,	O
since	O
it	O
can	O
use	O
both	O
left	O
and	O
right	O
context	O
at	O
every	O
layer.	O

We	O
demonstrate	O
the	O
importance	O
of	O
the	O
deep	O
bidirectionality	O
of	O
BERT	B-MethodName
by	O
evaluating	O
two	O
pretraining	O
objectives	O
using	O
exactly	O
the	O
same	O
pretraining	O
data,	O
fine-tuning	O
scheme,	O
and	O
hyperparameters	O
as	O
BERT	B-MethodName
BASE	I-MethodName
:	O
No	B-TaskName
NSP:	I-TaskName
A	O
bidirectional	O
model	O
which	O
is	O
trained	O
using	O
the	O
"masked	B-TaskName
LM"	I-TaskName
(MLM)	B-TaskName
but	O
without	O
the	O
"next	B-TaskName
sentence	I-TaskName
prediction"	I-TaskName
(NSP)	B-TaskName
task.	O

The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(SQuAD	B-DatasetName
v1.1)	I-DatasetName
is	O
a	O
collection	O
of	O
100k	O
crowdsourced	O
question/answer	O
pairs	O
(Rajpurkar	O
et	O
al.,	O
2016).	O
Given	O
a	O
question	O
and	O
a	O
passage	O
from	O
Wikipedia	O
containing	O
the	O
answer,	O
the	O
task	O
is	O
to	O
predict	B-TaskName
the	I-TaskName
answer	I-TaskName
text	I-TaskName
span	I-TaskName
in	I-TaskName
the	I-TaskName
passage.	I-TaskName
As	O
shown	O
in	O
Figure	O
1,	O
in	O
the	O
question	B-TaskName
answering	I-TaskName
task,	O
we	O
represent	O
the	O
input	O
question	O
and	O
passage	O
as	O
a	O
single	O
packed	O
sequence,	O
with	O
the	O
question	O
using	O
the	O
A	O
embedding	O
and	O
the	O
passage	O
using	O
the	O
B	O
embedding.	O
We	O
only	O
introduce	O
a	O
start	O
vector	O
S	O
2	O
R	O
H	O
and	O
an	O
end	O
vector	O
E	O
2	O
R	O
H	O
during	O
fine-tuning.	O
The	O
probability	O
of	O
word	O
i	O
being	O
the	O
start	O
of	O
the	O
answer	O
span	O
is	O
computed	O
as	O
a	O
dot	O
product	O
between	O
T	O
i	O
and	O
S	O
followed	O
by	O
a	O
softmax	O
over	O
all	O
of	O
the	O
words	O
in	O
the	O
paragraph:	O
P	O
i	O
=	O
e	O
S•T	O
i	O
P	O
j	O
e	O
S•T	O
j	O
.	O
The	O
analogous	O
formula	O
is	O
used	O
for	O
the	O
end	O
of	O
the	O
answer	O
span.	O
The	O
score	O
of	O
a	O
candidate	O
span	O
from	O
position	O
i	O
to	O
position	O
j	O
is	O
defined	O
as	O
S•T	O
i	O
+	O
E•T	O
j	O
,	O
and	O
the	O
maximum	O
scoring	O
span	O
where	O
j	O
i	O
is	O
used	O
as	O
a	O
prediction.	O
The	O
training	O
objective	O
is	O
the	O
sum	O
of	O
the	O
log-likelihoods	O
of	O
the	O
correct	O
start	O
and	O
end	O
positions.	O
We	O
fine-tune	O
for	O
3	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32.	B-HyperparameterValue
Table	O
2	O
shows	O
top	O
leaderboard	O
entries	O
as	O
well	O
as	O
results	O
from	O
top	O
published	O
systems	O
(Seo	O
et	O
al.,	O
2017;Clark	O
and	O
Gardner,	O
2018;Peters	O
et	O
al.,	O
2018a;Hu	O
et	O
al.,	O
2018).	O
The	O
top	O
results	O
from	O
the	O
SQuAD	B-DatasetName
leaderboard	O
do	O
not	O
have	O
up-to-date	O
public	O
system	O
descriptions	O
available,	O
11	O
and	O
are	O
allowed	O
to	O
use	O
any	O
public	O
data	O
when	O
training	O
their	O
systems.	O
We	O
therefore	O
use	O
modest	O
data	O
augmentation	O
in	O
our	O
system	O
by	O
first	O
fine-tuning	O
on	O
TriviaQA	B-DatasetName
(Joshi	O
et	O
al.,	O
2017)	O
befor	O
fine-tuning	O
on	O
SQuAD.	B-DatasetName
Our	O
best	O
performing	O
system	O
outperforms	O
the	O
top	O
leaderboard	O
system	O
by	O
+1.5	O
F1	B-MetricName
in	O
ensembling	O
and	O
+1.3	O
F1	B-MetricName
as	O
a	O
single	O
system.	O
In	O
fact,	O
our	O
single	O
BERT	B-MethodName
model	O
outperforms	O
the	O
top	O
ensemble	O
system	O
in	O
terms	O
of	O
F1	B-MetricName
score.	O
Without	O
TriviaQA	O
fine-	O
tuning	O
data,	O
we	O
only	O
lose	O
0.1-0.4	O
F1,	B-MetricName
still	O
outperforming	O
all	O
existing	O
systems	O
by	O
a	O
wide	O
margin.	O
12	O

In	O
this	O
section,	O
we	O
perform	O
ablation	O
experiments	O
over	O
a	O
number	O
of	O
facets	O
of	O
BERT	B-MethodName
in	O
order	O
to	O
better	O
understand	O
their	O
relative	O
importance.	O
Additional	O
ablation	O
studies	O
can	O
be	O
found	O
in	O
Appendix	O
C.	O

The	O
Situations	B-DatasetName
With	I-DatasetName
Adversarial	I-DatasetName
Generations	I-DatasetName
(SWAG)	B-DatasetName
dataset	O
contains	O
113k	O
sentence-pair	O
completion	O
examples	O
that	O
evaluate	O
grounded	O
commonsense	O
inference	O
(Zellers	O
et	O
al.,	O
2018).	O
Given	O
a	O
sentence,	O
the	O
task	O
is	O
to	O
choose	B-TaskName
the	I-TaskName
most	I-TaskName
plausible	I-TaskName
continuation	I-TaskName
among	I-TaskName
four	I-TaskName
choices.	I-TaskName
When	O
fine-tuning	O
on	O
the	O
SWAG	B-DatasetName
dataset,	O
we	O
construct	O
four	O
input	O
sequences,	O
each	O
containing	O
the	O
concatenation	O
of	O
the	O
given	O
sentence	O
(sentence	O
A)	O
and	O
a	O
possible	O
continuation	O
(sentence	O
B).	O
The	O
only	O
task-specific	O
parameters	O
introduced	O
is	O
a	O
vector	O
whose	O
dot	O
product	O
with	O
the	O
[CLS]	O
token	O
representation	O
C	O
denotes	O
a	O
score	O
for	O
each	O
choice	O
which	O
is	O
normalized	O
with	O
a	O
softmax	O
layer.	O
We	O
fine-tune	O
the	O
model	O
for	O
3	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16.	B-HyperparameterValue
Results	O
are	O
presented	O
in	O
Table	O
4.	O
BERT	B-MethodName
LARGE	I-MethodName
outperforms	O
the	O
authors'	O
baseline	O
ESIM+ELMo	B-MethodName
system	O
by	O
+27.1%	O
and	O
OpenAI	O
GPT	B-MethodName
by	O
8.3%.	O

The	O
SQuAD	B-DatasetName
2.0	I-DatasetName
task	O
extends	O
the	O
SQuAD	B-DatasetName
1.1	I-DatasetName
problem	O
definition	O
by	O
allowing	O
for	O
the	O
possibility	O
that	O
no	O
short	O
answer	O
exists	O
in	O
the	O
provided	O
paragraph,	O
making	O
the	O
problem	O
more	O
realistic.	O
We	O
use	O
a	O
simple	O
approach	O
to	O
extend	O
the	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
BERT	B-MethodName
model	O
for	O
this	O
task.	O
We	O
treat	O
questions	O
that	O
do	O
not	O
have	O
an	O
answer	O
as	O
having	O
an	O
answer	O
span	O
with	O
start	O
and	O
end	O
at	O
the	O
[CLS]	O
token.	O
The	O
probability	O
space	O
for	O
the	O
start	O
and	O
end	O
answer	O
span	O
positions	O
is	O
extended	O
to	O
include	O
the	O
position	O
of	O
the	O
[CLS]	O
token.	O
For	O
prediction,	O
we	O
compare	O
the	O
score	O
of	O
the	O
no-answer	O
span:	O
s	O
null	O
=	O
S•C	O
+	O
E•C	O
to	O
the	O
score	O
of	O
the	O
best	O
non-null	O
span	O
12	O
The	O
TriviaQA	B-DatasetName
data	O
we	O
used	O
consists	O
of	O
paragraphs	O
from	O
TriviaQA-Wiki	O
formed	O
of	O
the	O
first	O
400	O
tokens	O
in	O
documents,	O
that	O
contain	O
at	O
least	O
one	O
of	O
the	O
provided	O
possible	O
answers.	O
s	O
i,j	O
=	O
max	O
j	O
i	O
S•T	O
i	O
+	O
E•T	O
j	O
.	O
We	O
predict	O
a	O
non-null	O
answer	O
whenŝ	O
i,j	O
>	O
s	O
null	O
+	O
⌧	O
,	O
where	O
the	O
threshold	O
⌧	O
is	O
selected	O
on	O
the	O
dev	O
set	O
to	O
maximize	O
F1.	B-MetricName
We	O
did	O
not	O
use	O
TriviaQA	O
data	O
for	O
this	O
model.	O
We	O
fine-tuned	O
for	O
2	O
epochs	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
48.	B-HyperparameterValue
The	O
results	O
compared	O
to	O
prior	O
leaderboard	O
entries	O
and	O
top	O
published	O
work	O
(Sun	O
et	O
al.,	O
2018;Wang	O
et	O
al.,	O
2018b)	O
are	O
shown	O
in	O
Table	O
3,	O
excluding	O
systems	O
that	O
use	O
BERT	B-MethodName
as	O
one	O
of	O
their	O
components.	O
We	O
observe	O
a	O
+5.1	O
F1	O
improvement	O
over	O
the	O
previous	O
best	O
system.	O

The	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	B-DatasetName
benchmark	O
(Wang	O
et	O
al.,	O
2018a)	O
is	O
a	O
collection	O
of	O
diverse	O
natural	O
language	O
understanding	O
tasks.	O
Detailed	O
descriptions	O
of	O
GLUE	B-DatasetName
datasets	O
are	O
included	O
in	O
Appendix	O
B.1.	O
To	O
fine-tune	O
on	O
GLUE,	B-DatasetName
we	O
represent	O
the	O
input	O
sequence	O
(for	O
single	O
sentence	O
or	O
sentence	O
pairs)	O
as	O
described	O
in	O
Section	O
3,	O
and	O
use	O
the	O
final	O
hidden	O
vector	O
C	O
2	O
R	O
H	O
corresponding	O
to	O
the	O
first	O
input	O
token	O
([CLS])	O
as	O
the	O
aggregate	O
representation.	O
The	O
only	O
new	O
parameters	O
introduced	O
during	O
fine-tuning	O
are	O
classification	O
layer	O
weights	O
W	O
2	O
R	O
K⇥H	O
,	O
where	O
K	O
is	O
the	O
number	O
of	O
labels.	O
We	O
compute	O
a	O
standard	O
classification	O
loss	O
with	O
C	O
and	O
W	O
,	O
i.e.,	O
log(softmax(CW	O
T	O
)).	O
We	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
and	O
fine-tune	O
for	O
3	B-HyperparameterValue
epochs	B-HyperparameterName
over	O
the	O
data	O
for	O
all	O
GLUE	B-DatasetName
tasks.	O
For	O
each	O
task,	O
we	O
selected	O
the	O
best	O
fine-tuning	O
learning	B-HyperparameterName
rate	I-HyperparameterName
(among	O
5e-5,	B-HyperparameterValue
4e-5,	B-HyperparameterValue
3e-5,	B-HyperparameterValue
and	O
2e-5)	B-HyperparameterValue
on	O
the	O
Dev	O
set.	O
Additionally,	O
for	O
BERT	B-MethodName
LARGE	I-MethodName
we	O
found	O
that	O
finetuning	O
was	O
sometimes	O
unstable	O
on	O
small	O
datasets,	O
so	O
we	O
ran	O
several	O
random	O
restarts	O
and	O
selected	O
the	O
best	O
model	O
on	O
the	O
Dev	O
set.	O
With	O
random	O
restarts,	O
we	O
use	O
the	O
same	O
pre-trained	O
checkpoint	O
but	O
perform	O
different	O
fine-tuning	O
data	O
shuffling	O
and	O
classifier	O
layer	O
initialization.	O
9	O
Results	O
are	O
presented	O
in	O
Table	O
1.	O
Both	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
outperform	O
all	O
systems	O
on	O
all	O
tasks	O
by	O
a	O
substantial	O
margin,	O
obtaining	O
4.5%	O
and	O
7.0%	O
respective	O
average	B-MetricName
accuracy	I-MetricName
improvement	O
over	O
the	O
prior	O
state	O
of	O
the	O
art.	O
Note	O
that	O
BERT	B-MethodName
BASE	I-MethodName
and	O
OpenAI	O
GPT	B-MethodName
are	O
nearly	O
identical	O
in	O
terms	O
of	O
model	O
architecture	O
apart	O
from	O
the	O
attention	O
masking.	O
For	O
the	O
largest	O
and	O
most	O
widely	O
reported	O
GLUE	B-DatasetName
task,	O
MNLI,	B-DatasetName
BERT	B-MethodName
obtains	O
a	O
4.6%	O
absolute	O
accuracy	O
improvement.	O
On	O
the	O
official	O
GLUE	B-TaskName
leaderboard	O
10	O
,	O
BERT	B-MethodName
LARGE	I-MethodName
obtains	O
a	O
score	B-MetricName
of	O
80.5,	B-MetricValue
compared	O
to	O
OpenAI	O
GPT,	B-MethodName
which	O
obtains	O
72.8	B-MetricValue
as	O
of	O
the	O
date	O
of	O
writing.	O
We	O
find	O
that	O
BERT	B-MethodName
LARGE	I-MethodName
significantly	O
outperforms	O
BERT	B-MethodName
BASE	I-MethodName
across	O
all	O
tasks,	O
especially	O
those	O
with	O
very	O
little	O
training	O
data.	O
The	O
effect	O
of	O
model	O
size	O
is	O
explored	O
more	O
thoroughly	O
in	O
Section	O
5.2.	O

In	O
this	O
section,	O
we	O
present	O
BERT	O
fine-tuning	O
results	O
on	O
11	O
NLP	O
tasks.	O

Fine-tuning	O
is	O
straightforward	O
since	O
the	O
selfattention	O
mechanism	O
in	O
the	O
Transformer	O
allows	O
BERT	B-MethodName
to	O
model	O
many	O
downstream	O
taskswhether	O
they	O
involve	O
single	O
text	O
or	O
text	O
pairs-by	O
swapping	O
out	O
the	O
appropriate	O
inputs	O
and	O
outputs.	O
For	O
applications	O
involving	O
text	O
pairs,	O
a	O
common	O
pattern	O
is	O
to	O
independently	O
encode	O
text	O
pairs	O
before	O
applying	O
bidirectional	O
cross	O
attention,	O
such	O
as	O
Parikh	O
et	O
al.	O
(2016);	O
Seo	O
et	O
al.	O
(2017).	O
BERT	B-MethodName
instead	O
uses	O
the	O
self-attention	O
mechanism	O
to	O
unify	O
these	O
two	O
stages,	O
as	O
encoding	O
a	O
concatenated	O
text	O
pair	O
with	O
self-attention	O
effectively	O
includes	O
bidirectional	O
cross	O
attention	O
between	O
two	O
sentences.	O
For	O
each	O
task,	O
we	O
simply	O
plug	O
in	O
the	O
taskspecific	O
inputs	O
and	O
outputs	O
into	O
BERT	B-MethodName
and	O
finetune	O
all	O
the	O
parameters	O
end-to-end.	O
At	O
the	O
input,	O
sentence	O
A	O
and	O
sentence	O
B	O
from	O
pre-training	O
are	O
analogous	O
to	O
(1)	O
sentence	O
pairs	O
in	O
paraphrasing,	O
(2)	O
hypothesis-premise	O
pairs	O
in	O
entailment,	O
(3)	O
question-passage	O
pairs	O
in	O
question	B-TaskName
answering,	I-TaskName
and	O
(4)	O
a	O
degenerate	O
text-?	O
pair	O
in	O
text	O
classification	O
or	O
sequence	O
tagging.	O
At	O
the	O
output,	O
the	O
token	O
representations	O
are	O
fed	O
into	O
an	O
output	O
layer	O
for	O
tokenlevel	O
tasks,	O
such	O
as	O
sequence	B-TaskName
tagging	I-TaskName
or	O
question	B-TaskName
answering,	I-TaskName
and	O
the	O
[CLS]	O
representation	O
is	O
fed	O
into	O
an	O
output	O
layer	O
for	O
classification,	O
such	O
as	O
entailment	O
or	O
sentiment	O
analysis.	O
Compared	O
to	O
pre-training,	O
fine-tuning	O
is	O
relatively	O
inexpensive.	O
All	O
of	O
the	O
results	O
in	O
the	O
paper	O
can	O
be	O
replicated	O
in	O
at	O
most	O
1	O
hour	O
on	O
a	O
single	O
Cloud	O
TPU,	O
or	O
a	O
few	O
hours	O
on	O
a	O
GPU,	O
starting	O
from	O
the	O
exact	O
same	O
pre-trained	O
model.	O
7	O
We	O
describe	O
the	O
task-specific	O
details	O
in	O
the	O
corresponding	O
subsections	O
of	O
Section	O
4.	O
More	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
A.5.	O

The	O
pre-training	O
procedure	O
largely	O
follows	O
the	O
existing	O
literature	O
on	O
language	O
model	O
pre-training.	O
For	O
the	O
pre-training	O
corpus	O
we	O
use	O
the	O
BooksCorpus	B-DatasetName
(800M	O
words)	O
and	O
English	B-DatasetName
Wikipedia	I-DatasetName
(2,500M	O
words).	O
For	O
Wikipedia	B-DatasetName
we	O
extract	O
only	O
the	O
text	O
passages	O
and	O
ignore	O
lists,	O
tables,	O
and	O
headers.	O
It	O
is	O
critical	O
to	O
use	O
a	O
document-level	O
corpus	O
rather	O
than	O
a	O
shuffled	O
sentence-level	O
corpus	O
such	O
as	O
the	O
Billion	O
Word	O
Benchmark	O
(Chelba	O
et	O
al.,	O
2013)	O
in	O
order	O
to	O
extract	O
long	O
contiguous	O
sequences.	O

Unlike	O
Peters	O
et	O
al.	O
(2018a)	O
and	O
Radford	O
et	O
al.	O
(2018),	O
we	O
do	O
not	O
use	O
traditional	O
left-to-right	O
or	O
right-to-left	O
language	O
models	O
to	O
pre-train	O
BERT.	B-MethodName
Instead,	O
we	O
pre-train	O
BERT	B-MethodName
using	O
two	O
unsupervised	O
tasks,	O
described	O
in	O
this	O
section.	O
This	O
step	O
is	O
presented	O
in	O
the	O
left	O
part	O
of	O
Figure	O
1.	O
Task	O
#1:	O
Masked	B-TaskName
LM	I-TaskName
Intuitively,	O
it	O
is	O
reasonable	O
to	O
believe	O
that	O
a	O
deep	O
bidirectional	O
model	O
is	O
strictly	O
more	O
powerful	O
than	O
either	O
a	O
left-to-right	O
model	O
or	O
the	O
shallow	O
concatenation	O
of	O
a	O
left-toright	O
and	O
a	O
right-to-left	O
model.	O
Unfortunately,	O
standard	O
conditional	O
language	O
models	O
can	O
only	O
be	O
trained	O
left-to-right	O
or	O
right-to-left,	O
since	O
bidirectional	O
conditioning	O
would	O
allow	O
each	O
word	O
to	O
indirectly	O
"see	O
itself",	O
and	O
the	O
model	O
could	O
trivially	O
predict	O
the	O
target	O
word	O
in	O
a	O
multi-layered	O
context.	O
former	O
is	O
often	O
referred	O
to	O
as	O
a	O
"Transformer	O
encoder"	O
while	O
the	O
left-context-only	O
version	O
is	O
referred	O
to	O
as	O
a	O
"Transformer	O
decoder"	O
since	O
it	O
can	O
be	O
used	O
for	O
text	O
generation.	O
In	O
order	O
to	O
train	O
a	O
deep	O
bidirectional	O
representation,	O
we	O
simply	O
mask	O
some	O
percentage	O
of	O
the	O
input	O
tokens	O
at	O
random,	O
and	O
then	O
predict	O
those	O
masked	O
tokens.	O
We	O
refer	O
to	O
this	O
procedure	O
as	O
a	O
"masked	B-TaskName
LM"	I-TaskName
(MLM),	B-TaskName
although	O
it	O
is	O
often	O
referred	O
to	O
as	O
a	O
Cloze	O
task	O
in	O
the	O
literature	O
(Taylor,	O
1953).	O
In	O
this	O
case,	O
the	O
final	O
hidden	O
vectors	O
corresponding	O
to	O
the	O
mask	O
tokens	O
are	O
fed	O
into	O
an	O
output	O
softmax	O
over	O
the	O
vocabulary,	O
as	O
in	O
a	O
standard	O
LM.	O
In	O
all	O
of	O
our	O
experiments,	O
we	O
mask	O
15%	O
of	O
all	O
WordPiece	O
tokens	O
in	O
each	O
sequence	O
at	O
random.	O
In	O
contrast	O
to	O
denoising	O
auto-encoders	O
(Vincent	O
et	O
al.,	O
2008),	O
we	O
only	O
predict	O
the	O
masked	O
words	O
rather	O
than	O
reconstructing	O
the	O
entire	O
input.	O
Although	O
this	O
allows	O
us	O
to	O
obtain	O
a	O
bidirectional	O
pre-trained	O
model,	O
a	O
downside	O
is	O
that	O
we	O
are	O
creating	O
a	O
mismatch	O
between	O
pre-training	O
and	O
fine-tuning,	O
since	O
the	O
[MASK]	O
token	O
does	O
not	O
appear	O
during	O
fine-tuning.	O
To	O
mitigate	O
this,	O
we	O
do	O
not	O
always	O
replace	O
"masked"	O
words	O
with	O
the	O
actual	O
[MASK]	O
token.	O
The	O
training	O
data	O
generator	O
chooses	O
15%	O
of	O
the	O
token	O
positions	O
at	O
random	O
for	O
prediction.	O
If	O
the	O
i-th	O
token	O
is	O
chosen,	O
we	O
replace	O
the	O
i-th	O
token	O
with	O
(1)	O
the	O
[MASK]	O
token	O
80%	O
of	O
the	O
time	O
(2)	O
a	O
random	O
token	O
10%	O
of	O
the	O
time	O
(3)	O
the	O
unchanged	O
i-th	O
token	O
10%	O
of	O
the	O
time.	O
Then,	O
T	O
i	O
will	O
be	O
used	O
to	O
predict	O
the	O
original	O
token	O
with	O
cross	B-MetricName
entropy	I-MetricName
loss.	I-MetricName
We	O
compare	O
variations	O
of	O
this	O
procedure	O
in	O
Appendix	O
C.2.	O
Task	O
#2:	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
(NSP)	B-TaskName
Many	O
important	O
downstream	O
tasks	O
such	O
as	O
Question	B-TaskName
Answering	I-TaskName
(QA)	B-TaskName
and	O
Natural	B-TaskName
Language	I-TaskName
Inference	I-TaskName
(NLI)	B-TaskName
are	O
based	O
on	O
understanding	O
the	O
relationship	O
between	O
two	O
sentences,	O
which	O
is	O
not	O
directly	O
captured	O
by	O
language	O
modeling.	O
In	O
order	O
to	O
train	O
a	O
model	O
that	O
understands	O
sentence	O
relationships,	O
we	O
pre-train	O
for	O
a	O
binarized	O
next	O
sentence	O
prediction	O
task	O
that	O
can	O
be	O
trivially	O
generated	O
from	O
any	O
monolingual	O
corpus.	O
Specifically,	O
when	O
choosing	O
the	O
sentences	O
A	O
and	O
B	O
for	O
each	O
pretraining	O
example,	O
50%	O
of	O
the	O
time	O
B	O
is	O
the	O
actual	O
next	O
sentence	O
that	O
follows	O
A	O
(labeled	O
as	O
IsNext),	O
and	O
50%	O
of	O
the	O
time	O
it	O
is	O
a	O
random	O
sentence	O
from	O
the	O
corpus	O
(labeled	O
as	O
NotNext).	O
As	O
we	O
show	O
in	O
Figure	O
1,	O
C	O
is	O
used	O
for	O
next	O
sentence	O
prediction	O
(NSP).	B-TaskName
5	O
Despite	O
its	O
simplicity,	O
we	O
demonstrate	O
in	O
Section	O
5.1	O
that	O
pre-training	O
towards	O
this	O
task	O
is	O
very	O
beneficial	O
to	O
both	O
QA	B-TaskName
and	O
NLI.	B-TaskName
6	O
The	O
NSP	B-TaskName
task	O
is	O
closely	O
related	O
to	O
representationlearning	O
objectives	O
used	O
in	O
Jernite	O
et	O
al.	O
(2017)	O
and	O
Logeswaran	O
and	O
Lee	O
(2018).	O
However,	O
in	O
prior	O
work,	O
only	O
sentence	O
embeddings	O
are	O
transferred	O
to	O
down-stream	O
tasks,	O
where	O
BERT	B-MethodName
transfers	O
all	O
parameters	O
to	O
initialize	O
end-task	O
model	O
parameters.	O

We	O
introduce	O
BERT	B-MethodName
and	O
its	O
detailed	O
implementation	O
in	O
this	O
section.	O
There	O
are	O
two	O
steps	O
in	O
our	O
framework:	O
pre-training	O
and	O
fine-tuning.	O
During	O
pre-training,	O
the	O
model	O
is	O
trained	O
on	O
unlabeled	O
data	O
over	O
different	O
pre-training	O
tasks.	O
For	O
finetuning,	O
the	O
BERT	B-MethodName
model	O
is	O
first	O
initialized	O
with	O
the	O
pre-trained	O
parameters,	O
and	O
all	O
of	O
the	O
parameters	O
are	O
fine-tuned	O
using	O
labeled	O
data	O
from	O
the	O
downstream	O
tasks.	O
Each	O
downstream	O
task	O
has	O
separate	O
fine-tuned	O
models,	O
even	O
though	O
they	O
are	O
initialized	O
with	O
the	O
same	O
pre-trained	O
parameters.	O
The	O
question-answering	B-TaskName
example	O
in	O
Figure	O
1	O
will	O
serve	O
as	O
a	O
running	O
example	O
for	O
this	O
section.	O
A	O
distinctive	O
feature	O
of	O
BERT	B-MethodName
is	O
its	O
unified	O
architecture	O
across	O
different	O
tasks.	O
There	O
is	O
mini-mal	O
difference	O
between	O
the	O
pre-trained	O
architecture	O
and	O
the	O
final	O
downstream	O
architecture.	O
Model	O
Architecture	O
BERT's	B-MethodName
model	O
architecture	O
is	O
a	O
multi-layer	O
bidirectional	O
Transformer	O
encoder	O
based	O
on	O
the	O
original	O
implementation	O
described	O
in	O
Vaswani	O
et	O
al.	O
(2017)	O
and	O
released	O
in	O
the	O
tensor2tensor	O
library.	O
1	O
Because	O
the	O
use	O
of	O
Transformers	O
has	O
become	O
common	O
and	O
our	O
implementation	O
is	O
almost	O
identical	O
to	O
the	O
original,	O
we	O
will	O
omit	O
an	O
exhaustive	O
background	O
description	O
of	O
the	O
model	O
architecture	O
and	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al.	O
(2017)	O
as	O
well	O
as	O
excellent	O
guides	O
such	O
as	O
"The	O
Annotated	O
Transformer."	O
2	O
In	O
this	O
work,	O
we	O
denote	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
(i.e.,	O
Transformer	O
blocks)	O
as	O
L,	B-HyperparameterName
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
as	O
H,	B-HyperparameterName
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
self-attention	I-HyperparameterName
heads	I-HyperparameterName
as	O
A.	B-HyperparameterName
3	O
We	O
primarily	O
report	O
results	O
on	O
two	O
model	O
sizes:	O
BERT	B-MethodName
BASE	I-MethodName
was	O
chosen	O
to	O
have	O
the	O
same	O
model	O
size	O
as	O
OpenAI	O
GPT	B-MethodName
for	O
comparison	O
purposes.	O
Critically,	O
however,	O
the	O
BERT	B-MethodName
Transformer	O
uses	O
bidirectional	O
self-attention,	O
while	O
the	O
GPT	B-MethodName
Transformer	O
uses	O
constrained	O
self-attention	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
context	O
to	O
its	O
left.	O
4	O
Input/Output	O
Representations	O
To	O
make	O
BERT	B-MethodName
handle	O
a	O
variety	O
of	O
down-stream	O
tasks,	O
our	O
input	O
representation	O
is	O
able	O
to	O
unambiguously	O
represent	O
both	O
a	O
single	O
sentence	O
and	O
a	O
pair	O
of	O
sentences	O
(e.g.,	O
h	O
Question,	O
Answer	O
i)	O
in	O
one	O
token	O
sequence.	O
Throughout	O
this	O
work,	O
a	O
"sentence"	O
can	O
be	O
an	O
arbitrary	O
span	O
of	O
contiguous	O
text,	O
rather	O
than	O
an	O
actual	O
linguistic	O
sentence.	O
A	O
"sequence"	O
refers	O
to	O
the	O
input	O
token	O
sequence	O
to	O
BERT,	B-MethodName
which	O
may	O
be	O
a	O
single	O
sentence	O
or	O
two	O
sentences	O
packed	O
together.	O
BERT	B-MethodName
BASE	I-MethodName
(L=12,	B-HyperparameterName
We	O
use	O
WordPiece	O
embeddings	O
(Wu	O
et	O
al.,	O
2016)	O
with	O
a	O
30,000	B-HyperparameterValue
token	B-HyperparameterName
vocabulary.	I-HyperparameterName
The	O
first	O
token	O
of	O
every	O
sequence	O
is	O
always	O
a	O
special	O
classification	O
token	O
([CLS]).	O
The	O
final	O
hidden	O
state	O
corresponding	O
to	O
this	O
token	O
is	O
used	O
as	O
the	O
aggregate	O
sequence	O
representation	O
for	O
classification	O
tasks.	O
Sentence	O
pairs	O
are	O
packed	O
together	O
into	O
a	O
single	O
sequence.	O
We	O
differentiate	O
the	O
sentences	O
in	O
two	O
ways.	O
First,	O
we	O
separate	O
them	O
with	O
a	O
special	O
token	O
([SEP]).	O
Second,	O
we	O
add	O
a	O
learned	O
embedding	O
to	O
every	O
token	O
indicating	O
whether	O
it	O
belongs	O
to	O
sentence	O
A	O
or	O
sentence	O
B.	O
As	O
shown	O
in	O
Figure	O
1,	O
we	O
denote	O
input	O
embedding	B-HyperparameterName
as	O
E,	B-HyperparameterName
the	O
final	O
hidden	O
vector	O
of	O
the	O
special	O
[CLS]	O
token	O
as	O
C	O
2	O
R	O
H	O
,	O
and	O
the	O
final	O
hidden	O
vector	O
for	O
the	O
i	O
th	O
input	O
token	O
as	O
T	O
i	O
2	O
R	O
H	O
.	O
For	O
a	O
given	O
token,	O
its	O
input	O
representation	O
is	O
constructed	O
by	O
summing	O
the	O
corresponding	O
token,	O
segment,	O
and	O
position	O
embeddings.	O
A	O
visualization	O
of	O
this	O
construction	O
can	O
be	O
seen	O
in	O
Figure	O
2.	O

There	O
has	O
also	O
been	O
work	O
showing	O
effective	O
transfer	O
from	O
supervised	O
tasks	O
with	O
large	O
datasets,	O
such	O
as	O
natural	O
language	O
inference	O
(Conneau	O
et	O
al.,	O
2017)	O
and	O
machine	O
translation	O
(McCann	O
et	O
al.,	O
2017).	O
Computer	O
vision	O
research	O
has	O
also	O
demonstrated	O
the	O
importance	O
of	O
transfer	O
learning	O
from	O
large	O
pre-trained	O
models,	O
where	O
an	O
effective	O
recipe	O
is	O
to	O
fine-tune	O
models	O
pre-trained	O
with	O
Ima-geNet	O
(Deng	O
et	O
al.,	O
2009;Yosinski	O
et	O
al.,	O
2014).	O

There	O
is	O
a	O
long	O
history	O
of	O
pre-training	O
general	O
language	O
representations,	O
and	O
we	O
briefly	O
review	O
the	O
most	O
widely-used	O
approaches	O
in	O
this	O
section.	O

Learning	O
widely	O
applicable	O
representations	O
of	O
words	O
has	O
been	O
an	O
active	O
area	O
of	O
research	O
for	O
decades,	O
including	O
non-neural	O
(Brown	O
et	O
al.,	O
1992;Ando	O
and	O
Zhang,	O
2005;Blitzer	O
et	O
al.,	O
2006)	O
and	O
neural	O
Pennington	O
et	O
al.,	O
2014)	O
methods.	O
Pre-trained	O
word	O
embeddings	O
are	O
an	O
integral	O
part	O
of	O
modern	O
NLP	O
systems,	O
offering	O
significant	O
improvements	O
over	O
embeddings	O
learned	O
from	O
scratch	O
(Turian	O
et	O
al.,	O
2010).	O
To	O
pretrain	O
word	O
embedding	O
vectors,	O
left-to-right	O
language	O
modeling	O
objectives	O
have	O
been	O
used	O
(Mnih	O
and	O
Hinton,	O
2009),	O
as	O
well	O
as	O
objectives	O
to	O
discriminate	O
correct	O
from	O
incorrect	O
words	O
in	O
left	O
and	O
right	O
context	O
.	O
These	O
approaches	O
have	O
been	O
generalized	O
to	O
coarser	O
granularities,	O
such	O
as	O
sentence	O
embeddings	O
Logeswaran	O
and	O
Lee,	O
2018)	O
or	O
paragraph	O
embeddings	O
(Le	O
and	O
Mikolov,	O
2014).	O
To	O
train	O
sentence	O
representations,	O
prior	O
work	O
has	O
used	O
objectives	O
to	O
rank	O
candidate	O
next	O
sentences	O
(Jernite	O
et	O
al.,	O
2017;Logeswaran	O
and	O
Lee,	O
2018),	O
left-to-right	O
generation	O
of	O
next	O
sentence	O
words	O
given	O
a	O
representation	O
of	O
the	O
previous	O
sentence	O
,	O
or	O
denoising	O
autoencoder	O
derived	O
objectives	O
(Hill	O
et	O
al.,	O
2016).	O
ELMo	O
and	O
its	O
predecessor	O
(Peters	O
et	O
al.,	O
2017(Peters	O
et	O
al.,	O
,	O
2018a	O
generalize	O
traditional	O
word	O
embedding	O
research	O
along	O
a	O
different	O
dimension.	O
They	O
extract	O
context-sensitive	O
features	O
from	O
a	O
left-to-right	O
and	O
a	O
right-to-left	O
language	O
model.	O
The	O
contextual	O
representation	O
of	O
each	O
token	O
is	O
the	O
concatenation	O
of	O
the	O
left-to-right	O
and	O
right-to-left	O
representations.	O
When	O
integrating	O
contextual	O
word	O
embeddings	O
with	O
existing	O
task-specific	O
architectures,	O
ELMo	O
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
several	O
major	O
NLP	O
benchmarks	O
(Peters	O
et	O
al.,	O
2018a)	O
including	O
question	O
answering	O
(Rajpurkar	O
et	O
al.,	O
2016),	O
sentiment	O
analysis	O
(Socher	O
et	O
al.,	O
2013),	O
and	O
named	O
entity	O
recognition	O
(Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder,	O
2003).	O
Melamud	O
et	O
al.	O
(2016)	O
proposed	O
learning	O
contextual	O
representations	O
through	O
a	O
task	O
to	O
predict	O
a	O
single	O
word	O
from	O
both	O
left	O
and	O
right	O
context	O
using	O
LSTMs.	O
Similar	O
to	O
ELMo,	O
their	O
model	O
is	O
feature-based	O
and	O
not	O
deeply	O
bidirectional.	O
Fedus	O
et	O
al.	O
(2018)	O
shows	O
that	O
the	O
cloze	O
task	O
can	O
be	O
used	O
to	O
improve	O
the	O
robustness	O
of	O
text	O
generation	O
models.	O

As	O
with	O
the	O
feature-based	O
approaches,	O
the	O
first	O
works	O
in	O
this	O
direction	O
only	O
pre-trained	O
word	O
embedding	O
parameters	O
from	O
unlabeled	O
text	O
(Collobert	O
and	O
Weston,	O
2008).	O
More	O
recently,	O
sentence	O
or	O
document	O
encoders	O
which	O
produce	O
contextual	O
token	O
representations	O
have	O
been	O
pre-trained	O
from	O
unlabeled	O
text	O
and	O
fine-tuned	O
for	O
a	O
supervised	O
downstream	O
task	O
(Dai	O
and	O
Le,	O
2015;Howard	O
and	O
Ruder,	O
2018;Radford	O
et	O
al.,	O
2018).	O
The	O
advantage	O
of	O
these	O
approaches	O
is	O
that	O
few	O
parameters	O
need	O
to	O
be	O
learned	O
from	O
scratch.	O
At	O
least	O
partly	O
due	O
to	O
this	O
advantage,	O
OpenAI	O
GPT	O
(Radford	O
et	O
al.,	O
2018)	O
achieved	O
previously	O
state-of-the-art	O
results	O
on	O
many	O
sentencelevel	O
tasks	O
from	O
the	O
GLUE	B-DatasetName
benchmark	O
(Wang	O
et	O
al.,	O
2018a).	O
Left-to-right	O
language	O
model-BERT	B-MethodName
BERT	B-MethodName
E	O
[CLS]	O
E	O
1	O
E	O
[SEP]	O
...	O
E	O
N	O
E	O
1	O
'	O
...	O
E	O
M	O
'	O
C	O
T	O
1	O
T	O
[SEP]	O
...	O
...	O
ing	O
and	O
auto-encoder	O
objectives	O
have	O
been	O
used	O
for	O
pre-training	O
such	O
models	O
(Howard	O
and	O
Ruder,	O
2018;Radford	O
et	O
al.,	O
2018;Dai	O
and	O
Le,	O
2015).	O

Language	O
model	O
pre-training	O
has	O
been	O
shown	O
to	O
be	O
effective	O
for	O
improving	O
many	O
natural	O
language	O
processing	O
tasks	O
(Dai	O
and	O
Le,	O
2015;Peters	O
et	O
al.,	O
2018a;Radford	O
et	O
al.,	O
2018;Howard	O
and	O
Ruder,	O
2018).	O
These	O
include	O
sentence-level	O
tasks	O
such	O
as	O
natural	O
language	O
inference	O
(Bowman	O
et	O
al.,	O
2015;Williams	O
et	O
al.,	O
2018)	O
and	O
paraphrasing	O
(Dolan	O
and	O
Brockett,	O
2005),	O
which	O
aim	O
to	O
predict	O
the	O
relationships	O
between	O
sentences	O
by	O
analyzing	O
them	O
holistically,	O
as	O
well	O
as	O
token-level	O
tasks	O
such	O
as	O
named	O
entity	O
recognition	O
and	O
question	O
answering,	O
where	O
models	O
are	O
required	O
to	O
produce	O
fine-grained	O
output	O
at	O
the	O
token	O
level	O
(Tjong	O
Kim	O
Sang	O
and	O
De	O
Meulder,	O
2003;Rajpurkar	O
et	O
al.,	O
2016).	O
There	O
are	O
two	O
existing	O
strategies	O
for	O
applying	O
pre-trained	O
language	O
representations	O
to	O
downstream	O
tasks:	O
feature-based	O
and	O
fine-tuning.	O
The	O
feature-based	O
approach,	O
such	O
as	O
ELMo	O
(Peters	O
et	O
al.,	O
2018a),	O
uses	O
task-specific	O
architectures	O
that	O
include	O
the	O
pre-trained	O
representations	O
as	O
additional	O
features.	O
The	O
fine-tuning	O
approach,	O
such	O
as	O
the	O
Generative	O
Pre-trained	O
Transformer	O
(OpenAI	O
GPT)	O
(Radford	O
et	O
al.,	O
2018),	O
introduces	O
minimal	B-HyperparameterName
task-specific	O
parameters,	O
and	O
is	O
trained	O
on	O
the	O
downstream	O
tasks	O
by	O
simply	O
fine-tuning	O
all	O
pretrained	O
parameters.	O
The	O
two	O
approaches	O
share	O
the	O
same	O
objective	O
function	O
during	O
pre-training,	O
where	O
they	O
use	O
unidirectional	O
language	O
models	O
to	O
learn	O
general	O
language	O
representations.	O
We	O
argue	O
that	O
current	O
techniques	O
restrict	O
the	O
power	O
of	O
the	O
pre-trained	O
representations,	O
especially	O
for	O
the	O
fine-tuning	O
approaches.	O
The	O
major	O
limitation	O
is	O
that	O
standard	O
language	O
models	O
are	O
unidirectional,	O
and	O
this	O
limits	O
the	O
choice	O
of	O
architectures	O
that	O
can	O
be	O
used	O
during	O
pre-training.	O
For	O
example,	O
in	O
OpenAI	O
GPT,	O
the	O
authors	O
use	O
a	O
left-toright	O
architecture,	O
where	O
every	O
token	O
can	O
only	O
attend	O
to	O
previous	O
tokens	O
in	O
the	O
self-attention	O
layers	O
of	O
the	O
Transformer	O
(Vaswani	O
et	O
al.,	O
2017).	O
Such	O
restrictions	O
are	O
sub-optimal	O
for	O
sentence-level	O
tasks,	O
and	O
could	O
be	O
very	O
harmful	O
when	O
applying	O
finetuning	O
based	O
approaches	O
to	O
token-level	O
tasks	O
such	O
as	O
question	O
answering,	O
where	O
it	O
is	O
crucial	O
to	O
incorporate	O
context	O
from	O
both	O
directions.	O
In	O
this	O
paper,	O
we	O
improve	O
the	O
fine-tuning	O
based	O
approaches	O
by	O
proposing	O
BERT:	B-MethodName
Bidirectional	B-MethodName
Encoder	I-MethodName
Representations	I-MethodName
from	I-MethodName
Transformers.	I-MethodName
BERT	B-MethodName
alleviates	O
the	O
previously	O
mentioned	O
unidirectionality	O
constraint	O
by	O
using	O
a	O
"masked	O
language	O
model"	O
(MLM)	O
pre-training	O
objective,	O
inspired	O
by	O
the	O
Cloze	O
task	O
(Taylor,	O
1953).	O
The	O
masked	O
language	O
model	O
randomly	O
masks	O
some	O
of	O
the	O
tokens	O
from	O
the	O
input,	O
and	O
the	O
objective	O
is	O
to	O
predict	O
the	O
original	O
vocabulary	O
id	O
of	O
the	O
masked	O
word	O
based	O
only	O
on	O
its	O
context.	O
Unlike	O
left-toright	O
language	O
model	O
pre-training,	O
the	O
MLM	O
objective	O
enables	O
the	O
representation	O
to	O
fuse	O
the	O
left	O
and	O
the	O
right	O
context,	O
which	O
allows	O
us	O
to	O
pretrain	O
a	O
deep	O
bidirectional	O
Transformer.	O
In	O
addition	O
to	O
the	O
masked	O
language	O
model,	O
we	O
also	O
use	O
a	O
"next	O
sentence	O
prediction"	O
task	O
that	O
jointly	O
pretrains	O
text-pair	O
representations.	O
The	O
contributions	O
of	O
our	O
paper	O
are	O
as	O
follows:	O
•	O
We	O
demonstrate	O
the	O
importance	O
of	O
bidirectional	O
pre-training	O
for	O
language	O
representations.	O
Unlike	O
Radford	O
et	O
al.	O
(2018),	O
which	O
uses	O
unidirectional	O
language	O
models	O
for	O
pre-training,	O
BERT	B-MethodName
uses	O
masked	O
language	O
models	O
to	O
enable	O
pretrained	O
deep	O
bidirectional	O
representations.	O
This	O
is	O
also	O
in	O
contrast	O
to	O
Peters	O
et	O
al.	O
(2018a),	O
which	O
uses	O
a	O
shallow	O
concatenation	O
of	O
independently	O
trained	O
left-to-right	O
and	O
right-to-left	O
LMs.	O
•	O
We	O
show	O
that	O
pre-trained	O
representations	O
reduce	O
the	O
need	O
for	O
many	O
heavily-engineered	O
taskspecific	O
architectures.	O
BERT	B-MethodName
is	O
the	O
first	O
finetuning	O
based	O
representation	O
model	O
that	O
achieves	O
state-of-the-art	O
performance	O
on	O
a	O
large	O
suite	O
of	O
sentence-level	O
and	O
token-level	O
tasks,	O
outperforming	O
many	O
task-specific	O
architectures.	O
•	O
BERT	B-MethodName
advances	O
the	O
state	O
of	O
the	O
art	O
for	O
eleven	O
NLP	O
tasks.	O
The	O
code	O
and	O
pre-trained	O
models	O
are	O
available	O
at	O
google-research/bert.	O

We	O
introduce	O
a	O
new	O
language	O
representation	O
model	O
called	O
BERT,	B-MethodName
which	O
stands	O
for	O
Bidirectional	B-MethodName
Encoder	I-MethodName
Representations	I-MethodName
from	I-MethodName
Transformers.	I-MethodName
Unlike	O
recent	O
language	O
representation	O
models	O
(Peters	O
et	O
al.,	O
2018a;Radford	O
et	O
al.,	O
2018),	O
BERT	B-MethodName
is	O
designed	O
to	O
pretrain	O
deep	O
bidirectional	O
representations	O
from	O
unlabeled	O
text	O
by	O
jointly	O
conditioning	O
on	O
both	O
left	O
and	O
right	O
context	O
in	O
all	O
layers.	O
As	O
a	O
result,	O
the	O
pre-trained	O
BERT	B-MethodName
model	O
can	O
be	O
finetuned	O
with	O
just	O
one	O
additional	O
output	O
layer	O
to	O
create	O
state-of-the-art	O
models	O
for	O
a	O
wide	O
range	O
of	O
tasks,	O
such	O
as	O
question	O
answering	O
and	O
language	O
inference,	O
without	O
substantial	O
taskspecific	O
architecture	O
modifications.	O
BERT	B-MethodName
is	O
conceptually	O
simple	O
and	O
empirically	O
powerful.	O
It	O
obtains	O
new	O
state-of-the-art	O
results	O
on	O
eleven	O
natural	O
language	O
processing	O
tasks,	O
including	O
pushing	O
the	O
GLUE	O
score	O
to	O
80.5%	O
(7.7%	O
point	O
absolute	O
improvement),	O
MultiNLI	O
accuracy	O
to	O
86.7%	O
(4.6%	O
absolute	O
improvement),	O
SQuAD	O
v1.1	O
question	O
answering	O
Test	O
F1	O
to	O
93.2	O
(1.5	O
point	O
absolute	O
improvement)	O
and	O
SQuAD	O
v2.0	O
Test	O
F1	O
to	O
83.1	O
(5.1	O
point	O
absolute	O
improvement).	O

Acknowledgments	O
This	O
work	O
was	O
partially	O
supported	O
by	O
the	O
Defense	O
Advanced	O
Research	O
Projects	O
Agency	O
and	O
monitored	O
by	O
SPAWAR	O
under	O
contract	O
No.	O
N66001-99-2-8916.	O
The	O
views	O
and	O
findings	O
contained	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
necessarily	O
reflect	O
the	O
position	O
of	O
policy	O
of	O
the	O
Government	O
and	O
no	O
official	O
endorsement	O
should	O
be	O
inferred.	O
We	O
gratefully	O
acknowledge	O
comments	O
about	O
the	O
geometric	O
mean	O
by	O
John	O
Makhoul	O
of	O
BBN	O
and	O
discussions	O
with	O
George	O
Doddington	O
of	O
NIST.	O
We	O
especially	O
wish	O
to	O
thank	O
our	O
colleagues	O
who	O
served	O
in	O
the	O
monolingual	O
and	O
bilingual	O
judge	O
pools	O
for	O
their	O
perseverance	O
in	O
judging	O
the	O
output	O
of	O
Chinese-English	O
MT	O
systems.	O

We	O
believe	O
that	O
BLEU	B-MethodName
will	O
accelerate	O
the	O
MT	B-TaskName
R&D	O
cycle	O
by	O
allowing	O
researchers	O
to	O
rapidly	O
home	O
in	O
on	O
effective	O
modeling	O
ideas.	O
Our	O
belief	O
is	O
reinforced	O
by	O
a	O
recent	O
statistical	O
analysis	O
of	O
BLEU's	B-MethodName
correlation	O
with	O
human	O
judgment	O
for	O
translation	O
into	O
English	O
from	O
four	O
quite	O
different	O
languages	O
(Arabic,	O
Chinese,	O
French,	O
Spanish)	O
representing	O
3	O
different	O
language	O
families	O
(Papineni	O
et	O
al.,	O
2002)!	O
BLEU's	B-MethodName
strength	O
is	O
that	O
it	O
correlates	O
highly	O
with	O
human	O
judg-	O
8	O
Crossing	O
this	O
chasm	O
for	O
Chinese-English	O
translation	O
appears	O
to	O
be	O
a	O
significant	O
challenge	O
for	O
the	O
current	O
state-of-the-art	O
systems.	O
ments	O
by	O
averaging	O
out	O
individual	O
sentence	O
judgment	O
errors	O
over	O
a	O
test	O
corpus	O
rather	O
than	O
attempting	O
to	O
divine	O
the	O
exact	O
human	O
judgment	O
for	O
every	O
sentence:	O
quantity	O
leads	O
to	O
quality.	O
Finally,	O
since	O
MT	B-TaskName
and	O
summarization	O
can	O
both	O
be	O
viewed	O
as	O
natural	O
language	O
generation	O
from	O
a	O
textual	O
context,	O
we	O
believe	O
BLEU	B-MethodName
could	O
be	O
adapted	O
to	O
evaluating	O
summarization	O
or	O
similar	O
NLG	O
tasks.	O

Figure	O
5	O
shows	O
a	O
linear	O
regression	O
of	O
the	O
monolingual	O
group	O
scores	O
as	O
a	O
function	O
of	O
the	O
BLEU	B-MethodName
score	O
over	O
two	O
reference	O
translations	O
for	O
the	O
5	O
systems.	O
The	O
high	O
correlation	O
coefficient	O
of	O
0.99	O
indicates	O
that	O
BLEU	B-MethodName
tracks	O
human	B-MethodName
judgment	I-MethodName
well.	O
Particularly	O
interesting	O
is	O
how	O
well	O
BLEU	B-MethodName
distinguishes	O
between	O
S2	O
and	O
S3	O
which	O
are	O
quite	O
close.	O
Figure	O
6	O
shows	O
the	O
comparable	O
regression	O
results	O
for	O
the	O
bilingual	O
group.	O
The	O
correlation	O
coefficient	O
is	O
0.96.	O
We	O
now	O
take	O
the	O
worst	O
system	O
as	O
a	O
reference	O
point	O
and	O
compare	O
the	O
BLEU	B-MethodName
scores	O
with	O
the	O
human	B-MethodName
judg-ment	I-MethodName
scores	O
of	O
the	O
remaining	O
systems	O
relative	O
to	O
the	O
worst	O
system.	O
We	O
took	O
the	O
BLEU,	B-MethodName
monolingual	O
group,	O
and	O
bilingual	O
group	O
scores	O
for	O
the	O
5	O
systems	O
and	O
linearly	O
normalized	O
them	O
by	O
their	O
corresponding	O
range	O
(the	O
maximum	O
and	O
minimum	O
score	O
across	O
the	O
5	O
systems).	O
The	O
normalized	O
scores	O
are	O
shown	O
in	O
Figure	O
7.	O
This	O
figure	O
illustrates	O
the	O
high	O
correlation	O
between	O
the	O
BLEU	B-MethodName
score	O
and	O
the	O
monolingual	O
group.	O
Of	O
particular	O
interest	O
is	O
the	O
accuracy	O
of	O
BLEU's	B-MethodName
estimate	O
of	O
the	O
small	O
difference	O
between	O
S2	O
and	O
S3	O
and	O
the	O
larger	O
difference	O
between	O
S3	O
and	O
H1.	O
The	O
figure	O
also	O
highlights	O
the	O
relatively	O
large	O
gap	O
between	O
MT	B-TaskName
systems	O
and	O
human	O
translators.	O
8	O
In	O
addition,	O
we	O
surmise	O
that	O
the	O
bilingual	O
group	O
was	O
very	O
forgiving	O
in	O
judging	O
H1	O
relative	O
to	O
H2	O
because	O
the	O
monolingual	O
group	O
found	O
a	O
rather	O
large	O
difference	O
in	O
the	O
fluency	O
of	O
their	O
translations.	O

Figure	O
4	O
shows	O
the	O
same	O
results	O
for	O
the	O
bilingual	O
group.	O
They	O
also	O
find	O
that	O
S3	O
is	O
slightly	O
better	O
than	O
S2	O
(at	O
95%	O
confidence)	O
though	O
they	O
judge	O
that	O
the	O
human	O
translations	O
are	O
much	O
closer	O
(indistinguishable	O
at	O
95%	O
confidence),	O
suggesting	O
that	O
the	O
bilinguals	O
tended	O
to	O
focus	O
more	O
on	O
adequacy	O
than	O
on	O
fluency.	O

Figure	O
3	O
shows	O
the	O
mean	O
difference	O
between	O
the	O
scores	O
of	O
two	O
consecutive	O
systems	O
and	O
the	O
95%	O
confidence	O
interval	O
about	O
the	O
mean.	O
We	O
see	O
that	O
S2	O
is	O
quite	O
a	O
bit	O
better	O
than	O
S1	O
(by	O
a	O
mean	B-MetricName
opinion	I-MetricName
score	I-MetricName
difference	I-MetricName
of	O
0.326	B-MetricValue
on	O
the	O
5-point	O
scale),	O
while	O
S3	O
is	O
judged	O
a	O
little	O
better	O
(by	O
0.114).	B-MetricValue
Both	O
differences	O
are	O
significant	O
at	O
the	O
95%	O
level.	O
7	O
The	O
human	O
H1	O
is	O
much	O
better	O
than	O
the	O
best	O
system,	O
though	O
a	O
bit	O
worse	O
than	O
human	O
H2.	O
This	O
is	O
not	O
surprising	O
given	O
that	O
H1	O
is	O
not	O
a	O
native	O
speaker	O
of	O
either	O
Chinese	O
or	O
English,	O
whereas	O
H2	O
is	O
a	O
native	O
English	O
speaker.	O
Again,	O
the	O
difference	O
between	O
the	O
human	O
translators	O
is	O
significant	O
beyond	O
the	O
95%	O
level.	O

We	O
had	O
two	O
groups	O
of	O
human	O
judges.	O
The	O
first	O
group,	O
called	O
the	O
monolingual	O
group,	O
consisted	O
of	O
10	O
native	O
speakers	O
of	O
English.	O
The	O
second	O
group,	O
called	O
the	O
bilingual	O
group,	O
consisted	O
of	O
10	O
native	O
speakers	O
of	O
Chinese	O
who	O
had	O
lived	O
in	O
the	O
United	O
States	O
for	O
the	O
past	O
several	O
years.	O
None	O
of	O
the	O
human	B-MethodName
judges	I-MethodName
was	O
a	O
professional	O
translator.	O
The	O
humans	O
judged	O
our	O
5	O
standard	O
systems	O
on	O
a	O
Chinese	O
sentence	O
subset	O
extracted	O
at	O
random	O
from	O
our	O
500	O
sentence	O
test	O
corpus.	O
We	O
paired	O
each	O
source	O
sentence	O
with	O
each	O
of	O
its	O
5	O
translations,	O
for	O
a	O
total	O
of	O
250	O
pairs	O
of	O
Chinese	O
source	O
and	O
English	O
translations.	O
We	O
prepared	O
a	O
web	O
page	O
with	O
these	O
translation	O
pairs	O
randomly	O
ordered	O
to	O
disperse	O
the	O
five	O
translations	O
of	O
each	O
source	O
sentence.	O
All	O
judges	O
used	O
this	O
same	O
webpage	O
and	O
saw	O
the	O
sentence	O
pairs	O
in	O
the	O
same	O
order.	O
They	O
rated	O
each	O
translation	O
from	O
1	O
(very	O
bad)	O
to	O
5	O
(very	O
good).	O
The	O
monolingual	O
group	O
made	O
their	O
judgments	O
based	O
only	O
on	O
the	O
translations'	O
readability	O
and	O
fluency.	O
As	O
must	O
be	O
expected,	O
some	O
judges	O
were	O
more	O
liberal	O
than	O
others.	O
And	O
some	O
sentences	O
were	O
easier	O
to	O
translate	O
than	O
others.	O
To	O
account	O
for	O
the	O
intrinsic	O
difference	O
between	O
judges	O
and	O
the	O
sentences,	O
we	O
compared	O
each	O
judge's	O
rating	O
for	O
a	O
sentence	O
across	O
systems.	O
We	O
performed	O
four	O
pairwise	O
t-test	O
comparisons	O
between	O
adjacent	O
systems	O
as	O
ordered	O
by	O
their	O
aggregate	O
average	B-MetricName
score.	I-MetricName

The	O
BLEU	B-MethodName
metric	O
ranges	O
from	O
0	O
to	O
1.	O
Few	O
translations	O
will	O
attain	O
a	O
score	O
of	O
1	O
unless	O
they	O
are	O
identical	O
to	O
a	O
reference	O
translation.	O
For	O
this	O
reason,	O
even	O
a	O
human	O
translator	O
will	O
not	O
necessarily	O
score	O
1.	O
It	O
is	O
important	O
to	O
note	O
that	O
the	O
more	O
reference	O
translations	O
per	O
sentence	O
there	O
are,	O
the	O
higher	O
the	O
score	O
is.	O
Thus,	O
one	O
must	O
be	O
cautious	O
making	O
even	O
"rough"	O
comparisons	O
on	O
evaluations	O
with	O
different	O
numbers	O
of	O
reference	O
translations:	O
on	O
a	O
test	O
corpus	O
of	O
about	O
500	O
sentences	O
(40	O
general	O
news	O
stories),	O
a	O
human	O
translator	O
scored	O
0.3468	O
against	O
four	O
references	O
and	O
scored	O
0.2571	O
against	O
two	O
references.	O
Table	O
1	O
shows	O
the	O
BLEU	B-MethodName
scores	O
of	O
the	O
5	O
systems	O
against	O
two	O
references	O
on	O
this	O
test	O
corpus.	O
The	O
MT	B-TaskName
systems	O
S2	O
and	O
S3	O
are	O
very	O
close	O
in	O
this	O
metric.	O
Hence,	O
several	O
questions	O
arise:	O
•	O
Is	O
the	O
difference	O
in	O
BLEU	B-MethodName
metric	O
reliable?	O
•	O
What	O
is	O
the	O
variance	O
of	O
the	O
BLEU	B-MethodName
score?	O
•	O
If	O
we	O
were	O
to	O
pick	O
another	O
random	O
set	O
of	O
500	O
sentences,	O
would	O
we	O
still	O
judge	O
S3	O
to	O
be	O
better	O
than	O
S2?	O
To	O
answer	O
these	O
questions,	O
we	O
divided	O
the	O
test	O
corpus	O
into	O
20	O
blocks	O
of	O
25	O
sentences	O
each,	O
and	O
computed	O
the	O
BLEU	B-MethodName
metric	O
on	O
these	O
blocks	O
individually.	O
We	O
thus	O
have	O
20	O
samples	O
of	O
the	O
BLEU	B-MethodName
metric	O
for	O
each	O
system.	O
We	O
computed	O
the	O
means,	O
variances,	O
and	O
paired	O
t-statistics	O
which	O
are	O
displayed	O
in	O
Table	O
2.	O
The	O
t-statistic	O
compares	O
each	O
system	O
with	O
its	O
left	O
neighbor	O
in	O
the	O
table.	O
For	O
example,	O
t	B-HyperparameterName
=	O
6	B-HyperparameterValue
for	O
the	O
pair	O
S1	O
and	O
S2.	O
Note	O
that	O
the	O
numbers	O
in	O
Table	O
1	O
are	O
the	O
BLEU	B-MethodName
metric	O
on	O
an	O
aggregate	O
of	O
500	O
sentences,	O
but	O
the	O
means	O
in	O
Table	O
2	O
are	O
averages	O
of	O
the	O
BLEU	B-MethodName
metric	O
on	O
aggregates	O
of	O
25	O
sentences.	O
As	O
expected,	O
these	O
two	O
sets	O
of	O
results	O
are	O
close	O
for	O
each	O
system	O
and	O
differ	O
only	O
by	O
small	O
finite	O
block	O
size	O
effects.	O
Since	O
a	O
paired	O
t-statistic	O
of	O
1.7	O
or	O
above	O
is	O
95%	O
significant,	O
the	O
differences	O
between	O
the	O
systems'	O
scores	O
are	O
statistically	O
very	O
significant.	O
The	O
reported	O
variance	O
on	O
25-sentence	O
blocks	O
serves	O
as	O
an	O
upper	O
bound	O
to	O
the	O
variance	O
of	O
sizeable	O
test	O
sets	O
like	O
the	O
500	O
sentence	O
corpus.	O
How	O
many	O
reference	O
translations	O
do	O
we	O
need?	O
We	O
simulated	O
a	O
single-reference	O
test	O
corpus	O
by	O
randomly	O
selecting	O
one	O
of	O
the	O
4	O
reference	O
translations	O
as	O
the	O
single	O
reference	O
for	O
each	O
of	O
the	O
40	O
stories.	O
In	O
this	O
way,	O
we	O
ensured	O
a	O
degree	O
of	O
stylistic	O
variation.	O
The	O
systems	O
maintain	O
the	O
same	O
rank	O
order	O
as	O
with	O
multiple	O
references.	O
This	O
outcome	O
suggests	O
that	O
we	O
may	O
use	O
a	O
big	O
test	O
corpus	O
with	O
a	O
single	O
reference	O
translation,	O
provided	O
that	O
the	O
translations	O
are	O
not	O
all	O
from	O
the	O
same	O
translator.	O

We	O
take	O
the	O
geometric	O
mean	O
of	O
the	O
test	O
corpus'	O
modified	O
precision	O
scores	O
and	O
then	O
multiply	O
the	O
result	O
by	O
an	O
exponential	O
brevity	O
penalty	O
factor.	O
Currently,	O
case	O
folding	O
is	O
the	O
only	O
text	O
normalization	O
performed	O
before	O
computing	O
the	O
precision.	O
We	O
first	O
compute	O
the	O
geometric	O
average	O
of	O
the	O
modified	O
n-gram	O
precisions,	O
p	O
n	O
,	O
using	O
n-grams	O
up	O
to	O
length	B-HyperparameterName
N	B-HyperparameterValue
and	O
positive	O
weights	O
w	O
n	O
summing	O
to	O
one.	O
Next,	O
let	O
c	O
be	O
the	O
length	O
of	O
the	O
candidate	O
translation	O
and	O
r	O
be	O
the	O
effective	O
reference	O
corpus	O
length.	O
We	O
compute	O
the	O
brevity	B-MetricName
penalty	I-MetricName
BP,	B-MetricName
BP	B-MetricName
=	O
1	O
if	O
c	O
>	O
r	O
e	O
(1−r/c)	O
if	O
c	O
≤	O
r	O
.	O
Then,	O
BLEU=	B-MethodName
BP	B-MetricName
•	O
exp	O
N	O
∑	O
n=1	O
w	O
n	O
log	O
p	O
n	O
.	O
The	O
ranking	O
behavior	O
is	O
more	O
immediately	O
apparent	O
in	O
the	O
log	O
domain,	O
log	O
BLEU	B-MethodName
=	O
min(1	O
−	O
r	O
c	O
,	O
0)	O
+	O
N	O
∑	O
n=1	O
w	O
n	O
log	O
p	O
n	O
.	O
In	O
our	O
baseline,	O
we	O
use	O
N	B-HyperparameterName
=	O
4	B-HyperparameterValue
and	O
uniform	O
weights	O
w	B-HyperparameterName
n	I-HyperparameterName
=	O
1/N.	B-HyperparameterValue

Candidate	O
translations	O
longer	O
than	O
their	O
references	O
are	O
already	O
penalized	O
by	O
the	O
modified	O
n-gram	B-MetricName
precision	I-MetricName
measure:	O
there	O
is	O
no	O
need	O
to	O
penalize	O
them	O
again.	O
Consequently,	O
we	O
introduce	O
a	O
multiplicative	O
brevity	O
penalty	O
factor.	O
With	O
this	O
brevity	O
penalty	O
in	O
place,	O
a	O
high-scoring	O
candidate	O
translation	O
must	O
now	O
match	O
the	O
reference	O
translations	O
in	O
length,	O
in	O
word	O
choice,	O
and	O
in	O
word	O
order.	O
Note	O
that	O
neither	O
this	O
brevity	O
penalty	O
nor	O
the	O
modified	O
n-gram	O
precision	O
length	O
effect	O
directly	O
considers	O
the	O
source	O
length;	O
instead,	O
they	O
consider	O
the	O
range	O
of	O
reference	O
translation	O
lengths	O
in	O
the	O
target	O
language.	O
We	O
wish	O
to	O
make	O
the	O
brevity	O
penalty	O
1.0	O
when	O
the	O
candidate's	O
length	O
is	O
the	O
same	O
as	O
any	O
reference	O
translation's	O
length.	O
For	O
example,	O
if	O
there	O
are	O
three	O
references	O
with	O
lengths	O
12,	O
15,	O
and	O
17	O
words	O
and	O
the	O
candidate	O
translation	O
is	O
a	O
terse	O
12	O
words,	O
we	O
want	O
the	O
brevity	O
penalty	O
to	O
be	O
1.	O
We	O
call	O
the	O
closest	O
reference	O
sentence	O
length	O
the	O
"best	O
match	O
length."	O
One	O
consideration	O
remains:	O
if	O
we	O
computed	O
the	O
brevity	O
penalty	O
sentence	O
by	O
sentence	O
and	O
averaged	O
the	O
penalties,	O
then	O
length	O
deviations	O
on	O
short	O
sentences	O
would	O
be	O
punished	O
harshly.	O
Instead,	O
we	O
compute	O
the	O
brevity	O
penalty	O
over	O
the	O
entire	O
corpus	O
to	O
allow	O
some	O
freedom	O
at	O
the	O
sentence	O
level.	O
We	O
first	O
compute	O
the	O
test	O
corpus'	O
effective	O
reference	O
length,	O
r,	O
by	O
summing	O
the	O
best	O
match	O
lengths	O
for	O
each	O
candidate	O
sentence	O
in	O
the	O
corpus.	O
We	O
choose	O
the	O
brevity	O
penalty	O
to	O
be	O
a	O
decaying	O
exponential	O
in	O
r/c,	O
where	O
c	O
is	O
the	O
total	O
length	O
of	O
the	O
candidate	O
translation	O
corpus.	O

Traditionally,	O
precision	B-MetricName
has	O
been	O
paired	O
with	O
recall	O
to	O
overcome	O
such	O
length-related	O
problems.	O
However,	O
BLEU	B-MethodName
considers	O
multiple	O
reference	O
translations,	O
each	O
of	O
which	O
may	O
use	O
a	O
different	O
word	O
choice	O
to	O
translate	O
the	O
same	O
source	O
word.	O
Furthermore,	O
a	O
good	O
candidate	O
translation	O
will	O
only	O
use	O
(recall)	O
one	O
of	O
these	O
possible	O
choices,	O
but	O
not	O
all.	O
Indeed,	O
recalling	O
all	O
choices	O
leads	O
to	O
a	O
bad	O
translation.	O
Here	O
is	O
an	O
example.	O
The	O
first	O
candidate	O
recalls	O
more	O
words	O
from	O
the	O
references,	O
but	O
is	O
obviously	O
a	O
poorer	O
translation	O
than	O
the	O
second	O
candidate.	O
Thus,	O
naïve	O
recall	O
computed	O
over	O
the	O
set	O
of	O
all	O
reference	O
words	O
is	O
not	O
a	O
good	O
measure.	O
Admittedly,	O
one	O
could	O
align	O
the	O
reference	O
translations	O
to	O
discover	O
synonymous	O
words	O
and	O
compute	O
recall	O
on	O
concepts	O
rather	O
than	O
words.	O
But,	O
given	O
that	O
reference	O
translations	O
vary	O
in	O
length	O
and	O
differ	O
in	O
word	O
order	O
and	O
syntax,	O
such	O
a	O
computation	O
is	O
complicated.	O

It	O
is	O
the	O
guiding	O
principle	O
which	O
guarantees	O
the	O
military	O
forces	O
always	O
being	O
under	O
the	O
command	O
of	O
the	O
Party.	O
Reference	O
3:	O
It	O
is	O
the	O
practical	O
guide	O
for	O
the	O
army	O
always	O
to	O
heed	O
the	O
directions	O
of	O
the	O
party.	O
Because	O
this	O
candidate	O
is	O
so	O
short	O
compared	O
to	O
the	O
proper	O
length,	O
one	O
expects	O
to	O
find	O
inflated	O
precisions:	O
the	O
modified	B-MetricName
unigram	I-MetricName
precision	I-MetricName
is	O
2/2,	B-MetricValue
and	O
the	O
modified	B-MetricName
bigram	I-MetricName
precision	I-MetricName
is	O
1/1.	B-MetricValue

A	O
candidate	O
translation	O
should	O
be	O
neither	O
too	O
long	O
nor	O
too	O
short,	O
and	O
an	O
evaluation	O
metric	O
should	O
enforce	O
this.	O
To	O
some	O
extent,	O
the	O
n-gram	B-MetricName
precision	I-MetricName
already	O
accomplishes	O
this.	O
N-gram	B-MetricName
precision	I-MetricName
penalizes	O
spurious	O
words	O
in	O
the	O
candidate	O
that	O
do	O
not	O
appear	O
in	O
any	O
of	O
the	O
reference	O
translations.	O
Additionally,	O
modified	B-MetricName
precision	I-MetricName
is	O
penalized	O
if	O
a	O
word	O
occurs	O
more	O
frequently	O
in	O
a	O
candidate	O
translation	O
than	O
its	O
maximum	O
reference	O
count.	O
This	O
rewards	O
using	O
a	O
word	O
as	O
many	O
times	O
as	O
warranted	O
and	O
penalizes	O
using	O
a	O
word	O
more	O
times	O
than	O
it	O
occurs	O
in	O
any	O
of	O
the	O
references.	O
However,	O
modified	B-MetricName
n-gram	I-MetricName
precision	I-MetricName
alone	O
fails	O
to	O
enforce	O
the	O
proper	O
translation	O
length,	O
as	O
is	O
illustrated	O
in	O
the	O
short,	O
absurd	O
example	O
below.	O
Example	O
3:	O
Candidate:	O
of	O
the	O
Reference	O
1:	O
It	O
is	O
a	O
guide	O
to	O
action	O
that	O
ensures	O
that	O
the	O
military	O
will	O
forever	O
heed	O
Party	O
commands.	O

How	O
should	O
we	O
combine	O
the	O
modified	O
precisions	O
for	O
the	O
various	O
n-gram	O
sizes?	O
A	O
weighted	O
linear	O
average	O
of	O
the	O
modified	O
precisions	O
resulted	O
in	O
encouraging	O
results	O
for	O
the	O
5	O
systems.	O
However,	O
as	O
can	O
be	O
seen	O
in	O
Figure	O
2,	O
the	O
modified	O
n-gram	O
precision	O
decays	O
roughly	O
exponentially	O
with	O
n:	O
the	O
modified	O
unigram	O
precision	O
is	O
much	O
larger	O
than	O
the	O
modified	O
bigram	O
precision	O
which	O
in	O
turn	O
is	O
much	O
bigger	O
than	O
the	O
modified	O
trigram	O
precision.	O
A	O
reasonable	O
averaging	O
scheme	O
must	O
take	O
this	O
exponential	O
decay	O
into	O
account;	O
a	O
weighted	O
average	O
of	O
the	O
logarithm	O
of	O
modified	O
precisions	O
satisifies	O
this	O
requirement.	O
BLEU	B-MethodName
uses	O
the	O
average	O
logarithm	O
with	O
uniform	O
weights,	O
which	O
is	O
equivalent	O
to	O
using	O
the	O
geometric	O
mean	O
of	O
the	O
modified	O
n-gram	B-MetricName
precisions.	I-MetricName
5	O
,6	O
Experimentally,	O
we	O
obtain	O
the	O
best	O
correlation	O
with	O
mono-	O
5	O
The	O
geometric	O
average	O
is	O
harsh	O
if	O
any	O
of	O
the	O
modified	O
precisions	O
vanish,	O
but	O
this	O
should	O
be	O
an	O
extremely	O
rare	O
event	O
in	O
test	O
corpora	O
of	O
reasonable	O
size	O
(for	O
N	B-HyperparameterName
max	I-HyperparameterName
≤	O
4).	B-HyperparameterValue
6	O
Using	O
the	O
geometric	O
average	O
also	O
yields	O
slightly	O
stronger	O
correlation	O
with	O
human	O
judgments	O
than	O
our	O
best	O
results	O
using	O
an	O
arithmetic	O
average.	O
lingual	O
human	O
judgments	O
using	O
a	O
maximum	O
n-gram	B-HyperparameterName
order	I-HyperparameterName
of	O
4,	B-HyperparameterValue
although	O
3-grams	B-HyperparameterValue
and	O
5-grams	B-HyperparameterValue
give	O
comparable	O
results.	O

To	O
verify	O
that	O
modified	O
n-gram	O
precision	O
distinguishes	O
between	O
very	O
good	O
translations	O
and	O
bad	O
translations,	O
we	O
computed	O
the	O
modified	O
precision	B-MetricName
numbers	O
on	O
the	O
output	O
of	O
a	O
(good)	O
human	O
translator	O
and	O
a	O
standard	O
(poor)	O
machine	B-TaskName
translation	I-TaskName
system	O
using	O
4	O
reference	O
translations	O
for	O
each	O
of	O
127	O
source	O
sentences.	O
The	O
average	O
precision	O
results	O
are	O
shown	O
in	O
Figure	O
1.	O
The	O
strong	O
signal	O
differentiating	O
human	O
(high	O
precision)	O
from	O
machine	O
(low	O
precision)	O
is	O
striking.	O
The	O
difference	O
becomes	O
stronger	O
as	O
we	O
go	O
from	O
unigram	O
precision	O
to	O
4-gram	O
precision.	O
It	O
appears	O
that	O
any	O
single	O
n-gram	O
precision	O
score	O
can	O
distinguish	O
between	O
a	O
good	O
translation	O
and	O
a	O
bad	O
translation.	O
To	O
be	O
useful,	O
however,	O
the	O
metric	O
must	O
also	O
reliably	O
distinguish	O
between	O
translations	O
that	O
do	O
not	O
differ	O
so	O
greatly	O
in	O
quality.	O
Furthermore,	O
it	O
must	O
distinguish	O
between	O
two	O
human	O
translations	O
of	O
differing	O
quality.	O
This	O
latter	O
requirement	O
ensures	O
the	O
continued	O
validity	O
of	O
the	O
metric	O
as	O
MT	B-TaskName
approaches	O
human	O
translation	O
quality.	O
To	O
this	O
end,	O
we	O
obtained	O
a	O
human	O
translation	O
by	O
someone	O
lacking	O
native	O
proficiency	O
in	O
both	O
the	O
source	O
(Chinese)	O
and	O
the	O
target	O
language	O
(English).	O
For	O
comparison,	O
we	O
acquired	O
human	O
translations	O
of	O
the	O
same	O
documents	O
by	O
a	O
native	O
English	O
speaker.	O
We	O
also	O
obtained	O
machine	O
translations	O
by	O
three	O
commercial	O
systems.	O
These	O
five	O
"systems"	O
-two	O
humans	O
and	O
three	O
machines	O
-are	O
scored	O
against	O
two	O
reference	O
professional	O
human	O
translations.	O
The	O
average	O
modified	O
n-gram	B-MetricName
precision	I-MetricName
results	O
are	O
shown	O
in	O
Figure	O
2.	O
Each	O
of	O
these	O
n-gram	O
statistics	O
implies	O
the	O
same	O
Figure	O
2:	O
Machine	O
and	O
Human	O
Translations	O
ranking:	O
H2	O
(Human-2)	O
is	O
better	O
than	O
H1	O
(Human-1),	O
and	O
there	O
is	O
a	O
big	O
drop	O
in	O
quality	O
between	O
H1	O
and	O
S3	O
(Machine/System-3).	O
S3	O
appears	O
better	O
than	O
S2	O
which	O
in	O
turn	O
appears	O
better	O
than	O
S1.	O
Remarkably,	O
this	O
is	O
the	O
same	O
rank	O
order	O
assigned	O
to	O
these	O
"systems"	O
by	O
human	O
judges,	O
as	O
we	O
discuss	O
later.	O
While	O
there	O
seems	O
to	O
be	O
ample	O
signal	O
in	O
any	O
single	O
n-gram	B-MetricName
precision,	I-MetricName
it	O
is	O
more	O
robust	O
to	O
combine	O
all	O
these	O
signals	O
into	O
a	O
single	O
number	O
metric.	O

How	O
do	O
we	O
compute	O
modified	O
n-gram	O
precision	O
on	O
a	O
multi-sentence	O
test	O
set?	O
Although	O
one	O
typically	O
evaluates	O
MT	B-TaskName
systems	O
on	O
a	O
corpus	O
of	O
entire	O
documents,	O
our	O
basic	O
unit	O
of	O
evaluation	O
is	O
the	O
sentence.	O
A	O
source	O
sentence	O
may	O
translate	O
to	O
many	O
target	O
sentences,	O
in	O
which	O
case	O
we	O
abuse	O
terminology	O
and	O
refer	O
to	O
the	O
corresponding	O
target	O
sentences	O
as	O
a	O
"sentence."	O
We	O
first	O
compute	O
the	O
n-gram	O
matches	O
sentence	O
by	O
sentence.	O
Next,	O
we	O
add	O
the	O
clipped	O
n-gram	O
counts	O
for	O
all	O
the	O
candidate	O
sentences	O
and	O
divide	O
by	O
the	O
number	O
of	O
candidate	O
n-grams	O
in	O
the	O
test	O
corpus	O
to	O
compute	O
a	O
modified	O
precision	O
score,	O
p	O
n	O
,	O
for	O
the	O
entire	O
test	O
corpus.	O
p	O
n	O
=	O
∑	O
C	O
∈{Candidates}	O
∑	O
n-gram	O
∈	O
C	O
Count	O
clip	O
(n-gram)	O
∑	O
C	O
∈{Candidates}	O
∑	O
n-gram	O
∈	O
C	O
Count(n-gram	O
)	O
.	O
4	O
BLEU	B-MethodName
only	O
needs	O
to	O
match	O
human	B-MethodName
judgment	I-MethodName
when	O
averaged	O
over	O
a	O
test	O
corpus;	O
scores	O
on	O
individual	O
sentences	O
will	O
often	O
vary	O
from	O
human	B-MethodName
judgments.	I-MethodName
For	O
example,	O
a	O
system	O
which	O
produces	O
the	O
fluent	O
phrase	O
"East	O
Asian	O
economy"	O
is	O
penalized	O
heavily	O
on	O
the	O
longer	O
n-gram	O
precisions	O
if	O
all	O
the	O
references	O
happen	O
to	O
read	O
"economy	O
of	O
East	O
Asia."	O
The	O
key	O
to	O
BLEU's	B-MethodName
success	O
is	O
that	O
all	O
systems	O
are	O
treated	O
similarly	O
and	O
multiple	O
human	O
translators	O
with	O
different	O
styles	O
are	O
used,	O
so	O
this	O
effect	O
cancels	O
out	O
in	O
comparisons	O
between	O
systems.	O

The	O
cornerstone	O
of	O
our	O
metric	O
is	O
the	O
familiar	O
precision	B-MetricName
measure.	O
To	O
compute	O
precision,	B-MetricName
one	O
simply	O
counts	O
up	O
the	O
number	O
of	O
candidate	O
translation	O
words	O
(unigrams)	O
which	O
occur	O
in	O
any	O
reference	O
translation	O
and	O
then	O
divides	O
by	O
the	O
total	O
number	O
of	O
words	O
in	O
the	O
candidate	O
translation.	O
Unfortunately,	O
MT	B-TaskName
systems	O
can	O
overgenerate	O
"reasonable"	O
words,	O
resulting	O
in	O
improbable,	O
but	O
high-precision,	O
translations	O
like	O
that	O
of	O
example	O
2	O
below.	O
Intuitively	O
the	O
problem	O
is	O
clear:	O
a	O
reference	O
word	O
should	O
be	O
considered	O
exhausted	O
after	O
a	O
matching	O
candidate	O
word	O
is	O
identified.	O
We	O
formalize	O
this	O
intuition	O
as	O
the	O
modified	O
unigram	O
precision.	O
To	O
compute	O
this,	O
one	O
first	O
counts	O
the	O
maximum	O
number	O
of	O
times	O
a	O
word	O
occurs	O
in	O
any	O
single	O
reference	O
translation.	O
Next,	O
one	O
clips	O
the	O
total	O
count	O
of	O
each	O
candidate	O
word	O
by	O
its	O
maximum	O
reference	O
count,	O
2	O
adds	O
these	O
clipped	O
counts	O
up,	O
and	O
divides	O
by	O
the	O
total	O
(unclipped)	O
number	O
of	O
candidate	O
words.	O
Example	O
2.	O
Candidate:	O
the	O
the	O
the	O
the	O
the	O
the	O
the.	O
Reference	O
1:	O
The	O
cat	O
is	O
on	O
the	O
mat.	O
Reference	O
2:	O
There	O
is	O
a	O
cat	O
on	O
the	O
mat.	O
Modified	B-MetricName
Unigram	I-MetricName
Precision	I-MetricName
=	O
2/7.	B-MetricValue
3	O
In	O
Example	O
1,	O
Candidate	O
1	O
achieves	O
a	O
modified	B-MetricName
unigram	I-MetricName
precision	I-MetricName
of	O
17/18;	B-MetricValue
whereas	O
Candidate	O
2	O
achieves	O
a	O
modified	B-MetricName
unigram	I-MetricName
precision	I-MetricName
of	O
8/14.	B-MetricValue
Similarly,	O
the	O
modified	B-MetricName
unigram	I-MetricName
precision	I-MetricName
in	O
Example	O
2	O
is	O
2/7,	B-MetricValue
even	O
though	O
its	O
standard	O
unigram	O
precision	O
is	O
7/7.	B-MetricValue
Modified	B-MetricName
n-gram	I-MetricName
precision	I-MetricName
is	O
computed	O
similarly	O
for	O
any	O
n:	O
all	O
candidate	O
n-gram	O
counts	O
and	O
their	O
corresponding	O
maximum	O
reference	O
counts	O
are	O
collected.	O
The	O
candidate	O
counts	O
are	O
clipped	O
by	O
their	O
corresponding	O
reference	O
maximum	O
value,	O
summed,	O
and	O
divided	O
by	O
the	O
total	O
number	O
of	O
candidate	O
ngrams.	O
In	O
Example	O
1,	O
Candidate	O
1	O
achieves	O
a	O
modified	B-MetricName
bigram	I-MetricName
precision	I-MetricName
of	O
10/17,	B-MetricValue
whereas	O
the	O
lower	O
quality	O
Candidate	O
2	O
achieves	O
a	O
modified	B-MetricName
bigram	I-MetricName
precision	I-MetricName
of	O
1/13.	B-MetricValue
In	O
Example	O
2,	O
the	O
(implausible)	O
candidate	O
achieves	O
a	O
modified	B-MetricName
bigram	I-MetricName
precision	I-MetricName
of	O
0.	B-MetricValue
This	O
sort	O
of	O
modified	O
n-gram	O
precision	O
scoring	O
captures	O
two	O
aspects	O
of	O
translation:	O
adequacy	O
and	O
fluency.	O
A	O
translation	O
using	O
the	O
same	O
words	O
(1-grams)	O
as	O
in	O
the	O
references	O
tends	O
to	O
satisfy	O
adequacy.	O
The	O
longer	O
n-gram	O
matches	O
account	O
for	O
fluency.	O
4	O

It	O
is	O
the	O
guiding	O
principle	O
which	O
guarantees	O
the	O
military	O
forces	O
always	O
being	O
under	O
the	O
command	O
of	O
the	O
Party.	O
Reference	O
3:	O
It	O
is	O
the	O
practical	O
guide	O
for	O
the	O
army	O
always	O
to	O
heed	O
the	O
directions	O
of	O
the	O
party.	O
It	O
is	O
clear	O
that	O
the	O
good	O
translation,	O
Candidate	O
1,	O
shares	O
many	O
words	O
and	O
phrases	O
with	O
these	O
three	O
reference	O
translations,	O
while	O
Candidate	O
2	O
does	O
not.	O
We	O
will	O
shortly	O
quantify	O
this	O
notion	O
of	O
sharing	O
in	O
Section	O
2.1.	O
But	O
first	O
observe	O
that	O
Candidate	O
1	O
shares	O
"It	O
is	O
a	O
guide	O
to	O
action"	O
with	O
Reference	O
1,	O
"which"	O
with	O
Reference	O
2,	O
"ensures	O
that	O
the	O
military"	O
with	O
Reference	O
1,	O
"always"	O
with	O
References	O
2	O
and	O
3,	O
"commands"	O
with	O
Reference	O
1,	O
and	O
finally	O
"of	O
the	O
party"	O
with	O
Reference	O
2	O
(all	O
ignoring	O
capitalization).	O
In	O
contrast,	O
Candidate	O
2	O
exhibits	O
far	O
fewer	O
matches,	O
and	O
their	O
extent	O
is	O
less.	O
It	O
is	O
clear	O
that	O
a	O
program	O
can	O
rank	O
Candidate	O
1	O
higher	O
than	O
Candidate	O
2	O
simply	O
by	O
comparing	O
ngram	O
matches	O
between	O
each	O
candidate	O
translation	O
and	O
the	O
reference	O
translations.	O
Experiments	O
over	O
large	O
collections	O
of	O
translations	O
presented	O
in	O
Section	O
5	O
show	O
that	O
this	O
ranking	O
ability	O
is	O
a	O
general	O
phenomenon,	O
and	O
not	O
an	O
artifact	O
of	O
a	O
few	O
toy	O
examples.	O
The	O
primary	O
programming	O
task	O
for	O
a	O
BLEU	B-MethodName
implementor	O
is	O
to	O
compare	O
n-grams	O
of	O
the	O
candidate	O
with	O
the	O
n-grams	O
of	O
the	O
reference	O
translation	O
and	O
count	O
the	O
number	O
of	O
matches.	O
These	O
matches	O
are	O
positionindependent.	O
The	O
more	O
the	O
matches,	O
the	O
better	O
the	O
candidate	O
translation	O
is.	O
For	O
simplicity,	O
we	O
first	O
focus	O
on	O
computing	O
unigram	O
matches.	O

It	O
is	O
to	O
insure	O
the	O
troops	O
forever	O
hearing	O
the	O
activity	O
guidebook	O
that	O
party	O
direct.	O
Although	O
they	O
appear	O
to	O
be	O
on	O
the	O
same	O
subject,	O
they	O
differ	O
markedly	O
in	O
quality.	O
For	O
comparison,	O
we	O
provide	O
three	O
reference	O
human	O
translations	O
of	O
the	O
same	O
sentence	O
below.	O
Reference	O
1:	O
It	O
is	O
a	O
guide	O
to	O
action	O
that	O
ensures	O
that	O
the	O
military	O
will	O
forever	O
heed	O
Party	O
commands.	O

Typically,	O
there	O
are	O
many	O
"perfect"	O
translations	O
of	O
a	O
given	O
source	O
sentence.	O
These	O
translations	O
may	O
vary	O
in	O
word	O
choice	O
or	O
in	O
word	O
order	O
even	O
when	O
they	O
use	O
the	O
same	O
words.	O
And	O
yet	O
humans	O
can	O
clearly	O
distinguish	O
a	O
good	O
translation	O
from	O
a	O
bad	O
one.	O
For	O
example,	O
consider	O
these	O
two	O
candidate	O
translations	O
of	O
a	O
Chinese	O
source	O
sentence:	O
Example	O
1.	O
Candidate	O
1:	O
It	O
is	O
a	O
guide	O
to	O
action	O
which	O
ensures	O
that	O
the	O
military	O
always	O
obeys	O
the	O
commands	O
of	O
the	O
party.	O

How	O
does	O
one	O
measure	O
translation	O
performance?	O
The	O
closer	O
a	O
machine	O
translation	O
is	O
to	O
a	O
professional	O
human	O
translation,	O
the	O
better	O
it	O
is.	O
This	O
is	O
the	O
central	O
idea	O
behind	O
our	O
proposal.	O
To	O
judge	O
the	O
quality	O
of	O
a	O
machine	O
translation,	O
one	O
measures	O
its	O
closeness	O
to	O
one	O
or	O
more	O
reference	O
human	O
translations	O
according	O
to	O
a	O
numerical	O
metric.	O
Thus,	O
our	O
MT	B-TaskName
evaluation	O
system	O
requires	O
two	O
ingredients:	O
1.	O
a	O
numerical	O
"translation	O
closeness"	O
metric	O
2.	O
a	O
corpus	O
of	O
good	O
quality	O
human	O
reference	O
translations	O
We	O
fashion	O
our	O
closeness	O
metric	O
after	O
the	O
highly	O
successful	O
word	O
error	O
rate	O
metric	O
used	O
by	O
the	O
speech	O
recognition	O
community,	O
appropriately	O
modified	O
for	O
multiple	O
reference	O
translations	O
and	O
allowing	O
for	O
legitimate	O
differences	O
in	O
word	O
choice	O
and	O
word	O
order.	O
The	O
main	O
idea	O
is	O
to	O
use	O
a	O
weighted	O
average	O
of	O
variable	O
length	O
phrase	O
matches	O
against	O
the	O
reference	O
translations.	O
This	O
view	O
gives	O
rise	O
to	O
a	O
family	O
of	O
metrics	O
using	O
various	O
weighting	O
schemes.	O
We	O
have	O
selected	O
a	O
promising	O
baseline	O
metric	O
from	O
this	O
family.	O
In	O
Section	O
2,	O
we	O
describe	O
the	O
baseline	O
metric	O
in	O
detail.	O
In	O
Section	O
3,	O
we	O
evaluate	O
the	O
performance	O
of	O
BLEU.	B-MethodName
In	O
Section	O
4,	O
we	O
describe	O
a	B-MethodName
human	I-MethodName
evaluation	I-MethodName
experiment.	O
In	O
Section	O
5,	O
we	O
compare	O
our	O
baseline	O
metric	O
performance	O
with	O
human	O
evaluations.	O

Human	B-MethodName
evaluations	I-MethodName
of	O
machine	B-TaskName
translation	I-TaskName
(MT)	B-TaskName
weigh	O
many	O
aspects	O
of	O
translation,	O
including	O
adequacy,	O
fidelity	O
,	O
and	O
fluency	O
of	O
the	O
translation	O
(Hovy,	O
1999;White	O
and	O
O'Connell,	O
1994).	O
A	O
comprehensive	O
catalog	O
of	O
MT	B-TaskName
evaluation	O
techniques	O
and	O
their	O
rich	O
literature	O
is	O
given	O
by	O
Reeder	O
(2001).	O
For	O
the	O
most	O
part,	O
these	O
various	O
human	B-MethodName
evaluation	I-MethodName
approaches	O
are	O
quite	O
expensive	O
(Hovy,	O
1999).	O
Moreover,	O
they	O
can	O
take	O
weeks	O
or	O
months	O
to	O
finish.	O
This	O
is	O
a	O
big	O
problem	O
because	O
developers	O
of	O
machine	B-TaskName
translation	I-TaskName
systems	O
need	O
to	O
monitor	O
the	O
effect	O
of	O
daily	O
changes	O
to	O
their	O
systems	O
in	O
order	O
to	O
weed	O
out	O
bad	O
ideas	O
from	O
good	O
ideas.	O
We	O
believe	O
that	O
MT	B-TaskName
progress	O
stems	O
from	O
evaluation	O
and	O
that	O
there	O
is	O
a	O
logjam	O
of	O
fruitful	O
research	O
ideas	O
waiting	O
to	O
be	O
released	O
from	O
the	O
evaluation	O
bottleneck.	O
Developers	O
would	O
benefit	O
from	O
an	O
inexpensive	O
automatic	O
evaluation	O
that	O
is	O
quick,	O
language-independent,	O
and	O
correlates	O
highly	O
with	O
human	B-MethodName
evaluation.	I-MethodName
We	O
propose	O
such	O
an	O
evaluation	O
method	O
in	O
this	O
paper.	O

Human	B-MethodName
evaluations	I-MethodName
of	O
machine	O
translation	O
are	O
extensive	O
but	O
expensive.	O
Human	B-MethodName
evaluations	I-MethodName
can	O
take	O
months	O
to	O
finish	O
and	O
involve	O
human	O
labor	O
that	O
can	O
not	O
be	O
reused.	O
We	O
propose	O
a	O
method	O
of	O
automatic	O
machine	O
translation	O
evaluation	O
that	O
is	O
quick,	O
inexpensive,	O
and	O
language-independent,	O
that	O
correlates	O
highly	O
with	O
human	B-MethodName
evaluation,	I-MethodName
and	O
that	O
has	O
little	O
marginal	O
cost	O
per	O
run.	O
We	O
present	O
this	O
method	O
as	O
an	O
automated	O
understudy	O
to	O
skilled	O
human	O
judges	O
which	O
substitutes	O
for	O
them	O
when	O
there	O
is	O
need	O
for	O
quick	O
or	O
frequent	O
evaluations.	O
1	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
feedback.	O
This	O
work	O
is	O
funded	O
in	O
part	O
by	O
the	O
NSF	O
award	O
number	O
IIS-1844740	O
to	O
Anna	O
Rumshisky.	O

In	O
a	O
little	O
over	O
a	O
year,	O
BERT	B-MethodName
has	O
become	O
a	O
ubiquitous	O
baseline	O
in	O
NLP	O
experiments	O
and	O
inspired	O
numerous	O
studies	O
analyzing	O
the	O
model	O
and	O
proposing	O
various	O
improvements.	O
The	O
stream	O
of	O
papers	O
seems	O
to	O
be	O
accelerating	O
rather	O
than	O
slowing	O
down,	O
and	O
we	O
hope	O
that	O
this	O
survey	O
helps	O
the	O
community	O
to	O
focus	O
on	O
the	O
biggest	O
unresolved	O
questions.	O

BERTology	O
has	O
clearly	O
come	O
a	O
long	O
way,	O
but	O
it	O
is	O
fair	O
to	O
say	O
we	O
still	O
have	O
more	O
questions	O
than	O
answers	O
about	O
how	O
BERT	B-MethodName
works.	O
In	O
this	O
section,	O
we	O
list	O
what	O
we	O
believe	O
to	O
be	O
the	O
most	O
promising	O
directions	O
for	O
further	O
research.	O
Benchmarks	O
that	O
require	O
verbal	O
reasoning.	O
While	O
BERT	B-MethodName
enabled	O
breakthroughs	O
on	O
many	O
NLP	O
benchmarks,	O
a	O
growing	O
list	O
of	O
analysis	O
papers	O
are	O
showing	O
that	O
its	O
language	O
skills	O
are	O
not	O
as	O
impressive	O
as	O
it	O
seems.	O
In	O
particular,	O
it	O
was	O
shown	O
to	O
rely	O
on	O
shallow	O
heuristics	O
in	O
natural	O
language	O
inference	O
Zellers	O
et	O
al.,	O
2019;Jin	O
et	O
al.,	O
2020),	O
reading	O
comprehension	O
Sugawara	O
et	O
al.,	O
2020;Si	O
et	O
al.,	O
2019b;,	O
argument	O
reasoning	O
comprehension	O
(Niven	O
and	O
Kao,	O
2019),	O
and	O
text	O
classification	O
(Jin	O
et	O
al.,	O
2020).	O
Such	O
heuristics	O
can	O
even	O
be	O
used	O
to	O
reconstruct	O
a	O
non-publiclyavailable	O
model	O
(Krishna	O
et	O
al.,	O
2020).	O
As	O
with	O
any	O
optimization	O
method,	O
if	O
there	O
is	O
a	O
shortcut	O
in	O
the	O
data,	O
we	O
have	O
no	O
reason	O
to	O
expect	O
BERT	B-MethodName
to	O
not	O
learn	O
it.	O
But	O
harder	O
datasets	O
that	O
cannot	O
be	O
resolved	O
with	O
shallow	O
heuristics	O
are	O
unlikely	O
to	O
emerge	O
if	O
their	O
development	O
is	O
not	O
as	O
valued	O
as	O
modeling	O
work.	O
Benchmarks	O
for	O
the	O
full	O
range	O
of	O
linguistic	O
competence.	O
While	O
the	O
language	O
models	O
seem	O
to	O
acquire	O
a	O
great	O
deal	O
of	O
knowledge	O
about	O
language,	O
we	O
do	O
not	O
currently	O
have	O
comprehensive	O
stress	O
tests	O
for	O
different	O
aspects	O
of	O
linguistic	O
knowledge.	O
A	O
step	O
in	O
this	O
direction	O
is	O
the	O
"Checklist"	O
behavioral	O
testing	O
(Ribeiro	O
et	O
al.,	O
2020),	O
the	O
best	O
paper	O
at	O
ACL	O
2020.	O
Ideally,	O
such	O
tests	O
would	O
measure	O
not	O
only	O
errors,	O
but	O
also	O
sensitivity	O
(Ettinger,	O
2019).	O
Developing	O
methods	O
to	O
"teach"	O
reasoning.	O
While	O
large	O
pre-trained	O
models	O
have	O
a	O
lot	O
of	O
knowledge,	O
they	O
often	O
fail	O
if	O
any	O
reasoning	O
needs	O
to	O
be	O
performed	O
on	O
top	O
of	O
the	O
facts	O
they	O
possess	O
(Talmor	O
et	O
al.,	O
2019,	O
see	O
also	O
subsection	O
3.3).	O
For	O
instance,	O
Richardson	O
et	O
al.	O
(2020)	O
propose	O
a	O
method	O
to	O
"teach"	O
BERT	B-MethodName
quantification,	O
conditionals,	O
comparatives,	O
and	O
boolean	O
coordination.	O
Learning	O
what	O
happens	O
at	O
inference	O
time.	O
Most	O
BERT	B-MethodName
analysis	O
papers	O
focus	O
on	O
different	O
probes	O
of	O
the	O
model,	O
with	O
the	O
goal	O
to	O
find	O
what	O
the	O
language	O
model	O
"knows".	O
However,	O
probing	O
studies	O
have	O
limitations	O
(subsection	O
3.4),	O
and	O
to	O
this	O
point,	O
far	O
fewer	O
papers	O
have	O
focused	O
on	O
discovering	O
what	O
knowledge	O
actually	O
gets	O
used.	O
Several	O
promising	O
directions	O
are	O
the	O
"amnesic	O
probing"	O
(Elazar	O
et	O
al.,	O
2020),	O
identifying	O
features	O
important	O
for	O
prediction	O
for	O
a	O
given	O
task	O
(Arkhangelskaia	O
and	O
Dutta,	O
2019),	O
and	O
pruning	O
the	O
model	O
to	O
remove	O
the	O
nonimportant	O
components	O
(Voita	O
et	O
al.,	O
2019b;Michel	O
et	O
al.,	O
2019;Prasanna	O
et	O
al.,	O
2020).	O

There	O
is	O
a	O
nascent	O
discussion	O
around	O
pruning	O
as	O
a	O
model	O
analysis	O
technique.	O
The	O
basic	O
idea	O
is	O
that	O
a	O
compressed	O
model	O
a	O
priori	O
consists	O
of	O
elements	O
that	O
are	O
useful	O
for	O
prediction;	O
therefore	O
by	O
finding	O
out	O
what	O
they	O
do	O
we	O
may	O
find	O
out	O
what	O
the	O
whole	O
network	O
does.	O
For	O
instance,	O
BERT	B-MethodName
has	O
heads	O
that	O
seem	O
to	O
encode	O
frame-semantic	O
relations,	O
but	O
disabling	O
them	O
might	O
not	O
hurt	O
downstream	O
task	O
performance	O
Kovaleva	O
et	O
al.	O
(2019);	O
this	O
suggests	O
that	O
this	O
knowledge	O
is	O
not	O
actually	O
used.	O
For	O
the	O
base	O
Transformer,	O
Voita	O
et	O
al.	O
(2019b)	O
identify	O
the	O
functions	O
of	O
self-attention	O
heads	O
and	O
then	O
check	O
which	O
of	O
them	O
survive	O
the	O
pruning,	O
finding	O
that	O
the	O
syntactic	O
and	O
positional	O
heads	O
are	O
the	O
last	O
ones	O
to	O
go.	O
For	O
BERT,	B-MethodName
Prasanna	O
et	O
al.	O
(2020)	O
go	O
in	O
the	O
opposite	O
direction:	O
pruning	O
on	O
the	O
basis	O
of	O
importance	O
scores,	O
and	O
interpreting	O
the	O
remaining	O
"good"	O
subnetwork.	O
With	O
respect	O
to	O
self-attention	O
heads	O
specifically,	O
it	O
does	O
not	O
seem	O
to	O
be	O
the	O
case	O
that	O
only	O
the	O
heads	O
that	O
potentially	O
encode	O
nontrivial	O
linguistic	O
patterns	O
survive	O
the	O
pruning.	O
The	O
models	O
and	O
methodology	O
in	O
these	O
studies	O
differ,	O
so	O
the	O
evidence	O
is	O
inconclusive.	O
In	O
particular,	O
Voita	O
et	O
al.	O
(2019b)	O
find	O
that	O
before	O
pruning	O
the	O
majority	O
of	O
heads	O
are	O
syntactic,	O
and	O
Prasanna	O
et	O
al.	O
(2020)	O
-that	O
the	O
majority	O
of	O
heads	O
do	O
not	O
have	O
potentially	O
non-trivial	O
attention	O
patterns.	O
An	O
important	O
limitation	O
of	O
the	O
current	O
head	O
and	O
layer	O
ablation	O
studies	O
(Michel	O
et	O
al.,	O
2019;Kovaleva	O
et	O
al.,	O
2019)	O
is	O
that	O
they	O
inherently	O
assume	O
that	O
certain	O
knowledge	O
is	O
contained	O
in	O
heads/layers.	O
However,	O
there	O
is	O
evidence	O
of	O
more	O
diffuse	O
representations	O
spread	O
across	O
the	O
full	O
network,	O
such	O
as	O
the	O
gradual	O
increase	O
in	O
accuracy	O
on	O
difficult	O
semantic	O
parsing	O
tasks	O
(Tenney	O
et	O
al.,	O
2019a)	O
or	O
the	O
absence	O
of	O
heads	O
that	O
would	O
perform	O
parsing	O
"in	O
general"	O
(Clark	O
et	O
al.,	O
2019;Htut	O
et	O
al.,	O
2019).	O
If	O
so,	O
ablating	O
individual	O
components	O
harms	O
the	O
weightsharing	O
mechanism.	O
Conclusions	O
from	O
component	O
ablations	O
are	O
also	O
problematic	O
if	O
the	O
same	O
information	O
is	O
duplicated	O
elsewhere	O
in	O
the	O
network.	O

Given	O
the	O
above	O
evidence	O
of	O
overparameterization,	O
it	O
does	O
not	O
come	O
as	O
a	O
surprise	O
that	O
BERT	B-MethodName
can	O
be	O
efficiently	O
compressed	O
with	O
minimal	O
accuracy	O
loss,	O
which	O
would	O
be	O
highly	O
desirable	O
for	O
real-world	O
applications.	O
Such	O
efforts	O
to	O
date	O
are	O
summarized	O
in	O
Table	O
1.	O
The	O
main	O
approaches	O
are	O
knowledge	O
distillation,	O
quantization,	O
and	O
pruning.	O
The	O
studies	O
in	O
the	O
knowledge	O
distillation	O
framework	O
(Hinton	O
et	O
al.,	O
2014)	O
use	O
a	O
smaller	O
student-network	O
trained	O
to	O
mimic	O
the	O
behavior	O
of	O
a	O
larger	O
teacher-network.	O
For	O
BERT,	B-MethodName
this	O
has	O
been	O
achieved	O
through	O
experiments	O
with	O
loss	O
functions	O
(Sanh	O
et	O
al.,	O
2019b;Jiao	O
et	O
al.,	O
2019),	O
mimicking	O
the	O
activation	O
patterns	O
of	O
individual	O
portions	O
of	O
the	O
teacher	O
network	O
(Sun	O
et	O
al.,	O
2019a),	O
and	O
knowledge	O
transfer	O
at	O
the	O
pre-training	O
(Turc	O
et	O
al.,	O
2019;Jiao	O
et	O
al.,	O
2019;Sun	O
et	O
al.,	O
2020)	O
or	O
fine-tuning	O
stage	O
(Jiao	O
et	O
al.,	O
2019).	O
McCarley	O
et	O
al.	O
(2020)	O
suggest	O
that	O
distillation	O
has	O
so	O
far	O
worked	O
better	O
for	O
GLUE	B-DatasetName
than	O
for	O
reading	O
comprehension,	O
and	O
report	O
good	O
results	O
for	O
QA	B-TaskName
from	O
a	O
combination	O
of	O
structured	O
pruning	O
and	O
task-specific	O
distillation.	O
Quantization	O
decreases	O
BERT's	B-MethodName
memory	O
footprint	O
through	O
lowering	O
the	O
precision	O
of	O
its	O
weights	O
(Shen	O
et	O
al.,	O
2019;Zafrir	O
et	O
al.,	O
2019	O
this	O
strategy	O
often	O
requires	O
compatible	O
hardware.	O
As	O
discussed	O
in	O
section	O
6,	O
individual	O
selfattention	O
heads	O
and	O
BERT	B-MethodName
layers	O
can	O
be	O
disabled	O
without	O
significant	O
drop	O
in	O
performance	O
(Michel	O
et	O
al.,	O
2019;Kovaleva	O
et	O
al.,	O
2019;Baan	O
et	O
al.,	O
2019).	O
Pruning	O
is	O
a	O
compression	O
technique	O
that	O
takes	O
advantage	O
of	O
that	O
fact,	O
typically	O
reducing	O
the	O
amount	O
of	O
computation	O
via	O
zeroing	O
out	O
of	O
certain	O
parts	O
of	O
the	O
large	O
model.	O
In	O
structured	O
pruning,	O
architecture	O
blocks	O
are	O
dropped,	O
as	O
in	O
LayerDrop	O
.	O
In	O
unstructured,	O
the	O
weights	O
in	O
the	O
entire	O
model	O
are	O
pruned	O
irrespective	O
of	O
their	O
location,	O
as	O
in	O
magnitude	O
pruning	O
or	O
movement	O
pruning	O
(Sanh	O
et	O
al.,	O
2020).	O
Prasanna	O
et	O
al.	O
(2020)	O
and	O
explore	O
BERT	B-MethodName
from	O
the	O
perspective	O
of	O
the	O
lottery	O
ticket	O
hypothesis	O
(Frankle	O
and	O
Carbin,	O
2019),	O
looking	O
specifically	O
at	O
the	O
"winning"	O
subnetworks	O
in	O
pre-trained	O
BERT.	B-MethodName
They	O
independently	O
find	O
that	O
such	O
subnetworks	O
do	O
exist,	O
and	O
that	O
transferability	O
between	O
subnetworks	O
for	O
different	O
tasks	O
varies.	O
If	O
the	O
ultimate	O
goal	O
of	O
training	O
BERT	B-MethodName
is	O
compression,	O
recommend	O
training	O
larger	O
models	O
and	O
compressing	O
them	O
heavily	O
rather	O
than	O
compressing	O
smaller	O
models	O
lightly.	O
Other	O
techniques	O
include	O
decomposing	O
BERT's	B-MethodName
embedding	O
matrix	O
into	O
smaller	O
matrices	O
(Lan	O
et	O
al.,	O
2020a),	O
progressive	O
module	O
replacing	O
and	O
dynamic	O
elimination	O
of	O
intermediate	O
encoder	O
outputs	O
(Goyal	O
et	O
al.,	O
2020).	O
See	O
Ganesh	O
et	O
al.	O
(2020)	O
for	O
a	O
more	O
detailed	O
discussion	O
of	O
compression	O
methods.	O

Transformer-based	O
models	O
keep	O
growing	O
by	O
orders	O
of	O
magnitude:	O
the	O
110M	O
parameters	O
of	O
base	B-MethodName
BERT	I-MethodName
are	O
now	O
dwarfed	O
by	O
17B	O
parameters	O
of	O
Turing-NLG	O
(Microsoft,	O
2020),	O
which	O
is	O
dwarfed	O
by	O
175B	O
of	O
GPT-3	O
(Brown	O
et	O
al.,	O
2020).	O
This	O
trend	O
raises	O
concerns	O
about	O
computational	O
complexity	O
of	O
self-attention	O
,	O
environmental	O
issues	O
(Strubell	O
et	O
al.,	O
2019;Schwartz	O
et	O
al.,	O
2019),	O
fair	O
comparison	O
of	O
architectures	O
(Aßenmacher	O
and	O
Heumann,	O
2020),	O
and	O
reproducibility.	O
Human	O
language	O
is	O
incredibly	O
complex,	O
and	O
would	O
perhaps	O
take	O
many	O
more	O
parameters	O
to	O
describe	O
fully,	O
but	O
the	O
current	O
models	O
do	O
not	O
make	O
good	O
use	O
of	O
the	O
parameters	O
they	O
already	O
have.	O
Voita	O
et	O
al.	O
(2019b)	O
showed	O
that	O
all	O
but	O
a	O
few	O
Transformer	O
heads	O
could	O
be	O
pruned	O
without	O
significant	O
losses	O
in	O
performance.	O
For	O
BERT,	B-MethodName
Clark	O
et	O
al.	O
(2019)	O
observe	O
that	O
most	O
heads	O
in	O
the	O
same	O
layer	O
show	O
similar	O
self-attention	O
patterns	O
(perhaps	O
related	O
to	O
the	O
fact	O
that	O
the	O
output	O
of	O
all	O
self-attention	O
heads	O
in	O
a	O
layer	O
is	O
passed	O
through	O
the	O
same	O
MLP),	O
which	O
explains	O
why	O
Michel	O
et	O
al.	O
(2019)	O
were	O
able	O
to	O
reduce	O
most	O
layers	O
to	O
a	O
single	O
head.	O
Depending	O
on	O
the	O
task,	O
some	O
BERT	B-MethodName
heads/layers	O
are	O
not	O
only	O
redundant	O
,	O
but	O
also	O
harmful	O
to	O
the	O
downstream	O
task	O
performance.	O
Positive	O
effect	O
from	O
head	O
disabling	O
was	O
reported	O
for	O
machine	B-TaskName
translation	I-TaskName
(Michel	O
et	O
al.,	O
2019),	O
abstractive	B-TaskName
summarization	I-TaskName
(Baan	O
et	O
al.,	O
2019),	O
and	O
GLUE	B-DatasetName
tasks	O
(Kovaleva	O
et	O
al.,	O
2019).	O
Additionally,	O
Tenney	O
et	O
al.	O
(2019a)	O
examine	O
the	O
cumulative	O
gains	O
of	O
their	O
structural	O
probing	O
classifier,	O
observing	O
that	O
in	O
5	O
out	O
of	O
8	O
probing	O
tasks	O
some	O
layers	O
cause	O
a	O
drop	O
in	O
scores	O
(typically	O
in	O
the	O
final	O
layers).	O
Gordon	O
et	O
al.	O
(2020)	O
find	O
that	O
30-40%	O
of	O
the	O
weights	O
can	O
be	O
pruned	O
without	O
impact	O
on	O
downstream	O
tasks.	O
In	O
general,	O
larger	O
BERT	B-MethodName
models	O
perform	O
better	O
Roberts	O
et	O
al.,	O
2020),	O
but	O
not	O
always:	O
BERT-base	B-MethodName
outperformed	O
BERT-large	B-MethodName
on	O
subject-verb	O
agreement	O
(Goldberg,	O
2019)	O
and	O
sentence	O
subject	O
detection	O
.	O
Given	O
the	O
complexity	O
of	O
language,	O
and	O
amounts	O
of	O
pretraining	O
data,	O
it	O
is	O
not	O
clear	O
why	O
BERT	B-MethodName
ends	O
up	O
with	O
redundant	O
heads	O
and	O
layers.	O
Clark	O
et	O
al.	O
(2019)	O
suggest	O
that	O
one	O
possible	O
reason	O
is	O
the	O
use	O
of	O
attention	O
dropouts,	O
which	O
causes	O
some	O
attention	O
weights	O
to	O
be	O
zeroed-out	O
during	O
training.	O

Pre-training	O
+	O
fine-tuning	O
workflow	O
is	O
a	O
crucial	O
part	O
of	O
BERT.	B-MethodName
The	O
former	O
is	O
supposed	O
to	O
provide	O
task-independent	O
knowledge,	O
and	O
the	O
latter	O
would	O
presumably	O
teach	O
the	O
model	O
to	O
rely	O
more	O
on	O
the	O
representations	O
useful	O
for	O
the	O
task	O
at	O
hand.	O
Kovaleva	O
et	O
al.	O
(2019)	O
did	O
not	O
find	O
that	O
to	O
be	O
the	O
case	O
for	O
BERT	B-MethodName
fine-tuned	O
on	O
GLUE	B-DatasetName
tasks	O
5	O
:	O
during	O
fine-tuning,	O
the	O
most	O
changes	O
for	O
3	O
epochs	O
occurred	O
in	O
the	O
last	O
two	O
layers	O
of	O
the	O
models,	O
but	O
those	O
changes	O
caused	O
self-attention	O
to	O
focus	O
on	O
[SEP]	O
rather	O
than	O
on	O
linguistically	O
interpretable	O
patterns.	O
It	O
is	O
understandable	O
why	O
fine-tuning	O
would	O
increase	O
the	O
attention	O
to	O
[CLS],	O
but	O
not	O
[SEP].	O
If	O
Clark	O
et	O
al.	O
(2019)	O
are	O
correct	O
that	O
[SEP]	O
serves	O
as	O
"noop"	O
indicator,	O
fine-tuning	O
basically	O
tells	O
BERT	B-MethodName
what	O
to	O
ignore.	O
Several	O
studies	O
explored	O
the	O
possibilities	O
of	O
improving	O
the	O
fine-tuning	O
of	O
BERT:	B-MethodName
•	O
Taking	O
more	O
layers	O
into	O
account:	O
learning	O
a	O
complementary	O
representation	O
of	O
the	O
information	O
in	O
deep	O
and	O
output	O
layers	O
,	O
using	O
a	O
weighted	O
combination	O
of	O
all	O
layers	O
instead	O
of	O
the	O
final	O
one	O
(Su	O
and	O
Cheng,	O
2019;Kondratyuk	O
and	O
Straka,	O
2019),	O
and	O
layer	O
dropout	O
(Kondratyuk	O
and	O
Straka,	O
2019).	O
•	O
Two-stage	O
fine-tuning	O
introduces	O
an	O
intermediate	O
supervised	O
training	O
stage	O
between	O
pre-training	O
and	O
fine-tuning	O
Arase	O
and	O
Tsujii,	O
2019;Pruksachatkun	O
et	O
al.,	O
2020;Glavaš	O
and	O
Vulić,	O
2020).	O
Ben-David	O
et	O
al.	O
(2020)	O
propose	O
a	O
pivot-based	O
variant	O
of	O
MLM	B-TaskName
to	O
fine-tune	O
BERT	B-MethodName
for	O
domain	O
adaptation.	O
•	O
Adversarial	O
token	O
perturbations	O
improve	O
robustness	O
of	O
the	O
model	O
(Zhu	O
et	O
al.,	O
2019).	O
•	O
Adversarial	O
regularization	O
in	O
combination	O
with	O
Bregman	O
Proximal	O
Point	O
Optimization	O
helps	O
alleviate	O
pre-trained	O
knowledge	O
forgetting	O
and	O
therefore	O
prevents	O
BERT	B-MethodName
from	O
overfitting	O
to	O
downstream	O
tasks	O
).	O
•	O
Mixout	O
regularization	O
improves	O
the	O
stability	O
of	O
BERT	B-MethodName
fine-tuning	O
even	O
for	O
a	O
small	O
number	O
of	O
training	O
examples	O
.	O
With	O
large	O
models,	O
even	O
fine-tuning	O
becomes	O
expensive,	O
but	O
Houlsby	O
et	O
al.	O
(2019)	O
show	O
that	O
it	O
can	O
be	O
successfully	O
approximated	O
with	O
adapter	O
modules.	O
They	O
achieve	O
competitive	O
performance	O
on	O
26	O
classification	O
tasks	O
at	O
a	O
fraction	O
of	O
the	O
computational	O
cost.	O
Adapters	O
in	O
BERT	B-MethodName
were	O
also	O
used	O
for	O
multi-task	O
learning	O
(Stickland	O
and	O
Murray,	O
2019)	O
and	O
cross-lingual	O
transfer	O
(Artetxe	O
et	O
al.,	O
2019).	O
An	O
alternative	O
to	O
fine-tuning	O
is	O
extracting	O
features	O
from	O
frozen	O
representations,	O
but	O
fine-tuning	O
works	O
better	O
for	O
BERT	B-MethodName
(Peters	O
et	O
al.,	O
2019b).	O
A	O
big	O
methodological	O
challenge	O
in	O
the	O
current	O
NLP	O
is	O
that	O
the	O
reported	O
performance	O
improvements	O
of	O
new	O
models	O
may	O
well	O
be	O
within	O
variation	O
induced	O
by	O
environment	O
factors	O
(Crane,	O
2018).	O
BERT	B-MethodName
is	O
not	O
an	O
exception.	O
Dodge	O
et	O
al.	O
(2020)	O
report	O
significant	O
variation	O
for	O
BERT	B-MethodName
fine-tuned	O
on	O
GLUE	B-DatasetName
tasks	O
due	O
to	O
both	O
weight	O
initialization	O
and	O
training	O
data	O
order.	O
They	O
also	O
propose	O
early	O
stopping	O
on	O
the	O
less-promising	O
seeds.	O
Although	O
we	O
hope	O
that	O
the	O
above	O
observations	O
may	O
be	O
useful	O
for	O
the	O
practitioners,	O
this	O
section	O
does	O
not	O
exhaust	O
the	O
current	O
research	O
on	O
fine-tuning	O
and	O
its	O
alternatives.	O
For	O
example,	O
we	O
do	O
not	O
cover	O
such	O
topics	O
as	O
Siamese	O
architectures,	O
policy	O
gradient	O
training,	O
automated	O
curriculum	O
learning,	O
and	O
others.	O
6	O
How	O
big	O
should	O
BERT	B-MethodName
be?	O

The	O
original	B-MethodName
BERT	I-MethodName
is	O
a	O
bidirectional	O
Transformer	O
pre-trained	O
on	O
two	O
tasks:	O
next	O
sentence	B-TaskName
prediction	I-TaskName
(NSP)	B-TaskName
and	O
masked	B-TaskName
language	I-TaskName
model	I-TaskName
(MLM)	B-TaskName
(section	O
2).	O
Multiple	O
studies	O
have	O
come	O
up	O
with	O
alternative	O
training	O
objectives	O
to	O
improve	O
on	O
BERT,	B-MethodName
which	O
could	O
be	O
categorized	O
as	O
follows:	O
•	O
How	O
to	O
mask.	O
Raffel	O
et	O
al.	O
(2019)	O
(Devlin	O
et	O
al.,	O
2019;Cui	O
et	O
al.,	O
2019).	O
Similarly,	O
we	O
can	O
mask	O
spans	O
rather	O
than	O
single	O
tokens	O
(Joshi	O
et	O
al.,	O
2020),	O
predicting	O
how	O
many	O
are	O
missing	O
.	O
Masking	O
phrases	O
and	O
named	O
entities	O
(Sun	O
et	O
al.,	O
2019b)	O
improves	O
representation	O
of	O
structured	O
knowledge.	O
•	O
Where	O
to	O
mask.	O
Lample	O
and	O
Conneau	O
(2019)	O
use	O
arbitrary	O
text	O
streams	O
instead	O
of	O
sentence	O
pairs	O
and	O
subsample	O
frequent	O
outputs	O
similar	O
to	O
Mikolov	O
et	O
al.	O
(2013).	O
Bao	O
et	O
al.	O
(2020)	O
combine	O
the	O
standard	O
autoencoding	O
MLM	B-TaskName
with	O
partially	O
autoregressive	O
LM	O
objective	O
using	O
special	O
pseudo	O
mask	O
tokens.	O
•	O
Alternatives	O
to	O
masking.	O
Raffel	O
et	O
al.	O
(2019)	O
experiment	O
with	O
replacing	O
and	O
dropping	O
spans,	O
explore	O
deletion,	O
infilling,	O
sentence	O
permutation	O
and	O
document	O
rotation,	O
and	O
Sun	O
et	O
al.	O
(2019c)	O
predict	O
whether	O
a	O
token	O
is	O
capitalized	O
and	O
whether	O
it	O
occurs	O
in	O
other	O
segments	O
of	O
the	O
same	O
document.	O
Yang	O
et	O
al.	O
(2019)	O
train	O
on	O
different	O
permutations	O
of	O
word	O
order	O
in	O
the	O
input	O
sequence,	O
maximizing	O
the	O
probability	O
of	O
the	O
original	O
word	O
order	O
(cf.	O
the	O
n-gram	O
word	O
order	O
reconstruction	O
task	O
).	O
detect	O
tokens	O
that	O
were	O
replaced	O
by	O
a	O
generator	O
network	O
rather	O
than	O
masked.	O
•	O
NSP	B-TaskName
alternatives.	O
Removing	O
NSP	B-TaskName
does	O
not	O
hurt	O
or	O
slightly	O
improves	O
performance	O
(Liu	O
et	O
al.,	O
2019b;Joshi	O
et	O
al.,	O
2020;Clinchant	O
et	O
al.,	O
2019).	O
and	O
Another	O
obvious	O
source	O
of	O
improvement	O
is	O
pretraining	O
data.	O
Several	O
studies	O
explored	O
the	O
benefits	O
of	O
increasing	O
the	O
corpus	O
volume	O
(Liu	O
et	O
al.,	O
2019b;Baevski	O
et	O
al.,	O
2019)	O
and	O
longer	O
training	O
(Liu	O
et	O
al.,	O
2019b).	O
The	O
data	O
also	O
does	O
not	O
have	O
to	O
be	O
raw	O
text:	O
there	O
is	O
a	O
number	O
efforts	O
to	O
incorporate	O
explicit	O
linguistic	O
information,	O
both	O
syntactic	O
(Sundararaman	O
et	O
al.,	O
2019)	O
and	O
semantic	O
.	O
and	O
Kumar	O
et	O
al.	O
(2020)	O
include	O
the	O
label	O
for	O
a	O
given	O
sequence	O
from	O
an	O
annotated	O
task	O
dataset.	O
Schick	O
and	O
Schütze	O
(2020)	O
separately	O
learn	O
representations	O
for	O
rare	O
words.	O
Although	O
BERT	B-MethodName
is	O
already	O
actively	O
used	O
as	O
a	O
source	O
of	O
world	O
knowledge	O
(see	O
subsection	O
3.3),	O
there	O
is	O
also	O
work	O
on	O
explicitly	O
supplying	O
structured	O
knowledge.	O
One	O
approach	O
is	O
entityenhanced	O
models.	O
For	O
example,	O
;	O
include	O
entity	O
em-	O
Pre-training	O
is	O
the	O
most	O
expensive	O
part	O
of	O
training	O
BERT,	B-MethodName
and	O
it	O
would	O
be	O
informative	O
to	O
know	O
how	O
much	O
benefit	O
it	O
provides.	O
On	O
some	O
tasks,	O
a	O
randomly	O
initialized	O
and	O
fine-tuned	O
BERT	B-MethodName
obtains	O
competitive	O
or	O
higher	O
results	O
than	O
the	O
pre-trained	O
BERT	B-MethodName
with	O
the	O
task	O
classifier	O
and	O
frozen	O
weights	O
(Kovaleva	O
et	O
al.,	O
2019).	O
The	O
consensus	O
in	O
the	O
community	O
is	O
that	O
pre-training	O
does	O
help	O
in	O
most	O
situations,	O
but	O
the	O
degree	O
and	O
its	O
exact	O
contribution	O
requires	O
further	O
investigation.	O
Prasanna	O
et	O
al.	O
(2020)	O
found	O
that	O
most	O
weights	O
of	O
pre-trained	O
BERT	B-MethodName
are	O
useful	O
in	O
fine-tuning,	O
although	O
there	O
are	O
"better"	O
and	O
"worse"	O
subnetworks.	O
One	O
explanation	O
is	O
that	O
pre-trained	O
weights	O
help	O
the	O
fine-tuned	O
BERT	B-MethodName
find	O
wider	O
and	O
flatter	O
areas	O
with	O
smaller	O
generalization	O
error,	O
which	O
makes	O
the	O
model	O
more	O
robust	O
to	O
overfitting	O
(see	O
Figure	O
5	O
from	O
Hao	O
et	O
al.	O
(2019)).	O
Given	O
the	O
large	O
number	O
and	O
variety	O
of	O
proposed	O
modifications,	O
one	O
would	O
wish	O
to	O
know	O
how	O
much	O
impact	O
each	O
of	O
them	O
has.	O
However,	O
due	O
to	O
the	O
overall	O
trend	O
towards	O
large	O
model	O
sizes,	O
systematic	O
ablations	O
have	O
become	O
expensive.	O
Most	O
new	O
models	O
claim	O
superiority	O
on	O
standard	O
benchmarks,	O
but	O
gains	O
are	O
often	O
marginal,	O
and	O
estimates	O
of	O
model	O
stability	O
and	O
significance	O
testing	O
are	O
very	O
rare.	O

To	O
date,	O
the	O
most	O
systematic	O
study	O
of	O
BERT	B-MethodName
architecture	O
was	O
performed	O
by	O
,	O
who	O
experimented	O
with	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers,	I-HyperparameterName
heads,	B-HyperparameterName
and	O
model	O
parameters,	O
varying	O
one	O
option	O
and	O
freezing	O
the	O
others.	O
They	O
concluded	O
that	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
heads	I-HyperparameterName
was	O
not	O
as	O
significant	O
as	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers.	I-HyperparameterName
That	O
is	O
consistent	O
with	O
the	O
findings	O
of	O
Voita	O
et	O
al.	O
(2019b)	O
and	O
Michel	O
et	O
al.	O
(2019)	O
(section	O
6),	O
and	O
also	O
the	O
observation	O
by	O
that	O
the	O
middle	O
layers	O
were	O
the	O
most	O
transferable.	O
Larger	O
hidden	O
representation	O
size	O
was	O
con-sistently	O
better,	O
but	O
the	O
gains	O
varied	O
by	O
setting.	O
All	O
in	O
all,	O
changes	O
in	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
heads	I-HyperparameterName
and	O
layers	B-HyperparameterName
appear	O
to	O
perform	O
different	O
functions.	O
The	O
issue	O
of	O
model	O
depth	O
must	O
be	O
related	O
to	O
the	O
information	O
flow	O
from	O
the	O
most	O
task-specific	O
layers	O
closer	O
to	O
the	O
classifier	O
,	O
to	O
the	O
initial	O
layers	O
which	O
appear	O
to	O
be	O
the	O
most	O
task-invariant	O
(Hao	O
et	O
al.,	O
2019),	O
and	O
where	O
the	O
tokens	O
resemble	O
the	O
input	O
tokens	O
the	O
most	O
(Brunner	O
et	O
al.,	O
2020)	O
(see	O
subsection	O
4.3).	O
If	O
that	O
is	O
the	O
case,	O
a	O
deeper	O
model	O
has	O
more	O
capacity	O
to	O
encode	O
information	O
that	O
is	O
not	O
task-specific.	O
On	O
the	O
other	O
head,	O
many	O
self-attention	O
heads	O
in	O
vanilla	B-MethodName
BERT	I-MethodName
seem	O
to	O
naturally	O
learn	O
the	O
same	O
patterns	O
(Kovaleva	O
et	O
al.,	O
2019).	O
This	O
explains	O
why	O
pruning	O
them	O
does	O
not	O
have	O
too	O
much	O
impact.	O
The	O
question	O
that	O
arises	O
from	O
this	O
is	O
how	O
far	O
we	O
could	O
get	O
with	O
intentionally	O
encouraging	O
diverse	O
self-attention	O
patterns:	O
theoretically,	O
this	O
would	O
mean	O
increasing	O
the	O
amount	O
of	O
information	O
in	O
the	O
model	O
with	O
the	O
same	O
number	O
of	O
weights.	O
Raganato	O
et	O
al.	O
(2020)	O
show	O
for	O
Transformer-based	O
machine	O
translation	O
we	O
can	O
simply	O
pre-set	O
the	O
patterns	O
that	O
we	O
already	O
know	O
the	O
model	O
would	O
learn,	O
instead	O
of	O
learning	O
them	O
from	O
scratch.	O
Vanilla	B-MethodName
BERT	I-MethodName
is	O
symmetric	O
and	O
balanced	O
in	O
terms	O
of	O
self-attention	O
and	O
feed-forward	O
layers,	O
but	O
it	O
may	O
not	O
have	O
to	O
be.	O
For	O
the	O
base	O
Transformer,	O
Press	O
et	O
al.	O
(2020)	O
report	O
benefits	O
from	O
more	O
selfattention	O
sublayers	O
at	O
the	O
bottom	O
and	O
more	O
feedforward	O
sublayers	O
at	O
the	O
top.	O
Gong	O
et	O
al.	O
(2019)	O
note	O
that,	O
since	O
self-attention	O
patterns	O
in	O
higher	O
and	O
lower	O
layers	O
are	O
similar,	O
the	O
model	O
training	O
can	O
be	O
done	O
in	O
a	O
recursive	O
manner,	O
where	O
the	O
shallower	O
version	O
is	O
trained	O
first	O
and	O
then	O
the	O
trained	O
parameters	O
are	O
copied	O
to	O
deeper	O
layers.	O
Such	O
a	O
"warm-start"	O
can	O
lead	O
to	O
a	O
25%	O
faster	O
training	O
without	O
sacrificing	O
performance.	O

This	O
section	O
reviews	O
the	O
proposals	O
to	O
optimize	O
the	O
training	O
and	O
architecture	O
of	O
the	O
original	O
BERT.	B-MethodName

Figure	O
4	O
presents	O
the	O
performance	O
of	O
softmax	O
classifiers	O
trained	O
to	O
perform	O
the	O
bidirectional	O
language	O
modeling	O
task,	O
given	O
just	O
the	O
CWRs	O
as	O
input.	O
We	O
notice	O
that	O
higher	O
layers	O
in	O
recurrent	O
models	O
consistently	O
achieve	O
lower	O
perplexities.	O
Inter-is	O
the	O
vector	O
ori	O
during	O
pretrain	O
ing	O
performanc	O
tations	O
that	O
are	O
ing	O
are	O
also	O
tho	O
performance	O
(F	O
alizer	O
layers	O
tr	O
and	O
task-specifi	O
These	O
result	O
layerwise	O
beha	O
moving	O
up	O
the	O
specific	O
repres	O
hold	O
for	O
transf	O
differences	O
bet	O
an	O
active	O
area	O
o	O
et	O
al.	O
of	O
hierarchical	O
sentence	O
structure,	O
as	O
detected	O
by	O
the	O
probing	O
tasks	O
of	O
predicting	O
the	O
token	O
index,	O
the	O
main	O
auxiliary	O
verb	O
and	O
the	O
sentence	O
subject.	O
There	O
is	O
a	O
wide	O
consensus	O
in	O
studies	O
with	O
different	O
tasks,	O
datasets	O
and	O
methodologies	O
that	O
syntactic	O
information	O
is	O
most	O
prominent	O
in	O
the	O
middle	O
layers	O
of	O
BERT.	B-MethodName
4	O
Hewitt	O
and	O
Manning	O
(2019)	O
had	O
the	O
most	O
success	O
reconstructing	O
syntactic	O
tree	O
depth	O
from	O
the	O
middle	O
BERT	O
layers	B-HyperparameterName
(6-9	B-HyperparameterValue
for	O
base-BERT,	B-MethodName
14-19	B-HyperparameterValue
for	O
BERT-large).	B-MethodName
Goldberg	O
(2019)	O
reports	O
the	O
best	O
subject-verb	O
agreement	O
around	O
layers	B-HyperparameterName
8-9,	B-HyperparameterValue
and	O
the	O
performance	O
on	O
syntactic	O
probing	O
tasks	O
used	O
by	O
Jawahar	O
et	O
al.	O
(2019)	O
also	O
seems	O
to	O
peak	O
around	O
the	O
middle	O
of	O
the	O
model.	O
The	O
prominence	O
of	O
syntactic	O
information	O
in	O
the	O
middle	O
BERT	B-MethodName
layers	O
is	O
related	O
to	O
's	O
observation	O
that	O
the	O
middle	O
layers	O
of	O
Transformers	O
are	O
best-performing	O
overall	O
and	O
the	O
most	O
transferable	O
across	O
tasks	O
(see	O
Figure	O
4).	O
There	O
is	O
conflicting	O
evidence	O
about	O
syntactic	O
chunks.	O
Tenney	O
et	O
al.	O
(2019a)	O
conclude	O
that	O
"the	O
basic	O
syntactic	O
information	O
appears	O
earlier	O
in	O
the	O
network	O
while	O
high-level	O
semantic	O
features	O
appear	O
at	O
the	O
higher	O
layers",	O
drawing	O
parallels	O
between	O
this	O
order	O
and	O
the	O
order	O
of	O
components	O
in	O
a	O
typical	O
NLP	O
pipeline	O
-from	O
POS-tagging	O
to	O
dependency	O
parsing	O
to	O
semantic	O
role	O
labeling.	O
Jawahar	O
et	O
al.	O
(2019)	O
also	O
report	O
that	O
the	O
lower	O
layers	O
were	O
more	O
useful	O
for	O
chunking,	O
while	O
middle	O
layers	O
were	O
more	O
useful	O
for	O
parsing.	O
At	O
the	O
same	O
time,	O
the	O
probing	O
experiments	O
by	O
find	O
the	O
opposite:	O
both	O
POS-tagging	O
and	O
chunking	O
were	O
performed	O
best	O
at	O
the	O
middle	O
layers,	O
in	O
both	O
BERT-base	B-MethodName
and	O
BERT-large.	B-MethodName
However,	O
all	O
three	O
studies	O
use	O
different	O
suites	O
of	O
probing	O
tasks.	O
The	O
final	O
layers	O
of	O
BERT	B-MethodName
are	O
the	O
most	O
taskspecific.	O
In	O
pre-training,	O
this	O
means	O
specificity	O
to	O
the	O
MLM	B-TaskName
task,	O
which	O
explains	O
why	O
the	O
middle	O
layers	O
are	O
more	O
transferable	O
.	O
In	O
fine-tuning,	O
it	O
explains	O
why	O
the	O
final	O
layers	O
change	O
the	O
most	O
(Kovaleva	O
et	O
al.,	O
2019),	O
and	O
why	O
restoring	O
the	O
weights	O
of	O
lower	O
layers	O
of	O
fine-tuned	O
BERT	B-MethodName
to	O
their	O
original	O
values	O
does	O
not	O
dramatically	O
hurt	O
the	O
model	O
performance	O
(Hao	O
et	O
al.,	O
2019).	O
Tenney	O
et	O
al.	O
(2019a)	O
suggest	O
that	O
while	O
syntactic	O
information	O
appears	O
early	O
in	O
the	O
model	O
and	O
can	O
be	O
localized,	O
semantics	O
is	O
spread	O
across	O
the	O
entire	O
model,	O
which	O
explains	O
why	O
certain	O
non-trivial	O
examples	O
get	O
solved	O
incorrectly	O
at	O
first	O
but	O
correctly	O
at	O
the	O
later	O
layers.	O
This	O
is	O
rather	O
to	O
be	O
expected:	O
semantics	O
permeates	O
all	O
language,	O
and	O
linguists	O
debate	O
whether	O
meaningless	O
structures	O
can	O
exist	O
at	O
all	O
(Goldberg,	O
2006,	O
p.166-182).	O
But	O
this	O
raises	O
the	O
question	O
of	O
what	O
stacking	O
more	O
Transformer	O
layers	O
in	O
BERT	B-MethodName
actually	O
achieves	O
in	O
terms	O
of	O
the	O
spread	O
of	O
semantic	O
knowledge,	O
and	O
whether	O
that	O
is	O
beneficial.	O
Tenney	O
et	O
al.	O
compared	O
BERT-base	B-MethodName
and	O
BERT-large,	B-MethodName
and	O
found	O
that	O
the	O
overall	O
pattern	O
of	O
cumulative	O
score	O
gains	O
is	O
the	O
same,	O
only	O
more	O
spread	O
out	O
in	O
the	O
larger	O
model.	O
Note	O
that	O
Tenney	O
et	O
al.	O
(2019a)'s	O
experiments	O
concern	O
sentence-level	O
semantic	O
relations;	O
report	O
that	O
the	O
encoding	O
of	O
ConceptNet	O
semantic	O
relations	O
is	O
the	O
worst	O
in	O
the	O
early	O
layers	O
and	O
increases	O
towards	O
the	O
top.	O
Jawahar	O
et	O
al.	O
(2019)	O
place	O
"surface	O
features	O
in	O
lower	O
layers,	O
syntactic	O
features	O
in	O
middle	O
layers	O
and	O
semantic	O
features	O
in	O
higher	O
layers",	O
but	O
their	O
conclusion	O
is	O
surprising,	O
given	O
that	O
only	O
one	O
semantic	O
task	O
in	O
this	O
study	O
actually	O
topped	O
at	O
the	O
last	O
layer,	O
and	O
three	O
others	O
peaked	O
around	O
the	O
middle	O
and	O
then	O
considerably	O
degraded	O
by	O
the	O
final	O
layers.	O

The	O
first	O
layer	O
of	O
BERT	B-MethodName
receives	O
as	O
input	O
a	O
combination	O
of	O
token,	O
segment,	O
and	O
positional	O
embeddings.	O
It	O
stands	O
to	O
reason	O
that	O
the	O
lower	O
layers	O
have	O
the	O
most	O
information	O
about	O
linear	O
word	O
order.	O
report	O
a	O
decrease	O
in	O
the	O
knowledge	O
of	O
linear	O
word	O
order	O
around	O
layer	O
4	O
in	O
BERT-base.	B-MethodName
This	O
is	O
accompanied	O
by	O
an	O
increased	O
knowledge	O
textualizers.	O
Furthermore,	O
the	O
ELMo-based	O
models	O
facilitate	O
a	O
controlled	O
comparison-they	O
only	O
differ	O
in	O
the	O
contextualizer	O
architecture	O
used.	O
We	O
evaluate	O
how	O
well	O
CWR	O
features	O
perform	O
the	O
pretraining	O
task-bidirectional	B-TaskName
language	I-TaskName
modeling.	I-TaskName
Specifically,	O
we	O
take	O
the	O
pretrained	O
representations	O
for	O
each	O
layer	O
and	O
relearn	O
the	O
language	O
model	O
softmax	O
classifiers	O
used	O
to	O
predict	O
the	O
next	O
and	O
previous	O
token.	O
The	O
ELMo	O
models	O
are	O
trained	O
on	O
the	O
Billion	O
Word	O
Benchmark,	O
so	O
we	O
retrain	O
the	O
softmax	O
classifier	O
on	O
similar	O
data	O
to	O
mitigate	O
any	O
possible	O
effects	O
from	O
domain	O
shift.	O
We	O
split	O
the	O
held-out	O
portion	O
of	O
the	O
Billion	B-DatasetName
Word	I-DatasetName
Benchmark	I-DatasetName
into	O
train	B-HyperparameterName
(80%,	B-HyperparameterValue
6.2M	O
tokens)	O
and	O
evaluation	B-HyperparameterName
(20%,	B-HyperparameterValue
1.6M	O
tokens)	O
sets	O
and	O
use	O
this	O
data	O
to	O
retrain	O
and	O
evaluate	O
the	O
softmax	O
classifiers.	O
We	O
expect	O
that	O
biLM	O
perplexity	O
will	O
be	O
lower	O
when	O
training	O
the	O
softmax	O
classifiers	O
on	O
representations	O
from	O
layers	O
that	O
capture	O
more	O
information	O
about	O
the	O
pretraining	O
task.	O

More	O
recently,	O
Kobayashi	O
et	O
al.	O
(2020)	O
showed	O
that	O
the	O
norms	O
of	O
attention-weighted	O
input	O
vectors,	O
which	O
yield	O
a	O
more	O
intuitive	O
interpretation	O
of	O
self-attention,	O
reduce	O
the	O
attention	O
to	O
special	O
tokens.	O
However,	O
even	O
when	O
the	O
attention	O
weights	O
are	O
normed,	O
it	O
is	O
still	O
not	O
the	O
case	O
that	O
most	O
heads	O
that	O
do	O
the	O
"heavy	O
lifting"	O
are	O
even	O
potentially	O
interpretable	O
(Prasanna	O
et	O
al.,	O
2020).	O
One	O
methodological	O
choice	O
in	O
in	O
many	O
studies	O
of	O
attention	O
is	O
to	O
focus	O
on	O
inter-word	O
attention	O
and	O
simply	O
exclude	O
special	O
tokens	O
(e.g.	O
and	O
Htut	O
et	O
al.	O
(	O
2019)).	O
However,	O
if	O
attention	O
to	O
special	O
tokens	O
actually	O
matters	O
at	O
inference	O
time,	O
drawing	O
conclusions	O
purely	O
from	O
inter-word	O
attention	O
patterns	O
does	O
not	O
seem	O
warranted.	O
The	O
functions	O
of	O
special	O
tokens	O
are	O
not	O
yet	O
well	O
understood.	O
[CLS]	O
is	O
typically	O
viewed	O
as	O
an	O
aggregated	O
sentence-level	O
representation	O
(although	O
all	O
token	O
representations	O
also	O
contain	O
at	O
least	O
some	O
sentence-level	O
information,	O
as	O
discussed	O
in	O
subsection	O
4.1);	O
in	O
that	O
case,	O
we	O
may	O
not	O
see	O
e.g.	O
full	O
syntactic	O
trees	O
in	O
inter-word	O
attention	O
because	O
part	O
of	O
that	O
information	O
is	O
actually	O
packed	O
in	O
[CLS].	O
Clark	O
et	O
al.	O
(2019)	O
experiment	O
with	O
encoding	O
Wikipedia	O
paragraphs	O
with	O
base	B-MethodName
BERT	I-MethodName
to	O
consider	O
specifically	O
the	O
attention	O
to	O
special	O
tokens,	O
noting	O
that	O
heads	O
in	O
early	O
layers	O
attend	O
more	O
to	O
[CLS],	O
in	O
middle	O
layers	O
to	O
[SEP],	O
and	O
in	O
final	O
layers	O
to	O
periods	O
and	O
commas.	O
They	O
hypothesize	O
that	O
its	O
function	O
might	O
be	O
one	O
of	O
"no-op",	O
a	O
signal	O
to	O
ignore	O
the	O
head	O
if	O
its	O
pattern	O
is	O
not	O
applicable	O
to	O
the	O
current	O
case.	O
As	O
a	O
result,	O
for	O
example,	O
[SEP]	O
gets	O
increased	O
attention	O
starting	O
in	O
layer	O
5,	O
but	O
its	O
importance	O
for	O
prediction	O
drops.	O
However,	O
after	O
fine-tuning	O
both	O
[SEP]	O
and	O
[CLS]	O
get	O
a	O
lot	O
of	O
attention,	O
depending	O
on	O
the	O
task	O
(Kovaleva	O
et	O
al.,	O
2019).	O
Interestingly,	O
BERT	O
also	O
pays	O
a	O
lot	O
of	O
attention	O
to	O
punctuation,	O
which	O
Clark	O
et	O
al.	O
(2019)	O
explain	O
by	O
the	O
fact	O
that	O
periods	O
and	O
commas	O
are	O
simply	O
almost	O
as	O
frequent	O
as	O
the	O
special	O
tokens,	O
and	O
so	O
the	O
model	O
might	O
learn	O
to	O
rely	O
on	O
them	O
for	O
the	O
same	O
reasons.	O

The	O
"heterogeneous"	O
attention	O
pattern	O
shown	O
in	O
Figure	O
3	O
could	O
potentially	O
be	O
linguistically	O
interpretable,	O
and	O
a	O
number	O
of	O
studies	O
focused	O
on	O
identifying	O
the	O
functions	O
of	O
self-attention	O
heads.	O
In	O
particular,	O
some	O
BERT	B-MethodName
heads	O
seem	O
to	O
specialize	O
in	O
certain	O
types	O
of	O
syntactic	O
relations.	O
Htut	O
et	O
al.	O
(2019)	O
and	O
Clark	O
et	O
al.	O
(2019)	O
report	O
that	O
there	O
are	O
BERT	B-MethodName
heads	O
that	O
attended	O
significantly	O
more	O
than	O
a	O
random	O
baseline	O
to	O
words	O
in	O
certain	O
syntactic	O
positions.	O
The	O
datasets	O
and	O
methods	O
used	O
in	O
these	O
studies	O
differ,	O
but	O
they	O
both	O
find	O
that	O
there	O
are	O
heads	O
that	O
attend	O
to	O
words	O
in	O
obj	O
role	O
more	O
than	O
the	O
positional	O
baseline.	O
The	O
evidence	O
for	O
nsubj,	O
advmod,	O
and	O
amod	O
varies	O
between	O
these	O
two	O
studies.	O
The	O
overall	O
conclusion	O
is	O
also	O
supported	O
by	O
Voita	O
et	O
al.	O
(2019b)'s	O
study	O
of	O
the	O
base	B-MethodName
Transformer	I-MethodName
in	O
machine	B-TaskName
translation	I-TaskName
context.	O
Hoover	O
et	O
al.	O
(2019)	O
hypothesize	O
that	O
even	O
complex	O
dependencies	O
like	O
dobj	O
are	O
encoded	O
by	O
a	O
combination	O
of	O
heads	O
rather	O
than	O
a	O
single	O
head,	O
but	O
this	O
work	O
is	O
limited	O
to	O
qualitative	O
analysis.	O
Zhao	O
and	O
Bethard	O
(2020)	O
looked	O
specifically	O
for	O
the	O
heads	O
encoding	O
negation	O
scope.	O
Both	O
Clark	O
et	O
al.	O
(2019)	O
and	O
Htut	O
et	O
al.	O
(2019)	O
conclude	O
that	O
no	O
single	O
head	O
has	O
the	O
complete	O
syntactic	O
tree	O
information,	O
in	O
line	O
with	O
evidence	O
of	O
partial	O
knowledge	O
of	O
syntax	O
(cf.	O
subsection	O
3.1).	O
However,	O
Clark	O
et	O
al.	O
(2019)	O
identify	O
a	O
BERT	B-MethodName
head	O
that	O
can	O
be	O
directly	O
used	O
as	O
a	O
classifier	O
to	O
perform	O
coreference	O
resolution	O
on	O
par	O
with	O
a	O
rule-based	O
system,	O
which	O
by	O
itself	O
would	O
seem	O
to	O
require	O
quite	O
a	O
lot	O
of	O
syntactic	O
knowledge.	O
present	O
evidence	O
that	O
attention	O
weights	O
are	O
weak	O
indicators	O
of	O
subjectverb	O
agreement	O
and	O
reflexive	O
anaphora.	O
Instead	O
of	O
serving	O
as	O
strong	O
pointers	O
between	O
tokens	O
that	O
should	O
be	O
related,	O
BERT's	B-MethodName
self-attention	O
weights	O
were	O
close	O
to	O
a	O
uniform	O
attention	O
baseline,	O
but	O
there	O
was	O
some	O
sensitivity	O
to	O
different	O
types	O
of	O
distractors	O
coherent	O
with	O
psycholinguistic	O
data.	O
This	O
is	O
consistent	O
with	O
conclusions	O
by	O
Ettinger	O
(2019).	O
To	O
our	O
knowledge,	O
morphological	O
information	O
in	O
BERT	B-MethodName
heads	O
has	O
not	O
been	O
addressed,	O
but	O
with	O
the	O
sparse	O
attention	O
variant	O
by	O
Correia	O
et	O
al.	O
(2019)	O
in	O
the	O
base	B-MethodName
Transformer,	I-MethodName
some	O
attention	O
heads	O
appear	O
to	O
merge	O
BPE-tokenized	O
words.	O
For	O
semantic	O
relations,	O
there	O
are	O
reports	O
of	O
self-attention	O
heads	O
encoding	O
core	O
frame-semantic	O
relations	O
(Kovaleva	O
et	O
al.,	O
2019),	O
as	O
well	O
as	O
lexicographic	O
and	O
commonsense	O
relations	O
.	O
The	O
overall	O
popularity	O
of	O
self-attention	O
as	O
an	O
interpretability	O
mechanism	O
is	O
due	O
to	O
the	O
idea	O
that	O
"attention	O
weight	O
has	O
a	O
clear	O
meaning:	O
how	O
much	O
a	O
particular	O
word	O
will	O
be	O
weighted	O
when	O
computing	O
the	O
next	O
representation	O
for	O
the	O
current	O
word"	O
(Clark	O
et	O
al.,	O
2019).	O
This	O
view	O
is	O
currently	O
debated	O
(Jain	O
and	O
Wallace,	O
2019;Serrano	O
and	O
Smith,	O
2019;Wiegreffe	O
and	O
Pinter,	O
2019;Brunner	O
et	O
al.,	O
2020),	O
and	O
in	O
a	O
multi-layer	O
model	O
where	O
attention	O
is	O
followed	O
by	O
non-linear	O
transformations,	O
the	O
patterns	O
in	O
individual	O
heads	O
do	O
not	O
provide	O
a	O
full	O
picture.	O
Also,	O
while	O
many	O
current	O
papers	O
are	O
accompanied	O
by	O
attention	O
visualizations,	O
and	O
there	O
is	O
a	O
growing	O
number	O
of	O
visualization	O
tools	O
(Vig,	O
2019;Hoover	O
et	O
al.,	O
2019),	O
the	O
visualization	O
is	O
typically	O
limited	O
to	O
qualitative	O
analysis	O
(often	O
with	O
cherry-picked	O
examples)	O
(Belinkov	O
and	O
Glass,	O
2019),	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
definitive	O
evidence.	O
Kovaleva	O
et	O
al.	O
(2019)	O
show	O
that	O
most	O
selfattention	O
heads	O
do	O
not	O
directly	O
encode	O
any	O
nontrivial	O
linguistic	O
information,	O
at	O
least	O
when	O
finetuned	O
on	O
GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2018),	O
since	O
only	O
less	O
than	O
50%	O
of	O
heads	O
exhibit	O
the	O
"heterogeneous"	O
pattern.	O
Much	O
of	O
the	O
model	O
produced	O
the	O
vertical	O
pattern	O
(attention	O
to	O
[CLS],	O
[SEP],	O
and	O
punctuation	O
tokens),	O
consistent	O
with	O
the	O
observations	O
by	O
Clark	O
et	O
al.	O
(2019).	O
This	O
redundancy	O
is	O
likely	O
related	O
to	O
the	O
overparameterization	O
issue	O
(see	O
section	O
6).	O

Several	O
studies	O
proposed	O
classification	O
of	O
attention	O
head	O
types.	O
Raganato	O
and	O
Tiedemann	O
(2018)	O
discuss	O
attending	O
to	O
the	O
token	O
itself,	O
previous/next	O
tokens	O
and	O
the	O
sentence	O
end.	O
Clark	O
et	O
al.	O
(2019)	O
distinguish	O
between	O
attending	O
to	O
previous/next	O
tokens,	O
[CLS],	O
[SEP],	O
punctuation,	O
and	O
"attending	O
broadly"	O
over	O
the	O
sequence.	O
Kovaleva	O
et	O
al.	O
(2019)	O
propose	O
5	O
patterns	O
shown	O
in	O
Figure	O
3.	O

In	O
studies	O
of	O
BERT,	B-MethodName
the	O
term	O
"embedding"	O
refers	O
to	O
the	O
output	O
of	O
a	O
Transformer	O
layer	O
(typically,	O
the	O
final	O
one).	O
Both	O
conventional	O
static	O
embeddings	O
(Mikolov	O
et	O
al.,	O
2013)	O
and	O
BERT-style	B-MethodName
embeddings	O
can	O
be	O
viewed	O
in	O
terms	O
of	O
mutual	O
information	O
maximization	O
,	O
but	O
the	O
latter	O
are	O
contextualized.	O
Every	O
token	O
is	O
represented	O
by	O
a	O
vector	O
dependent	O
on	O
the	O
particular	O
context	O
of	O
occurrence,	O
and	O
contains	O
at	O
least	O
some	O
information	O
about	O
that	O
context	O
(Miaschi	O
and	O
Dell'Orletta,	O
2020).	O
Several	O
studies	O
reported	O
that	O
distilled	O
contextualized	O
embeddings	O
better	O
encode	O
lexical	O
semantic	O
information	O
(i.e.	O
they	O
are	O
better	O
at	O
traditional	O
word-level	O
tasks	O
such	O
as	O
word	O
similarity).	O
The	O
methods	O
to	O
distill	O
a	O
contextualized	O
representation	O
into	O
static	O
include	O
aggregating	O
the	O
information	O
across	O
multiple	O
contexts	O
(Akbik	O
et	O
al.,	O
2019;Bommasani	O
et	O
al.,	O
2020),	O
encoding	O
"semantically	O
bleached"	O
sentences	O
that	O
rely	O
almost	O
exclusively	O
on	O
the	O
meaning	O
of	O
a	O
given	O
word	O
(e.g.	O
"This	O
is	O
<>")	O
(May	O
et	O
al.,	O
2019),	O
and	O
even	O
using	O
contextualized	O
embeddings	O
to	O
train	O
static	O
embeddings	O
(Wang	O
et	O
al.,	O
2020d).	O
But	O
this	O
is	O
not	O
to	O
say	O
that	O
there	O
is	O
no	O
room	O
for	O
improvement.	O
Ethayarajh	O
(2019)	O
measure	O
how	O
similar	O
the	O
embeddings	O
for	O
identical	O
words	O
are	O
in	O
every	O
layer,	O
reporting	O
that	O
later	O
BERT	B-MethodName
layers	O
produce	O
more	O
context-specific	O
representations	O
3	O
.	O
They	O
also	O
find	O
that	O
BERT	B-MethodName
embeddings	O
occupy	O
a	O
narrow	O
cone	O
in	O
the	O
vector	O
space,	O
and	O
this	O
effect	O
increases	O
from	O
the	O
earlier	O
to	O
later	O
layers.	O
That	O
is,	O
two	O
random	O
words	O
will	O
on	O
average	O
have	O
a	O
much	O
higher	O
cosine	B-MetricName
similarity	I-MetricName
than	O
expected	O
if	O
embeddings	O
were	O
directionally	O
uniform	O
(isotropic).	O
Since	O
isotropy	O
was	O
shown	O
to	O
be	O
beneficial	O
for	O
static	O
word	O
embeddings	O
(Mu	O
and	O
Viswanath,	O
2018),	O
this	O
might	O
be	O
a	O
fruitful	O
direction	O
to	O
explore	O
for	O
BERT.	B-MethodName
Since	O
BERT	B-MethodName
embeddings	O
are	O
contextualized,	O
an	O
interesting	O
question	O
is	O
to	O
what	O
extent	O
they	O
capture	O
phenomena	O
like	O
polysemy	O
and	O
homonymy.	O
There	O
is	O
indeed	O
evidence	O
that	O
BERT's	B-MethodName
contextualized	O
embeddings	O
form	O
distinct	O
clusters	O
corresponding	O
to	O
word	O
senses	O
(Wiedemann	O
et	O
al.,	O
2019;Schmidt	O
and	O
Hofmann,	O
2020),	O
making	O
BERT	B-MethodName
successful	O
at	O
word	B-TaskName
sense	I-TaskName
disambiguation	I-TaskName
task.	O
However,	O
Mickus	O
et	O
al.	O
(2019)	O
note	O
that	O
the	O
representations	O
of	O
the	O
same	O
word	O
depend	O
on	O
the	O
position	O
of	O
the	O
sentence	O
in	O
which	O
it	O
occurs,	O
likely	O
due	O
to	O
the	O
NSP	B-TaskName
objective.	O
This	O
is	O
not	O
desirable	O
from	O
the	O
linguistic	O
point	O
of	O
view,	O
and	O
could	O
be	O
a	O
promising	O
avenue	O
for	O
future	O
work.	O
The	O
above	O
discussion	O
concerns	O
token	O
embeddings,	O
but	O
BERT	B-MethodName
is	O
typically	O
used	O
as	O
a	O
sentence	O
or	O
text	O
encoder.	O
The	O
standard	O
way	O
to	O
generate	O
sentence	O
or	O
text	O
representations	O
for	O
classification	O
is	O
to	O
use	O
the	O
[CLS]	O
token,	O
but	O
alternatives	O
are	O
also	O
being	O
discussed,	O
including	O
concatenation	O
of	O
token	O
representations	O
(Tanaka	O
et	O
al.,	O
2020),	O
normalized	O
mean	O
(Tanaka	O
et	O
al.,	O
2020),	O
and	O
layer	O
activations	O
.	O
See	O
Toshniwal	O
et	O
al.	O
(2020)	O
for	O
a	O
systematic	O
comparison	O
of	O
several	O
methods	O
across	O
tasks	O
and	O
sentence	O
encoders.	O

Multiple	O
probing	O
studies	O
in	O
section	O
3	O
and	O
section	O
4	O
report	O
that	O
BERT	B-MethodName
possesses	O
a	O
surprising	O
amount	O
of	O
syntactic,	O
semantic,	O
and	O
world	O
knowledge.	O
However,	O
Tenney	O
et	O
al.	O
(2019a)	O
remarks,	O
"the	O
fact	O
that	O
a	O
linguistic	O
pattern	O
is	O
not	O
observed	O
by	O
our	O
probing	O
classifier	O
does	O
not	O
guarantee	O
that	O
it	O
is	O
not	O
there,	O
and	O
the	O
observation	O
of	O
a	O
pattern	O
does	O
not	O
tell	O
us	O
how	O
it	O
is	O
used."	O
There	O
is	O
also	O
the	O
issue	O
of	O
how	O
complex	O
a	O
probe	O
should	O
be	O
allowed	O
to	O
be	O
.	O
If	O
a	O
more	O
complex	O
probe	O
recovers	O
more	O
information,	O
to	O
what	O
extent	O
are	O
we	O
still	O
relying	O
on	O
the	O
original	O
model?	O
Furthermore,	O
different	O
probing	O
methods	O
may	O
lead	O
to	O
complementary	O
or	O
even	O
contradictory	O
conclusions,	O
which	O
makes	O
a	O
single	O
test	O
(as	O
in	O
most	O
stud-	O
(Kovaleva	O
et	O
al.,	O
2019)	O
ies)	O
insufficient	O
(Warstadt	O
et	O
al.,	O
2019).	O
A	O
given	O
method	O
might	O
also	O
favor	O
one	O
model	O
over	O
another,	O
e.g.,	O
RoBERTa	O
trails	O
BERT	B-MethodName
with	O
one	O
tree	O
extraction	O
method,	O
but	O
leads	O
with	O
another	O
(Htut	O
et	O
al.,	O
2019).	O
The	O
choice	O
of	O
linguistic	O
formalism	O
also	O
matters	O
(Kuznetsov	O
and	O
Gurevych,	O
2020).	O
In	O
view	O
of	O
all	O
that,	O
the	O
alternative	O
is	O
to	O
focus	O
on	O
identifying	O
what	O
BERT	B-MethodName
actually	O
relies	O
on	O
at	O
inference	O
time.	O
This	O
direction	O
is	O
currently	O
pursued	O
both	O
at	O
the	O
level	O
of	O
architecture	O
blocks	O
(to	O
be	O
discussed	O
in	O
detail	O
in	O
subsection	O
6.3),	O
and	O
at	O
the	O
level	O
of	O
information	O
encoded	O
in	O
model	O
weights.	O
Amnesic	O
probing	O
(Elazar	O
et	O
al.,	O
2020)	O
aims	O
to	O
specifically	O
remove	O
certain	O
information	O
from	O
the	O
model	O
and	O
see	O
how	O
it	O
changes	O
performance,	O
finding,	O
for	O
example,	O
that	O
language	O
modeling	O
does	O
rely	O
on	O
part-of-speech	O
information.	O
Another	O
direction	O
is	O
information-theoretic	B-TaskName
probing.	I-TaskName
Pimentel	O
et	O
al.	O
(2020)	O
operationalize	O
probing	O
as	O
estimating	O
mutual	O
information	O
between	O
the	O
learned	O
representation	O
and	O
a	O
given	O
linguistic	O
property,	O
which	O
highlights	O
that	O
the	O
focus	O
should	O
be	O
not	O
on	O
the	O
amount	O
of	O
information	O
contained	O
in	O
a	O
representation,	O
but	O
rather	O
on	O
how	O
easily	O
it	O
can	O
be	O
extracted	O
from	O
it.	O
Voita	O
and	O
Titov	O
(2020)	O
quantify	O
the	O
amount	O
of	O
effort	O
needed	O
to	O
extract	O
information	O
from	O
a	O
given	O
representation	O
as	O
minimum	O
description	O
length	O
needed	O
to	O
communicate	O
both	O
the	O
probe	O
size	O
and	O
the	O
amount	O
of	O
data	O
required	O
for	O
it	O
to	O
do	O
well	O
on	O
a	O
task.	O

Recently,	O
pretrained	O
high-capacity	O
language	O
models	O
such	O
as	O
ELMo	O
(Peters	O
et	O
al.,	O
2018a)	O
and	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2018a)	O
have	O
become	O
increasingly	O
important	O
in	O
NLP.	O
They	O
are	O
optimised	O
to	O
either	O
predict	O
the	O
next	O
word	O
in	O
a	O
sequence	O
or	O
some	O
masked	O
word	O
anywhere	O
in	O
a	O
given	O
sequence	O
(e.g.	O
"Dante	O
was	O
born	O
in	O
[Mask]	O
in	O
the	O
year	O
1265.").	O
The	O
parameters	O
of	O
these	O
models	O
appear	O
to	O
store	O
vast	O
amounts	O
of	O
linguistic	O
knowledge	O
(Peters	O
et	O
al.,	O
2018b;Goldberg,	O
2019;Tenney	O
et	O
al.,	O
2019)	O
useful	O
for	O
downstream	O
tasks.	O
This	O
knowledge	O
is	O
usually	O
accessed	O
either	O
by	O
conditioning	O
on	O
latent	O
context	O
representations	O
produced	O
by	O
the	O
original	O
model	O
or	O
by	O
using	O
the	O
original	O
model	O
weights	O
to	O
initialize	O
a	O
task-specific	O
model	O
which	O
is	O
then	O
further	O
fine-tuned.	O
This	O
type	O
of	O
knowledge	O
transfer	O
is	O
crucial	O
for	O
current	O
state-of-the-art	O
results	O
on	O
a	O
wide	O
range	O
of	O
tasks.	O
In	O
contrast,	O
knowledge	O
bases	O
are	O
effective	O
solutions	O
for	O
accessing	O
annotated	O
gold-standard	O
relational	O
data	O
by	O
enabling	O
queries	O
such	O
as	O
(Dante,	O
born-in,	O
X).	O
However,	O
in	O
practice	O
we	O
often	O
need	O
to	O
extract	O
relational	O
data	O
from	O
text	O
or	O
other	O
modalities	O
to	O
populate	O
these	O
knowledge	O
bases.	O
This	O
requires	O
complex	O
NLP	O
pipelines	O
involving	O
entity	O
extraction,	O
coreference	O
resolution,	O
entity	O
linking	O
and	O
relation	O
extraction	O
(Surdeanu	O
and	O
Ji,	O
2014)components	O
that	O
often	O
need	O
supervised	O
data	O
and	O
fixed	O
schemas.	O
Moreover,	O
errors	O
can	O
easily	O
propagate	O
and	O
accumulate	O
throughout	O
the	O
pipeline.	O
Instead,	O
we	O
could	O
attempt	O
to	O
query	O
neural	O
language	O
models	O
for	O
relational	O
data	O
by	O
asking	O
them	O
to	O
fill	O
in	O
masked	O
tokens	O
in	O
sequences	O
like	O
"Dante	O
was	O
born	O
arXiv:1909.01066v2	O
[cs.CL]	O
4	O
Sep	O
2019	O
(Petroni	O
et	O
al.,	O
2019)	O
blanks	O
(e.g.	O
"Cats	O
like	O
to	O
chase	O
[___]").	O
Petroni	O
et	O
al.	O
(2019)	O
showed	O
that,	O
for	O
some	O
relation	O
types,	O
vanilla	B-MethodName
BERT	I-MethodName
is	O
competitive	O
with	O
methods	O
relying	O
on	O
knowledge	O
bases	O
(Figure	O
2),	O
and	O
Roberts	O
et	O
al.	O
(2020)	O
show	O
the	O
same	O
for	O
open-domain	B-TaskName
QA	I-TaskName
using	O
T5	B-MethodName
model	O
(Raffel	O
et	O
al.,	O
2019).	O
Davison	O
et	O
al.	O
(2019)	O
suggest	O
that	O
it	O
generalizes	O
better	O
to	O
unseen	O
data.	O
In	O
order	O
to	O
retrieve	O
BERT's	O
knowledge,	O
we	O
need	O
good	O
template	O
sentences,	O
and	O
there	O
is	O
work	O
on	O
their	O
automatic	O
extraction	O
and	O
augmentation	O
(Bouraoui	O
et	O
al.,	O
2019;.	O
However,	O
BERT	O
cannot	O
reason	O
based	O
on	O
its	O
world	O
knowledge.	O
Forbes	O
et	O
al.	O
(2019)	O
show	O
that	O
BERT	B-MethodName
can	O
"guess"	O
the	O
affordances	O
and	O
properties	O
of	O
many	O
objects,	O
but	O
can	O
not	O
reason	O
about	O
the	O
relationship	O
between	O
properties	O
and	O
affordances.	O
For	O
example,	O
it	O
"knows"	O
that	O
people	O
can	O
walk	O
into	O
houses,	O
and	O
that	O
houses	O
are	O
big,	O
but	O
it	O
cannot	O
infer	O
that	O
houses	O
are	O
bigger	O
than	O
people.	O
and	O
Richardson	O
and	O
Sabharwal	O
(2019)	O
also	O
show	O
that	O
the	O
performance	O
drops	O
with	O
the	O
number	O
of	O
necessary	O
inference	O
steps.	O
Some	O
of	O
BERT's	B-MethodName
world	O
knowledge	O
success	O
comes	O
from	O
learning	O
stereotypical	O
associations	O
(Poerner	O
et	O
al.,	O
2019),	O
e.g.,	O
a	O
person	O
with	O
an	O
Italian-sounding	O
name	O
is	O
predicted	O
to	O
be	O
Italian,	O
even	O
when	O
it	O
is	O
incorrect.	O

Recent	O
progress	O
in	O
pretraining	O
language	O
models	O
on	O
large	O
textual	O
corpora	O
led	O
to	O
a	O
surge	O
of	O
improvements	O
for	O
downstream	O
NLP	O
tasks.	O
Whilst	O
learning	O
linguistic	O
knowledge,	O
these	O
models	O
may	O
also	O
be	O
storing	O
relational	O
knowledge	O
present	O
in	O
the	O
training	O
data,	O
and	O
may	O
be	O
able	O
to	O
answer	O
queries	O
structured	O
as	O
"fillin-the-blank"	O
cloze	O
statements.	O
Language	O
models	O
have	O
many	O
advantages	O
over	O
structured	O
knowledge	O
bases:	O
they	O
require	O
no	O
schema	O
engineering,	O
allow	O
practitioners	O
to	O
query	O
about	O
an	O
open	O
class	O
of	O
relations,	O
are	O
easy	O
to	O
extend	O
to	O
more	O
data,	O
and	O
require	O
no	O
human	O
supervision	O
to	O
train.	O
We	O
present	O
an	O
in-depth	O
analysis	O
of	O
the	O
relational	O
knowledge	O
already	O
present	O
(without	O
fine-tuning)	O
in	O
a	O
wide	O
range	O
of	O
state-of-theart	O
pretrained	O
language	O
models.	O
We	O
find	O
that	O
(i)	O
without	O
fine-tuning,	O
BERT	B-MethodName
contains	O
relational	O
knowledge	O
competitive	O
with	O
traditional	O
NLP	O
methods	O
that	O
have	O
some	O
access	O
to	O
oracle	O
knowledge,	O
(ii)	O
BERT	B-MethodName
also	O
does	O
remarkably	O
well	O
on	O
open-domain	B-TaskName
question	I-TaskName
answering	I-TaskName
against	O
a	O
supervised	O
baseline,	O
and	O
(iii)	O
certain	O
types	O
of	O
factual	O
knowledge	O
are	O
learned	O
much	O
more	O
readily	O
than	O
others	O
by	O
standard	O
language	O
model	O
pretraining	O
approaches.	O
The	O
surprisingly	O
strong	O
ability	O
of	O
these	O
models	O
to	O
recall	O
factual	O
knowledge	O
without	O
any	O
fine-tuning	O
demonstrates	O
their	O
potential	O
as	O
unsupervised	O
open-domain	O
QA	O
systems.	O
The	O
code	O
to	O
reproduce	O
our	O
analysis	O
is	O
available	O
at	O
https:	O
//github.com/facebookresearch/LAMA.	O

The	O
bulk	O
of	O
evidence	O
about	O
commonsense	O
knowledge	O
captured	O
in	O
BERT	B-MethodName
comes	O
from	O
practitioners	O
using	O
it	O
to	O
extract	O
such	O
knowledge.	O
One	O
direct	O
probing	O
study	O
of	O
BERT	B-MethodName
reports	O
that	O
BERT	B-MethodName
struggles	O
with	O
pragmatic	O
inference	O
and	O
role-based	O
event	O
knowledge	O
(Ettinger,	O
2019).	O
BERT	B-MethodName
also	O
struggles	O
with	O
abstract	O
attributes	O
of	O
objects,	O
as	O
well	O
as	O
visual	O
and	O
perceptual	O
properties	O
that	O
are	O
likely	O
to	O
be	O
assumed	O
rather	O
than	O
mentioned	O
(Da	O
and	O
Kasai,	O
2019).	O
The	O
MLM	B-TaskName
component	O
of	O
BERT	B-MethodName
is	O
easy	O
to	O
adapt	O
for	O
knowledge	O
induction	O
by	O
filling	O
in	O
the	O

To	O
date,	O
more	O
studies	O
have	O
been	O
devoted	O
to	O
BERT's	B-MethodName
knowledge	O
of	O
syntactic	O
rather	O
than	O
semantic	O
phenomena.	O
However,	O
we	O
do	O
have	O
evidence	O
from	O
an	O
MLM	B-TaskName
probing	I-TaskName
study	O
that	O
BERT	B-MethodName
has	O
some	O
knowledge	O
of	O
semantic	O
roles	O
(Ettinger,	O
2019).	O
BERT	B-MethodName
even	O
displays	O
some	O
preference	O
for	O
the	O
incorrect	O
fillers	O
for	O
semantic	O
roles	O
that	O
are	O
semantically	O
related	O
to	O
the	O
correct	O
ones,	O
as	O
opposed	O
to	O
those	O
that	O
are	O
unrelated	O
(e.g.	O
"to	O
tip	O
a	O
chef"	O
is	O
better	O
than	O
"to	O
tip	O
a	O
robin",	O
but	O
worse	O
than	O
"to	O
tip	O
a	O
waiter").	O
Tenney	O
et	O
al.	O
(2019b)	O
showed	O
that	O
BERT	O
encodes	O
information	O
about	O
entity	O
types,	O
relations,	O
semantic	O
roles,	O
and	O
proto-roles,	O
since	O
this	O
information	O
can	O
be	O
detected	O
with	O
probing	O
classifiers.	O
BERT	B-MethodName
struggles	O
with	O
representations	O
of	O
numbers.	O
Addition	O
and	O
number	O
decoding	O
tasks	O
showed	O
that	O
BERT	B-MethodName
does	O
not	O
form	O
good	O
representations	O
for	O
floating	O
point	O
numbers	O
and	O
fails	O
to	O
generalize	O
away	O
from	O
the	O
training	O
data	O
(Wallace	O
et	O
al.,	O
2019b).	O
A	O
part	O
of	O
the	O
problem	O
is	O
BERT's	B-MethodName
wordpiece	O
tokenization,	O
since	O
numbers	O
of	O
similar	O
values	O
can	O
be	O
divided	O
up	O
into	O
substantially	O
different	O
word	O
chunks.	O
Out-of-the-box	O
BERT	B-MethodName
is	O
surprisingly	O
brittle	O
to	O
named	O
entity	O
replacements:	O
e.g.	O
replacing	O
names	O
in	O
the	O
coreference	O
task	O
changes	O
85%	O
of	O
predictions	O
(Balasubramanian	O
et	O
al.,	O
2020).	O
This	O
suggests	O
that	O
the	O
model	O
does	O
not	O
actually	O
form	O
a	O
generic	O
idea	O
of	O
named	O
entities,	O
although	O
its	O
F1	B-MetricName
scores	O
on	O
NER	B-TaskName
probing	I-TaskName
tasks	O
are	O
high	O
(Tenney	O
et	O
al.,	O
2019a).	O
Broscheit	O
(2019)	O
find	O
that	O
fine-tuning	O
BERT	B-MethodName
on	O
Wikipedia	O
entity	O
linking	O
"teaches"	O
it	O
additional	O
entity	O
knowledge,	O
which	O
would	O
suggest	O
that	O
it	O
did	O
not	O
absorb	O
all	O
the	O
relevant	O
entity	O
information	O
during	O
pre-training	O
on	O
Wikipedia.	O

With	O
the	O
goal	O
of	O
exploring	O
the	O
extent	O
dependency	O
relations	O
are	O
captured	O
in	O
BERT,	B-MethodName
we	O
set	O
out	O
to	O
answer	O
the	O
following	O
question:	O
Can	O
BERT	B-MethodName
outperform	O
linguistically	O
uninformed	O
baselines	O
in	O
unsupervised	O
dependency	O
parsing?	O
If	O
so,	O
to	O
what	O
extent?	O
We	O
begin	O
by	O
using	O
the	O
token-level	B-TaskName
perturbed	I-TaskName
masking	I-TaskName
technique	O
to	O
extract	O
an	O
impact	O
matrix	O
F	O
for	O
each	O
sentence.	O
We	O
then	O
utilize	O
graph-based	O
algorithms	O
to	O
induce	O
a	O
dependency	O
tree	O
from	O
F,	O
and	O
compare	O
it	O
against	O
ground-truth	O
whose	O
annotations	O
Figure	O
1:	O
Parameter-free	O
probe	O
for	O
syntactic	O
knowledge:	O
words	O
sharing	O
syntactic	O
subtrees	O
have	O
larger	O
impact	O
on	O
each	O
other	O
in	O
the	O
MLM	B-TaskName
prediction	O
parameter-free	O
approach	O
based	O
on	O
measuring	O
the	O
impact	O
that	O
one	O
word	O
has	O
on	O
predicting	O
another	O
word	O
within	O
a	O
sequence	O
in	O
the	O
MLM	B-TaskName
task	O
(Figure	O
1).	O
They	O
concluded	O
that	O
BERT	B-MethodName
"naturally"	O
learns	O
some	O
syntactic	O
information,	O
although	O
it	O
is	O
not	O
very	O
similar	O
to	O
linguistic	O
annotated	O
resources.	O
The	O
fill-in-the-gap	O
probes	O
of	O
MLM	B-TaskName
showed	O
that	O
BERT	B-MethodName
takes	O
subject-predicate	O
agreement	O
into	O
account	O
when	O
performing	O
the	O
cloze	B-TaskName
task	O
(Goldberg,	O
2019;van	O
Schijndel	O
et	O
al.,	O
2019),	O
even	O
for	O
meaningless	O
sentences	O
and	O
sentences	O
with	O
distractor	O
clauses	O
between	O
the	O
subject	O
and	O
the	O
verb	O
(Goldberg,	O
2019).	O
A	O
study	O
of	O
negative	O
polarity	O
items	O
(NPIs)	O
by	O
Warstadt	O
et	O
al.	O
(2019)	O
showed	O
that	O
BERT	B-MethodName
is	O
better	O
able	O
to	O
detect	O
the	O
presence	O
of	O
NPIs	O
(e.g.	O
"ever")	O
and	O
the	O
words	O
that	O
allow	O
their	O
use	O
(e.g.	O
"whether")	O
than	O
scope	O
violations.	O
The	O
above	O
claims	O
of	O
syntactic	O
knowledge	O
are	O
belied	O
by	O
the	O
evidence	O
that	O
BERT	B-MethodName
does	O
not	O
"understand"	O
negation	O
and	O
is	O
insensitive	O
to	O
malformed	O
input.	O
In	O
particular,	O
its	O
predictions	O
were	O
not	O
altered	O
2	O
even	O
with	O
shuffled	O
word	O
order,	O
truncated	O
sentences,	O
removed	O
subjects	O
and	O
objects	O
(Ettinger,	O
2019).	O
This	O
could	O
mean	O
that	O
either	O
BERT's	B-MethodName
syntactic	O
knowledge	O
is	O
incomplete,	O
or	O
it	O
does	O
not	O
need	O
to	O
rely	O
on	O
it	O
for	O
solving	O
its	O
tasks.	O
The	O
latter	O
seems	O
more	O
likely,	O
since	O
Glavaš	O
and	O
Vulić	O
(2020)	O
report	O
that	O
an	O
intermediate	O
fine-tuning	O
step	O
with	O
supervised	O
parsing	O
does	O
not	O
make	O
much	O
difference	O
for	O
downstream	O
task	O
performance.	O

We	O
start	O
with	O
two	O
syntactic	O
probes	O
-dependency	O
probe	O
and	O
constituency	O
probe.	O

Fundamentally,	O
BERT	B-MethodName
is	O
a	O
stack	O
of	O
Transformer	O
encoder	O
layers	O
(Vaswani	O
et	O
al.,	O
2017)	O
which	O
consist	O
of	O
multiple	O
self-attention	O
"heads".	O
For	O
every	O
input	O
token	O
in	O
a	O
sequence,	O
each	O
head	O
computes	O
key,	O
value	O
and	O
query	O
vectors,	O
used	O
to	O
create	O
a	O
weighted	O
representation.	O
The	O
outputs	O
of	O
all	O
heads	O
in	O
the	O
same	O
layer	O
are	O
combined	O
and	O
run	O
through	O
a	O
fully-connected	O
layer.	O
Each	O
layer	O
is	O
wrapped	O
with	O
a	O
skip	O
connection	O
and	O
followed	O
by	O
layer	O
normalization.	O
The	O
conventional	O
workflow	O
for	O
BERT	B-MethodName
consists	O
of	O
two	O
stages:	O
pre-training	O
and	O
fine-tuning.	O
Pretraining	O
uses	O
two	O
self-supervised	O
tasks:	O
masked	B-TaskName
language	I-TaskName
modeling	I-TaskName
(MLM,	B-TaskName
prediction	O
of	O
randomly	O
masked	O
input	O
tokens)	O
and	O
next	B-TaskName
sentence	I-TaskName
prediction	I-TaskName
(NSP,	B-TaskName
predicting	O
if	O
two	O
input	O
sentences	O
are	O
adjacent	O
to	O
each	O
other).	O
In	O
fine-tuning	O
for	O
downstream	O
applications,	O
one	O
or	O
more	O
fully-connected	O
layers	O
are	O
typically	O
added	O
on	O
top	O
of	O
the	O
final	O
encoder	O
layer.	O
The	O
input	O
representations	O
are	O
computed	O
as	O
follows:	O
each	O
word	O
in	O
the	O
input	O
is	O
first	O
tokenized	O
into	O
wordpieces	O
(Wu	O
et	O
al.,	O
2016),	O
and	O
then	O
three	O
embedding	O
layers	O
(token,	O
position,	O
and	O
segment)	O
are	O
combined	O
to	O
obtain	O
a	O
fixed-length	O
vector.	O
Special	O
token	O
[CLS]	O
is	O
used	O
for	O
classification	O
predictions,	O
and	O
[SEP]	O
separates	O
input	O
segments.	O
Google	O
1	O
and	O
HuggingFace	O
(Wolf	O
et	O
al.,	O
2020)	O
provide	O
many	O
variants	O
of	O
BERT,	B-MethodName
including	O
the	O
original	O
"base"	O
and	O
"large"	O
versions.	O
They	O
vary	O
in	O
the	O
number	O
of	O
heads,	O
layers,	O
and	O
hidden	O
state	O
size.	O
3	O
What	O
knowledge	O
does	O
BERT	B-MethodName
have?	O
A	O
number	O
of	O
studies	O
have	O
looked	O
at	O
the	O
knowledge	O
encoded	O
in	O
BERT	B-MethodName
weights.	O
The	O
popular	O
approaches	O
include	O
fill-in-the-gap	O
probes	O
of	O
MLM,	B-TaskName
analysis	O
of	O
self-attention	O
weights,	O
and	O
probing	O
classifiers	O
with	O
different	O
BERT	B-MethodName
representations	O
as	O
inputs.	O
3.1	O
Syntactic	O
knowledge	O
showed	O
that	O
BERT	B-MethodName
representations	O
are	O
hierarchical	O
rather	O
than	O
linear,	O
i.e.	O
there	O
is	O
something	O
akin	O
to	O
syntactic	O
tree	O
structure	O
in	O
addition	O
to	O
the	O
word	O
order	O
information.	O
Tenney	O
et	O
al.	O
(2019b)	O
and	O
also	O
showed	O
that	O
BERT	B-MethodName
embeddings	O
encode	O
information	O
about	O
parts	O
of	O
speech,	O
syntactic	O
chunks	O
and	O
roles.	O
Enough	O
syntactic	O
information	O
seems	O
to	O
be	O
captured	O
in	O
the	O
token	O
embeddings	O
themselves	O
to	O
recover	O
syntactic	O
trees	O
(Vilares	O
et	O
al.,	O
2020;Kim	O
et	O
al.,	O
2020;Rosa	O
and	O
Mareček,	O
2019),	O
although	O
probing	O
classifiers	O
could	O
not	O
recover	O
the	O
labels	O
of	O
distant	O
parent	O
nodes	O
in	O
the	O
syntactic	O
tree	O
.	O
Warstadt	O
and	O
Bowman	O
(2020)	O
report	O
evidence	O
of	O
hierarchical	O
structure	O
in	O
three	O
out	O
of	O
four	O
probing	O
tasks.	O
As	O
far	O
as	O
how	O
syntax	O
is	O
represented,	O
it	O
seems	O
that	O
syntactic	O
structure	O
is	O
not	O
directly	O
encoded	O
in	O
self-attention	O
weights.	O
Htut	O
et	O
al.	O
(2019)	O
were	O
unable	O
to	O
extract	O
full	O
parse	O
trees	O
from	O
BERT	B-MethodName
heads	O
even	O
with	O
the	O
gold	O
annotations	O
for	O
the	O
root.	O
Jawahar	O
et	O
al.	O
(2019)	O
include	O
a	O
brief	O
illustration	O
of	O
a	O
dependency	O
tree	O
extracted	O
directly	O
from	O
self-attention	O
weights,	O
but	O
provide	O
no	O
quantitative	O
evaluation.	O
However,	O
syntactic	O
information	O
can	O
be	O
recovered	O
from	O
BERT	B-MethodName
token	O
representations.	O
Hewitt	O
and	O
Manning	O
(2019)	O
were	O
able	O
to	O
learn	O
transformation	O
matrices	O
that	O
successfully	O
recovered	O
syntactic	O
dependencies	O
in	O
PennTreebank	O
data	O
from	O
BERT's	B-MethodName
token	O
embeddings	O
(see	O
also	O
.	O
Jawahar	O
et	O
al.	O
(2019)	O
experimented	O
with	O
transformations	O
of	O
the	O
[CLS]	O
token	O
using	O
Tensor	O
Product	O
Decomposition	O
Networks	O
(McCoy	O
et	O
al.,	O
2019a),	O
concluding	O
that	O
dependency	O
trees	O
are	O
the	O
best	O
match	O
among	O
5	O
decomposition	O
schemes	O
(although	O
the	O
reported	O
MSE	B-MetricName
differences	O
are	O
very	O
small).	O
Miaschi	O
and	O
Dell'Orletta	O
(2020)	O
performs	O
a	O
range	O
of	O
syntactic	O
probing	O
experiments	O
with	O
concatenated	O
token	O
representations	O
as	O
input.	O
Note	O
that	O
all	O
these	O
approaches	O
look	O
for	O
the	O
evidence	O
of	O
gold-standard	O
linguistic	O
structures,	O
and	O
add	O
some	O
amount	O
of	O
extra	O
knowledge	O
to	O
the	O
probe.	O
Most	O
recently,	O
proposed	O
a	O
4168	O
3	O
Visualization	O
with	O
Impact	O
Maps	O
Before	O
we	O
discuss	O
specific	O
syntactic	O
phenomena,	O
let	O
us	O
first	O
analyze	O
some	O
example	O
impact	O
matrices	O
derived	O
from	O
sample	O
sentences.	O
We	O
visualize	O
an	O
impact	O
matrix	O
of	O
a	O
sentence	O
by	O
displaying	O
a	O
heatmap.	O
We	O
use	O
the	O
term	O
"impact	O
map"	O
to	O
refer	O
to	O
a	O
heatmap	O
of	O
an	O
impact	O
matrix.	O
Setup.	O
We	O
extract	O
impact	O
matrices	O
by	O
feeding	O
BERT	B-MethodName
with	O
1,000	O
sentences	O
from	O
the	O
English	B-DatasetName
Parallel	I-DatasetName
Universal	I-DatasetName
Dependencies	I-DatasetName
(PUD)	B-DatasetName
treebank	O
of	O
the	O
CoNLL	O
2017	O
Shared	O
Task	O
(Zeman	O
et	O
al.,	O
2017).	O
We	O
follow	O
the	O
setup	O
and	O
pre-processing	O
steps	O
employed	O
in	O
pre-training	O
BERT.	B-MethodName
An	O
example	O
impact	O
map	O
is	O
shown	O
in	O
Figure	O
1.	O
Dependency.	O
We	O
notice	O
that	O
the	O
impact	O
map	O
contains	O
many	O
stripes,	O
which	O
are	O
short	O
series	O
of	O
vertical/horizontal	O
cells,	O
typically	O
located	O
along	O
the	O
diagonal.	O
Take	O
the	O
word	O
"different"	O
as	O
an	O
example	O
(which	O
is	O
illustrated	O
by	O
the	O
second-to-last	O
column	O
in	O
the	O
impact	O
matrix).	O
We	O
observe	O
a	O
clear	O
vertical	O
stripe	O
above	O
the	O
main	O
diagonal.	O
The	O
interpretation	O
is	O
that	O
this	O
particular	O
occurrence	O
of	O
the	O
word	O
"different"	O
strongly	O
affects	O
the	O
occurrences	O
of	O
those	O
words	O
before	O
it.	O
These	O
strong	O
influences	O
are	O
shown	O
by	O
the	O
darker-colored	O
pixels	O
seen	O
in	O
the	O
second	O
last	O
column	O
of	O
the	O
impact	O
map.	O
This	O
observation	O
agrees	O
with	O
the	O
ground-truth	O
dependency	O
tree,	O
which	O
selects	O
"different"	O
as	O
the	O
head	O
of	O
all	O
remaining	O
words	O
in	O
the	O
phrase	O
"this	O
will	O
be	O
a	O
little	O
different."	O
We	O
also	O
observe	O
similar	O
patterns	O
on	O
"transitions"	O
and	O
"Hill".	O
Such	O
correlations	O
lead	O
us	O
to	O
explore	O
the	O
idea	O
of	O
extracting	O
dependency	O
trees	O
from	O
the	O
matrices	O
(see	O
Section	O
4.1).	O
3	O
Visualization	O
with	O
Impact	O
Maps	O
Before	O
we	O
discuss	O
specific	O
syntactic	O
phenomena,	O
let	O
us	O
first	O
analyze	O
some	O
example	O
impact	O
matrices	O
derived	O
from	O
sample	O
sentences.	O
We	O
visualize	O
an	O
impact	O
matrix	O
of	O
a	O
sentence	O
by	O
displaying	O
a	O
heatmap.	O
We	O
use	O
the	O
term	O
"impact	O
map"	O
to	O
refer	O
to	O
a	O
heatmap	O
of	O
an	O
impact	O
matrix.	O
Setup.	O
We	O
extract	O
impact	O
matrices	O
by	O
feeding	O
BERT	B-MethodName
with	O
1,000	O
sentences	O
from	O
the	O
English	B-DatasetName
Parallel	I-DatasetName
Universal	I-DatasetName
Dependencies	I-DatasetName
(PUD)	B-DatasetName
treebank	O
of	O
the	O
CoNLL	B-TaskName
2017	I-TaskName
Shared	I-TaskName
Task	I-TaskName
(Zeman	O
et	O
al.,	O
2017).	O
We	O
follow	O
the	O
setup	O
and	O
pre-processing	O
steps	O
employed	O
in	O
pre-training	O
BERT.	B-MethodName
An	O
example	O
impact	O
map	O
is	O
shown	O
in	O
Figure	O
1.	O
Dependency.	O
We	O
notice	O
that	O
the	O
impact	O
map	O
contains	O
many	O
stripes,	O
which	O
are	O
short	O
series	O
of	O
vertical/horizontal	O
cells,	O
typically	O
located	O
along	O
the	O
diagonal.	O
Take	O
the	O
word	O
"different"	O
as	O
an	O
example	O
(which	O
is	O
illustrated	O
by	O
the	O
second-to-last	O
column	O
in	O
the	O
impact	O
matrix).	O
We	O
observe	O
a	O
clear	O
vertical	O
stripe	O
above	O
the	O
main	O
diagonal.	O
The	O
interpretation	O
is	O
that	O
this	O
particular	O
occurrence	O
of	O
the	O
word	O
"different"	O
strongly	O
affects	O
the	O
occurrences	O
of	O
those	O
words	O
before	O
it.	O
These	O
strong	O
influences	O
are	O
shown	O
by	O
the	O
darker-colored	O
pixels	O
seen	O
in	O
the	O
second	O
last	O
column	O
of	O
the	O
impact	O
map.	O
This	O
observation	O
agrees	O
with	O
the	O
ground-truth	O
dependency	O
tree,	O
which	O
selects	O
"different"	O
as	O
the	O
head	O
of	O
all	O
remaining	O
words	O
in	O
the	O
phrase	O
"this	O
will	O
be	O
a	O
little	O
different."	O
We	O
also	O
observe	O
similar	O
patterns	O
on	O
"transitions"	O
and	O
"Hill".	O
Such	O
correlations	O
lead	O
us	O
to	O
explore	O
the	O
idea	O
of	O
extracting	O
dependency	O
trees	O
from	O
the	O
matrices	O
(see	O
Section	O
4.1).	O
Constituency.	O
Figure	O
2	O
shows	O
part	O
of	O
the	O
constituency	O
tree	O
of	O
our	O
example	O
sentence	O
generated	O
by	O
Stanford	O
CoreNLP	O
(Manning	O
et	O
al.,	O
2014).	O
In	O
this	O
sentence,	O
"media"	O
and	O
"on"	O
are	O
two	O
words	O
that	O
are	O
adjacent	O
to	O
"transitions".	O
From	O
the	O
tree,	O
however,	O
we	O
see	O
that	O
"media"	O
is	O
closer	O
to	O
"transitions"	O
than	O
"on"	O
is	O
in	O
terms	O
of	O
syntactic	O
distance.	O
If	O
a	O
model	O
is	O
syntactically	O
uninformed,	O
we	O
would	O
expect	O
"media"	O
and	O
"on"	O
to	O
have	O
comparable	O
impacts	O
on	O
the	O
prediction	O
of	O
"transitions",	O
and	O
vice	O
versa.	O
However,	O
we	O
observe	O
a	O
far	O
greater	O
impact	O
(darker	O
color)	O
between	O
"media"	O
and	O
"transitions"	O
than	O
that	O
between	O
"on"	O
and	O
"transitions".	O
We	O
will	O
further	O
support	O
this	O
observation	O
with	O
empirical	O
experiments	O
in	O
Section	O
4.2.	O
Other	O
Structures.	O
Along	O
the	O
diagonal	O
of	O
the	O
impact	O
map,	O
we	O
see	O
that	O
words	O
are	O
grouped	O
into	O
four	O
contiguous	O
chunks	O
that	O
have	O
specific	O
intents	O
(e.g.,	O
a	O
noun	O
phrase	O
-on	O
Capitol	O
Hill).	O
We	O
also	O
observe	O
that	O
the	O
two	O
middle	O
chunks	O
have	O
relatively	O
strong	O
inter-chunk	O
word	O
impacts	O
and	O
thus	O
a	O
bonding	O
that	O
groups	O
them	O
together,	O
forming	O
a	O
larger	O
verb	O
phrase.	O
This	O
observation	O
suggest	O
that	O
BERT	B-MethodName
may	O
capture	O
the	O
compositionality	O
of	O
the	O
language.	O
In	O
the	O
following	O
sections	O
we	O
quantitatively	O
evaluate	O
these	O
observations.	O

Since	O
their	O
introduction	O
in	O
2017,	O
Transformers	O
(Vaswani	O
et	O
al.,	O
2017)	O
have	O
taken	O
NLP	O
by	O
storm,	O
offering	O
enhanced	O
parallelization	O
and	O
better	O
modeling	O
of	O
long-range	O
dependencies.	O
The	O
best	O
known	O
Transformer-based	O
model	O
is	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019);	O
it	O
obtained	O
state-of-the-art	O
results	O
in	O
numerous	O
benchmarks	O
and	O
is	O
still	O
a	O
must-have	O
baseline.	O
While	O
it	O
is	O
clear	O
that	O
BERT	B-MethodName
works	O
remarkably	O
well,	O
it	O
is	O
less	O
clear	O
why,	O
which	O
limits	O
further	O
hypothesis-driven	O
improvement	O
of	O
the	O
architecture.	O
Unlike	O
CNNs,	O
the	O
Transformers	O
have	O
little	O
cognitive	O
motivation,	O
and	O
the	O
size	O
of	O
these	O
models	O
limits	O
our	O
ability	O
to	O
experiment	O
with	O
pre-training	O
and	O
perform	O
ablation	O
studies.	O
This	O
explains	O
a	O
large	O
number	O
of	O
studies	O
over	O
the	O
past	O
year	O
that	O
attempted	O
to	O
understand	O
the	O
reasons	O
behind	O
BERT's	B-MethodName
performance.	O
In	O
this	O
paper,	O
we	O
provide	O
an	O
overview	O
of	O
what	O
has	O
been	O
learned	O
to	O
date,	O
highlighting	O
the	O
questions	O
which	O
are	O
still	O
unresolved.	O
We	O
first	O
consider	O
the	O
linguistic	O
aspects	O
of	O
it,	O
i.e.,	O
the	O
current	O
evidence	O
regarding	O
the	O
types	O
of	O
linguistic	O
and	O
world	O
knowledge	O
learned	O
by	O
BERT,	B-MethodName
as	O
well	O
as	O
where	O
and	O
how	O
this	O
knowledge	O
may	O
be	O
stored	O
in	O
the	O
model.	O
We	O
then	O
turn	O
to	O
the	O
technical	O
aspects	O
of	O
the	O
model	O
and	O
provide	O
an	O
overview	O
of	O
the	O
current	O
proposals	O
to	O
improve	O
BERT's	B-MethodName
architecture,	O
pre-training	O
and	O
finetuning.	O
We	O
conclude	O
by	O
discussing	O
the	O
issue	O
of	O
overparameterization,	O
the	O
approaches	O
to	O
compressing	O
BERT,	B-MethodName
and	O
the	O
nascent	O
area	O
of	O
pruning	O
as	O
a	O
model	O
analysis	O
technique.	O

Transformer-based	O
models	O
have	O
pushed	O
state	O
of	O
the	O
art	O
in	O
many	O
areas	O
of	O
NLP,	O
but	O
our	O
understanding	O
of	O
what	O
is	O
behind	O
their	O
success	O
is	O
still	O
limited.	O
This	O
paper	O
is	O
the	O
first	O
survey	O
of	O
over	O
150	O
studies	O
of	O
the	O
popular	O
BERT	B-MethodName
model.	O
We	O
review	O
the	O
current	O
state	O
of	O
knowledge	O
about	O
how	O
BERT	B-MethodName
works,	O
what	O
kind	O
of	O
information	O
it	O
learns	O
and	O
how	O
it	O
is	O
represented,	O
common	O
modifications	O
to	O
its	O
training	O
objectives	O
and	O
architecture,	O
the	O
overparameterization	O
issue	O
and	O
approaches	O
to	O
compression.	O
We	O
then	O
outline	O
directions	O
for	O
future	O
research.	O

Task-specific	O
distillation	O
Most	O
of	O
the	O
prior	O
works	O
focus	O
on	O
building	O
task-specific	O
distillation	O
setups.	O
Tang	O
et	O
al.	O
transfer	O
fine-tune	O
classification	O
model	O
BERT	B-MethodName
to	O
an	O
LSTM-based	B-MethodName
classifier.	O
Chatterjee	O
distill	B-MethodName
BERT	I-MethodName
model	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
in	O
a	O
smaller	O
Transformer	B-MethodName
model	O
previously	O
initialized	O
from	O
BERT.	B-MethodName
In	O
the	O
present	O
work,	O
we	O
found	O
it	O
beneficial	O
to	O
use	O
a	O
general-purpose	O
pre-training	O
distillation	O
rather	O
than	O
a	O
task-specific	O
distillation.	O
Turc	O
et	O
al.	O
use	O
the	O
original	O
pretraining	O
objective	O
to	O
train	O
smaller	O
student,	O
then	O
fine-tuned	O
via	O
distillation.	B-MethodName
As	O
shown	O
in	O
the	O
ablation	O
study,	O
we	O
found	O
it	O
beneficial	O
to	O
leverage	O
the	O
teacher's	O
knowledge	O
to	O
pre-train	O
with	O
additional	O
distillation	O
signal.	O
Multi-distillation	O
Yang	O
et	O
al.	O
combine	O
the	O
knowledge	O
of	O
an	O
ensemble	O
of	O
teachers	O
using	O
multi-task	O
learning	O
to	O
regularize	O
the	O
distillation.	O
The	O
authors	O
apply	O
Multi-Task	O
Knowledge	O
Distillation	O
to	O
learn	O
a	O
compact	O
question	O
answering	O
model	O
from	O
a	O
set	O
of	O
large	O
question	B-TaskName
answering	I-TaskName
models.	O
An	O
application	O
of	O
multi-distillation	O
is	O
multi-linguality:	B-TaskName
Tsai	O
et	O
al.	O
adopts	O
a	O
similar	O
approach	O
to	O
us	O
by	O
pre-training	O
a	O
multilingual	O
model	O
from	O
scratch	O
solely	O
through	O
distillation.	O
However,	O
as	O
shown	O
in	O
the	O
ablation	O
study,	O
leveraging	O
the	O
teacher's	O
knowledge	O
with	O
initialization	O
and	O
additional	O
losses	O
leads	O
to	O
substantial	O
gains.	O
Other	O
compression	O
techniques	O
have	O
been	O
studied	O
to	O
compress	O
large	O
models.	O
Recent	O
developments	O
in	O
weights	O
pruning	O
reveal	O
that	O
it	O
is	O
possible	O
to	O
remove	O
some	O
heads	O
in	O
the	O
self-attention	O
at	O
test	O
time	O
without	O
significantly	O
degrading	O
the	O
performance	O
Michel	O
et	O
al.	O
.	O
Some	O
layers	O
can	O
be	O
reduced	O
to	O
one	O
head.	O
A	O
separate	O
line	O
of	O
study	O
leverages	O
quantization	O
to	O
derive	O
smaller	O
models	O
(Gupta	O
et	O
al.	O
).	O
Pruning	O
and	O
quantization	O
are	O
orthogonal	O
to	O
the	O
present	O
work.	O

We	O
introduced	O
DistilBERT,	B-MethodName
a	O
general-purpose	O
pre-trained	O
version	O
of	O
BERT,	B-MethodName
40%	B-MetricValue
smaller,	O
60%	B-MetricValue
faster,	O
that	O
retains	O
97%	B-MetricValue
of	O
the	O
language	O
understanding	O
capabilities.	O
We	O
showed	O
that	O
a	O
general-purpose	O
language	O
model	O
can	O
be	O
successfully	O
trained	O
with	O
distillation	O
and	O
analyzed	O
the	O
various	O
components	O
with	O
an	O
ablation	O
study.	O
We	O
further	O
demonstrated	O
that	O
DistilBERT	B-MethodName
is	O
a	O
compelling	O
option	O
for	O
edge	O
applications.	O

In	O
this	O
section,	O
we	O
investigate	O
the	O
influence	O
of	O
various	O
components	O
of	O
the	O
triple	O
loss	O
and	O
the	O
student	O
initialization	O
on	O
the	O
performances	O
of	O
the	O
distilled	O
model.	O
We	O
report	O
the	O
macro-score	O
on	O
GLUE.	B-DatasetName
Table	O
4	O
presents	O
the	O
deltas	O
with	O
the	O
full	O
triple	O
loss:	O
removing	O
the	O
Masked	B-MetricName
Language	I-MetricName
Modeling	I-MetricName
loss	I-MetricName
has	O
little	O
impact	O
while	O
the	O
two	O
distillation	O
losses	O
account	O
for	O
a	O
large	O
portion	O
of	O
the	O
performance.	O

To	O
further	O
investigate	O
the	O
speed-up/size	O
trade-off	O
of	O
DistilBERT,	B-MethodName
we	O
compare	O
(in	O
Table	O
3)	O
the	O
number	O
of	O
parameters	O
of	O
each	O
model	O
along	O
with	O
the	O
inference	O
time	O
needed	O
to	O
do	O
a	O
full	O
pass	O
on	O
the	O
STS-B	B-DatasetName
development	O
set	O
on	O
CPU	O
(Intel	O
Xeon	O
E5-2690	O
v3	O
Haswell	O
@2.9GHz)	O
using	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1.	B-HyperparameterValue
DistilBERT	B-MethodName
has	O
40%	B-MetricValue
fewer	O
parameters	O
than	O
BERT	B-MethodName
and	O
is	O
60%	B-MetricValue
faster	O
than	O
BERT.	B-MethodName
On	O
device	O
computation	O
We	O
studied	O
whether	O
DistilBERT	B-MethodName
could	O
be	O
used	O
for	O
on-the-edge	O
applications	O
by	O
building	O
a	O
mobile	O
application	O
for	O
question	O
answering.	O
We	O
compare	O
the	O
average	O
inference	O
time	O
on	O
a	O
recent	O
smartphone	O
(iPhone	O
7	O
Plus)	O
against	O
our	O
previously	O
trained	O
question	O
answering	O
model	O
based	O
on	O
BERT-base.	B-MethodName
Excluding	O
the	O
tokenization	O
step,	O
DistilBERT	B-MethodName
is	O
71%	B-MetricValue
faster	O
than	O
BERT,	B-MethodName
and	O
the	O
whole	O
model	O
weighs	O
207	O
MB	O
(which	O
could	O
be	O
further	O
reduced	O
with	O
quantization).	O
Our	O
code	O
is	O
available	O
5	O
.	O

Downstream	O
tasks	O
We	O
further	O
study	O
the	O
performances	O
of	O
DistilBERT	B-MethodName
on	O
several	O
downstream	O
tasks	O
under	O
efficient	O
inference	O
constraints:	O
a	O
classification	O
task	O
(IMDb	O
sentiment	O
classification	O
-	O
Maas	O
et	O
al.	O
)	O
and	O
a	O
question	O
answering	O
task	O
(SQuAD	B-DatasetName
v1.1	O
-Rajpurkar	O
et	O
al.	O
).	O
As	O
shown	O
in	O
Table	O
2,	O
DistilBERT	B-MethodName
is	O
only	O
0.6%	B-MetricValue
point	O
behind	O
BERT	B-MethodName
in	O
test	O
accuracy	B-MetricName
on	O
the	O
IMDb	B-DatasetName
benchmark	O
while	O
being	O
40%	B-MetricValue
smaller.	O
On	O
SQuAD,	B-DatasetName
DistilBERT	B-MethodName
is	O
within	O
3.9	B-MetricValue
points	I-MetricValue
of	O
the	O
full	O
BERT.	B-MethodName
We	O
also	O
studied	O
whether	O
we	O
could	O
add	O
another	O
step	O
of	O
distillation	B-MethodName
during	O
the	O
adaptation	O
phase	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
on	O
SQuAD	B-DatasetName
using	O
a	O
BERT	B-MethodName
model	O
previously	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
as	O
a	O
∅	O
-L	O
cos	O
-L	O
mlm	O
-2.96	O
L	O
ce	O
-∅	O
-L	O
mlm	O
-1.46	O
L	O
ce	O
-L	O
cos	O
-∅	O
-0.	O
31	O
Triple	B-MetricName
loss	I-MetricName
+	O
random	O
weights	O
initialization	O
-3.69	O
teacher	O
for	O
an	O
additional	O
term	O
in	O
the	O
loss	O
(knowledge	O
distillation).	O
In	O
this	O
setting,	O
there	O
are	O
thus	O
two	O
successive	O
steps	O
of	O
distillation,	B-MethodName
one	O
during	O
the	O
pre-training	O
phase	O
and	O
one	O
during	O
the	O
adaptation	O
phase.	O
In	O
this	O
case,	O
we	O
were	O
able	O
to	O
reach	O
interesting	O
performances	O
given	O
the	O
size	O
of	O
the	O
model:	O
79.8	O
F1	O
and	O
70.4	O
EM,	O
i.e.	O
within	O
3	O
points	O
of	O
the	O
full	O
model.	O

General	O
Language	O
Understanding	O
We	O
assess	O
the	O
language	O
understanding	O
and	O
generalization	O
capabilities	O
of	B-MethodName
DistilBERT	I-MethodName
on	O
the	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	I-DatasetName
[Wang	O
et	O
al.,	O
2018],	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
We	O
report	O
scores	O
on	O
the	O
development	O
sets	O
for	O
each	O
task	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
without	O
the	O
use	O
of	O
ensembling	O
or	O
multi-tasking	O
scheme	O
for	O
fine-tuning	O
(which	O
are	O
mostly	O
orthogonal	O
to	O
the	O
present	O
work).	O
We	O
compare	O
the	O
results	O
to	O
the	O
baseline	O
provided	O
by	O
the	O
authors	O
of	O
GLUE:	B-DatasetName
an	O
ELMo	B-MethodName
(Peters	O
et	O
al.	O
)	O
encoder	O
followed	O
by	O
two	O
BiLSTMs.	B-MethodName
4	O
The	O
results	O
on	O
each	O
of	O
the	O
9	O
tasks	O
are	O
showed	O
on	O
Table	O
1	O
along	O
with	O
the	O
macro-score	O
(average	O
of	O
individual	O
scores).	O
Among	O
the	O
9	O
tasks,	O
DistilBERT	B-MethodName
is	O
always	O
on	O
par	O
or	O
improving	O
over	O
the	O
ELMo	B-MethodName
baseline	O
(up	O
to	O
19	B-MetricValue
points	I-MetricValue
of	O
accuracy	B-MetricName
on	O
STS-B).	B-DatasetName
DistilBERT	B-MethodName
also	O
compares	O
surprisingly	O
well	O
to	O
BERT,	B-MethodName
retaining	O
97%	B-MetricValue
of	O
the	O
performance	B-MetricName
with	O
40%	B-MetricValue
fewer	O
parameters.	O

In	O
addition	O
to	O
the	O
previously	O
described	O
optimization	O
and	O
architectural	O
choices,	O
an	O
important	O
element	O
in	O
our	O
training	O
procedure	O
is	O
to	O
find	O
the	O
right	O
initialization	O
for	O
the	O
sub-network	O
to	O
converge.	O
Taking	O
advantage	O
of	O
the	O
common	O
dimensionality	O
between	O
teacher	O
and	O
student	O
networks,	O
we	O
initialize	O
the	O
student	O
from	O
the	O
teacher	O
by	O
taking	O
one	O
layer	O
out	O
of	O
two.	O

Knowledge	B-MethodName
distillation	I-MethodName
[Bucila	O
et	O
al.,	O
2006,	O
Hinton	O
et	O
al.,	O
2015]	O
is	O
a	O
compression	O
technique	O
in	O
which	O
a	O
compact	O
model	O
-the	O
student	O
-is	O
trained	O
to	O
reproduce	O
the	O
behaviour	O
of	O
a	O
larger	O
model	O
-the	O
teacheror	O
an	O
ensemble	O
of	O
models.	O
In	O
supervised	O
learning,	O
a	O
classification	O
model	O
is	O
generally	O
trained	O
to	O
predict	O
an	O
instance	O
class	O
by	O
maximizing	O
the	O
estimated	O
probability	O
of	O
gold	O
labels.	O
A	O
standard	O
training	O
objective	O
thus	O
involves	O
minimizing	O
the	O
cross-entropy	O
between	O
the	O
model's	O
predicted	O
distribution	O
and	O
the	O
one-hot	O
empirical	O
distribution	O
of	O
training	O
labels.	O
A	O
model	O
performing	O
well	O
on	O
the	O
training	O
set	O
will	O
predict	O
an	O
output	O
distribution	O
with	O
high	O
probability	O
on	O
the	O
correct	O
class	O
and	O
with	O
near-zero	O
probabilities	O
on	O
other	O
classes.	O
But	O
some	O
of	O
these	O
"near-zero"	O
probabilities	O
are	O
larger	O
than	O
others	O
and	O
reflect,	O
in	O
part,	O
the	O
generalization	O
capabilities	O
of	O
the	O
model	O
and	O
how	O
well	O
it	O
will	O
perform	O
on	O
the	O
test	O
set	O
3	O
.	O
Training	O
loss	O
The	O
student	O
is	O
trained	O
with	O
a	O
distillation	B-MetricName
loss	I-MetricName
over	O
the	O
soft	O
target	O
probabilities	O
of	O
the	O
teacher:	O
L	O
ce	O
=	O
i	O
t	O
i	O
*	O
log(s	O
i	O
)	O
where	O
t	O
i	O
(resp.	O
s	O
i	O
)	O
is	O
a	O
probability	O
estimated	O
by	O
the	O
teacher	O
(resp.	O
the	O
student).	O
This	O
objective	O
results	O
in	O
a	O
rich	O
training	O
signal	O
by	O
leveraging	O
the	O
full	O
teacher	O
distribution.	O
Following	O
Hinton	O
et	O
al.	O
we	O
used	O
a	O
softmax-temperature:	B-HyperparameterName
p	O
i	O
=	O
exp(zi/T	O
)	O
j	O
exp(zj	O
/T	O
)	O
where	O
T	B-HyperparameterName
controls	O
the	O
smoothness	O
of	O
the	O
output	O
distribution	O
and	O
z	O
i	O
is	O
the	O
model	O
score	O
for	O
the	O
class	O
i.	O
The	O
same	O
temperature	O
T	O
is	O
applied	O
to	O
the	O
student	O
and	O
the	O
teacher	O
at	O
training	O
time,	O
while	O
at	O
inference,	O
T	B-HyperparameterName
is	O
set	O
to	O
1	B-HyperparameterValue
to	O
recover	O
a	O
standard	O
softmax.	O
The	O
final	O
training	O
objective	O
is	O
a	O
linear	O
combination	O
of	O
the	O
distillation	B-MetricName
loss	I-MetricName
L	I-MetricName
ce	O
with	O
the	O
supervised	O
training	O
loss,	O
in	O
our	O
case	O
the	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
loss	I-MetricName
L	O
mlm	O
[Devlin	O
et	O
al.,	O
2018].	O
We	O
found	O
it	O
beneficial	O
to	O
add	O
a	O
cosine	B-MetricName
embedding	I-MetricName
loss	I-MetricName
(L	I-MetricName
cos	I-MetricName
)	I-MetricName
which	O
will	O
tend	O
to	O
align	O
the	O
directions	O
of	O
the	O
student	O
and	O
teacher	O
hidden	O
states	O
vectors.	O
3	O
DistilBERT:	B-MethodName
a	O
distilled	O
version	O
of	O
BERT	B-MethodName
Student	O
architecture	O
In	O
the	O
present	O
work,	O
the	O
student	O
-DistilBERT	B-MethodName
-has	O
the	O
same	O
general	O
architecture	O
as	O
BERT.	B-MethodName
The	O
token-type	O
embeddings	O
and	O
the	O
pooler	O
are	O
removed	O
while	O
the	O
number	O
of	O
layers	O
is	O
reduced	O
by	O
a	O
factor	O
of	O
2.	O
Most	O
of	O
the	O
operations	O
used	O
in	O
the	O
Transformer	B-MethodName
architecture	O
(linear	O
layer	O
and	O
layer	O
normalisation)	O
are	O
highly	O
optimized	O
in	O
modern	O
linear	O
algebra	O
frameworks	O
and	O
our	O
investigations	O
showed	O
that	O
variations	O
on	O
the	O
last	O
dimension	O
of	O
the	O
tensor	O
(hidden	O
size	O
dimension)	O
have	O
a	O
smaller	O
impact	O
on	O
computation	O
efficiency	O
(for	O
a	O
fixed	O
parameters	O
budget)	O
than	O
variations	O
on	O
other	O
factors	O
like	O
the	O
number	O
of	O
layers.	O
Thus	O
we	O
focus	O
on	O
reducing	O
the	O
number	O
of	O
layers.	O

The	O
last	O
two	O
years	O
have	O
seen	O
the	O
rise	O
of	O
Transfer	B-MethodName
Learning	I-MethodName
approaches	O
in	O
Natural	O
Language	O
Processing	O
(NLP)	O
with	O
large-scale	O
pre-trained	O
language	O
models	O
becoming	O
a	O
basic	O
tool	O
in	O
many	O
NLP	O
tasks	O
[Devlin	O
et	O
al.,	O
2018,	O
Radford	O
et	O
al.,	O
2019.	O
While	O
these	O
models	O
lead	O
to	O
significant	O
improvement,	O
they	O
often	O
have	O
several	O
hundred	O
million	O
parameters	O
and	O
current	O
research	O
1	O
on	O
pre-trained	O
models	O
indicates	O
that	O
training	O
even	O
larger	O
models	O
still	O
leads	O
to	O
better	O
performances	O
on	O
downstream	O
tasks.	O
The	O
trend	O
toward	O
bigger	O
models	O
raises	O
several	O
concerns.	O
First	O
is	O
the	O
environmental	O
cost	O
of	O
exponentially	O
scaling	O
these	O
models'	O
computational	O
requirements	O
as	O
mentioned	O
in	O
Schwartz	O
et	O
al.	O
,	O
Strubell	O
et	O
al.	O
.	O
Second,	O
while	O
operating	O
these	O
models	O
on-device	O
in	O
real-time	O
has	O
the	O
potential	O
to	O
enable	O
novel	O
and	O
interesting	O
language	O
processing	O
applications,	O
the	O
growing	O
computational	O
and	O
memory	O
requirements	O
of	O
these	O
models	O
may	O
hamper	O
wide	O
adoption.	O
In	O
this	O
paper,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
reach	O
similar	O
performances	O
on	O
many	O
downstream-tasks	O
using	O
much	O
smaller	O
language	O
models	O
pre-trained	O
with	O
knowledge	O
distillation,	B-MethodName
resulting	O
in	O
models	O
that	O
are	O
lighter	O
and	O
faster	O
at	O
inference	O
time,	O
while	O
also	O
requiring	O
a	O
smaller	O
computational	O
training	O
budget.	O
Our	O
general-purpose	O
pre-trained	O
models	O
can	O
be	O
fine-tuned	O
with	O
good	O
performances	O
on	O
several	O
downstream	O
tasks,	O
keeping	O
the	O
flexibility	O
of	O
larger	O
models.	O
We	O
also	O
show	O
that	O
our	O
compressed	O
models	O
are	O
small	O
enough	O
to	O
run	O
on	O
the	O
edge,	O
e.g.	O
on	O
mobile	O
devices.	O
Using	O
a	O
triple	O
loss,	O
we	O
show	O
that	O
a	O
40%	B-MetricValue
smaller	O
Transformer	O
(Vaswani	O
et	O
al.	O
)	O
pre-trained	O
through	O
distillation	B-MethodName
via	O
the	O
supervision	O
of	O
a	O
bigger	O
Transformer	B-MethodName
language	O
model	O
can	O
achieve	O
similar	O
performance	O
on	O
a	O
variety	O
of	O
downstream	O
tasks,	O
while	O
being	O
60%	O
faster	O
at	O
inference	O
time.	O
Further	O
ablation	O
studies	O
indicate	O
that	O
all	O
the	O
components	O
of	O
the	O
triple	O
loss	O
are	O
important	O
for	O
best	O
performances.	O
We	O
have	O
made	O
the	O
trained	O
weights	O
available	O
along	O
with	O
the	O
training	O
code	O
in	O
the	O
Transformers	O
2	O
library	O
from	O
HuggingFace	O
[Wolf	O
et	O
al.,	O
2019].	O

As	O
Transfer	B-MethodName
Learning	I-MethodName
from	O
large-scale	O
pre-trained	O
models	O
becomes	O
more	O
prevalent	O
in	O
Natural	O
Language	O
Processing	O
(NLP),	O
operating	O
these	O
large	O
models	O
in	O
on-theedge	O
and/or	O
under	O
constrained	O
computational	O
training	O
or	O
inference	O
budgets	O
remains	O
challenging.	O
In	O
this	O
work,	O
we	O
propose	O
a	O
method	O
to	O
pre-train	O
a	O
smaller	O
generalpurpose	O
language	O
representation	O
model,	O
called	O
DistilBERT,	B-MethodName
which	O
can	O
then	O
be	O
finetuned	O
with	O
good	O
performances	O
on	O
a	O
wide	O
range	O
of	O
tasks	O
like	O
its	O
larger	O
counterparts.	O
While	O
most	O
prior	O
work	O
investigated	O
the	O
use	O
of	O
distillation	B-MethodName
for	O
building	O
task-specific	O
models,	O
we	O
leverage	O
knowledge	O
distillation	B-MethodName
during	O
the	O
pre-training	O
phase	O
and	O
show	O
that	O
it	O
is	O
possible	O
to	O
reduce	O
the	O
size	O
of	O
a	O
BERT	B-MethodName
model	O
by	O
40%,	B-MetricValue
while	O
retaining	O
97%	B-MetricValue
of	O
its	O
language	O
understanding	O
capabilities	O
and	O
being	O
60%	B-MetricValue
faster.	O
To	O
leverage	O
the	O
inductive	O
biases	O
learned	O
by	O
larger	O
models	O
during	O
pre-training,	O
we	O
introduce	O
a	O
triple	O
loss	O
combining	O
language	O
modeling,	O
distillation	O
and	O
cosine-distance	O
losses.	O
Our	O
smaller,	O
faster	O
and	O
lighter	O
model	O
is	O
cheaper	O
to	O
pre-train	O
and	O
we	O
demonstrate	O
its	O
capabilities	O
for	O
on-device	O
computations	O
in	O
a	O
proof-of-concept	O
experiment	O
and	O
a	O
comparative	O
on-device	O
study.	O

Table	O
16	O
shows	O
the	O
details	O
of	O
accuracy	O
on	O
each	O
language	O
in	O
the	O
cross-lingual	O
retrieval	O
task.	O
For	O
a	O
fair	O
comparison	O
with	O
VECO,	B-MethodName
we	O
use	O
the	O
averaged	O
representation	O
in	O
the	O
middle	O
layer	O
of	O
best	O
XNLI	B-DatasetName
model	O
for	O
cross-lingual	B-TaskName
retrieval	I-TaskName
task.	O
ERNIE-M	B-MethodName
outperforms	O
VECO	B-MethodName
in	O
most	O
languages	O
and	O
achieves	O
state-of-the-art	O
results.	O
We	O
also	O
proposed	O
a	O
new	O
method	O
for	O
cross-lingual	B-TaskName
retrieval.	I-TaskName
We	O
use	O
hardest	O
negative	B-MetricName
binary	I-MetricName
cross-entropy	I-MetricName
loss	I-MetricName
(Wang	O
et	O
al.,	O
2019b;Faghri	O
et	O
al.,	O
2017)	O
to	O
fine-tune	O
ERNIE-M	B-MethodName
with	O
the	O
same	O
bilingual	O
corpora	O
in	O
pre-training.	O

To	O
better	O
evaluate	O
the	O
performance	O
of	O
ERNIE-M,	B-MethodName
we	O
train	O
the	O
ERNIE-M-15	B-MethodName
model	O
for	O
15	O
languages.	O
The	O
languages	O
of	O
training	O
corpora	O
is	O
the	O
same	O
as	O
that	O
of	O
HICTL	B-MethodName
.	O
We	O
evaluate	O
ERNIE-M-15	B-MethodName
on	O
the	O
XNLI	B-DatasetName
dataset.	O
Table	O
15	O
shows	O
the	O
results	O
of	O
15	O
languages	O
models.	O
The	O
ERNIE-M-15	B-MethodName
model	O
outperforms	O
the	O
current	O
best	O
15-language	O
cross-lingual	O
model	O
on	O
the	O
XNLI	B-DatasetName
task,	O
achieving	O
a	O
score	B-MetricName
of	O
77.5	B-MetricValue
in	O
the	O
cross-lingual	O
transfer	O
setting,	O
outperforming	O
HICTL	B-MethodName
0.2	B-MetricValue
and	O
a	O
score	B-MetricName
of	O
80.7	B-MetricValue
in	O
the	O
translate-train-all	O
setting,	O
outperforming	O
HICTL	B-MethodName
0.7.	B-MetricValue

Tables	O
12	O
and	O
13	O
list	O
the	O
fine-tuning	O
parameters	O
on	O
XNLI,	B-DatasetName
MLQA,	B-DatasetName
CoNLL	B-DatasetName
and	O
PAWS-X.	B-DatasetName
For	O
each	O
task,	O
we	O
select	O
the	O
model	O
with	O
the	O
best	O
performance	O
on	O
the	O
validation	O
set,	O
and	O
the	O
test	O
set	O
score	O
is	O
the	O
average	O
of	O
five	O
runs	O
with	O
different	O
random	O
seeds.	O
Tables	O
14	O
list	O
the	O
fine-tuning	O
parameters	O
on	O
Tatoeba.	O

We	O
follow	O

To	O
learn	O
the	O
alignment	O
of	O
cross-lingual	O
semantic	O
representations	O
in	O
parallel	O
corpora,	O
we	O
propose	O
a	O
new	O
pre-training	O
objective,	O
CAMLM.	B-TaskName
We	O
denote	O
a	O
parallel	O
sentence	O
pair	O
as	O
<source	O
sentence,	O
target	O
sentence>.	O
In	O
CAMLM,	B-TaskName
we	O
learn	O
the	O
multilingual	O
semantic	O
representation	O
by	O
restoring	O
the	O
MASK	O
token	O
in	O
the	O
input	O
sentences.	O
When	O
the	O
model	O
restores	O
the	O
MASK	O
token	O
in	O
the	O
source	O
sentence,	O
the	O
model	O
can	O
only	O
rely	O
on	O
the	O
semantics	O
of	O
the	O
target	O
sentence,	O
which	O
means	O
that	O
the	O
model	O
has	O
to	O
learn	O
how	O
to	O
represent	O
the	O
source	O
language	O
with	O
the	O
semantics	O
of	O
the	O
target	O
sentence	O
and	O
thus	O
align	O
the	O
semantics	O
of	O
multiple	O
languages.	O
Figure	O
1	O
(b)	O
and	O
(c)	O
show	O
the	O
differences	O
between	O
TLM	O
(Lample	O
and	O
Conneau,	O
2019)	O
and	O
CAMLM.	B-TaskName
TLM	O
learns	O
the	O
semantic	O
alignment	O
between	O
languages	O
with	O
both	O
the	O
source	O
and	O
target	O
sentences	O
while	O
CAMLM	B-TaskName
only	O
relies	O
on	O
one	O
side	O
of	O
the	O
sentence	O
to	O
restore	O
the	O
MASK	O
token.	O
The	O
advantage	O
of	O
CAMLM	B-TaskName
is	O
that	O
it	O
avoids	O
the	O
information	O
leakage	O
that	O
the	O
model	O
can	O
attend	O
to	O
a	O
pair	O
of	O
input	O
sentences	O
at	O
the	O
same	O
time,	O
which	O
makes	O
learning	O
of	O
BTMLM	O
possible.	O
The	O
selfattention	O
matrix	O
of	O
the	O
example	O
in	O
Figure	O
1	O
is	O
shown	O
in	O
Figure	O
2.	O
For	O
TLM,	O
the	O
prediction	O
of	O
the	O
MASK	O
token	O
relies	O
on	O
the	O
input	O
sentence	O
pair.	O
When	O
the	O
model	O
learns	O
CAMLM,	B-TaskName
the	O
model	O
can	O
only	O
predict	O
the	O
MASK	O
token	O
based	O
on	O
the	O
sentence	O
of	O
its	O
corresponding	O
parallel	O
sentence	O
and	O
the	O
MASK	O
symbol	O
of	O
this	O
sentence,	O
which	O
provides	O
the	O
position	O
and	O
language	O
information.	O
Thus,	O
the	O
probability	O
of	O
the	O
MASK	O
token	O
M	O
2	O
is	O
p(x	O
2	O
|M	O
2	O
,	O
y	O
4	O
,	O
y	O
5	O
,	O
y	O
6	O
,	O
y	O
7	O
),	O
p(y	O
5	O
|x	O
1	O
,	O
x	O
2	O
,	O
x	O
3	O
,	O
M	O
5	O
)	O
for	O
M	O
5	O
,	O
and	O
p(y	O
Given	O
the	O
input	O
in	O
a	O
bilingual	O
corpus	O
6	O
|x	O
1	O
,	O
x	O
2	O
,	O
x	O
3	O
,	O
M	O
6	O
)	O
for	O
M	O
6	O
in	O
CAMLM.	B-TaskName
x1	O
M2	O
x	O
3	O
y	O
4	O
y7	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
M5	O
M6	O
x	O
2	O
y5	O
y6	O
x2	O
y5	O
y6	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
x2	O
y5	O
y6	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
x2	O
y5	O
y6	O
(a)	O
MMLM	O
x1	O
M2	O
x	O
3	O
y	O
4	O
y7	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
M5	O
M6	O
x	O
2	O
y5	O
y6	O
x2	O
y5	O
y6	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
x2	O
y5	O
y6	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
x2	O
y5	O
y6	O
(b)	O
TLM	O
x1	O
M2	O
x	O
3	O
y	O
4	O
y7	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
M5	O
M6	O
x	O
2	O
y5	O
y6	O
x2	O
y5	O
y6	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
x2	O
y5	O
y6	O
x1	O
M	O
2	O
x3	O
y4	O
M	O
5	O
M	O
6	O
y7	O
x2	O
y5	O
y6	O
(c)	O
CAMLM	B-TaskName
X	O
src	O
=	O
{x	O
1	O
,	O
x	O
2	O
,	O
•	O
•	O
•	O
,	O
x	O
s	O
},	O
and	O
its	O
corresponding	O
MASK	O
po-	O
sition,	O
M	O
src	O
=	O
{m	O
1	O
,	O
m	O
2	O
,	O
•	O
•	O
•	O
,	O
m	O
ms	O
},	O
the	O
tar-	O
get	O
sentence	O
is	O
X	O
tgt	O
=	O
{x	O
s+1	O
,	O
x	O
s+2	O
,	O
•	O
•	O
•	O
,	O
x	O
s+t	O
},	O
and	O
its	O
corresponding	O
MASK	O
position	O
is	O
M	O
tgt	O
=	O
{m	O
ms+1	O
,	O
m	O
ms+2	O
,	O
•	O
•	O
•	O
,	O
m	O
ms+mt	O
}.	O
In	O
TLM,	O
the	O
model	O
can	O
attend	O
to	O
the	O
tokens	O
in	O
the	O
source	O
and	O
target	O
sentences,	O
so	O
the	O
probability	O
of	O
masked	O
tokens	O
is	O
m∈M	O
p(x	O
m	O
|X/	O
M	O
),	O
where	O
M	O
=	O
M	O
src	O
∪	O
M	O
tgt	O
.	O
X/	O
M	O
denotes	O
all	O
input	O
tokens	O
x	O
in	O
X	O
except	O
x	O
in	O
M	O
,	O
where	O
X	O
=	O
X	O
src	O
∪	O
X	O
tgt	O
.	O
x	O
m	O
denotes	O
the	O
token	O
with	O
position	O
m.	O
In	O
CAMLM,	O
the	O
probability	O
of	O
the	O
MASK	O
token	O
in	O
the	O
source	O
sentence	O
is	O
m∈Msrc	O
p(x	O
m	O
|X/	O
M	O
∪Xsrc	O
),	O
which	O
means	O
that	O
when	O
predicting	O
the	O
MASK	O
tokens	O
in	O
the	O
source	O
sentence,	O
we	O
only	O
focus	O
on	O
the	O
target	O
sentence.	O
As	O
for	O
the	O
target	O
sentence,	O
the	O
probability	O
of	O
the	O
MASK	O
token	O
is	O
m∈Mtgt	O
p(x	O
m	O
|X/	O
M	O
∪Xtgt	O
),	O
which	O
means	O
that	O
the	O
MASK	O
tokens	O
in	O
the	O
target	O
sentence	O
will	O
be	O
predicted	O
based	O
only	O
on	O
the	O
source	O
sentence.	O
Therefore,	O
the	O
model	O
must	O
learn	O
to	O
use	O
the	O
corresponding	O
sentence	O
to	O
predict	O
and	O
learn	O
the	O
alignment	O
across	O
multiple	O
languages.	O
The	O
pre-training	O
loss	O
of	O
CAMLM	O
in	O
the	O
source/target	O
sentence	O
is	O
L	O
CAM	O
LM	O
(src)	O
=	O
−	O
x∈D	O
B	O
log	O
m∈Msrc	O
p(x	O
m	O
|X/	O
M	O
∪Xsrc	O
)	O
L	O
CAM	O
LM	O
(tgt)	O
=	O
−	O
x∈D	O
B	O
log	O
m∈Mtgt	O
p(x	O
m	O
|X/	O
M	O
∪Xtgt	O
)	O
where	O
D	O
B	O
is	O
the	O
bilingual	O
training	O
corpus.	O
The	O
CAMLM	B-TaskName
loss	O
is	O
L	O
CAM	O
LM	O
=	O
L	O
CAM	O
LM	O
(src)	O
+	O
L	O
CAM	O
LM	O
(tgt)	O

Existing	O
multilingual	O
language	O
models	O
can	O
be	O
classified	O
into	O
two	O
main	O
categories:	O
(1)	O
discriminative	O
models;	O
(2)	O
generative	O
models.	O
In	O
the	O
first	O
category,	O
a	O
multilingual	O
bidirectional	O
encoder	O
representation	O
from	O
transformers	O
(mBERT;	B-MethodName
Devlin	O
et	O
al.	O
2018)	O
is	O
pre-trained	O
using	O
MMLM	O
on	O
a	O
monolingual	O
corpus,	O
which	O
learns	O
a	O
shared	O
language-invariant	O
feature	O
space	O
among	O
multiple	O
languages.	O
The	O
evaluation	O
results	O
show	O
that	O
the	O
mBERT	B-MethodName
achieves	O
significant	O
performance	O
in	O
downstream	O
tasks	O
(Wu	O
and	O
Dredze,	O
2019).	O
XLM	B-MethodName
(Lample	O
and	O
is	O
extended	O
on	O
the	O
basis	O
of	O
mBERT	B-MethodName
using	O
TLM,	O
which	O
enables	O
the	O
model	O
to	O
learn	O
cross-lingual	O
token	O
alignment	O
from	O
parallel	O
corpora.	O
XLM-R	B-MethodName
demonstrates	O
the	O
effects	O
of	O
models	O
when	O
trained	O
on	O
a	O
large-scale	O
corpus.	O
It	O
used	O
2.5T	O
data	O
extracted	O
from	O
Common	O
Crawl	O
that	O
involves	O
100	O
languages	O
for	O
MMLM	O
training.	O
The	O
results	O
show	O
that	O
a	O
large-scale	O
training	O
corpus	O
can	O
significantly	O
improve	O
the	O
performance	O
of	O
the	O
cross-lingual	O
model.	O
Unicoder	O
(Huang	O
et	O
al.,	O
2019)	O
achieves	O
gains	O
on	O
downstream	O
tasks	O
by	O
employing	O
a	O
multitask	O
learning	O
framework	O
to	O
learn	O
cross-lingual	O
semantic	O
representations	O
with	O
monolingual	O
and	O
parallel	O
corpora.	O
ALM	O
(Yang	O
et	O
al.,	O
2020)	O
improves	O
the	O
model's	O
transferability	O
by	O
enabling	O
the	O
model	O
to	O
learn	O
cross-lingual	O
code-switch	O
sentences.	O
IN-FOXLM	O
(Chi	O
et	O
al.,	O
2020b)	O
adds	O
a	O
contrastive	O
learning	O
task	O
for	O
cross-lingual	O
model	O
training.	O
HICTL	O
learns	O
cross-lingual	O
semantic	O
representation	O
from	O
multiple	O
facets	O
(at	O
word-levels	O
and	O
sentence-levels)	O
to	O
improve	O
the	O
performance	O
of	O
cross-lingual	O
models.	O
VECO	O
presents	O
a	O
variable	O
encoder-decoder	O
framework	O
to	O
unify	O
the	O
understanding	O
and	O
generation	O
tasks	O
and	O
achieves	O
significant	O
improvement	O
in	O
both	O
downstream	O
tasks.	O
The	O
second	O
category	O
includes	O
MASS	O
(Song	O
et	O
al.,	O
2019),	O
mBART	O
,	O
XNLG	O
(Chi	O
et	O
al.,	O
2020a)	O
and	O
mT5	O
(Xue	O
et	O
al.,	O
2020).	O
MASS	O
(Vaswani	O
et	O
al.,	O
2017)	O
proposed	O
a	O
training	O
objective	O
for	O
restore	O
the	O
input	O
sentences	O
in	O
which	O
successive	O
token	O
fragments	O
are	O
masked	O
which	O
improved	O
the	O
model's	O
performance	O
on	O
machine	O
translation.	O
Similar	O
to	O
MASS,	O
mBART	O
pre-trains	O
a	O
denoised	O
sequence-to-sequence	O
model	O
and	O
uses	O
an	O
autoregressive	O
task	O
to	O
train	O
the	O
model.	O
XNLG	O
focuses	O
on	O
multilingual	O
question	O
generation	O
and	O
abstractive	O
summarization	O
and	O
updates	O
the	O
parameters	O
of	O
the	O
encoder	O
and	O
decoder	O
through	O
auto-encoding	O
and	O
autoregressive	O
tasks.	O
mT5	O
uses	O
the	O
same	O
model	O
structure	O
and	O
pre-training	O
method	O
as	O
T5	O
(Raffel	O
et	O
al.,	O
2019),	O
and	O
extends	O
the	O
parameters	O
of	O
the	O
cross-lingual	O
model	O
to	O
13B,	O
significantly	O
improving	O
the	O
performance	O
of	O
the	O
cross-language	O
downstream	O
tasks.	O

Back	O
translation	O
(BT)	O
is	O
an	O
effective	O
neuralnetwork-based	O
machine	O
translation	O
method	O
proposed	O
by	O
Sennrich	O
et	O
al.	O
(2015).	O
It	O
can	O
significantly	O
improve	O
the	O
performance	O
of	O
both	O
supervised	O
and	O
unsupervised	O
machine	O
translation	O
via	O
augment	O
the	O
parallel	O
training	O
corpus	O
(Lample	O
et	O
al.,	O
2017;Edunov	O
et	O
al.,	O
2018).	O
BT	O
has	O
been	O
found	O
to	O
particularly	O
useful	O
when	O
the	O
parallel	O
corpus	O
is	O
sparse	O
(Karakanta	O
et	O
al.,	O
2018).	O
Predicting	O
the	O
token	O
of	O
the	O
target	O
language	O
in	O
one	O
batch	O
can	O
also	O
improve	O
the	O
speed	O
of	O
non-auto	O
regressive	O
machine	O
translation	O
(NAT;	O
Gu	O
et	O
al.	O
2017;Wang	O
et	O
al.	O
2019a).	O
Our	O
work	O
is	O
inspired	O
by	O
NAT	O
and	O
BT.	O
We	O
generate	O
the	O
tokens	O
of	O
another	O
language	O
in	O
batches	O
and	O
then	O
use	O
these	O
in	O
pre-training	O
to	O
help	O
sentence	O
alignment	O
learning.	O

To	O
overcome	O
the	O
constraint	O
that	O
the	O
parallel	O
corpus	O
size	O
places	O
on	O
the	O
model	O
performance,	O
we	O
propose	O
a	O
novel	O
pre-training	O
objective	O
inspired	O
by	O
NAT	O
(Gu	O
et	O
al.,	O
2017;Wang	O
et	O
al.,	O
2019a)	O
and	O
BT	O
methods	O
called	O
BTMLM	O
to	O
align	O
cross-lingual	O
semantics	O
with	O
the	O
monolingual	O
corpus.	O
We	O
use	O
BTMLM	O
to	O
train	O
our	O
model,	O
which	O
builds	O
on	O
the	O
transferability	O
learned	O
through	O
CAMLM,	B-TaskName
generating	O
pseudoparallel	O
sentences	O
from	O
the	O
monolingual	O
sentences	O
and	O
the	O
generated	O
pseudo-parallel	O
sentences	O
are	O
then	O
used	O
as	O
the	O
input	O
of	O
the	O
model	O
to	O
align	O
the	O
cross-lingual	O
semantics,	O
thus	O
enhancing	O
the	O
multilingual	O
representation.	O
The	O
training	O
process	O
for	O
BTMLM	O
is	O
shown	O
in	O
Figure	O
3.	O
The	O
learning	O
process	O
for	O
the	O
BTMLM	O
is	O
divided	O
into	O
two	O
stages.	O
Stage	O
1	O
involves	O
the	O
generation	O
of	O
pseudo-parallel	O
tokens	O
from	O
monolingual	O
corpora.	O
Specifically,	O
we	O
fill	O
in	O
several	O
placeholder	O
MASK	O
at	O
the	O
end	O
of	O
the	O
monolingual	O
sentence	O
to	O
indicate	O
the	O
location	O
and	O
the	O
language	O
we	O
want	O
to	O
generate,	O
and	O
let	O
the	O
model	O
generate	O
its	O
corresponding	O
parallel	O
language	O
token	O
based	O
on	O
the	O
original	O
monolingual	O
sentence	O
and	O
the	O
corresponding	O
position	O
of	O
the	O
pseudo-token.	O
In	O
this	O
way,	O
we	O
generate	O
the	O
tokens	O
of	O
another	O
language	O
from	O
the	O
monolingual	O
sentence,	O
which	O
will	O
be	O
used	O
in	O
learning	O
cross-lingual	O
semantic	O
alignment	O
for	O
multiple	O
languages.	O
The	O
self-attention	O
matrix	O
for	O
generating	O
pseudotokens	O
in	O
Figure	O
3	O
is	O
shown	O
in	O
Figure	O
4.	O
In	O
the	O
pseudo-token	O
generating	O
process,	O
the	O
model	O
can	O
only	O
attend	O
to	O
the	O
source	O
sentence	O
and	O
the	O
placeholder	O
MASK	O
tokens,	O
which	O
indicate	O
the	O
language	O
and	O
position	O
we	O
want	O
to	O
predict	O
by	O
using	O
language	O
embedding	O
and	O
position	O
embedding.	O
The	O
probability	O
of	O
mask	O
token	O
M	O
5	O
is	O
p(y	O
5	O
|x	O
1	O
,	O
x	O
2	O
,	O
x	O
3	O
,	O
x	O
4	O
,	O
M	O
5	O
),	O
p(y	O
6	O
|x	O
1	O
,	O
x	O
2	O
,	O
x	O
3	O
,	O
x	O
4	O
,	O
M	O
6	O
)	O
for	O
M	O
6	O
and	O
p(y	O
7	O
|x	O
1	O
,	O
x	O
2	O
,	O
x	O
3	O
,	O
x	O
4	O
,	O
M	O
7	O
)	O
for	O
M	O
7	O
.	O
Stage	O
2	O
uses	O
the	O
pseudo-tokens	O
generated	O
in	O
Stage	O
1	O
to	O
learn	O
the	O
cross-lingual	O
semantics	O
alignment.	O
The	O
process	O
in	O
Stage	O
2	O
is	O
shown	O
in	O
the	O
righthand	O
diagram	O
of	O
Figure	O
3.	O
In	O
the	O
training	O
process	O
of	O
Stage	O
2,	O
the	O
input	O
of	O
the	O
model	O
is	O
the	O
concatenation	O
of	O
the	O
monolingual	O
sentences	O
and	O
the	O
generated	O
pseudo-parallel	O
tokens,	O
and	O
the	O
learning	O
objective	O
is	O
to	O
restore	O
the	O
MASK	O
tokens	O
based	O
on	O
the	O
original	O
sentences	O
and	O
the	O
generated	O
pseudo-parallel	O
tokens.	O
Because	O
the	O
model	O
can	O
rely	O
not	O
only	O
on	O
the	O
input	O
monolingual	O
sentence	O
but	O
also	O
the	O
generated	O
pseudo-tokens	O
in	O
the	O
process	O
of	O
inference	O
MASK	O
to-kens,	O
the	O
model	O
can	O
explicitly	O
learn	O
the	O
alignment	O
of	O
the	O
cross-lingual	O
semantic	O
representation	O
from	O
the	O
monolingual	O
sentences.	O
The	O
learning	O
process	O
of	O
the	O
BTMLM	O
can	O
be	O
interpreted	O
as	O
follows:	O
given	O
the	O
input	O
in	O
monolin-	O
gual	O
corpora	O
X	O
=	O
{x	O
1	O
,	O
x	O
2	O
,	O
•	O
•	O
•	O
,	O
x	O
s	O
},	O
the	O
positions	O
of	O
masked	O
tokens	O
M	O
=	O
{m	O
1	O
,	O
m	O
2	O
,	O
•	O
•	O
•	O
,	O
m	O
m	O
}	O
and	O
the	O
position	O
of	O
the	O
pseudo-token	O
to	O
be	O
predicted,	O
M	O
pseudo	O
=	O
{m	O
s+1	O
,	O
m	O
s+2	O
,	O
•	O
•	O
•	O
,	O
m	O
s+p	O
},	O
we	O
first	O
generate	O
pseudo-tokens	O
P	O
=	O
{h	O
s+1	O
,	O
h	O
s+2	O
,	O
•	O
•	O
•	O
,	O
h	O
s+p	O
},	O
as	O
described	O
earlier;	O
we	O
then	O
concatenate	O
the	O
generated	O
pseudo-token	O
with	O
input	O
monolingual	O
sentence	O
as	O
a	O
new	O
parallel	O
sentence	O
pair	O
and	O
use	O
it	O
to	O
train	O
our	O
model.	O
Thus,	O
the	O
probability	O
of	O
the	O
masked	O
tokens	O
in	O
BTMLM	O
is	O
m∈M	O
p(x	O
m	O
|X/	O
M	O
,	O
P	O
),	O
where	O
X/	O
M	O
denotes	O
all	O
input	O
tokens	O
x	O
in	O
X	O
except	O
x	O
in	O
M	O
.	O
The	O
pre-training	O
loss	O
of	O
BTMLM	O
is	O
L	O
BT	O
M	O
LM	O
=	O
−	O
x∈D	O
M	O
log	O
m∈M	O
p(x	O
m	O
|X/	O
M	O
,	O
P	O
)	O
where	O
D	O
M	O
is	O
the	O
monolingual	O
training	O
corpus.	O

We	O
consider	O
five	O
cross-lingual	O
evaluation	O
benchmarks:	O
XNLI	B-DatasetName
for	O
cross-lingual	B-TaskName
natural	I-TaskName
language	I-TaskName
inference,	I-TaskName
MLQA	B-DatasetName
for	O
cross-lingual	B-TaskName
question	I-TaskName
answering,	I-TaskName
CoNLL	B-DatasetName
for	O
cross-lingual	B-TaskName
named	I-TaskName
entity	I-TaskName
recognition,	I-TaskName
PAWS-X	B-DatasetName
for	O
cross-lingual	B-TaskName
paraphrase	I-TaskName
identification,	I-TaskName
and	O
Tatoeba	B-DatasetName
for	O
cross-lingual	B-TaskName
retrieval.	I-TaskName
Next,	O
we	O
first	O
describe	O
the	O
data	O
and	O
pre-training	O
details	O
and	O
then	O
compare	O
the	O
ERNIE-M	B-MethodName
with	O
the	O
existing	O
state-of-the-art	O
models.	O

ERNIE-M	B-MethodName
is	O
trained	O
with	O
monolingual	O
and	O
parallel	O
corpora	O
that	O
involved	O
96	O
languages.	O
For	O
the	O
monolingual	O
corpus,	O
we	O
extract	O
it	O
from	O
CC-100	B-DatasetName
according	O
to	O
;	O
.	O
For	O
the	O
bilingual	O
corpus,	O
we	O
use	O
the	O
same	O
corpus	O
as	O
INFOXLM	O
(Chi	O
et	O
al.,	O
2020b),	O
including	O
MultiUN	B-DatasetName
(Ziemski	O
et	O
al.,	O
2016),	O
IIT	B-DatasetName
Bombay	I-DatasetName
(Kunchukuttan	O
et	O
al.,	O
2017),	O
OPUS	B-DatasetName
(Tiedemann,	O
2012),	O
and	O
WikiMatrix	B-DatasetName
We	O
use	O
a	O
transformer-encoder	O
(Vaswani	O
et	O
al.,	O
2017)	O
as	O
the	O
backbone	O
of	O
the	O
model.	O
For	O
the	O
ERNIE-M	B-MethodName
BASE	I-MethodName
model,	O
we	O
adopt	O
a	O
structure	O
with	O
12	B-HyperparameterValue
layers,	B-HyperparameterName
768	B-HyperparameterValue
hidden	B-HyperparameterName
units,	I-HyperparameterName
12	B-HyperparameterValue
heads.	B-HyperparameterName
For	O
ERNIE-M	B-MethodName
LARGE	I-MethodName
model	O
,	O
we	O
adopt	O
a	O
structure	O
with	O
24	B-HyperparameterValue
layers,	B-HyperparameterName
1024	B-HyperparameterValue
hidden	B-HyperparameterName
units,	I-HyperparameterName
16	B-HyperparameterValue
heads.	B-HyperparameterName
The	O
activation	B-HyperparameterName
function	I-HyperparameterName
used	O
is	O
GeLU	B-HyperparameterValue
(Hendrycks	O
and	O
Gimpel,	O
2016).	O
Following	O
Chi	O
et	O
al.	O
2020b	O
and,	O
we	O
initialize	O
the	O
parameters	O
of	O
ERNIE-M	B-MethodName
with	O
XLM-R.	B-MethodName
We	O
use	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
(Kingma	O
and	O
Ba,	O
2014)	O
to	O
train	O
ERNIE-M;	B-MethodName
the	O
learning	O
rate	O
is	O
scheduled	O
with	O
a	O
linear	O
decay	O
with	O
10K	B-HyperparameterValue
warm-up	B-HyperparameterName
steps,	I-HyperparameterName
and	O
the	O
peak	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
is	O
2e	B-HyperparameterValue
−	I-HyperparameterValue
4	I-HyperparameterValue
for	O
the	O
base	O
model	O
and	O
1e	B-HyperparameterValue
−	I-HyperparameterValue
4	I-HyperparameterValue
for	O
the	O
large	O
model.	O
We	O
conduct	O
the	O
pre-training	O
experiments	O
using	O
64	O
Nvidia	O
V100-32GB	O
GPUs	O
with	O
2048	B-HyperparameterValue
batch	B-HyperparameterName
size	I-HyperparameterName
and	O
512	B-HyperparameterValue
max	B-HyperparameterName
length.	I-HyperparameterName

Cross-lingual	O
Natural	O
Language	O
Inference.	O
The	O
cross-lingual	O
natural	O
language	O
inference	O
(XNLI;	O
Conneau	O
et	O
al.	O
2018)	O
task	O
is	O
a	O
multilingual	O
language	O
inference	O
task.	O
The	O
goal	O
of	O
XNLI	B-DatasetName
is	O
to	O
determine	O
the	O
relationship	O
between	O
the	O
two	O
input	O
sentences.	O
We	O
evaluate	O
ERNIE-M	B-MethodName
in	O
(1)	O
cross-lingual	B-TaskName
transfer	I-TaskName
(Conneau	O
et	O
al.,	O
2018)	O
setting:	O
fine-tune	O
the	O
model	O
with	O
an	O
English	O
training	O
set	O
and	O
evaluate	O
the	O
foreign	O
language	O
XNLI	O
test	O
and	O
(2)	O
translatetrain-all	O
(Huang	O
et	O
al.,	O
2019)	O
setting:	O
fine-tune	O
the	O
model	O
on	O
the	O
concatenation	O
of	O
all	O
other	O
languages	O
and	O
evaluate	O
on	O
each	O
language	O
test	O
set.	O
Table	O
1	O
shows	O
the	O
results	O
of	O
ERNIE-M	B-MethodName
in	O
XNLI	B-DatasetName
task.	O
The	O
result	O
shows	O
that	O
ERNIE-M	B-MethodName
outperforms	O
all	O
baseline	O
models	O
including	O
XLM	B-MethodName
(Lample	O
and	O
Conneau,	O
2019)	O
Named	B-TaskName
Entity	I-TaskName
Recognition.	I-TaskName
For	O
the	O
namedentity-recognition	B-TaskName
task,	O
we	O
evaluate	O
ERNIE-M	B-MethodName
on	O
the	O
CoNLL-2002and	B-DatasetName
CoNLL-2003datasets	B-DatasetName
(Sang	O
and	O
De	O
Meulder,	O
2003,	O
which	O
is	O
a	O
cross-lingual	B-TaskName
named-entity-recognition	I-TaskName
task	O
including	O
English,	O
Dutch,	O
Spanish	O
and	O
German.	O
We	O
consider	O
ERNIE-M	B-MethodName
in	O
the	O
following	O
setting:	O
(1)	O
fine-tune	O
on	O
the	O
,	O
and	O
(Wu	O
and	O
Dredze,	O
2019),	O
respectively.	O
English	O
dataset	O
and	O
evaluate	O
on	O
each	O
cross-lingual	O
dataset	O
to	O
evaluate	O
cross-lingual	O
transfer	O
and	O
(2)	O
fine-tune	O
on	O
all	O
training	O
datasets	O
to	O
evaluate	O
crosslingual	O
learning.	O
For	O
each	O
setting,	O
we	O
reported	O
the	O
F1	O
score	O
for	O
each	O
language.	O
Table	O
2	O
shows	O
the	O
results	O
of	O
ERNIE-M,	B-MethodName
XLM-R,	B-MethodName
and	O
mBERT	B-MethodName
on	O
CoNLL-2002	B-DatasetName
andCoNLL-2003.	B-DatasetName
The	O
results	O
of	O
XLM-R	B-MethodName
and	O
mBERT	B-MethodName
are	O
reported	O
from	O
.	O
ERNIE-M	B-MethodName
model	O
yields	O
SoTA	O
performance	O
on	O
both	O
settings	O
and	O
outperforms	O
XLM-R	B-MethodName
by	O
0.45	B-MetricValue
F1	B-MetricName
when	O
trained	O
on	O
English	O
and	O
0.70	B-MetricValue
F1	B-MetricName
when	O
trained	O
on	O
all	O
languages	O
in	O
the	O
base	O
model.	O
Similar	O
to	O
the	O
performance	O
in	O
the	O
XNLI	B-DatasetName
task,	O
ERNIE-M	B-MethodName
shows	O
better	O
performance	O
on	O
low-resource	O
languages.	O
For	O
large	O
models	O
and	O
finetune	O
in	O
all	O
languages	O
setting,	O
ERNIE-M	B-MethodName
is	O
2.21	B-MetricValue
F1	B-MetricName
higher	O
than	O
SoTA	O
in	O
Dutch	O
(nl)	O
and	O
1.6	B-MetricValue
F1	B-MetricName
higher	O
than	O
SoTA	O
in	O
German	O
(de).	O
Cross-lingual	B-TaskName
Question	I-TaskName
Answering.	I-TaskName
For	O
the	O
question	O
answering	O
task,	O
we	O
use	O
a	O
multilingual	B-DatasetName
question	I-DatasetName
answering	I-DatasetName
(MLQA)	B-DatasetName
dataset	O
to	O
evaluate	O
ERNIE-M.	B-MethodName
MLQA	B-DatasetName
has	O
the	O
same	O
format	O
as	O
SQuAD	O
v1.1	O
(Rajpurkar	O
et	O
al.,	O
2016)	O
and	O
is	O
a	O
multilingual	O
language	O
question	O
answering	O
task	O
composed	O
of	O
seven	O
languages.	O
We	O
fine-tune	O
ERNIE-M	B-MethodName
by	O
training	O
on	O
English	O
data	O
and	O
evaluating	O
on	O
seven	O
crosslingual	O
datasets.	O
The	O
fine-tune	O
method	O
is	O
the	O
same	O
as	O
in	O
Lewis	O
et	O
al.	O
(2019),	O
which	O
concatenates	O
the	O
question-passage	O
pair	O
as	O
the	O
input.	O
Table	O
3	O
presents	O
a	O
comparison	O
of	O
ERNIE-M	B-MethodName
and	O
several	O
baseline	O
models	O
on	O
MLQA.	B-DatasetName
We	O
report	O
the	O
F1	B-MetricName
and	O
extract	B-MetricName
match	I-MetricName
(EM)	B-MetricName
scores	O
based	O
on	O
the	O
average	O
over	O
five	O
runs.	O
The	O
performance	O
of	O
ERNIE-M	B-MethodName
in	O
MLQA	B-DatasetName
is	O
significantly	O
better	O
than	O
the	O
previous	O
models,	O
and	O
it	O
achieves	O
a	O
SoTA	O
score.	O
We	O
outperform	O
INFOXLM	O
0.8	B-MetricValue
in	O
F1	B-MetricName
and	O
0.5	B-MetricValue
in	O
EM.	B-MetricName
Cross-lingual	O
Paraphrase	O
Identification.	O
For	O
cross-lingual	O
paraphrase	O
identification	O
task,	O
we	O
use	O
the	O
PAWS-X	B-DatasetName
dataset	O
to	O
evaluate	O
our	O
model.	O
The	O
goal	O
of	O
PAWS-X	B-DatasetName
was	O
to	O
determine	O
whether	O
two	O
sentences	O
were	O
paraphrases.	O
We	O
evaluate	O
ERNIE-M	B-MethodName
on	O
both	O
the	O
cross-lingual	O
transfer	O
setting	O
and	O
translate-train-all	O
setting.	O
Table	O
4	O
shows	O
a	O
comparison	O
of	O
ERNIE-M	B-MethodName
and	O
various	O
baseline	O
models	O
on	O
PAWS-X.	B-DatasetName
We	O
report	O
the	O
accuracy	O
score	O
on	O
each	O
language	O
test	O
set	O
based	O
on	O
the	O
average	O
over	O
five	O
runs.	O
The	O
results	O
show	O
that	O
and	O
,	O
respectively.	O

Cross-lingual	B-TaskName
Sentence	I-TaskName
Retrieval.	I-TaskName
The	O
goal	O
of	O
the	O
cross-lingual	B-TaskName
sentence	I-TaskName
retrieval	I-TaskName
task	O
was	O
to	O
extract	O
parallel	O
sentences	O
from	O
bilingual	O
corpora.	O
We	O
used	O
a	O
subset	O
of	O
the	O
Tatoeba	B-DatasetName
dataset,	O
which	O
contains	O
36	O
language	O
pairs	O
to	O
evaluate	O
ERNIE-M.	B-MethodName
Following	O
,	O
we	O
used	O
the	O
averaged	O
representation	O
in	O
the	O
middle	O
layer	O
of	O
the	O
best	O
XNLI	B-DatasetName
model	O
to	O
evaluate	O
the	O
retrieval	O
task.	O
Table	O
5	O
shows	O
the	O
results	O
of	O
ERNIE-M	B-MethodName
in	O
the	O
retrieval	O
task;	O
XLM-R	B-MethodName
results	O
are	O
reported	O
from	O
.	O
ERNIE-M	B-MethodName
achieves	O
a	O
score	O
of	O
87.9	O
in	O
the	O
Tatoeba	B-DatasetName
dataset,	O
outperforming	O
VECO	O
1.0	O
and	O
obtaining	O
new	O
SoTA	O
results.	O

XLM-R	B-HyperparameterName
LARGE	I-HyperparameterName
To	O
further	O
evaluate	O
the	O
performance	O
of	O
ERNIE-M	B-HyperparameterName
in	O
retrieval	O
task,	O
we	O
use	O
hardest	O
negative	B-MetricName
binary	I-MetricName
cross-entropy	I-MetricName
loss	I-MetricName
(Wang	O
et	O
al.,	O
2019b;Faghri	O
et	O
al.,	O
2017)	O
to	O
fine-tune	O
ERNIE-M	B-MethodName
with	O
the	O
same	O
bilingual	O
corpus	O
in	O
pre-training.	O
Figure	O
5	O
shows	O
the	O
details	O
of	O
accuracy	O
on	O
each	O
language	O
in	O
Tatoeba.	B-DatasetName
After	O
fine-tuning,	O
ERNIE-M	B-MethodName
shows	O
a	O
significant	O
improvement	O
in	O
all	O
languages,	O
with	O
the	O
average	B-MetricName
accuracy	I-MetricName
in	O
all	O
languages	O
increasing	O
from	O
87.9	B-MetricValue
to	O
93.3.	B-MetricValue

To	O
understand	O
the	O
effect	O
of	O
aligning	O
semantic	O
representations	O
of	O
multiple	O
languages	O
in	O
the	O
training	O
process	O
of	O
ERNIE-M,	B-MethodName
we	O
conducted	O
an	O
ablation	O
study	O
as	O
reported	O
in	O
Table	O
6.	O
Comparing	O
exp	O
0	O
and	O
exp	O
1	O
,	O
we	O
can	O
observer	O
that	O
there	O
is	O
no	O
gain	O
in	O
the	O
performance	O
of	O
the	O
crosslingual	O
model	O
by	O
continuing	O
pre-training	O
XLM-	O
Comparing	O
exp	O
3	O
to	O
exp	O
4	O
,	O
we	O
find	O
that	O
there	O
is	O
a	O
0.5	B-MetricValue
improvement	O
on	O
XNLI	B-DatasetName
and	O
0.1	B-MetricValue
improvement	O
on	O
CoNLL	B-DatasetName
after	O
the	O
model	O
learns	O
BTMLM.	B-TaskName
This	O
demonstrates	O
that	O
our	O
proposed	O
BTMLM	B-TaskName
can	O
learn	O
cross-lingual	O
semantic	O
alignment	O
and	O
improve	O
the	O
performance	O
of	O
our	O
model.	O
To	O
further	O
analyze	O
the	O
effect	O
of	O
our	O
strategy,	O
we	O
trained	O
the	O
small-sized	O
ERNIE-M	B-MethodName
model	O
from	O
scratch.	O
Table	O
8	O
shows	O
the	O
results	O
of	O
XNLI	B-DatasetName
and	O
CoNLL.	B-DatasetName
Both	O
XNLI	B-DatasetName
and	O
CoNLL	B-DatasetName
results	O
are	O
the	O
average	O
of	O
each	O
languages.	O
We	O
observe	O
that,	O
ERNIE-M	B-MethodName
SMALL	I-MethodName
can	O
outperform	O
XLM-R	B-MethodName
SMALL	I-MethodName
by	O
4.4	B-MetricValue
in	O
XNLI	B-DatasetName
and	O
6.6	B-MetricValue
in	O
CoNLL.	B-DatasetName
It	O
suggests	O
that	O
our	O
models	O
can	O
benefit	O
from	O
align	O
cross-lingual	O
semantic	O
representation.	O
Table	O
7	O
shows	O
the	O
gap	B-MetricName
scores	I-MetricName
for	O
English	O
and	O
other	O
languages	O
in	O
the	O
downstream	O
task.	O
This	O
gap	B-MetricName
score	I-MetricName
is	O
the	O
difference	O
between	O
the	O
English	O
testset	O
and	O
the	O
average	O
performance	O
on	O
the	O
testset	O
in	O
other	O
languages.	O
So,	O
a	O
smaller	O
gap	B-MetricName
score	I-MetricName
represents	O
a	O
better	O
transferability	O
of	O
the	O
model.	O
We	O
can	O
no-	O
With	O
the	O
same	O
computational	O
overhead,	O
the	O
performance	O
of	O
ERNIE-M	B-MethodName
is	O
69.9	B-MetricValue
in	O
XNLI	B-DatasetName
and	O
69.7	B-MetricValue
in	O
CoNLL,	B-DatasetName
while	O
XLM-R's	O
performance	O
is	O
67.3	B-MetricValue
in	O
XNLI	B-DatasetName
and	O
65.6	B-MetricValue
in	O
CoNLL.	B-DatasetName
The	O
results	O
demonstrate	O
that	O
ERNIE-M	B-MethodName
performs	O
better	O
than	O
XLM-R	B-MethodName
even	O
with	O
the	O
same	O
computational	O
overhead.	O
In	O
addition,	O
we	O
explored	O
the	O
effect	O
of	O
the	O
number	O
of	O
generated	O
pseudo-parallel	O
tokens	O
on	O
the	O
convergence	O
of	O
the	O
model.	O
In	O
particular,	O
we	O
compare	O
the	O
impact	O
on	O
the	O
convergence	O
speed	O
of	O
the	O
model	O
when	O
generating	O
a	O
5%,	B-HyperparameterValue
10%,	B-HyperparameterValue
15%,	B-HyperparameterValue
and	O
20%	B-HyperparameterValue
proportion	B-HyperparameterName
of	I-HyperparameterName
pseudo-tokens.	I-HyperparameterName
As	O
shown	O
in	O
Figure	O
6,	O
we	O
can	O
find	O
that	O
the	O
perplexity	O
(PPL)	O
of	O
the	O
model	O
decreases	O
as	O
the	O
proportion	O
of	O
generated	O
tokens	O
increases,	O
which	O
indicates	O
that	O
the	O
generated	O
pseudo-parallel	O
tokens	O
are	O
helpful	O
for	O
model	O
convergence.	O

To	O
overcome	O
the	O
constraint	O
that	O
the	O
parallel	O
corpus	O
size	O
places	O
on	O
the	O
cross-lingual	O
models	O
performance,	O
we	O
propose	O
a	O
new	O
cross-lingual	O
model,	O
ERNIE-M,	B-MethodName
which	O
is	O
trained	O
using	O
both	O
monolingual	O
and	O
parallel	O
corpora.	O
The	O
contribution	O
of	O
ERNIE-M	B-MethodName
is	O
to	O
propose	O
two	O
training	O
objectives.	O
The	O
first	O
objective	O
is	O
to	O
enhance	O
the	O
multilingual	O
representation	O
on	O
parallel	O
corpora	O
by	O
applying	O
CAMLM,	B-TaskName
and	O
the	O
second	O
objective	O
is	O
to	O
help	O
the	O
model	O
to	O
align	O
cross-lingual	O
semantic	O
representations	O
from	O
a	O
monolingual	O
corpus	O
by	O
using	O
BTMLM.	O
Experiments	O
show	O
that	O
ERNIE-M	B-MethodName
achieves	O
SoTA	O
results	O
in	O
various	O
downstream	O
tasks	O
on	O
the	O
XNLI,	B-DatasetName
MLQA,	B-DatasetName
CoNLL,	B-DatasetName
PAWS-X,	B-DatasetName
and	O
Tatoeba	B-DatasetName
datasets.	O

In	O
this	O
section,	O
we	O
first	O
introduce	O
the	O
general	O
workflow	O
of	O
ERNIE-M	B-MethodName
and	O
then	O
present	O
the	O
details	O
of	O
the	O
model	O
training.	O

Recent	O
studies	O
have	O
demonstrated	O
that	O
the	O
pretraining	O
of	O
cross-lingual	O
language	O
models	O
can	O
significantly	O
improve	O
their	O
performance	O
in	O
crosslingual	O
natural	O
language	O
processing	O
tasks	O
(Devlin	O
et	O
al.,	O
2018;Lample	O
and	O
Conneau,	O
2019;.	O
Existing	O
pretraining	O
methods	O
include	O
multilingual	O
masked	O
language	O
modeling	O
(MMLM;	O
Devlin	O
et	O
al.	O
2018)	O
and	O
translation	O
language	O
modeling	O
(TLM;	O
Lample	O
and	O
Conneau	O
2019),	O
of	O
which	O
the	O
key	O
point	O
is	O
to	O
learn	O
a	O
shared	O
language-invariant	O
feature	O
space	O
among	O
multiple	O
languages.	O
MMLM	O
implicitly	O
models	O
the	O
semantic	O
representation	O
of	O
each	O
language	O
in	O
a	O
unified	O
feature	O
space	O
by	O
learning	O
them	O
separately.	O
TLM	O
is	O
an	O
extension	O
of	O
MMLM	O
that	O
is	O
trained	O
with	O
a	O
parallel	O
corpus	O
and	O
captures	O
semantic	O
alignment	O
by	O
learning	O
a	O
pair	O
of	O
parallel	O
sentences	O
simultaneously.	O
This	O
study	O
shows	O
that	O
the	O
use	O
of	O
parallel	O
corpora	O
can	O
significantly	O
improve	O
the	O
performance	O
in	O
downstream	O
cross-lingual	O
understanding	O
and	O
generation	O
tasks.	O
However,	O
the	O
sizes	O
of	O
parallel	O
corpora	O
are	O
limited	O
(Tran	O
et	O
al.,	O
2020),	O
restricting	O
the	O
performance	O
of	O
the	O
cross-lingual	O
language	O
model.	O
To	O
overcome	O
the	O
constraint	O
of	O
the	O
parallel	O
corpus	O
size	O
on	O
the	O
model	O
performance,	O
we	O
propose	O
ERNIE-M,	B-MethodName
a	O
novel	O
cross-lingual	O
pre-training	O
method	O
to	O
learn	O
semantic	O
alignment	O
among	O
multiple	O
languages	O
on	O
monolingual	O
corpora.	O
Specifically,	O
we	O
propose	O
cross-attention	B-TaskName
masked	I-TaskName
language	I-TaskName
modeling	I-TaskName
(CAMLM)	B-TaskName
to	O
improve	O
the	O
cross-lingual	O
transferability	O
of	O
the	O
model	O
on	O
parallel	O
corpora,	O
and	O
it	O
trains	O
the	O
model	O
to	O
predict	O
the	O
tokens	O
of	O
one	O
language	O
by	O
using	O
another	O
language.	O
Then,	O
we	O
utilize	O
the	O
transferability	O
learned	O
from	O
parallel	O
corpora	O
to	O
enhance	O
multilingual	O
representation.	O
We	O
propose	O
back-translation	O
masked	O
language	O
modeling	O
(BTMLM)	O
to	O
train	O
the	O
model,	O
and	O
this	O
helps	O
the	O
model	O
to	O
learn	O
sentence	O
alignment	O
from	O
monolingual	O
corpora.	O
In	O
BTMLM,	O
a	O
part	O
of	O
the	O
tokens	O
in	O
the	O
input	O
monolingual	O
sentences	O
is	O
predicted	O
into	O
the	O
tokens	O
of	O
another	O
language.	O
We	O
then	O
concatenate	O
the	O
predicted	O
tokens	O
and	O
the	O
input	O
sentences	O
as	O
pseudo-parallel	O
sentences	O
to	O
train	O
the	O
model.	O
In	O
this	O
way,	O
the	O
model	O
can	O
learn	O
sentence	O
alignment	O
with	O
only	O
monolingual	O
corpora	O
and	O
overcome	O
the	O
constraint	O
of	O
the	O
parallel	O
corpus	O
size	O
while	O
improving	O
the	O
model	O
performance.	O
ERNIE-M	B-MethodName
is	O
implemented	O
on	O
the	O
basis	O
of	O
XLM-R	O
,	O
and	O
we	O
evaluate	O
its	O
performance	O
on	O
five	O
widely	O
used	O
cross-lingual	O
benchmarks:	O
XNLI	B-DatasetName
(Conneau	O
et	O
al.,	O
2018)	O
for	O
crosslingual	B-TaskName
natural	I-TaskName
language	I-TaskName
inference,	I-TaskName
MLQA	B-DatasetName
(Lewis	O
et	O
al.,	O
2019)	O
for	O
cross-lingual	B-TaskName
question	I-TaskName
answering,	I-TaskName
CoNLL	B-DatasetName
(Sang	O
and	O
De	O
Meulder,	O
2003)	O
for	O
named	B-TaskName
entity	I-TaskName
recognition,	I-TaskName
cross-lingual	B-TaskName
paraphrase	I-TaskName
adversaries	I-TaskName
from	I-TaskName
word	I-TaskName
scrambling	I-TaskName
(PAWS-X)	B-DatasetName
for	O
cross-lingual	B-TaskName
paraphrase	I-TaskName
identification,	I-TaskName
and	O
Tatoeba	B-DatasetName
for	O
cross-lingual	B-TaskName
retrieval.	I-TaskName
The	O
experimental	O
results	O
demonstrate	O
that	O
ERNIE-M	B-MethodName
outperforms	O
existing	O
cross-lingual	O
models	O
and	O
achieves	O
new	O
state-of-the-art	O
(SoTA)	O
results.	O
2	O
Related	O
Work	O

Recent	O
studies	O
have	O
demonstrated	O
that	O
pretrained	O
cross-lingual	O
models	O
achieve	O
impressive	O
performance	O
in	O
downstream	O
cross-lingual	O
tasks.	O
This	O
improvement	O
benefits	O
from	O
learning	O
a	O
large	O
amount	O
of	O
monolingual	O
and	O
parallel	O
corpora.	O
Although	O
it	O
is	O
generally	O
acknowledged	O
that	O
parallel	O
corpora	O
are	O
critical	O
for	O
improving	O
the	O
model	O
performance,	O
existing	O
methods	O
are	O
often	O
constrained	O
by	O
the	O
size	O
of	O
parallel	O
corpora,	O
especially	O
for	O
lowresource	O
languages.	O
In	O
this	O
paper,	O
we	O
propose	O
ERNIE-M,	B-MethodName
a	O
new	O
training	O
method	O
that	O
encourages	O
the	O
model	O
to	O
align	O
the	O
representation	O
of	O
multiple	O
languages	O
with	O
monolingual	O
corpora,	O
to	O
overcome	O
the	O
constraint	O
that	O
the	O
parallel	O
corpus	O
size	O
places	O
on	O
the	O
model	O
performance.	O
Our	O
key	O
insight	O
is	O
to	O
integrate	O
back-translation	O
into	O
the	O
pre-training	O
process.	O
We	O
generate	O
pseudo-parallel	O
sentence	O
pairs	O
on	O
a	O
monolingual	O
corpus	O
to	O
enable	O
the	O
learning	O
of	O
semantic	O
alignments	O
between	O
different	O
languages,	O
thereby	O
enhancing	O
the	O
semantic	O
modeling	O
of	O
cross-lingual	O
models.	O
Experimental	O
results	O
show	O
that	O
ERNIE-M	B-MethodName
outperforms	O
existing	O
cross-lingual	O
models	O
and	O
delivers	O
new	O
state-of-the-art	O
results	O
in	O
various	O
cross-lingual	O
downstream	O
tasks.	O
1	O

We	O
On	O
LAMBADA,	B-DatasetName
the	O
few-shot	O
capability	O
of	O
language	O
models	O
results	O
in	O
a	O
strong	O
boost	O
to	O
accuracy.	O
GPT-3	B-MethodName
2.7B	O
outperforms	O
the	O
SOTA	O
17B	O
parameter	O
Turing-NLG	B-MethodName
[Tur20]	O
in	O
this	O
setting,	O
and	O
GPT-3	B-MethodName
175B	O
advances	O
the	O
state	O
of	O
the	O
art	O
by	O
18%.	O
Note	O
zero-shot	O
uses	O
a	O
different	O
format	O
from	O
one-shot	O
and	O
few-shot	O
as	O
described	O
in	O
the	O
text.	O
and	O
[Tur20])	O
and	O
argue	O
that	O
"continuing	O
to	O
expand	O
hardware	O
and	O
data	O
sizes	O
by	O
orders	O
of	O
magnitude	O
is	O
not	O
the	O
path	O
forward".	O
We	O
find	O
that	O
path	O
is	O
still	O
promising	O
and	O
in	O
a	O
zero-shot	O
setting	O
GPT-3	B-MethodName
achieves	O
76%	B-MetricValue
on	O
LAMBADA,	B-DatasetName
a	O
gain	O
of	O
8%	O
over	O
the	O
previous	O
state	O
of	O
the	O
art.	O
LAMBADA	B-DatasetName
is	O
also	O
a	O
demonstration	O
of	O
the	O
flexibility	O
of	O
few-shot	B-TaskName
learning	O
as	O
it	O
provides	O
a	O
way	O
to	O
address	O
a	O
problem	O
that	O
classically	O
occurs	O
with	O
this	O
dataset.	O
Although	O
the	O
completion	O
in	O
LAMBADA	B-DatasetName
is	O
always	O
the	O
last	O
word	O
in	O
a	O
sentence,	O
a	O
standard	O
language	O
model	O
has	O
no	O
way	O
of	O
knowing	O
this	O
detail.	O
It	O
thus	O
assigns	O
probability	O
not	O
only	O
to	O
the	O
correct	O
ending	O
but	O
also	O
to	O
other	O
valid	O
continuations	O
of	O
the	O
paragraph.	O
This	O
problem	O
has	O
been	O
partially	O
addressed	O
in	O
the	O
past	O
with	O
stop-word	O
filters	O
[RWC	O
+	O
19]	O
(which	O
ban	O
"continuation"	O
words).	O
The	O
few-shot	B-TaskName
setting	O
instead	O
allows	O
us	O
to	O
"frame"	O
the	O
task	O
as	O
a	O
cloze-test	O
and	O
allows	O
the	O
language	O
model	O
to	O
infer	O
from	O
examples	O
that	O
a	O
completion	O
of	O
exactly	O
one	O
word	O
is	O
desired.	O
We	O
use	O
the	O
following	O
fill-in-the-blank	O
format:	O
Alice	O
was	O
friends	O
with	O
Bob.	O
Alice	O
went	O
to	O
visit	O
her	O
friend	O
.	O
→	O
Bob	O
George	O
bought	O
some	O
baseball	O
equipment,	O
a	O
ball,	O
a	O
glove,	O
and	O
a	O
.	O
→	O
When	O
presented	O
with	O
examples	O
formatted	O
this	O
way,	O
GPT-3	B-MethodName
achieves	O
86.4%	B-MetricValue
accuracy	B-MetricName
in	O
the	O
few-shot	B-TaskName
setting,	O
an	O
increase	O
of	O
over	O
18%	O
from	O
the	O
previous	O
state-of-the-art.	O
We	O
observe	O
that	O
few-shot	O
performance	O
improves	O
strongly	O
with	O
model	O
size.	O
While	O
this	O
setting	O
decreases	O
the	O
performance	O
of	O
the	O
smallest	O
model	O
by	O
almost	O
20%,	O
for	O
GPT-3	B-MethodName
it	O
improves	O
accuracy	O
by	O
10%.	O
Finally,	O
the	O
fill-in-blank	O
method	O
is	O
not	O
effective	O
one-shot,	B-TaskName
where	O
it	O
always	O
performs	O
worse	O
than	O
the	O
zero-shot	B-TaskName
setting.	O
Perhaps	O
this	O
is	O
because	O
all	O
models	O
still	O
require	O
several	O
examples	O
to	O
recognize	O
the	O
pattern.	O
One	O
note	O
of	O
caution	O
is	O
that	O
an	O
analysis	O
of	O
test	O
set	O
contamination	O
identified	O
that	O
a	O
significant	O
minority	O
of	O
the	O
LAMBADA	B-DatasetName
dataset	O
appears	O
to	O
be	O
present	O
in	O
our	O
training	O
data	O
-however	O
analysis	O
performed	O
in	O
Section	O
4	O
suggests	O
negligible	O
impact	O
on	O
performance.	O

In	O
this	O
section	O
we	O
measure	O
GPT-3's	B-MethodName
ability	O
to	O
answer	O
questions	O
about	O
broad	O
factual	O
knowledge.	O
Due	O
to	O
the	O
immense	O
amount	O
of	O
possible	O
queries,	O
this	O
task	O
has	O
normally	O
been	O
approached	O
by	O
using	O
an	O
information	O
retrieval	O
system	O
to	O
find	O
relevant	O
text	O
in	O
combination	O
with	O
a	O
model	O
which	O
learns	O
to	O
generate	O
an	O
answer	O
given	O
the	O
question	O
and	O
the	O
retrieved	O
text.	O
Since	O
this	O
setting	O
allows	O
a	O
system	O
to	O
search	O
for	O
and	O
condition	O
on	O
text	O
which	O
potentially	O
contains	O
the	O
answer	O
it	O
is	O
denoted	O
"open-book".	O
[RRS20]	O
recently	O
demonstrated	O
that	O
a	O
large	O
language	O
model	O
can	O
perform	O
surprisingly	O
well	O
directly	O
answering	O
the	O
questions	O
without	O
conditioning	O
on	O
auxilliary	O
information.	O
They	O
denote	O
this	O
more	O
restrictive	O
evaluation	O
setting	O
as	O
"closed-book".	O
Their	O
work	O
suggests	O
that	O
even	O
higher-capacity	O
models	O
could	O
perform	O
even	O
better	O
and	O
we	O
test	O
this	O
hypothesis	O
with	O
GPT-3.	B-MethodName
We	O
evaluate	O
GPT-3	B-MethodName
on	O
the	O
3	O
datasets	O
in	O
[RRS20]:	O
Natural	B-DatasetName
Questions	I-DatasetName
[KPR	O
+	O
19],	O
WebQuestions	B-DatasetName
[BCFL13],	O
and	O
TriviaQA	B-DatasetName
[JCWZ17],	O
using	O
the	O
same	O
splits.	O
Note	O
that	O
in	O
addition	O
to	O
all	O
results	O
being	O
in	O
the	O
closed-book	O
setting,	O
our	O
use	O
of	O
few-shot,	B-TaskName
one-shot,	B-TaskName
and	O
zero-shot	B-TaskName
evaluations	O
represent	O
an	O
even	O
stricter	O
setting	O
than	O
previous	O
closed-book	O
QA	O
work:	O
in	O
addition	O
to	O
external	O
content	O
not	O
being	O
allowed,	O
fine-tuning	O
on	O
the	O
Q&A	O
dataset	O
itself	O
is	O
also	O
not	O
permitted.	O
The	O
results	O
for	O
GPT-3	B-MethodName
are	O
shown	O
in	O
GPT-3's	B-MethodName
few-shot	O
result	O
further	O
improves	O
performance	O
another	O
3.2%	O
beyond	O
this.	O
On	O
WebQuestions	B-DatasetName
(WebQs),	B-DatasetName
GPT-3	B-MethodName
achieves	O
14.4%	B-MetricValue
in	O
the	O
zero-shot	O
setting,	O
25.3%	B-MetricValue
in	O
the	O
one-shot	O
setting,	O
and	O
41.5%	B-MetricValue
in	O
the	O
few-shot	O
setting.	O
This	O
compares	O
to	O
37.4%	B-MetricValue
for	O
fine-tuned	O
T5-11B,	O
and	O
44.7%	B-MetricValue
for	O
fine-tuned	O
T5-11B+SSM,	O
which	O
uses	O
a	O
Q&A-specific	O
pre-training	O
procedure.	O
GPT-3	B-MethodName
in	O
the	O
few-shot	O
setting	O
approaches	O
the	O
performance	O
of	O
state-of-the-art	O
fine-tuned	O
models.	O
Notably,	O
compared	O
to	O
TriviaQA,	B-DatasetName
WebQS	B-MetricValue
shows	O
a	O
much	O
larger	O
gain	O
from	O
zero-shot	B-TaskName
to	O
few-shot	B-TaskName
(and	O
indeed	O
its	O
zero-shot	B-TaskName
and	O
one-shot	B-TaskName
performance	O
are	O
poor),	O
perhaps	O
suggesting	O
that	O
the	O
WebQs	O
questions	O
and/or	O
the	O
style	O
of	O
their	O
answers	O
are	O
out-of-distribution	O
for	O
GPT-3.	B-MethodName
Nevertheless,	O
GPT-3	B-MethodName
appears	O
able	O
to	O
adapt	O
to	O
this	O
distribution,	O
recovering	O
strong	O
performance	O
in	O
the	O
few-shot	O
setting.	O
On	O
Natural	O
Questions	O
(NQs)	O
GPT-3	O
achieves	O
14.6%	B-MetricValue
in	O
the	O
zero-shot	O
setting,	O
23.0%	O
in	O
the	O
one-shot	O
setting,	O
and	O
29.9%	B-MetricValue
in	O
the	O
few-shot	O
setting,	O
compared	O
to	O
36.6%	B-MetricValue
for	O
fine-tuned	O
T5	O
11B+SSM.	O
Similar	O
to	O
WebQS,	B-DatasetName
the	O
large	O
gain	O
from	O
zero-shot	B-TaskName
to	O
few-shot	B-TaskName
may	O
suggest	O
a	O
distribution	O
shift,	O
and	O
may	O
also	O
explain	O
the	O
less	O
competitive	O
performance	O
compared	O
to	O
TriviaQA	B-DatasetName
and	O
WebQS.	B-DatasetName
In	O
particular,	O
the	O
questions	O
in	O
NQs	B-DatasetName
tend	O
towards	O
very	O
fine-grained	O
knowledge	O
on	O
Wikipedia	O
specifically	O
which	O
could	O
be	O
testing	O
the	O
limits	O
of	O
GPT-3's	B-MethodName
capacity	O
and	O
broad	O
pretraining	O
distribution.	O
Overall,	O
on	O
one	O
of	O
the	O
three	O
datasets	O
GPT-3's	B-DatasetName
one-shot	O
matches	O
the	O
open-domain	O
fine-tuning	O
SOTA.	O
On	O
the	O
other	O
two	O
datasets	O
it	O
approaches	O
the	O
performance	O
of	O
the	O
closed-book	O
SOTA	O
despite	O
not	O
using	O
fine-tuning.	O
On	O
all	O
3	O
datasets,	O
we	O
find	O
that	O
performance	O
scales	O
very	O
smoothly	O
with	O
model	O
size	O
(Figure	O
3.3	O
and	O
Appendix	O
H	O
Figure	O
H.7),	O
possibly	O
reflecting	O
the	O
idea	O
that	O
model	O
capacity	O
translates	O
directly	O
to	O
more	O
'knowledge'	O
absorbed	O
in	O
the	O
parameters	O
of	O
the	O
model.	O

We	O
next	O
evaluate	B-MethodName
GPT-3	I-MethodName
on	O
the	O
StoryCloze	B-DatasetName
2016	I-DatasetName
dataset	O
[MCH	O
+	O
16],	O
which	O
involves	O
selecting	O
the	O
correct	O
ending	O
sentence	O
for	O
five-sentence	O
long	O
stories.	O
Here	O
GPT-3	B-MethodName
achieves	O
83.2%	B-MetricValue
in	O
the	O
zero-shot	B-TaskName
setting	O
and	O
87.7%	B-MetricValue
in	O
the	O
few-shot	B-TaskName
setting	O
(with	O
K	B-HyperparameterName
=	O
70).	B-HyperparameterValue
This	O
is	O
still	O
4.1%	O
lower	O
than	O
the	O
fine-tuned	O
SOTA	O
using	O
a	O
BERT	B-MethodName
based	O
model	O
[LDL19]	O
but	O
improves	O
over	O
previous	O
zero-shot	O
results	O
by	O
roughly	O
10%.	O

The	O
HellaSwag	B-DatasetName
dataset	O
[ZHB	O
+	O
19]	O
involves	O
picking	O
the	O
best	O
ending	O
to	O
a	O
story	O
or	O
set	O
of	O
instructions.	O
The	O
examples	O
were	O
adversarially	O
mined	O
to	O
be	O
difficult	O
for	O
language	O
models	O
while	O
remaining	O
easy	O
for	O
humans	O
(who	O
achieve	O
95.6%	B-MetricValue
accuracy).	B-MetricName
GPT-3	O
achieves	O
78.1%	B-MetricValue
accuracy	B-MetricName
in	O
the	O
one-shot	O
setting	O
and	O
79.3%	B-MetricValue
accuracy	B-MetricName
in	O
the	O
few-shot	O
setting,	O
outperforming	O
the	O
75.4%	B-MetricValue
accuracy	B-MetricName
of	O
a	O
fine-tuned	O
1.5B	O
parameter	O
language	O
model	O
[ZHR	O
+	O
19]	O
but	O
still	O
a	O
fair	O
amount	O
lower	O
than	O
the	O
overall	O
SOTA	O
of	O
85.6%	B-MetricValue
achieved	O
by	O
the	O
fine-tuned	O
multi-task	O
model	O
ALUM.	O

In	O
this	O
section	O
we	O
test	O
GPT-3's	B-MethodName
performance	O
on	O
the	O
traditional	O
task	O
of	O
language	O
modeling,	O
as	O
well	O
as	O
related	O
tasks	O
that	O
involve	O
predicting	O
a	O
single	O
word	O
of	O
interest,	O
completing	O
a	O
sentence	O
or	O
paragraph,	O
or	O
choosing	O
between	O
possible	O
completions	O
of	O
a	O
piece	O
of	O
text.	O

In	O
Figure	O
3.1	O
we	O
display	O
training	O
curves	O
for	O
the	O
8	O
models	O
described	O
in	O
Section	O
2.	O
For	O
this	O
graph	O
we	O
also	O
include	O
6	O
additional	O
extra-small	O
models	O
with	O
as	O
few	O
as	O
100,000	O
parameters.	O
As	O
observed	O
in	O
[KMH	O
+	O
20],	O
language	O
modeling	O
performance	O
follows	O
a	O
power-law	O
when	O
making	O
efficient	O
use	O
of	O
training	O
compute.	O
After	O
extending	O
this	O
trend	O
by	O
two	O
more	O
orders	O
of	O
magnitude,	O
we	O
observe	O
only	O
a	O
slight	O
(if	O
any)	O
departure	O
from	O
the	O
power-law.	O
One	O
might	O
worry	O
that	O
these	O
improvements	O
in	O
cross-entropy	O
loss	O
come	O
only	O
from	O
modeling	O
spurious	O
details	O
of	O
our	O
training	O
corpus.	O
However,	O
we	O
will	O
see	O
in	O
the	O
following	O
sections	O
that	O
improvements	O
in	O
cross-entropy	O
loss	O
lead	O
to	O
consistent	O
performance	O
gains	O
across	O
a	O
broad	O
spectrum	O
of	O
natural	O
language	O
tasks.	O
Below,	O
we	O
evaluate	O
the	O
8	O
models	O
described	O
in	O
Section	O
2	O
(the	O
175	O
billion	O
parameter	O
parameter	O
GPT-3	B-MethodName
and	O
7	O
smaller	O
models)	O
on	O
a	O
wide	O
range	O
of	O
datasets.	O
We	O
group	O
the	O
datasets	O
into	O
9	O
categories	O
representing	O
roughly	O
similar	O
tasks.	O
In	O
Section	O
3.1	O
we	O
evaluate	O
on	O
traditional	O
language	O
modeling	O
tasks	O
and	O
tasks	O
that	O
are	O
similar	O
to	O
language	O
modeling,	O
such	O
as	O
Cloze	B-TaskName
tasks	O
and	O
sentence/paragraph	O
completion	O
tasks.	O
In	O
Section	O
3.2	O
we	O
evaluate	O
on	O
"closed	B-TaskName
book"	I-TaskName
question	I-TaskName
answering	I-TaskName
tasks:	O
tasks	O
which	O
require	O
using	O
the	O
information	O
stored	O
in	O
the	O
model's	O
parameters	O
to	O
answer	O
general	O
knowledge	O
questions.	O
In	O
Section	O
3.3	O
we	O
evaluate	O
the	O
model's	O
ability	O
to	O
translate	O
between	O
languages	O
(especially	O
one-shot	B-TaskName
and	O
few-shot).	B-TaskName
In	O
Section	O
3.4	O
we	O
evaluate	O
the	O
model's	O
performance	O
on	O
Winograd	B-TaskName
Schema-like	I-TaskName
tasks.	O
In	O
Section	O
3.5	O
we	O
evaluate	O
on	O
datasets	O
that	O
involve	O
commonsense	O
reasoning	O
or	O
question	O
answering.	O
In	O
Section	O
3.6	O
we	O
evaluate	O
on	O
reading	O
comprehension	O
tasks,	O
in	O
Section	O
3.7	O
we	O
evaluate	O
on	O
the	O
SuperGLUE	B-DatasetName
benchmark	O
suite,	O
and	O
in	O
3.8	O
we	O
briefly	O
explore	O
NLI.	B-TaskName
Finally,	O
in	O
Section	O
3.9,	O
we	O
invent	O
some	O
additional	O
tasks	O
designed	O
especially	O
to	O
probe	O
in-context	O
learning	O
abilitiesthese	O
tasks	O
focus	O
on	O
on-the-fly	O
reasoning,	O
adaptation	O
skills,	O
or	O
open-ended	O
text	O
synthesis.	O
We	O
evaluate	O
all	O
tasks	O
in	O
the	O
few-shot,	B-TaskName
one-shot,	B-TaskName
and	O
zero-shot	B-TaskName
settings.	O

For	O
few-shot	B-TaskName
learning,	I-TaskName
we	O
evaluate	O
each	O
example	O
in	O
the	O
evaluation	O
set	O
by	O
randomly	O
drawing	O
K	B-HyperparameterName
examples	O
from	O
that	O
task's	O
training	O
set	O
as	O
conditioning,	O
delimited	O
by	O
1	O
or	O
2	O
newlines	O
depending	O
on	O
the	O
task.	O
For	O
LAMBADA	B-DatasetName
and	O
Storycloze	B-DatasetName
there	O
is	O
no	O
supervised	O
training	O
set	O
available	O
so	O
we	O
draw	O
conditioning	O
examples	O
from	O
the	O
development	O
set	O
and	O
evaluate	O
on	O
the	O
test	O
set.	O
For	O
Winograd	B-DatasetName
(the	O
original,	O
not	O
SuperGLUE	O
version)	O
there	O
is	O
only	O
one	O
dataset,	O
so	O
we	O
draw	O
conditioning	O
examples	O
directly	O
from	O
it.	O
K	B-HyperparameterName
can	O
be	O
any	O
value	O
from	O
0	B-HyperparameterValue
to	O
the	O
maximum	O
amount	O
allowed	O
by	O
the	O
model's	O
context	O
window,	O
which	O
is	O
n	B-HyperparameterName
ctx	I-HyperparameterName
=	O
2048	B-HyperparameterValue
for	O
all	O
models	O
and	O
typically	O
fits	O
10	B-HyperparameterValue
to	I-HyperparameterValue
100	I-HyperparameterValue
examples.	O
Larger	O
values	O
of	O
K	B-HyperparameterName
are	O
usually	O
but	O
not	O
always	O
better,	O
so	O
when	O
a	O
separate	O
development	O
and	O
test	O
set	O
are	O
available,	O
we	O
experiment	O
with	O
a	O
few	O
values	O
of	O
K	B-HyperparameterName
on	O
the	O
development	O
set	O
and	O
then	O
run	O
the	O
best	O
value	O
on	O
the	O
test	O
set.	O
For	O
some	O
tasks	O
(see	O
Appendix	O
G)	O
we	O
also	O
use	O
a	O
natural	O
language	O
prompt	O
in	O
addition	O
to	O
(or	O
for	O
K	B-HyperparameterName
=	O
0,	B-HyperparameterValue
instead	O
of)	O
demonstrations.	O
On	O
tasks	O
that	O
involve	O
choosing	O
one	O
correct	O
completion	O
from	O
several	O
options	O
(multiple	O
choice),	O
we	O
provide	O
K	B-HyperparameterName
examples	O
of	O
context	O
plus	O
correct	O
completion,	O
followed	O
by	O
one	O
example	O
of	O
context	O
only,	O
and	O
compare	O
the	O
LM	O
likelihood	O
of	O
each	O
completion.	O
For	O
most	O
tasks	O
we	O
compare	O
the	O
per-token	O
likelihood	O
(to	O
normalize	O
for	O
length),	O
however	O
on	O
a	O
small	O
number	O
of	O
datasets	O
(ARC,	B-DatasetName
OpenBookQA,	B-DatasetName
and	O
RACE)	B-DatasetName
we	O
gain	O
additional	O
benefit	O
as	O
measured	O
on	O
the	O
development	O
set	O
by	O
normalizing	O
by	O
the	O
unconditional	O
probability	O
of	O
each	O
completion,	O
by	O
computing	O
P	O
(completion|context)	O
P	O
(completion|answer	O
context)	O
,	O
where	O
answer	O
context	O
is	O
the	O
string	O
"Answer:	O
"	O
or	O
"A:	O
"	O
and	O
is	O
used	O
to	O
prompt	O
that	O
the	O
completion	O
should	O
be	O
an	O
answer	O
but	O
is	O
otherwise	O
generic.	O
On	O
tasks	O
that	O
involve	O
binary	O
classification,	O
we	O
give	O
the	O
options	O
more	O
semantically	O
meaningful	O
names	O
(e.g.	O
"True"	O
or	O
"False"	O
rather	O
than	O
0	O
or	O
1)	O
and	O
then	O
treat	O
the	O
task	O
like	O
multiple	O
choice;	O
we	O
also	O
sometimes	O
frame	O
the	O
task	O
similar	O
to	O
what	O
is	O
done	O
by	O
[RSR	O
+	O
19]	O
(see	O
Appendix	O
G)	O
for	O
details.	O
On	O
tasks	O
with	O
free-form	O
completion,	O
we	O
use	O
beam	O
search	O
with	O
the	O
same	O
parameters	O
as	O
[RSR	O
+	O
19]:	O
a	O
beam	B-HyperparameterName
width	I-HyperparameterName
of	O
4	B-HyperparameterValue
and	O
a	O
length	O
penalty	O
of	O
α	B-HyperparameterName
=	O
0.6.	B-HyperparameterValue
We	O
score	O
the	O
model	O
using	O
F1	B-MetricName
similarity	O
score,	O
BLEU,	B-MetricName
or	O
exact	O
match,	O
depending	O
on	O
what	O
is	O
standard	O
for	O
the	O
dataset	O
at	O
hand.	O
Final	O
results	O
are	O
reported	O
on	O
the	O
test	O
set	O
when	O
publicly	O
available,	O
for	O
each	O
model	O
size	O
and	O
learning	O
setting	O
(zero-,	B-TaskName
one-,	B-TaskName
and	O
few-shot).	B-TaskName
When	O
the	O
test	O
set	O
is	O
private,	O
our	O
model	O
is	O
often	O
too	O
large	O
to	O
fit	O
on	O
the	O
test	O
server,	O
so	O
we	O
report	O
results	O
on	O
the	O
development	O
set.	O
We	O
do	O
submit	O
to	O
the	O
test	O
server	O
on	O
a	O
small	O
number	O
of	O
datasets	O
(SuperGLUE,	B-DatasetName
TriviaQA,	B-DatasetName
PiQa)	B-DatasetName
where	O
we	O
were	O
able	O
to	O
make	O
submission	O
work,	O
and	O
we	O
submit	O
only	O
the	O
200B	O
few-shot	B-TaskName
results,	O
and	O
report	O
development	O
set	O
results	O
for	O
everything	O
else.	O

As	O
found	O
in	O
[KMH	O
+	O
20,MKAT18],	O
larger	O
models	O
can	O
typically	O
use	O
a	O
larger	O
batch	B-HyperparameterName
size,	I-HyperparameterName
but	O
require	O
a	O
smaller	O
learning	B-HyperparameterName
rate.	I-HyperparameterName
We	O
measure	O
the	O
gradient	O
noise	O
scale	O
during	O
training	O
and	O
use	O
it	O
to	O
guide	O
our	O
choice	O
of	O
batch	B-HyperparameterName
size	I-HyperparameterName
[MKAT18].	O
Table	O
2.1	O
shows	O
the	O
parameter	O
settings	O
we	O
used.	O
To	O
train	O
the	O
larger	O
models	O
without	O
running	O
out	O
of	O
memory,	O
we	O
use	O
a	O
mixture	O
of	O
model	O
parallelism	O
within	O
each	O
matrix	O
multiply	O
and	O
model	O
parallelism	O
across	O
the	O
layers	O
of	O
the	O
network.	O
All	O
models	O
were	O
trained	O
on	O
V100	O
GPU's	O
on	O
part	O
of	O
a	O
high-bandwidth	O
cluster	O
provided	O
by	O
Microsoft.	O
Details	O
of	O
the	O
training	O
process	O
and	O
hyperparameter	O
settings	O
are	O
described	O
in	O
Appendix	O
B.	O

Datasets	O
for	O
language	O
models	O
have	O
rapidly	O
expanded,	O
culminating	O
in	O
the	O
Common	B-DatasetName
Crawl	I-DatasetName
dataset	I-DatasetName
2	I-DatasetName
[RSR	O
+	O
19]	O
constituting	O
nearly	O
a	O
trillion	O
words.	O
This	O
size	O
of	O
dataset	O
is	O
sufficient	O
to	O
train	O
our	O
largest	O
models	O
without	O
ever	O
updating	O
on	O
the	O
same	O
sequence	O
twice.	O
However,	O
we	O
have	O
found	O
that	O
unfiltered	O
or	O
lightly	O
filtered	O
versions	O
of	O
Common	B-DatasetName
Crawl	I-DatasetName
tend	O
to	O
have	O
lower	O
quality	O
than	O
more	O
curated	O
datasets.	O
Therefore,	O
we	O
took	O
3	O
steps	O
to	O
improve	O
the	O
average	O
quality	O
of	O
our	O
datasets:	O
(1)	O
we	O
downloaded	O
and	O
filtered	O
a	B-DatasetName
version	I-DatasetName
of	I-DatasetName
CommonCrawl	I-DatasetName
based	O
on	O
similarity	O
to	O
a	O
range	O
of	O
high-quality	O
reference	O
corpora,	O
(2)	O
we	O
performed	O
fuzzy	O
deduplication	O
at	O
the	O
document	O
level,	O
within	O
and	O
across	O
datasets,	O
to	O
prevent	O
redundancy	O
and	O
preserve	O
the	O
integrity	O
of	O
our	O
held-out	O
validation	O
set	O
as	O
an	O
accurate	O
measure	O
of	O
overfitting,	O
and	O
(3)	O
we	O
also	O
added	O
known	O
high-quality	O
reference	O
corpora	O
to	O
the	O
training	O
mix	O
to	O
augment	O
CommonCrawl	O
and	O
increase	O
its	O
diversity.	O
Details	O
of	O
the	O
first	O
two	O
points	O
(processing	O
of	O
Common	B-DatasetName
Crawl)	I-DatasetName
are	O
described	O
in	O
Appendix	O
A.	O
For	O
the	O
third,	O
we	O
added	O
several	O
curated	O
high-quality	O
datasets,	O
including	O
an	O
expanded	O
version	O
of	O
the	O
WebText	O
dataset	O
[RWC	O
+	O
19],	O
collected	O
by	O
scraping	O
links	O
over	O
a	O
longer	O
period	O
of	O
time,	O
and	O
first	O
described	O
in	O
[KMH	O
+	O
20],	O
two	O
internet-based	O
books	O
corpora	O
(Books1	O
and	O
Books2)	O
and	O
English-language	O
Wikipedia.	O
Table	O
2.2	O
shows	O
the	O
final	O
mixture	O
of	O
datasets	O
that	O
we	O
used	O
in	O
training.	O
The	O
CommonCrawl	B-DatasetName
data	O
was	O
downloaded	O
from	O
41	O
shards	O
of	O
monthly	O
CommonCrawl	B-DatasetName
covering	O
2016	O
to	O
2019,	O
constituting	O
45TB	O
of	O
compressed	O
plaintext	O
before	O
filtering	O
and	O
570GB	O
after	O
filtering,	O
roughly	O
equivalent	O
to	O
400	O
billion	O
byte-pair-encoded	O
tokens.	O
Note	O
that	O
during	O
training,	O
datasets	O
are	O
not	O
sampled	O
in	O
proportion	O
to	O
their	O
size,	O
but	O
rather	O
datasets	O
we	O
view	O
as	O
higher-quality	O
are	O
sampled	O
more	O
frequently,	O
such	O
that	O
CommonCrawl	B-DatasetName
and	O
Books2	B-DatasetName
datasets	O
are	O
sampled	O
less	O
than	O
once	O
during	O
training,	O
but	O
the	O
other	O
datasets	O
are	O
sampled	O
2-3	O
times.	O
This	O
essentially	O
accepts	O
a	O
small	O
amount	O
of	O
overfitting	O
in	O
exchange	O
for	O
higher	O
quality	O
training	O
data.	O
that	O
are	O
drawn	O
from	O
a	O
given	O
dataset,	O
which	O
we	O
intentionally	O
do	O
not	O
make	O
proportional	O
to	O
the	O
size	O
of	O
the	O
dataset.	O
As	O
a	O
result,	O
when	O
we	O
train	O
for	O
300	O
billion	O
tokens,	O
some	O
datasets	O
are	O
seen	O
up	O
to	O
3.4	O
times	O
during	O
training	O
while	O
other	O
datasets	O
are	O
seen	O
less	O
than	O
once.	O
A	O
major	O
methodological	O
concern	O
with	O
language	O
models	O
pretrained	O
on	O
a	O
broad	O
swath	O
of	O
internet	O
data,	O
particularly	O
large	O
models	O
with	O
the	O
capacity	O
to	O
memorize	O
vast	O
amounts	O
of	O
content,	O
is	O
potential	O
contamination	O
of	O
downstream	O
tasks	O
by	O
having	O
their	O
test	O
or	O
development	O
sets	O
inadvertently	O
seen	O
during	O
pre-training.	O
To	O
reduce	O
such	O
contamination,	O
we	O
searched	O
for	O
and	O
attempted	O
to	O
remove	O
any	O
overlaps	O
with	O
the	O
development	O
and	O
test	O
sets	O
of	O
all	O
benchmarks	O
studied	O
in	O
this	O
paper.	O
Unfortunately,	O
a	O
bug	O
in	O
the	O
filtering	O
caused	O
us	O
to	O
ignore	O
some	O
overlaps,	O
and	O
due	O
to	O
the	O
cost	O
of	O
training	O
it	O
was	O
not	O
feasible	O
to	O
retrain	O
the	O
model.	O
In	O
Section	O
4	O
we	O
characterize	O
the	O
impact	O
of	O
the	O
remaining	O
overlaps,	O
and	O
in	O
future	O
work	O
we	O
will	O
more	O
aggressively	O
remove	O
data	O
contamination.	O

We	O
use	O
the	O
same	O
model	O
and	O
architecture	O
as	O
GPT-2	O
[RWC	O
+	O
19],	O
including	O
the	O
modified	O
initialization,	O
pre-normalization,	O
and	O
reversible	O
tokenization	O
described	O
therein,	O
with	O
the	O
exception	O
that	O
we	O
use	O
alternating	O
dense	O
and	O
locally	O
banded	O
sparse	O
attention	O
patterns	O
in	O
the	O
layers	O
of	O
the	O
transformer,	O
similar	O
to	O
the	O
Sparse	O
Transformer	O
[CGRS19].	O
To	O
study	O
the	O
dependence	O
of	O
ML	O
performance	O
on	O
model	O
size,	O
we	O
train	O
8	O
different	O
sizes	O
of	O
model,	O
ranging	O
over	O
three	O
orders	O
of	O
magnitude	O
from	O
125	O
million	O
parameters	O
to	O
175	O
billion	O
parameters,	O
with	O
the	O
last	O
being	O
the	O
model	O
we	O
call	O
GPT-3.	B-MethodName
Previous	O
work	O
[KMH	O
+	O
20]	O
suggests	O
that	O
with	O
enough	O
training	O
data,	O
scaling	O
of	O
validation	O
loss	O
should	O
be	O
approximately	O
a	O
smooth	O
power	O
law	O
as	O
a	O
function	O
of	O
size;	O
training	O
models	O
of	O
many	O
different	O
sizes	O
allows	O
us	O
to	O
test	O
this	O
hypothesis	O
both	O
for	O
validation	O
loss	O
and	O
for	O
downstream	O
language	O
tasks.	O
Table	O
2.1	O
shows	O
the	O
sizes	O
and	O
architectures	O
of	O
our	O
8	O
models.	O
Here	O
n	O
params	O
is	O
the	O
total	O
number	O
of	O
trainable	O
parameters,	O
n	B-HyperparameterName
layers	I-HyperparameterName
is	O
the	O
total	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
layers,	I-HyperparameterName
d	B-HyperparameterName
model	I-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
units	I-HyperparameterName
in	I-HyperparameterName
each	I-HyperparameterName
bottleneck	I-HyperparameterName
layer	I-HyperparameterName
(we	O
always	O
have	O
the	O
feedforward	O
layer	O
four	O
times	O
the	O
size	O
of	O
the	O
bottleneck	O
layer,	O
d	B-HyperparameterName
ff	I-HyperparameterName
=	O
4	B-HyperparameterValue
*	I-HyperparameterValue
d	I-HyperparameterValue
model	I-HyperparameterValue
),	O
and	O
d	B-HyperparameterName
head	I-HyperparameterName
is	O
the	O
dimension	B-HyperparameterName
of	I-HyperparameterName
each	I-HyperparameterName
attention	I-HyperparameterName
head.	I-HyperparameterName
All	O
models	O
use	O
a	O
context	B-HyperparameterName
window	I-HyperparameterName
of	O
n	B-HyperparameterName
ctx	I-HyperparameterName
=	O
2048	B-HyperparameterValue
tokens.	I-HyperparameterValue
We	O
partition	O
the	O
model	O
across	O
GPUs	O
along	O
both	O
the	O
depth	O
and	O
width	O
dimension	O
in	O
order	O
to	O
minimize	O
data-transfer	O
between	O
nodes.	O
The	O
precise	O
architectural	O
parameters	O
for	O
each	O
model	O
are	O
chosen	O
based	O
on	O
computational	O
efficiency	O
and	O
load-balancing	O
in	O
the	O
layout	O
of	O
models	O
across	O
GPU's.	O
Previous	O
work	O
[KMH	O
+	O
20]	O
suggests	O
that	O
validation	O
loss	O
is	O
not	O
strongly	O
sensitive	O
to	O
these	O
parameters	O
within	O
a	O
reasonably	O
broad	O
range.	O

Our	O
basic	O
pre-training	O
approach,	O
including	O
model,	O
data,	O
and	O
training,	O
is	O
similar	O
to	O
the	O
process	O
described	O
in	O
[RWC	O
+	O
19],	O
with	O
relatively	O
straightforward	O
scaling	O
up	O
of	O
the	O
model	O
size,	O
dataset	O
size	O
and	O
diversity,	O
and	O
length	O
of	O
training.	O
Our	O
use	O
of	O
in-context	O
learning	O
is	O
also	O
similar	O
to	O
[RWC	O
+	O
19],	O
but	O
in	O
this	O
work	O
we	O
systematically	O
explore	O
different	O
settings	O
for	O
learning	O
within	O
the	O
context.	O
Therefore,	O
we	O
start	O
this	O
section	O
by	O
explicitly	O
defining	O
and	O
contrasting	O
the	O
different	O
settings	O
that	O
we	O
will	O
be	O
evaluating	O
GPT-3	B-MethodName
on	O
or	O
could	O
in	O
principle	O
evaluate	O
GPT-3	B-MethodName
on.	O
These	O
settings	O
can	O
be	O
seen	O
as	O
lying	O
on	O
a	O
spectrum	O
of	O
how	O
much	O
task-specific	O
data	O
they	O
tend	O
to	O
rely	O
on.	O
Specifically,	O
we	O
can	O
identify	O
at	O
least	O
four	O
points	O
on	O
this	O
spectrum	O
(see	O
Figure	O
2.1	O
for	O
an	O
illustration):	O
•	O
Fine-Tuning	O
(FT)	O
has	O
been	O
the	O
most	O
common	O
approach	O
in	O
recent	O
years,	O
and	O
involves	O
updating	O
the	O
weights	O
of	O
a	O
pre-trained	O
model	O
by	O
training	O
on	O
a	O
supervised	O
dataset	O
specific	O
to	O
the	O
desired	O
task.	O
Typically	O
thousands	O
to	O
hundreds	O
of	O
thousands	O
of	O
labeled	O
examples	O
are	O
used.	O
The	O
main	O
advantage	O
of	O
fine-tuning	O
is	O
strong	O
performance	O
on	O
many	O
benchmarks.	O
The	O
main	O
disadvantages	O
are	O
the	O
need	O
for	O
a	O
new	O
large	O
dataset	O
for	O
every	O
task,	O
the	O
potential	O
for	O
poor	O
generalization	O
out-of-distribution	O
[MPL19],	O
and	O
the	O
potential	O
to	O
exploit	O
spurious	O
features	O
of	O
the	O
training	O
data	O
[GSL	O
+	O
18,	O
NK19],	O
potentially	O
resulting	O
in	O
an	O
unfair	O
comparison	O
with	O
human	O
performance.	O
In	O
this	O
work	O
we	O
do	O
not	O
fine-tune	O
GPT-3	B-MethodName
because	O
our	O
focus	O
is	O
on	O
task-agnostic	O
performance,	O
but	O
GPT-3	B-MethodName
can	O
be	O
fine-tuned	O
in	O
principle	O
and	O
this	O
is	O
a	O
promising	O
direction	O
for	O
future	O
work.	O
•	O
Few-Shot	B-TaskName
(FS)	B-TaskName
is	O
the	O
term	O
we	O
will	O
use	O
in	O
this	O
work	O
to	O
refer	O
to	O
the	O
setting	O
where	O
the	O
model	O
is	O
given	O
a	O
few	O
demonstrations	O
of	O
the	O
task	O
at	O
inference	O
time	O
as	O
conditioning	O
[RWC	O
+	O
19],	O
but	O
no	O
weight	O
updates	O
are	O
allowed.	O
As	O
shown	O
in	O
Figure	O
2.1,	O
for	O
a	O
typical	O
dataset	O
an	O
example	O
has	O
a	O
context	O
and	O
a	O
desired	O
completion	O
(for	O
example	O
an	O
English	O
sentence	O
and	O
the	O
French	O
translation),	O
and	O
few-shot	B-TaskName
works	O
by	O
giving	O
K	B-HyperparameterName
examples	O
of	O
context	O
and	O
completion,	O
and	O
then	O
one	O
final	O
example	O
of	O
context,	O
with	O
the	O
model	O
expected	O
to	O
provide	O
the	O
completion.	O
We	O
typically	O
set	O
K	B-HyperparameterName
in	O
the	O
range	O
of	O
10	B-HyperparameterValue
to	I-HyperparameterValue
100	I-HyperparameterValue
as	O
this	O
is	O
how	O
many	O
examples	O
can	O
fit	O
in	O
the	O
model's	O
context	O
window	O
(n	B-HyperparameterName
ctx	O
=	O
2048).	B-HyperparameterValue
The	O
main	O
advantages	O
of	O
few-shot	O
are	O
a	O
major	O
reduction	O
in	O
the	O
need	O
for	O
task-specific	O
data	O
and	O
reduced	O
potential	O
to	O
learn	O
an	O
overly	O
narrow	O
distribution	O
from	O
a	O
large	O
but	O
narrow	O
fine-tuning	O
dataset.	O
The	O
main	O
disadvantage	O
is	O
that	O
results	O
from	O
this	O
method	O
have	O
so	O
far	O
been	O
much	O
worse	O
than	O
state-of-the-art	O
fine-tuned	O
models.	O
Also,	O
a	O
small	O
amount	O
of	O
task	O
specific	O
data	O
is	O
still	O
required.	O
As	O
indicated	O
by	O
the	O
name,	O
few-shot	B-TaskName
learning	I-TaskName
as	O
described	O
here	O
for	O
language	O
models	O
is	O
related	O
to	O
few-shot	B-TaskName
learning	I-TaskName
as	O
used	O
in	O
other	O
contexts	O
in	O
ML	O
[HYC01,	O
VBL	O
+	O
16]	O
-both	O
involve	O
learning	O
based	O
on	O
a	O
broad	O
distribution	O
of	O
tasks	O
(in	O
this	O
case	O
implicit	O
in	O
the	O
pre-training	O
data)	O
and	O
then	O
rapidly	O
adapting	O
to	O
a	O
new	O
task.	O
•	O
One-Shot	B-TaskName
(1S)	B-TaskName
is	O
the	O
same	O
as	O
few-shot	B-TaskName
except	O
that	O
only	O
one	O
demonstration	O
is	O
allowed,	O
in	O
addition	O
to	O
a	O
natural	O
language	O
description	O
of	O
the	O
task,	O
as	O
shown	O
in	O
Figure	O
1.	O
The	O
reason	O
to	O
distinguish	O
one-shot	B-TaskName
from	O
few-shot	B-TaskName
and	O
zero-shot	B-TaskName
(below)	O
is	O
that	O
it	O
most	O
closely	O
matches	O
the	O
way	O
in	O
which	O
some	O
tasks	O
are	O
communicated	O
to	O
humans.	O
For	O
example,	O
when	O
asking	O
humans	O
to	O
generate	O
a	O
dataset	O
on	O
a	O
human	O
worker	O
service	O
(for	O
example	O
Mechanical	O
Turk),	O
it	O
is	O
common	O
to	O
give	O
one	O
demonstration	O
of	O
the	O
task.	O
By	O
contrast	O
it	O
is	O
sometimes	O
difficult	O
to	O
communicate	O
the	O
content	O
or	O
format	O
of	O
a	O
task	O
if	O
no	O
examples	O
are	O
given.	O
Figure	O
2.1:	O
Zero-shot,	B-TaskName
one-shot	B-TaskName
and	O
few-shot,	B-TaskName
contrasted	O
with	O
traditional	O
fine-tuning.	O
The	O
panels	O
above	O
show	O
four	O
methods	O
for	O
performing	O
a	O
task	O
with	O
a	O
language	O
model	O
-fine-tuning	O
is	O
the	O
traditional	O
method,	O
whereas	O
zero-,	O
one-,	O
and	O
few-shot,	O
which	O
we	O
study	O
in	O
this	O
work,	O
require	O
the	O
model	O
to	O
perform	O
the	O
task	O
with	O
only	O
forward	O
passes	O
at	O
test	O
time.	O
We	O
typically	O
present	O
the	O
model	O
with	O
a	O
few	O
dozen	O
examples	O
in	O
the	O
few	O
shot	O
setting.	O
Exact	O
phrasings	O
for	O
all	O
task	O
descriptions,	O
examples	O
and	O
prompts	O
can	O
be	O
found	O
in	O
Appendix	O
G.	O
•	O
Zero-Shot	B-TaskName
(0S)	B-TaskName
is	O
the	O
same	O
as	O
one-shot	B-DatasetName
except	O
that	O
no	O
demonstrations	O
are	O
allowed,	O
and	O
the	O
model	O
is	O
only	O
given	O
a	O
natural	O
language	O
instruction	O
describing	O
the	O
task.	O
This	O
method	O
provides	O
maximum	O
convenience,	O
potential	O
for	O
robustness,	O
and	O
avoidance	O
of	O
spurious	O
correlations	O
(unless	O
they	O
occur	O
very	O
broadly	O
across	O
the	O
large	O
corpus	O
of	O
pre-training	O
data),	O
but	O
is	O
also	O
the	O
most	O
challenging	O
setting.	O
In	O
some	O
cases	O
it	O
may	O
even	O
be	O
difficult	O
for	O
humans	O
to	O
understand	O
the	O
format	O
of	O
the	O
task	O
without	O
prior	O
examples,	O
so	O
this	O
setting	O
is	O
in	O
some	O
cases	O
"unfairly	O
hard".	O
For	O
example,	O
if	O
someone	O
is	O
asked	O
to	O
"make	O
a	O
Figure	O
2.1	O
shows	O
the	O
four	O
methods	O
using	O
the	O
example	O
of	O
translating	O
English	O
to	O
French.	O
In	O
this	O
paper	O
we	O
focus	O
on	O
zero-shot,	O
one-shot	O
and	O
few-shot,	O
with	O
the	O
aim	O
of	O
comparing	O
them	O
not	O
as	O
competing	O
alternatives,	O
but	O
as	O
different	O
problem	O
settings	O
which	O
offer	O
a	O
varying	O
trade-off	O
between	O
performance	O
on	O
specific	O
benchmarks	O
and	O
sample	O
efficiency.	O
We	O
especially	O
highlight	O
the	O
few-shot	O
results	O
as	O
many	O
of	O
them	O
are	O
only	O
slightly	O
behind	O
state-of-the-art	O
fine-tuned	O
models.	O
Ultimately,	O
however,	O
one-shot,	O
or	O
even	O
sometimes	O
zero-shot,	O
seem	O
like	O
the	O
fairest	O
comparisons	O
to	O
human	O
performance,	O
and	O
are	O
important	O
targets	O
for	O
future	O
work.	O
Table	O
2.1:	O
Sizes,	O
architectures,	O
and	O
learning	O
hyper-parameters	O
(batch	O
size	O
in	O
tokens	O
and	O
learning	O
rate)	O
of	O
the	O
models	O
which	O
we	O
trained.	O
All	O
models	O
were	O
trained	O
for	O
a	O
total	O
of	O
300	O
billion	O
tokens.	O

Recent	O
years	O
have	O
featured	O
a	O
trend	O
towards	O
pre-trained	O
language	O
representations	O
in	O
NLP	O
systems,	O
applied	O
in	O
increasingly	O
flexible	O
and	O
task-agnostic	O
ways	O
for	O
downstream	O
transfer.	O
First,	O
single-layer	O
representations	O
were	O
learned	O
using	O
word	O
vectors	O
[MCCD13,PSM14]	O
and	O
fed	O
to	O
task-specific	O
architectures,	O
then	O
RNNs	O
with	O
multiple	O
layers	O
of	O
representations	O
and	O
contextual	O
state	O
were	O
used	O
to	O
form	O
stronger	O
representations	O
[DL15,MBXS17,PNZtY18]	O
(though	O
still	O
applied	O
to	O
task-specific	O
architectures),	O
and	O
more	O
recently	O
pre-trained	O
recurrent	O
or	O
transformer	O
language	O
models	O
[VSP	O
+	O
17]	O
have	O
been	O
directly	O
fine-tuned,	O
entirely	O
removing	O
the	O
need	O
for	O
task-specific	O
architectures	O
[RNSS18,DCLT18,HR18].	O
This	O
last	O
paradigm	O
has	O
led	O
to	O
substantial	O
progress	O
on	O
many	O
challenging	O
NLP	O
tasks	O
such	O
as	O
reading	O
comprehension,	O
question	O
answering,	O
textual	O
entailment,	O
and	O
many	O
others,	O
and	O
has	O
continued	O
to	O
advance	O
based	O
on	O
new	O
architectures	O
and	O
algorithms	O
[RSR	O
+	O
19,	O
LOG	O
+	O
19,	O
YDY	O
+	O
19,	O
LCG	O
+	O
19].	O
However,	O
a	O
major	O
limitation	O
to	O
this	O
approach	O
is	O
that	O
while	O
the	O
architecture	O
is	O
task-agnostic,	O
there	O
is	O
still	O
a	O
need	O
for	O
task-specific	O
datasets	O
and	O
task-specific	O
fine-tuning:	O
to	O
achieve	O
strong	O
performance	O
on	O
a	O
desired	O
task	O
typically	O
requires	O
fine-tuning	O
on	O
a	O
dataset	O
of	O
thousands	O
to	O
hundreds	O
of	O
thousands	O
of	O
examples	O
specific	O
to	O
that	O
task.	O
Removing	O
this	O
limitation	O
would	O
be	O
desirable,	O
for	O
several	O
reasons.	O
First,	O
from	O
a	O
practical	O
perspective,	O
the	O
need	O
for	O
a	O
large	O
dataset	O
of	O
labeled	O
examples	O
for	O
every	O
new	O
task	O
limits	O
the	O
applicability	O
of	O
language	O
models.	O
There	O
exists	O
a	O
very	O
wide	O
range	O
of	O
possible	O
useful	O
language	O
tasks,	O
encompassing	O
anything	O
from	O
correcting	O
grammar,	O
to	O
generating	O
examples	O
of	O
an	O
abstract	O
concept,	O
to	O
critiquing	O
a	O
short	O
story.	O
For	O
many	O
of	O
these	O
tasks	O
it	O
is	O
difficult	O
to	O
collect	O
a	O
large	O
supervised	O
training	O
dataset,	O
especially	O
when	O
the	O
process	O
must	O
be	O
repeated	O
for	O
every	O
new	O
task.	O
Second,	O
the	O
potential	O
to	O
exploit	O
spurious	O
correlations	O
in	O
training	O
data	O
fundamentally	O
grows	O
with	O
the	O
expressiveness	O
of	O
the	O
model	O
and	O
the	O
narrowness	O
of	O
the	O
training	O
distribution.	O
This	O
can	O
create	O
problems	O
for	O
the	O
pre-training	O
plus	O
fine-tuning	O
paradigm,	O
where	O
models	O
are	O
designed	O
to	O
be	O
large	O
to	O
absorb	O
information	O
during	O
pre-training,	O
but	O
are	O
then	O
fine-tuned	O
on	O
very	O
narrow	O
task	O
distributions.	O
For	O
instance	O
[HLW	O
+	O
20]	O
observe	O
that	O
larger	O
models	O
do	O
not	O
necessarily	O
generalize	O
better	O
out-of-distribution.	O
There	O
is	O
evidence	O
that	O
suggests	O
that	O
the	O
generalization	O
achieved	O
under	O
this	O
paradigm	O
can	O
be	O
poor	O
because	O
the	O
model	O
is	O
overly	O
specific	O
to	O
the	O
training	O
distribution	O
and	O
does	O
not	O
generalize	O
well	O
outside	O
it	O
[YdC	O
+	O
19,	O
MPL19].	O
Thus,	O
the	O
performance	O
of	O
fine-tuned	O
models	O
on	O
specific	O
benchmarks,	O
even	O
when	O
it	O
is	O
nominally	O
at	O
human-level,	O
may	O
exaggerate	O
actual	O
performance	O
on	O
the	O
underlying	O
task	O
[GSL	O
+	O
18,	O
NK19].	O
Third,	O
humans	O
do	O
not	O
require	O
large	O
supervised	O
datasets	O
to	O
learn	O
most	O
language	O
tasks	O
-a	O
brief	O
directive	O
in	O
natural	O
language	O
(e.g.	O
"please	O
tell	O
me	O
if	O
this	O
sentence	O
describes	O
something	O
happy	O
or	O
something	O
sad")	O
or	O
at	O
most	O
a	O
tiny	O
number	O
of	O
demonstrations	O
(e.g.	O
"here	O
are	O
two	O
examples	O
of	O
people	O
acting	O
brave;	O
please	O
give	O
a	O
third	O
example	O
of	O
bravery")	O
is	O
often	O
Figure	O
1.1:	O
Language	O
model	O
meta-learning.	O
During	O
unsupervised	O
pre-training,	O
a	O
language	O
model	O
develops	O
a	O
broad	O
set	O
of	O
skills	O
and	O
pattern	O
recognition	O
abilities.	O
It	O
then	O
uses	O
these	O
abilities	O
at	O
inference	O
time	O
to	O
rapidly	O
adapt	O
to	O
or	O
recognize	O
the	O
desired	O
task.	O
We	O
use	O
the	O
term	O
"in-context	O
learning"	O
to	O
describe	O
the	O
inner	O
loop	O
of	O
this	O
process,	O
which	O
occurs	O
within	O
the	O
forward-pass	O
upon	O
each	O
sequence.	O
The	O
sequences	O
in	O
this	O
diagram	O
are	O
not	O
intended	O
to	O
be	O
representative	O
of	O
the	O
data	O
a	O
model	O
would	O
see	O
during	O
pre-training,	O
but	O
are	O
intended	O
to	O
show	O
that	O
there	O
are	O
sometimes	O
repeated	O
sub-tasks	O
embedded	O
within	O
a	O
single	O
sequence.	O
Figure	O
1.2:	O
Larger	O
models	O
make	O
increasingly	O
efficient	O
use	O
of	O
in-context	O
information.	O
We	O
show	O
in-context	O
learning	O
performance	O
on	O
a	O
simple	O
task	O
requiring	O
the	O
model	O
to	O
remove	O
random	O
symbols	O
from	O
a	O
word,	O
both	O
with	O
and	O
without	O
a	O
natural	O
language	O
task	O
description	O
(see	O
Sec.	O
3.9.2).	O
The	O
steeper	O
"in-context	O
learning	O
curves"	O
for	O
large	O
models	O
demonstrate	O
improved	O
ability	O
to	O
learn	O
a	O
task	O
from	O
contextual	O
information.	O
We	O
see	O
qualitatively	O
similar	O
behavior	O
across	O
a	O
wide	O
range	O
of	O
tasks.	O
sufficient	O
to	O
enable	O
a	O
human	O
to	O
perform	O
a	O
new	O
task	O
to	O
at	O
least	O
a	O
reasonable	O
degree	O
of	O
competence.	O
Aside	O
from	O
pointing	O
to	O
a	O
conceptual	O
limitation	O
in	O
our	O
current	O
NLP	O
techniques,	O
this	O
adaptability	O
has	O
practical	O
advantages	O
-it	O
allows	O
humans	O
to	O
seamlessly	O
mix	O
together	O
or	O
switch	O
between	O
many	O
tasks	O
and	O
skills,	O
for	O
example	O
performing	O
addition	O
during	O
a	O
lengthy	O
dialogue.	O
To	O
be	O
broadly	O
useful,	O
we	O
would	O
someday	O
like	O
our	O
NLP	O
systems	O
to	O
have	O
this	O
same	O
fluidity	O
and	O
generality.	O
One	O
potential	O
route	O
towards	O
addressing	O
these	O
issues	O
is	O
meta-learning	O
1	O
-which	O
in	O
the	O
context	O
of	O
language	O
models	O
means	O
the	O
model	O
develops	O
a	O
broad	O
set	O
of	O
skills	O
and	O
pattern	O
recognition	O
abilities	O
at	O
training	O
time,	O
and	O
then	O
uses	O
those	O
abilities	O
at	O
inference	O
time	O
to	O
rapidly	O
adapt	O
to	O
or	O
recognize	O
the	O
desired	O
task	O
(illustrated	O
in	O
Figure	O
1.1).	O
Recent	O
work	O
[RWC	O
+	O
19]	O
attempts	O
to	O
do	O
this	O
via	O
what	O
we	O
call	O
"in-context	O
learning",	O
using	O
the	O
text	O
input	O
of	O
a	O
pretrained	O
language	O
model	O
as	O
a	O
form	O
of	O
task	O
specification:	O
the	O
model	O
is	O
conditioned	O
on	O
a	O
natural	O
language	O
instruction	O
and/or	O
a	O
few	O
demonstrations	O
of	O
the	O
task	O
and	O
is	O
then	O
expected	O
to	O
complete	O
further	O
instances	O
of	O
the	O
task	O
simply	O
by	O
predicting	O
what	O
comes	O
next.	O
While	O
it	O
has	O
shown	O
some	O
initial	O
promise,	O
this	O
approach	O
still	O
achieves	O
results	O
far	O
inferior	O
to	O
fine-tuning	O
-for	O
example	O
[RWC	O
+	O
19]	O
achieves	O
only	O
4%	O
on	O
Natural	O
Questions,	O
and	O
even	O
its	O
55	O
F1	O
CoQa	O
result	O
is	O
now	O
more	O
than	O
35	O
points	O
behind	O
the	O
state	O
of	O
the	O
art.	O
Meta-learning	O
clearly	O
requires	O
substantial	O
improvement	O
in	O
order	O
to	O
be	O
viable	O
as	O
a	O
practical	O
method	O
of	O
solving	O
language	O
tasks.	O
Another	O
recent	O
trend	O
in	O
language	O
modeling	O
may	O
offer	O
a	O
way	O
forward.	O
In	O
recent	O
years	O
the	O
capacity	O
of	O
transformer	O
language	O
models	O
has	O
increased	O
substantially,	O
from	O
100	O
million	O
parameters	O
[RNSS18],	O
to	O
300	O
million	O
parameters	O
[DCLT18],	O
to	O
1.5	O
billion	O
parameters	O
[RWC	O
+	O
19],	O
to	O
8	O
billion	O
parameters	O
[SPP	O
+	O
19],	O
11	O
billion	O
parameters	O
[RSR	O
+	O
19],	O
and	O
finally	O
17	O
billion	O
parameters	O
[Tur20].	O
Each	O
increase	O
has	O
brought	O
improvements	O
in	O
text	O
synthesis	O
and/or	O
downstream	O
NLP	O
tasks,	O
and	O
there	O
is	O
evidence	O
suggesting	O
that	O
log	O
loss,	O
which	O
correlates	O
well	O
with	O
many	O
downstream	O
tasks,	O
follows	O
a	O
smooth	O
trend	O
of	O
improvement	O
with	O
scale	O
[KMH	O
+	O
20].	O
Since	O
in-context	O
learning	O
involves	O
absorbing	O
many	O
skills	O
and	O
tasks	O
within	O
the	O
parameters	O
of	O
the	O
model,	O
it	O
is	O
plausible	O
that	O
in-context	O
learning	O
abilities	O
might	O
show	O
similarly	O
strong	O
gains	O
with	O
scale.	O
Figure	O
1.3:	O
Aggregate	O
performance	O
for	O
all	O
42	O
accuracy-denominated	O
benchmarks	O
While	O
zero-shot	O
performance	O
improves	O
steadily	O
with	O
model	O
size,	O
few-shot	O
performance	O
increases	O
more	O
rapidly,	O
demonstrating	O
that	O
larger	O
models	O
are	O
more	O
proficient	O
at	O
in-context	O
learning.	O
See	O
Figure	O
3.8	O
for	O
a	O
more	O
detailed	O
analysis	O
on	O
SuperGLUE,	O
a	O
standard	O
NLP	O
benchmark	O
suite.	O
In	O
this	O
paper,	O
we	O
test	O
this	O
hypothesis	O
by	O
training	O
a	O
175	O
billion	O
parameter	O
autoregressive	O
language	O
model,	O
which	O
we	O
call	O
GPT-3,	B-MethodName
and	O
measuring	O
its	O
in-context	O
learning	O
abilities.	O
Specifically,	O
we	O
evaluate	O
GPT-3	B-MethodName
on	O
over	O
two	O
dozen	O
NLP	O
datasets,	O
as	O
well	O
as	O
several	O
novel	O
tasks	O
designed	O
to	O
test	O
rapid	O
adaptation	O
to	O
tasks	O
unlikely	O
to	O
be	O
directly	O
contained	O
in	O
the	O
training	O
set.	O
For	O
each	O
task,	O
we	O
evaluate	O
GPT-3	B-MethodName
under	O
3	O
conditions:	O
(a)	O
"few-shot	B-TaskName
learning",	I-TaskName
or	O
in-context	B-TaskName
learning	I-TaskName
where	O
we	O
allow	O
as	O
many	O
demonstrations	O
as	O
will	O
fit	O
into	O
the	O
model's	O
context	O
window	O
(typically	O
10	O
to	O
100),	O
(b)	O
"one-shot	B-TaskName
learning",	I-TaskName
where	O
we	O
allow	O
only	O
one	O
demonstration,	O
and	O
(c)	O
"zero-shot"	B-TaskName
learning,	O
where	O
no	O
demonstrations	O
are	O
allowed	O
and	O
only	O
an	O
instruction	O
in	O
natural	O
language	O
is	O
given	O
to	O
the	O
model.	O
GPT-3	B-MethodName
could	O
also	O
in	O
principle	O
be	O
evaluated	O
in	O
the	O
traditional	O
fine-tuning	O
setting,	O
but	O
we	O
leave	O
this	O
to	O
future	O
work.	O
Figure	O
1.2	O
illustrates	O
the	O
conditions	O
we	O
study,	O
and	O
shows	O
few-shot	B-TaskName
learning	O
of	O
a	O
simple	O
task	O
requiring	O
the	O
model	O
to	O
remove	O
extraneous	O
symbols	O
from	O
a	O
word.	O
Model	O
performance	O
improves	O
with	O
the	O
addition	O
of	O
a	O
natural	O
language	O
task	O
description,	O
and	O
with	O
the	O
number	O
of	O
examples	O
in	O
the	O
model's	O
context,	O
K.	O
Few-shot	B-TaskName
learning	I-TaskName
also	O
improves	O
dramatically	O
with	O
model	O
size.	O
Though	O
the	O
results	O
in	O
this	O
case	O
are	O
particularly	O
striking,	O
the	O
general	O
trends	O
with	O
both	O
model	O
size	O
and	O
number	O
of	O
examples	O
in-context	O
hold	O
for	O
most	O
tasks	O
we	O
study.	O
We	O
emphasize	O
that	O
these	O
"learning"	O
curves	O
involve	O
no	O
gradient	O
updates	O
or	O
fine-tuning,	O
just	O
increasing	O
numbers	O
of	O
demonstrations	O
given	O
as	O
conditioning.	O
Broadly,	O
on	O
NLP	O
tasks	O
GPT-3	B-MethodName
achieves	O
promising	O
results	O
in	O
the	O
zero-shot	O
and	O
one-shot	O
settings,	O
and	O
in	O
the	O
the	O
few-shot	O
setting	O
is	O
sometimes	O
competitive	O
with	O
or	O
even	O
occasionally	O
surpasses	O
state-of-the-art	O
(despite	O
state-of-the-art	O
being	O
held	O
by	O
fine-tuned	O
models).	O
For	O
example,	O
GPT-3	B-MethodName
achieves	O
81.5	B-MetricValue
F1	B-MetricName
on	O
CoQA	B-DatasetName
in	O
the	O
zero-shot	O
setting,	O
84.0	B-MetricValue
F1	B-MetricName
on	O
CoQA	B-DatasetName
in	O
the	O
one-shot	O
setting,	O
85.0	B-MetricValue
F1	B-MetricName
in	O
the	O
few-shot	B-TaskName
setting.	O
Similarly,	O
GPT-3	O
achieves	O
64.3%	B-MetricValue
accuracy	B-MetricName
on	O
TriviaQA	B-DatasetName
in	O
the	O
zero-shot	B-TaskName
setting,	O
68.0%	B-MetricValue
in	O
the	O
one-shot	B-TaskName
setting,	O
and	O
71.2%	B-MetricValue
in	O
the	O
few-shot	B-TaskName
setting,	O
the	O
last	O
of	O
which	O
is	O
state-of-the-art	O
relative	O
to	O
fine-tuned	O
models	O
operating	O
in	O
the	O
same	O
closed-book	O
setting.	O
GPT-3	B-MethodName
also	O
displays	O
one-shot	B-TaskName
and	O
few-shot	O
proficiency	O
at	O
tasks	O
designed	O
to	O
test	O
rapid	O
adaption	O
or	O
on-the-fly	O
reasoning,	O
which	O
include	O
unscrambling	O
words,	O
performing	O
arithmetic,	O
and	O
using	O
novel	O
words	O
in	O
a	O
sentence	O
after	O
seeing	O
them	O
defined	O
only	O
once.	O
We	O
also	O
show	O
that	O
in	O
the	O
few-shot	O
setting,	O
GPT-3	B-MethodName
can	O
generate	O
synthetic	O
news	O
articles	O
which	O
human	O
evaluators	O
have	O
difficulty	O
distinguishing	O
from	O
human-generated	O
articles.	O
At	O
the	O
same	O
time,	O
we	O
also	O
find	O
some	O
tasks	O
on	O
which	O
few-shot	O
performance	O
struggles,	O
even	O
at	O
the	O
scale	O
of	O
GPT-3.	B-MethodName
This	O
includes	B-TaskName
natural	I-TaskName
language	I-TaskName
inference	I-TaskName
tasks	I-TaskName
like	O
the	O
ANLI	B-DatasetName
dataset,	O
and	O
some	O
reading	O
comprehension	O
datasets	O
like	O
RACE	B-DatasetName
or	O
QuAC.	B-DatasetName
By	O
presenting	O
a	O
broad	O
characterization	O
of	O
GPT-3's	B-MethodName
strengths	O
and	O
weaknesses,	O
including	O
these	O
limitations,	O
we	O
hope	O
to	O
stimulate	O
study	O
of	O
few-shot	O
learning	O
in	O
language	O
models	O
and	O
draw	O
attention	O
to	O
where	O
progress	O
is	O
most	O
needed.	O
A	O
heuristic	O
sense	O
of	O
the	O
overall	O
results	O
can	O
be	O
seen	O
in	O
Figure	O
1.3,	O
which	O
aggregates	O
the	O
various	O
tasks	O
(though	O
it	O
should	O
not	O
be	O
seen	O
as	O
a	O
rigorous	O
or	O
meaningful	O
benchmark	O
in	O
itself).	O
We	O
also	O
undertake	O
a	O
systematic	O
study	O
of	O
"data	O
contamination"	O
-a	O
growing	O
problem	O
when	O
training	O
high	O
capacity	O
models	O
on	O
datasets	O
such	O
as	O
Common	O
Crawl,	O
which	O
can	O
potentially	O
include	O
content	O
from	O
test	O
datasets	O
simply	O
because	O
such	O
content	O
often	O
exists	O
on	O
the	O
web.	O
In	O
this	O
paper	O
we	O
develop	O
systematic	O
tools	O
to	O
measure	O
data	O
contamination	O
and	O
quantify	O
its	O
distorting	O
effects.	O
Although	O
we	O
find	O
that	O
data	O
contamination	O
has	O
a	O
minimal	O
effect	O
on	O
GPT-3's	B-MethodName
performance	O
on	O
most	O
datasets,	O
we	O
do	O
identify	O
a	O
few	O
datasets	O
where	O
it	O
could	O
be	O
inflating	O
results,	O
and	O
we	O
either	O
do	O
not	O
report	O
results	O
on	O
these	O
datasets	O
or	O
we	O
note	O
them	O
with	O
an	O
asterisk,	O
depending	O
on	O
the	O
severity.	O
In	O
addition	O
to	O
all	O
the	O
above,	O
we	O
also	O
train	O
a	O
series	O
of	O
smaller	O
models	O
(ranging	O
from	O
125	O
million	O
parameters	O
to	O
13	O
billion	O
parameters)	O
in	O
order	O
to	O
compare	O
their	O
performance	O
to	O
GPT-3	B-MethodName
in	O
the	O
zero,	O
one	O
and	O
few-shot	O
settings.	O
Broadly,	O
for	O
most	O
tasks	O
we	O
find	O
relatively	O
smooth	O
scaling	O
with	O
model	O
capacity	O
in	O
all	O
three	O
settings;	O
one	O
notable	O
pattern	O
is	O
that	O
the	O
gap	O
between	O
zero-,	O
one-,	O
and	O
few-shot	O
performance	O
often	O
grows	O
with	O
model	O
capacity,	O
perhaps	O
suggesting	O
that	O
larger	O
models	O
are	O
more	O
proficient	O
meta-learners.	O
Finally,	O
given	O
the	O
broad	O
spectrum	O
of	O
capabilities	O
displayed	O
by	O
GPT-3,	B-MethodName
we	O
discuss	O
concerns	O
about	O
bias,	O
fairness,	O
and	O
broader	O
societal	O
impacts,	O
and	O
attempt	O
a	O
preliminary	O
analysis	O
of	O
GPT-3's	B-MethodName
characteristics	O
in	O
this	O
regard.	O
The	O
remainder	O
of	O
this	O
paper	O
is	O
organized	O
as	O
follows.	O
In	O
Section	O
2,	O
we	O
describe	O
our	O
approach	O
and	O
methods	O
for	O
training	O
GPT-3	B-MethodName
and	O
evaluating	O
it.	O
Section	O
3	O
presents	O
results	O
on	O
the	O
full	O
range	O
of	O
tasks	O
in	O
the	O
zero-,	O
one-and	O
few-shot	O
settings.	O
Section	O
4	O
addresses	O
questions	O
of	O
data	O
contamination	O
(train-test	O
overlap).	O
Section	O
5	O
discusses	O
limitations	O
of	O
GPT-3.	B-MethodName
Section	O
6	O
discusses	O
broader	O
impacts.	O
Section	O
7	O
reviews	O
related	O
work	O
and	O
Section	O
8	O
concludes.	O

Recent	O
work	O
has	O
demonstrated	O
substantial	O
gains	O
on	O
many	O
NLP	O
tasks	O
and	O
benchmarks	O
by	O
pre-training	O
on	O
a	O
large	O
corpus	O
of	O
text	O
followed	O
by	O
fine-tuning	O
on	O
a	O
specific	O
task.	O
While	O
typically	O
task-agnostic	O
in	O
architecture,	O
this	O
method	O
still	O
requires	O
task-specific	O
fine-tuning	O
datasets	O
of	O
thousands	O
or	O
tens	O
of	O
thousands	O
of	O
examples.	O
By	O
contrast,	O
humans	O
can	O
generally	O
perform	O
a	O
new	O
language	O
task	O
from	O
only	O
a	O
few	O
examples	O
or	O
from	O
simple	O
instructions	O
-something	O
which	O
current	O
NLP	O
systems	O
still	O
largely	O
struggle	O
to	O
do.	O
Here	O
we	O
show	O
that	O
scaling	O
up	O
language	O
models	O
greatly	O
improves	O
task-agnostic,	O
few-shot	O
performance,	O
sometimes	O
even	O
reaching	O
competitiveness	O
with	O
prior	O
state-of-the-art	O
finetuning	O
approaches.	O
Specifically,	O
we	O
train	O
GPT-3,	B-MethodName
an	O
autoregressive	O
language	O
model	O
with	O
175	O
billion	O
parameters,	O
10x	O
more	O
than	O
any	O
previous	O
non-sparse	O
language	O
model,	O
and	O
test	O
its	O
performance	O
in	O
the	O
few-shot	O
setting.	O
For	O
all	O
tasks,	O
GPT-3	B-MethodName
is	O
applied	O
without	O
any	O
gradient	O
updates	O
or	O
fine-tuning,	O
with	O
tasks	O
and	O
few-shot	O
demonstrations	O
specified	O
purely	O
via	O
text	O
interaction	O
with	O
the	O
model.	O
GPT-3	B-MethodName
achieves	O
strong	O
performance	O
on	O
many	O
NLP	O
datasets,	O
including	O
translation,	B-TaskName
question-answering,	B-TaskName
and	O
cloze	B-TaskName
tasks,	O
as	O
well	O
as	O
several	O
tasks	O
that	O
require	O
on-the-fly	O
reasoning	O
or	O
domain	O
adaptation,	O
such	O
as	O
unscrambling	B-TaskName
words,	I-TaskName
using	B-TaskName
a	I-TaskName
novel	I-TaskName
word	I-TaskName
in	I-TaskName
a	I-TaskName
sentence,	I-TaskName
or	O
performing	B-TaskName
3-digit	I-TaskName
arithmetic.	I-TaskName
At	O
the	O
same	O
time,	O
we	O
also	O
identify	O
some	O
datasets	O
where	O
GPT-3's	B-MethodName
few-shot	O
learning	O
still	O
struggles,	O
as	O
well	O
as	O
some	O
datasets	O
where	O
GPT-3	B-MethodName
faces	O
methodological	O
issues	O
related	O
to	O
training	O
on	O
large	O
web	O
corpora.	O
Finally,	O
we	O
find	O
that	O
GPT-3	B-MethodName
can	O
generate	O
samples	O
of	O
news	O
articles	O
which	O
human	O
evaluators	O
have	O
difficulty	O
distinguishing	O
from	O
articles	O
written	O
by	O
humans.	O
We	O
discuss	O
broader	O
societal	O
impacts	O
of	O
this	O
finding	O
and	O
of	O
GPT-3	B-MethodName
in	O
general.	O

In	O
Table	O
1,	O
we	O
list	O
the	O
translation	O
performances	O
measured	O
in	O
BLEU	B-MetricName
score.	O
It	O
is	O
clear	O
from	O
the	O
table	O
that	O
in	O
all	O
the	O
cases,	O
the	O
proposed	O
RNNsearch	B-MethodName
outperforms	O
the	O
conventional	O
RNNencdec.	B-MethodName
More	O
importantly,	O
the	O
performance	O
of	O
the	O
RNNsearch	B-MethodName
is	O
as	O
high	O
as	O
that	O
of	O
the	O
conventional	O
phrase-based	O
translation	O
system	O
(Moses),	O
when	O
only	O
the	O
sentences	O
consisting	O
of	O
known	O
words	O
are	O
considered.	O
This	O
is	O
a	O
significant	O
achievement,	O
considering	O
that	O
Moses	O
uses	O
a	O
separate	O
monolingual	O
corpus	O
(418M	O
words)	O
in	O
addition	O
to	O
the	O
parallel	O
corpora	O
we	O
used	O
to	O
train	O
the	O
RNNsearch	B-MethodName
and	O
RNNencdec.	B-MethodName
(a	O
)	O
(b)	O
(c)	O
(d)	O
Figure	O
3:	O
Four	O
sample	O
alignments	O
found	O
by	O
RNNsearch-50.	B-MethodName
The	O
x-axis	O
and	O
y-axis	O
of	O
each	O
plot	O
correspond	O
to	O
the	O
words	O
in	O
the	O
source	O
sentence	O
(English)	O
and	O
the	O
generated	O
translation	O
(French),	O
respectively.	O
Each	O
pixel	O
shows	O
the	O
weight	O
↵	O
ij	O
of	O
the	O
annotation	O
of	O
the	O
j-th	O
source	O
word	O
for	O
the	O
i-th	O
target	O
word	O
(see	O
Eq.	O
(	O
6)),	O
in	O
grayscale	O
(0:	O
black,	O
1:	O
white).	O
(a)	O
an	O
arbitrary	O
sentence.	O
(b-d)	O
three	O
randomly	O
selected	O
samples	O
among	O
the	O
sentences	B-HyperparameterName
without	I-HyperparameterName
any	I-HyperparameterName
unknown	I-HyperparameterName
words	I-HyperparameterName
and	I-HyperparameterName
of	I-HyperparameterName
length	I-HyperparameterName
between	O
10	B-HyperparameterValue
and	I-HyperparameterValue
20	I-HyperparameterValue
words	I-HyperparameterValue
from	O
the	O
test	O
set.	O
One	O
of	O
the	O
motivations	O
behind	O
the	O
proposed	O
approach	O
was	O
the	O
use	O
of	O
a	O
fixed-length	O
context	O
vector	O
in	O
the	O
basic	O
encoder-decoder	O
approach.	O
We	O
conjectured	O
that	O
this	O
limitation	O
may	O
make	O
the	O
basic	O
encoder-decoder	O
approach	O
to	O
underperform	O
with	O
long	O
sentences.	O
In	O
Fig.	O
2,	O
we	O
see	O
that	O
the	O
performance	O
of	O
RNNencdec	B-MethodName
dramatically	O
drops	O
as	O
the	O
length	O
of	O
the	O
sentences	O
increases.	O
On	O
the	O
other	O
hand,	O
both	O
RNNsearch-30	B-MethodName
and	O
RNNsearch-50	B-MethodName
are	O
more	O
robust	O
to	O
the	O
length	O
of	O
the	O
sentences.	O
RNNsearch-50,	B-MethodName
especially,	O
shows	O
no	O
performance	O
deterioration	O
even	O
with	O
sentences	O
of	O
length	O
50	O
or	O
more.	O
This	O
superiority	O
of	O
the	O
proposed	O
model	O
over	O
the	O
basic	O
encoder-decoder	O
is	O
further	O
confirmed	O
by	O
the	O
fact	O
that	O
the	O
RNNsearch-30	B-MethodName
even	O
outperforms	O
RNNencdec-50	B-MethodName
(see	O
Table	O
1).	O
tokens	O
when	O
only	O
the	O
sentences	O
having	O
no	O
unknown	O
words	O
were	O
evaluated	O
(last	O
column).	O

We	O
train	O
two	O
types	O
of	O
models.	O
The	O
first	O
one	O
is	O
an	O
RNN	B-MethodName
Encoder-Decoder	I-MethodName
(RNNencdec,	B-MethodName
Cho	O
et	O
al.,	O
2014a),	O
and	O
the	O
other	O
is	O
the	O
proposed	O
model,	O
to	O
which	O
we	O
refer	O
as	O
RNNsearch.	B-MethodName
We	O
train	O
each	O
model	O
twice:	O
first	O
with	O
the	O
sentences	B-HyperparameterName
of	I-HyperparameterName
length	I-HyperparameterName
up	O
to	O
30	B-HyperparameterValue
words	I-HyperparameterValue
(RNNencdec-30,	B-MethodName
RNNsearch-30)	B-MethodName
and	O
then	O
with	O
the	O
sentences	B-HyperparameterName
of	I-HyperparameterName
length	I-HyperparameterName
up	O
to	O
50	B-HyperparameterValue
word	I-HyperparameterValue
(RNNencdec-50,	B-MethodName
RNNsearch-50).	B-MethodName
The	O
encoder	O
and	O
decoder	O
of	O
the	O
RNNencdec	B-MethodName
have	O
1000	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
each.	O
7	O
The	O
encoder	O
of	O
the	O
RNNsearch	B-MethodName
consists	O
of	O
forward	O
and	O
backward	O
recurrent	O
neural	O
networks	O
(RNN)	B-MethodName
each	O
having	O
1000	B-HyperparameterValue
hidden	B-HyperparameterName
units.	I-HyperparameterName
Its	O
decoder	O
has	O
1000	B-HyperparameterValue
hidden	B-HyperparameterName
units.	I-HyperparameterName
In	O
both	O
cases,	O
we	O
use	O
a	O
multilayer	O
network	O
with	O
a	O
single	O
maxout	O
(Goodfellow	O
et	O
al.,	O
2013)	O
hidden	O
layer	O
to	O
compute	O
the	O
conditional	O
probability	O
of	O
each	O
target	O
word	O
(Pascanu	O
et	O
al.,	O
2014).	O
We	O
use	O
a	O
minibatch	O
stochastic	O
gradient	O
descent	O
(SGD)	O
algorithm	O
together	O
with	O
Adadelta	O
(Zeiler,	O
2012)	O
to	O
train	O
each	O
model.	O
Each	O
SGD	O
update	O
direction	O
is	O
computed	O
using	O
a	O
minibatch	B-HyperparameterName
of	O
80	B-HyperparameterValue
sentences.	I-HyperparameterValue
We	O
trained	O
each	O
model	O
for	O
approximately	O
5	O
days.	O
Once	O
a	O
model	O
is	O
trained,	O
we	O
use	O
a	O
beam	O
search	O
to	O
find	O
a	O
translation	O
that	O
approximately	O
maximizes	O
the	O
conditional	O
probability	O
(see,	O
e.g.,	O
Graves,	O
2012;Boulanger-Lewandowski	O
et	O
al.,	O
2013).	O
Sutskever	O
et	O
al.	O
(2014)	O
used	O
this	O
approach	O
to	O
generate	O
translations	O
from	O
their	O
neural	B-MethodName
machine	I-MethodName
translation	I-MethodName
model.	O
For	O
more	O
details	O
on	O
the	O
architectures	O
of	O
the	O
models	O
and	O
training	O
procedure	O
used	O
in	O
the	O
experiments,	O
see	O
Appendices	O
A	O
and	O
B.	O

We	O
evaluate	O
the	O
proposed	O
approach	O
on	O
the	O
task	O
of	O
English-to-French	B-TaskName
translation.	I-TaskName
We	O
use	O
the	O
bilingual,	B-DatasetName
parallel	I-DatasetName
corpora	I-DatasetName
provided	I-DatasetName
by	I-DatasetName
ACL	I-DatasetName
WMT	I-DatasetName
'14.	I-DatasetName
3	O
As	O
a	O
comparison,	O
we	O
also	O
report	O
the	O
performance	O
of	O
an	O
RNN	B-MethodName
Encoder-Decoder	I-MethodName
which	O
was	O
proposed	O
recently	O
by	O
Cho	O
et	O
al.	O
(2014a).	O
We	O
use	O
the	O
same	O
training	O
procedures	O
and	O
the	O
same	O
dataset	O
for	O
both	O
models.	O
4	O
4.1	O
DATASET	O
WMT	B-DatasetName
'14	I-DatasetName
contains	O
the	O
following	O
English-French	O
parallel	O
corpora:	O
Europarl	O
(61M	O
words),	O
news	O
commentary	O
(5.5M),	O
UN	O
(421M)	O
and	O
two	O
crawled	O
corpora	O
of	O
90M	O
and	O
272.5M	O
words	O
respectively,	O
totaling	O
850M	O
words.	O
Following	O
the	O
procedure	O
described	O
in	O
Cho	O
et	O
al.	O
(2014a),	O
we	O
reduce	O
the	O
size	O
of	O
the	O
combined	O
corpus	O
to	O
have	O
348M	O
words	O
using	O
the	O
data	O
selection	O
method	O
by	O
Axelrod	O
et	O
al.	O
(2011).	O
5	O
We	O
do	O
not	O
use	O
any	O
monolingual	O
data	O
other	O
than	O
the	O
mentioned	O
parallel	O
corpora,	O
although	O
it	O
may	O
be	O
possible	O
to	O
use	O
a	O
much	O
larger	O
monolingual	O
corpus	O
to	O
pretrain	O
an	O
encoder.	O
We	O
concatenate	O
news-test-	O
After	O
a	O
usual	O
tokenization	O
6	O
,	O
we	O
use	O
a	O
shortlist	O
of	O
30,000	B-HyperparameterValue
most	B-HyperparameterName
frequent	I-HyperparameterName
words	I-HyperparameterName
in	O
each	O
language	O
to	O
train	O
our	O
models.	O
Any	O
word	O
not	O
included	O
in	O
the	O
shortlist	O
is	O
mapped	O
to	O
a	O
special	O
token	O
([UNK]).	O
We	O
do	O
not	O
apply	O
any	O
other	O
special	O
preprocessing,	O
such	O
as	O
lowercasing	O
or	O
stemming,	O
to	O
the	O
data.	O

The	O
usual	O
RNN,	B-MethodName
described	O
in	O
Eq.	O
(1),	O
reads	O
an	O
input	O
sequence	O
x	O
in	O
order	O
starting	O
from	O
the	O
first	O
symbol	O
x	O
1	O
to	O
the	O
last	O
one	O
x	O
Tx	O
.	O
However,	O
in	O
the	O
proposed	O
scheme,	O
we	O
would	O
like	O
the	O
annotation	O
of	O
each	O
word	O
to	O
summarize	O
not	O
only	O
the	O
preceding	O
words,	O
but	O
also	O
the	O
following	O
words.	O
Hence,	O
we	O
propose	O
to	O
use	O
a	O
bidirectional	B-MethodName
RNN	I-MethodName
(BiRNN,	B-MethodName
Schuster	O
and	O
Paliwal,	O
1997),	O
which	O
has	O
been	O
successfully	O
used	O
recently	O
in	O
speech	O
recognition	O
(see,	O
e.g.,	O
Graves	O
et	O
al.,	O
2013).	O
A	O
BiRNN	B-MethodName
consists	O
of	O
forward	O
and	O
backward	O
RNN's.	B-MethodName
The	O
forward	O
RNN	B-MethodName
!	O
f	O
reads	O
the	O
input	O
sequence	O
as	O
it	O
is	O
ordered	O
(from	O
x	O
1	O
to	O
x	O
Tx	O
)	O
and	O
calculates	O
a	O
sequence	O
of	O
forward	O
hidden	O
states	O
(	O
!	O
h	O
1	O
,	O
•	O
•	O
•	O
,	O
!	O
h	O
Tx	O
).	O
The	O
backward	O
RNN	B-MethodName
f	O
reads	O
the	O
sequence	O
in	O
the	O
reverse	O
order	O
(from	O
x	O
Tx	O
to	O
x	O
1	O
),	O
resulting	O
in	O
a	O
sequence	O
of	O
backward	O
hidden	O
states	O
(	O
h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
Tx	O
).	O
We	O
obtain	O
an	O
annotation	O
for	O
each	O
word	O
x	O
j	O
by	O
concatenating	O
the	O
forward	O
hidden	O
state	O
!	O
h	O
j	O
and	O
the	O
backward	O
one	O
h	O
j	O
,	O
i.e.,	O
h	O
j	O
=	O
h	O
!	O
h	O
>	O
j	O
;	O
h	O
>	O
j	O
i	O
>	O
.	O
In	O
this	O
way,	O
the	O
annotation	O
h	O
j	O
contains	O
the	O
summaries	O
of	O
both	O
the	O
preceding	O
words	O
and	O
the	O
following	O
words.	O
Due	O
to	O
the	O
tendency	O
of	O
RNNs	B-MethodName
to	O
better	O
represent	O
recent	O
inputs,	O
the	O
annotation	O
h	O
j	O
will	O
be	O
focused	O
on	O
the	O
words	O
around	O
x	O
j	O
.	O
This	O
sequence	O
of	O
annotations	O
is	O
used	O
by	O
the	O
decoder	O
and	O
the	O
alignment	O
model	O
later	O
to	O
compute	O
the	O
context	O
vector	O
(Eqs.	O
(	O
5)-(	O
6)).	O
See	O
Fig.	O
1	O
for	O
the	O
graphical	O
illustration	O
of	O
the	O
proposed	O
model.	O

DESCRIPTION	O
x	O
1	O
x	O
2	O
x	O
3	O
x	O
T	O
+	O
α	O
t,1	O
α	O
t,2	O
α	O
t,3	O
α	O
t,T	O
y	O
t-1	O
y	O
t	O
h	O
1	O
h	O
2	O
h	O
3	O
h	O
T	O
h	O
1	O
h	O
2	O
h	O
3	O
h	O
T	O
s	O
t-1	O
s	O
t	O
Figure	O
1:	O
The	O
graphical	O
illustration	O
of	O
the	O
proposed	O
model	O
trying	O
to	O
generate	O
the	O
t-th	O
target	O
word	O
y	O
t	O
given	O
a	O
source	O
sentence	O
(x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O
x	O
T	O
).	O
In	O
a	O
new	O
model	O
architecture,	O
we	O
define	O
each	O
conditional	O
probability	O
in	O
Eq.	O
(2)	O
as:	O
p(y	O
i	O
|y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
i	O
1	O
,	O
x)	O
=	O
g(y	O
i	O
1	O
,	O
s	O
i	O
,	O
c	O
i	O
),(4)	O
where	O
s	O
i	O
is	O
an	O
RNN	B-MethodName
hidden	O
state	O
for	O
time	O
i,	O
computed	O
by	O
s	O
i	O
=	O
f	O
(s	O
i	O
1	O
,	O
y	O
i	O
1	O
,	O
c	O
i	O
).	O
It	O
should	O
be	O
noted	O
that	O
unlike	O
the	O
existing	O
encoder-decoder	B-MethodName
approach	O
(see	O
Eq.	O
(2)),	O
here	O
the	O
probability	O
is	O
conditioned	O
on	O
a	O
distinct	O
context	O
vector	O
c	O
i	O
for	O
each	O
target	O
word	O
y	O
i	O
.	O
The	O
context	O
vector	O
c	O
i	O
depends	O
on	O
a	O
sequence	O
of	O
annotations	O
(h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
Tx	O
)	O
to	O
which	O
an	O
encoder	O
maps	O
the	O
input	O
sentence.	O
Each	O
annotation	O
h	O
i	O
contains	O
information	O
about	O
the	O
whole	O
input	O
sequence	O
with	O
a	O
strong	O
focus	O
on	O
the	O
parts	O
surrounding	O
the	O
i-th	O
word	O
of	O
the	O
input	O
sequence.	O
We	O
explain	O
in	O
detail	O
how	O
the	O
annotations	O
are	O
computed	O
in	O
the	O
next	O
section.	O
The	O
context	O
vector	O
c	O
i	O
is,	O
then,	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
these	O
annotations	O
h	O
i	O
:	O
c	O
i	O
=	O
Tx	O
X	O
j=1	O
↵	O
ij	O
h	O
j	O
.(5)	O
The	O
weight	O
↵	O
ij	O
of	O
each	O
annotation	O
h	O
j	O
is	O
computed	O
by	O
↵	O
ij	O
=	O
exp	O
(e	O
ij	O
)	O
P	O
Tx	O
k=1	O
exp	O
(e	O
ik	O
)	O
,(6)	O
where	O
e	O
ij	O
=	O
a(s	O
i	O
1	O
,	O
h	O
j	O
)	O
is	O
an	O
alignment	O
model	O
which	O
scores	O
how	O
well	O
the	O
inputs	O
around	O
position	O
j	O
and	O
the	O
output	O
at	O
position	O
i	O
match.	O
The	O
score	O
is	O
based	O
on	O
the	O
RNN	B-MethodName
hidden	O
state	O
s	O
i	O
1	O
(just	O
before	O
emitting	O
y	O
i	O
,	O
Eq.	O
(4))	O
and	O
the	O
j-th	O
annotation	O
h	O
j	O
of	O
the	O
input	O
sentence.	O
We	O
parametrize	O
the	O
alignment	O
model	O
a	O
as	O
a	O
feedforward	O
neural	O
network	O
which	O
is	O
jointly	O
trained	O
with	O
all	O
the	O
other	O
components	O
of	O
the	O
proposed	O
system.	O
Note	O
that	O
unlike	O
in	O
traditional	O
machine	O
translation,	O
the	O
alignment	O
is	O
not	O
considered	O
to	O
be	O
a	O
latent	O
variable.	O
Instead,	O
the	O
alignment	O
model	O
directly	O
computes	O
a	O
soft	O
alignment,	O
which	O
allows	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
to	O
be	O
backpropagated	O
through.	O
This	O
gradient	O
can	O
be	O
used	O
to	O
train	O
the	O
alignment	O
model	O
as	O
well	O
as	O
the	O
whole	O
translation	O
model	O
jointly.	O
We	O
can	O
understand	O
the	O
approach	O
of	O
taking	O
a	O
weighted	O
sum	O
of	O
all	O
the	O
annotations	O
as	O
computing	O
an	O
expected	O
annotation,	O
where	O
the	O
expectation	O
is	O
over	O
possible	O
alignments.	O
Let	O
↵	O
ij	O
be	O
a	O
probability	O
that	O
the	O
target	O
word	O
y	O
i	O
is	O
aligned	O
to,	O
or	O
translated	O
from,	O
a	O
source	O
word	O
x	O
j	O
.	O
Then,	O
the	O
i-th	O
context	O
vector	O
c	O
i	O
is	O
the	O
expected	O
annotation	O
over	O
all	O
the	O
annotations	O
with	O
probabilities	O
↵	O
ij	O
.	O
The	O
probability	O
↵	O
ij	O
,	O
or	O
its	O
associated	O
energy	O
e	O
ij	O
,	O
reflects	O
the	O
importance	O
of	O
the	O
annotation	O
h	O
j	O
with	O
respect	O
to	O
the	O
previous	O
hidden	O
state	O
s	O
i	O
1	O
in	O
deciding	O
the	O
next	O
state	O
s	O
i	O
and	O
generating	O
y	O
i	O
.	O
Intuitively,	O
this	O
implements	O
a	O
mechanism	O
of	O
attention	B-MethodName
in	I-MethodName
the	I-MethodName
decoder.	I-MethodName
The	O
decoder	O
decides	O
parts	O
of	O
the	O
source	O
sentence	O
to	O
pay	O
attention	O
to.	O
By	O
letting	O
the	O
decoder	O
have	O
an	O
attention	O
mechanism,	O
we	O
relieve	O
the	O
encoder	O
from	O
the	O
burden	O
of	O
having	O
to	O
encode	O
all	O
information	O
in	O
the	O
source	O
sentence	O
into	O
a	O
fixedlength	O
vector.	O
With	O
this	O
new	O
approach	O
the	O
information	O
can	O
be	O
spread	O
throughout	O
the	O
sequence	O
of	O
annotations,	O
which	O
can	O
be	O
selectively	O
retrieved	O
by	O
the	O
decoder	O
accordingly.	O

In	O
this	O
section,	O
we	O
propose	O
a	O
novel	O
architecture	O
for	O
neural	O
machine	O
translation.	O
The	O
new	O
architecture	O
consists	O
of	O
a	O
bidirectional	B-MethodName
RNN	I-MethodName
as	I-MethodName
an	I-MethodName
encoder	I-MethodName
(Sec.	O
3.2)	O
and	O
a	O
decoder	O
that	O
emulates	O
searching	O
through	O
a	O
source	O
sentence	O
during	O
decoding	O
a	O
translation	O
(Sec.	O
3.1).	O

Neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
is	O
a	O
recently	O
proposed	O
approach	O
to	O
machine	O
translation.	O
Unlike	O
the	O
traditional	O
statistical	O
machine	O
translation,	O
the	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
aims	O
at	O
building	O
a	O
single	O
neural	O
network	O
that	O
can	O
be	O
jointly	O
tuned	O
to	O
maximize	O
the	O
translation	O
performance.	O
The	O
models	O
proposed	O
recently	O
for	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
often	O
belong	O
to	O
a	O
family	O
of	O
encoder-decoders	B-MethodName
and	O
encode	O
a	O
source	O
sentence	O
into	O
a	O
fixed-length	O
vector	O
from	O
which	O
a	O
decoder	O
generates	O
a	O
translation.	O
In	O
this	O
paper,	O
we	O
conjecture	O
that	O
the	O
use	O
of	O
a	O
fixed-length	O
vector	O
is	O
a	O
bottleneck	O
in	O
improving	O
the	O
performance	O
of	O
this	O
basic	B-MethodName
encoder-decoder	I-MethodName
architecture,	O
and	O
propose	O
to	O
extend	O
this	O
by	O
allowing	O
a	O
model	O
to	O
automatically	O
(soft-)search	O
for	O
parts	O
of	O
a	O
source	O
sentence	O
that	O
are	O
relevant	O
to	O
predicting	O
a	O
target	O
word,	O
without	O
having	O
to	O
form	O
these	O
parts	O
as	O
a	O
hard	O
segment	O
explicitly.	O
With	O
this	O
new	O
approach,	O
we	O
achieve	O
a	O
translation	O
performance	O
comparable	O
to	O
the	O
existing	O
state-of-the-art	O
phrase-based	O
system	O
on	O
the	O
task	O
of	O
English-to-French	B-TaskName
translation.	I-TaskName
Furthermore,	O
qualitative	O
analysis	O
reveals	O
that	O
the	O
(soft-)alignments	O
found	O
by	O
the	O
model	O
agree	O
well	O
with	O
our	O
intuition.	O

Neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
is	O
a	O
newly	O
emerging	O
approach	O
to	O
machine	O
translation,	O
recently	O
proposed	O
by	O
Kalchbrenner	O
and	O
Blunsom	O
(2013),	O
Sutskever	O
et	O
al.	O
(2014)	O
and	O
Cho	O
et	O
al.	O
(2014b).	O
Unlike	O
the	O
traditional	O
phrase-based	O
translation	O
system	O
(see,	O
e.g.,	O
Koehn	O
et	O
al.,	O
2003)	O
which	O
consists	O
of	O
many	O
small	O
sub-components	O
that	O
are	O
tuned	O
separately,	O
neural	O
machine	O
translation	O
attempts	O
to	O
build	O
and	O
train	O
a	O
single,	O
large	O
neural	O
network	O
that	O
reads	O
a	O
sentence	O
and	O
outputs	O
a	O
correct	O
translation.	O
Most	O
of	O
the	O
proposed	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
models	O
belong	O
to	O
a	O
family	O
of	O
encoderdecoders	O
(Sutskever	O
et	O
al.,	O
2014;Cho	O
et	O
al.,	O
2014a),	O
with	O
an	O
encoder	B-MethodName
and	I-MethodName
a	I-MethodName
decoder	I-MethodName
for	O
each	O
language,	O
or	O
involve	O
a	O
language-specific	O
encoder	O
applied	O
to	O
each	O
sentence	O
whose	O
outputs	O
are	O
then	O
compared	O
(Hermann	O
and	O
Blunsom,	O
2014).	O
An	O
encoder	O
neural	O
network	O
reads	O
and	O
encodes	O
a	O
source	O
sentence	O
into	O
a	O
fixed-length	O
vector.	O
A	O
decoder	O
then	O
outputs	O
a	O
translation	O
from	O
the	O
encoded	O
vector.	O
The	O
whole	O
encoder-decoder	O
system,	O
which	O
consists	O
of	O
the	O
encoder	O
and	O
the	O
decoder	O
for	O
a	O
language	O
pair,	O
is	O
jointly	O
trained	O
to	O
maximize	O
the	O
probability	O
of	O
a	O
correct	O
translation	O
given	O
a	O
source	O
sentence.	O
A	O
potential	O
issue	O
with	O
this	O
encoder-decoder	B-MethodName
approach	O
is	O
that	O
a	O
neural	O
network	O
needs	O
to	O
be	O
able	O
to	O
compress	O
all	O
the	O
necessary	O
information	O
of	O
a	O
source	O
sentence	O
into	O
a	O
fixed-length	O
vector.	O
This	O
may	O
make	O
it	O
difficult	O
for	O
the	O
neural	O
network	O
to	O
cope	O
with	O
long	O
sentences,	O
especially	O
those	O
that	O
are	O
longer	O
than	O
the	O
sentences	O
in	O
the	O
training	O
corpus.	O
Cho	O
et	O
al.	O
(2014b)	O
showed	O
that	O
indeed	O
the	O
performance	O
of	O
a	O
basic	O
encoder-decoder	O
deteriorates	O
rapidly	O
as	O
the	O
length	O
of	O
an	O
input	O
sentence	O
increases.	O
In	O
order	O
to	O
address	O
this	O
issue,	O
we	O
introduce	O
an	O
extension	O
to	O
the	O
encoder-decoder	O
model	O
which	O
learns	O
to	O
align	O
and	O
translate	O
jointly.	O
Each	O
time	O
the	O
proposed	O
model	O
generates	O
a	O
word	O
in	O
a	O
translation,	O
it	O
(soft-)searches	O
for	O
a	O
set	O
of	O
positions	O
in	O
a	O
source	O
sentence	O
where	O
the	O
most	O
relevant	O
information	O
is	O
concentrated.	O
The	O
model	O
then	O
predicts	O
a	O
target	O
word	O
based	O
on	O
the	O
context	O
vectors	O
associated	O
with	O
these	O
source	O
positions	O
and	O
all	O
the	O
previous	O
generated	O
target	O
words.	O

The	O
most	O
important	O
distinguishing	O
feature	O
of	O
this	O
approach	O
from	O
the	O
basic	B-MethodName
encoder-decoder	I-MethodName
is	O
that	O
it	O
does	O
not	O
attempt	O
to	O
encode	O
a	O
whole	O
input	O
sentence	O
into	O
a	O
single	O
fixed-length	O
vector.	O
Instead,	O
it	O
encodes	O
the	O
input	O
sentence	O
into	O
a	O
sequence	O
of	O
vectors	O
and	O
chooses	O
a	O
subset	O
of	O
these	O
vectors	O
adaptively	O
while	O
decoding	O
the	O
translation.	O
This	O
frees	O
a	O
neural	O
translation	O
model	O
from	O
having	O
to	O
squash	O
all	O
the	O
information	O
of	O
a	O
source	O
sentence,	O
regardless	O
of	O
its	O
length,	O
into	O
a	O
fixed-length	O
vector.	O
We	O
show	O
this	O
allows	O
a	O
model	O
to	O
cope	O
better	O
with	O
long	O
sentences.	O
In	O
this	O
paper,	O
we	O
show	O
that	O
the	O
proposed	O
approach	O
of	O
jointly	B-MethodName
learning	I-MethodName
to	I-MethodName
align	I-MethodName
and	I-MethodName
translate	I-MethodName
achieves	O
significantly	O
improved	O
translation	O
performance	O
over	O
the	O
basic	B-MethodName
encoder-decoder	I-MethodName
approach.	O
The	O
improvement	O
is	O
more	O
apparent	O
with	O
longer	O
sentences,	O
but	O
can	O
be	O
observed	O
with	O
sentences	O
of	O
any	O
length.	O
On	O
the	O
task	O
of	O
English-to-French	B-TaskName
translation,	I-TaskName
the	O
proposed	O
approach	O
achieves,	O
with	O
a	O
single	O
model,	O
a	O
translation	O
performance	O
comparable,	O
or	O
close,	O
to	O
the	O
conventional	O
phrase-based	O
system.	O
Furthermore,	O
qualitative	O
analysis	O
reveals	O
that	O
the	O
proposed	O
model	O
finds	O
a	O
linguistically	O
plausible	O
(soft-)alignment	O
between	O
a	O
source	O
sentence	O
and	O
the	O
corresponding	O
target	O
sentence.	O

From	O
a	O
probabilistic	O
perspective,	O
translation	O
is	O
equivalent	O
to	O
finding	O
a	O
target	O
sentence	O
y	O
that	O
maximizes	O
the	O
conditional	O
probability	O
of	O
y	O
given	O
a	O
source	O
sentence	O
x,	O
i.e.,	O
arg	O
max	O
y	O
p(y	O
|	O
x).	O
In	O
neural	B-TaskName
machine	I-TaskName
translation,	I-TaskName
we	O
fit	O
a	O
parameterized	O
model	O
to	O
maximize	O
the	O
conditional	O
probability	O
of	O
sentence	O
pairs	O
using	O
a	O
parallel	O
training	O
corpus.	O
Once	O
the	O
conditional	O
distribution	O
is	O
learned	O
by	O
a	O
translation	O
model,	O
given	O
a	O
source	O
sentence	O
a	O
corresponding	O
translation	O
can	O
be	O
generated	O
by	O
searching	O
for	O
the	O
sentence	O
that	O
maximizes	O
the	O
conditional	O
probability.	O
Recently,	O
a	O
number	O
of	O
papers	O
have	O
proposed	O
the	O
use	O
of	O
neural	O
networks	O
to	O
directly	O
learn	O
this	O
conditional	O
distribution	O
(see,	O
e.g.,	O
Kalchbrenner	O
and	O
Blunsom,	O
2013;Cho	O
et	O
al.,	O
2014a;Sutskever	O
et	O
al.,	O
2014;Cho	O
et	O
al.,	O
2014b;Forcada	O
andÑeco,	O
1997).	O
This	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
approach	O
typically	O
consists	O
of	O
two	O
components,	O
the	O
first	O
of	O
which	O
encodes	O
a	O
source	O
sentence	O
x	O
and	O
the	O
second	O
decodes	O
to	O
a	O
target	O
sentence	O
y.	O
For	O
instance,	O
two	O
recurrent	O
neural	O
networks	O
(RNN)	O
were	O
used	O
by	O
(Cho	O
et	O
al.,	O
2014a)	O
and	O
(Sutskever	O
et	O
al.,	O
2014)	O
to	O
encode	O
a	O
variable-length	O
source	O
sentence	O
into	O
a	O
fixed-length	O
vector	O
and	O
to	O
decode	O
the	O
vector	O
into	O
a	O
variable-length	O
target	O
sentence.	O
Despite	O
being	O
a	O
quite	O
new	O
approach,	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
has	O
already	O
shown	O
promising	O
results.	O
Sutskever	O
et	O
al.	O
(2014)	O
reported	O
that	O
the	O
neural	O
machine	O
translation	O
based	O
on	O
RNNs	O
with	O
long	O
shortterm	O
memory	O
(LSTM)	O
units	O
achieves	O
close	O
to	O
the	O
state-of-the-art	O
performance	O
of	O
the	O
conventional	O
phrase-based	O
machine	O
translation	O
system	O
on	O
an	O
English-to-French	B-TaskName
translation	I-TaskName
task.	O
1	O
Adding	O
neural	O
components	O
to	O
existing	O
translation	O
systems,	O
for	O
instance,	O
to	O
score	O
the	O
phrase	O
pairs	O
in	O
the	O
phrase	O
table	O
(Cho	O
et	O
al.,	O
2014a)	O
or	O
to	O
re-rank	O
candidate	O
translations	O
(Sutskever	O
et	O
al.,	O
2014),	O
has	O
allowed	O
to	O
surpass	O
the	O
previous	O
state-of-the-art	O
performance	O
level.	O

Ce	O
type	O
d'expérience	O
entre	O
dans	O
le	O
cadre	O
des	O
efforts	O
de	O
Disney	O
pour	O
"étendre	O
la	O
durée	O
de	O
vie	O
de	O
ses	O
séries	O
et	O
construire	O
de	O
nouvelles	O
relations	O
avec	O
son	O
public	O
grâceà	O
des	O
plateformes	O
numériques	O
qui	O
sont	O
de	O
plus	O
en	O
plus	O
importantes",	O
a-t-il	O
ajouté.	O

Ce	O
type	O
d'expérience	O
fait	O
partie	O
des	O
initiatives	O
du	O
Disney	O
pour	O
"prolonger	O
la	O
durée	O
de	O
vie	O
de	O
ses	O
nouvelles	O
et	O
de	O
développer	O
des	O
liens	O
avec	O
les	O
lecteurs	O
numériques	O
qui	O
deviennent	O
plus	O
complexes.	O
RNNsearch-50	B-MethodName
Ce	O
genre	O
d'expérience	O
fait	O
partie	O
des	O
efforts	O
de	O
Disney	O
pour	O
"prolonger	O
la	O
durée	O
de	O
vie	O
de	O
ses	O
séries	O
et	O
créer	O
de	O
nouvelles	O
relations	O
avec	O
des	O
publics	O
via	O
des	O
plateformes	O
numériques	O
de	O
plus	O
en	O
plus	O
importantes",	O
a-t-il	O
ajouté.	O
Google	B-MethodName
Translate	I-MethodName
Ce	O
genre	O
d'expérience	O
fait	O
partie	O
des	O
efforts	O
de	O
Disneyà	O
"étendre	O
la	O
durée	O
de	O
vie	O
de	O
sa	O
série	O
et	O
construire	O
de	O
nouvelles	O
relations	O
avec	O
le	O
public	O
par	O
le	O
biais	O
des	O
plates-formes	O
numériques	O
qui	O
deviennent	O
de	O
plus	O
en	O
plus	O
important",	O
at-il	O
ajouté.	O

In	O
a	O
press	O
conference	O
on	O
Thursday,	O
Mr	O
Blair	O
stated	O
that	O
there	O
was	O
nothing	O
in	O
this	O
video	O
that	O
might	O
constitute	O
a	O
"reasonable	O
motive"	O
that	O
could	O
lead	O
to	O
criminal	O
charges	O
being	O
brought	O
against	O
the	O
mayor.	O
Reference	O
En	O
conférence	O
de	O
presse,	O
jeudi,	O
M.	O
Blair	O
a	O
affirmé	O
qu'il	O
n'y	O
avait	O
rien	O
dans	O
cette	O
vidéo	O
qui	O
puisse	O
constituer	O
des	O
"motifs	O
raisonnables"	O
pouvant	O
mener	O
au	O
dépôt	O
d'une	O
accusation	O
criminelle	O
contre	O
le	O
maire.	O
RNNenc-50	B-MethodName
Lors	O
de	O
la	O
conférence	O
de	O
presse	O
de	O
jeudi,	O
M.	O
Blair	O
a	O
dit	O
qu'il	O
n'y	O
avait	O
rien	O
dans	O
cette	O
vidéo	O
qui	O
pourrait	O
constituer	O
une	O
"motivation	O
raisonnable"	O
pouvant	O
entraîner	O
des	O
accusations	O
criminelles	O
portées	O
contre	O
le	O
maire.	O
RNNsearch-50	B-MethodName
Lors	O
d'une	O
conférence	O
de	O
presse	O
jeudi,	O
M.	O
Blair	O
a	O
déclaré	O
qu'il	O
n'y	O
avait	O
rien	O
dans	O
cette	O
vidéo	O
qui	O
pourrait	O
constituer	O
un	O
"motif	O
raisonnable"	O
qui	O
pourrait	O
conduireà	O
des	O
accusations	O
criminelles	O
contre	O
le	O
maire.	O
Google	B-MethodName
Translate	I-MethodName
Lors	O
d'une	O
conférence	O
de	O
presse	O
jeudi,	O
M.	O
Blair	O
a	O
déclaré	O
qu'il	O
n'y	O
avait	O
rien	O
dans	O
cette	O
vido	O
qui	O
pourrait	O
constituer	O
un	O
"motif	O
raisonnable"	O
qui	O
pourrait	O
menerà	O
des	O
accusations	O
criminelles	O
portes	O
contre	O
le	O
maire.	O

Un	O
privilège	O
d'admission	O
est	O
le	O
droit	O
d'un	O
médecin	O
de	O
reconnaître	O
un	O
patientà	O
l'hôpital	O
ou	O
un	O
centre	O
médical	O
d'un	O
diagnostic	O
ou	O
de	O
prendre	O
un	O
diagnostic	O
en	O
fonction	O
de	O
sonétat	O
de	O
santé.	O
RNNsearch-50	B-MethodName
Un	O
privilège	O
d'admission	O
est	O
le	O
droit	O
d'un	O
médecin	O
d'admettre	O
un	O
patientà	O
un	O
hôpital	O
ou	O
un	O
centre	O
médical	O
pour	O
effectuer	O
un	O
diagnostic	O
ou	O
une	O
procédure,	O
selon	O
son	O
statut	O
de	O
travailleur	O
des	O
soins	O
de	O
santéà	O
l'hôpital.	O
Google	B-MethodName
Translate	I-MethodName
Un	O
privilège	O
admettre	O
est	O
le	O
droit	O
d'un	O
médecin	O
d'admettre	O
un	O
patient	O
dans	O
un	O
hôpital	O
ou	O
un	O
centre	O
médical	O
pour	O
effectuer	O
un	O
diagnostic	O
ou	O
une	O
procédure,	O
fondée	O
sur	O
sa	O
situation	O
en	O
tant	O
que	O
travailleur	O
de	O
soins	O
de	O
santé	O
dans	O
un	O
hôpital.	O

This	O
kind	O
of	O
experience	O
is	O
part	O
of	O
Disney's	O
efforts	O
to	O
"extend	O
the	O
lifetime	O
of	O
its	O
series	O
and	O
build	O
new	O
relationships	O
with	O
audiences	O
via	O
digital	O
platforms	O
that	O
are	O
becoming	O
ever	O
more	O
important,"	O
he	O
added.	O

Le	O
privilège	O
d'admission	O
est	O
le	O
droit	O
d'un	O
médecin,	O
en	O
vertu	O
de	O
son	O
statut	O
de	O
membre	O
soignant	O
d'un	O
hôpital,	O
d'admettre	O
un	O
patient	O
dans	O
un	O
hôpital	O
ou	O
un	O
centre	O
médical	O
afin	O
d'y	O
délivrer	O
un	O
diagnostic	O
ou	O
un	O
traitement.	O

An	O
admitting	O
privilege	O
is	O
the	O
right	O
of	O
a	O
doctor	O
to	O
admit	O
a	O
patient	O
to	O
a	O
hospital	O
or	O
a	O
medical	O
centre	O
to	O
carry	O
out	O
a	O
diagnosis	O
or	O
a	O
procedure,	O
based	O
on	O
his	O
status	O
as	O
a	O
health	O
care	O
worker	O
at	O
a	O
hospital.	O

We	O
used	O
the	O
stochastic	O
gradient	O
descent	O
(SGD)	O
algorithm.	O
Adadelta	O
(Zeiler,	O
2012)	O
was	O
used	O
to	O
automatically	O
adapt	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
each	O
parameter	O
(✏	O
=	O
10	B-HyperparameterValue
6	I-HyperparameterValue
and	O
⇢	O
=	O
0.95).	B-HyperparameterValue
We	O
explicitly	O
normalized	O
the	O
L	O
2	O
-norm	O
of	O
the	O
gradient	O
of	O
the	O
cost	O
function	O
each	O
time	O
to	O
be	O
at	O
most	O
a	O
predefined	O
threshold	O
of	O
1,	O
when	O
the	O
norm	O
was	O
larger	O
than	O
the	O
threshold	O
(Pascanu	O
et	O
al.,	O
2013b).	O
Each	O
SGD	O
update	O
direction	O
was	O
computed	O
with	O
a	O
minibatch	B-HyperparameterName
of	O
80	B-HyperparameterValue
sentences.	I-HyperparameterValue
At	O
each	O
update	O
our	O
implementation	O
requires	O
time	O
proportional	O
to	O
the	O
length	O
of	O
the	O
longest	O
sentence	O
in	O
a	O
minibatch.	B-HyperparameterName
Hence,	O
to	O
minimize	O
the	O
waste	O
of	O
computation,	O
before	O
every	O
20-th	O
update,	O
we	O
retrieved	O
1600	O
sentence	O
pairs,	O
sorted	O
them	O
according	O
to	O
the	O
lengths	O
and	O
split	O
them	O
into	O
20	O
minibatches.	B-HyperparameterName
The	O
training	O
data	O
was	O
shuffled	O
once	O
before	O
training	O
and	O
was	O
traversed	O
sequentially	O
in	O
this	O
manner.	O
In	O
Tables	O
2	O
we	O
present	O
the	O
statistics	O
related	O
to	O
training	O
all	O
the	O
models	O
used	O
in	O
the	O
experiments.	O

For	O
all	O
the	O
models	O
used	O
in	O
this	O
paper,	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
a	I-HyperparameterName
hidden	I-HyperparameterName
layer	I-HyperparameterName
n	I-HyperparameterName
is	O
1000,	B-HyperparameterValue
the	O
word	B-HyperparameterName
embedding	I-HyperparameterName
dimensionality	I-HyperparameterName
m	B-HyperparameterName
is	O
620	B-HyperparameterValue
and	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
maxout	I-HyperparameterName
hidden	I-HyperparameterName
layer	I-HyperparameterName
in	I-HyperparameterName
the	I-HyperparameterName
deep	I-HyperparameterName
output	I-HyperparameterName
l	B-HyperparameterName
is	O
500.	B-HyperparameterValue
The	O
number	O
of	O
hidden	O
units	O
in	O
the	O
alignment	O
model	O
n	O
0	O
is	O
1000.	B-HyperparameterValue
U	O
r	O
as	O
random	O
orthogonal	O
matrices.	O
For	O
W	O
a	O
and	O
U	O
a	O
,	O
we	O
initialized	O
them	O
by	O
sampling	O
each	O
element	O
from	O
the	O
Gaussian	O
distribution	O
of	O
mean	O
0	O
and	O
variance	O
0.001	O
2	O
.	O
All	O
the	O
elements	O
of	O
V	O
a	O
and	O
all	O
the	O
bias	O
vectors	O
were	O
initialized	O
to	O
zero.	O
Any	O
other	O
weight	O
matrix	O
was	O
initialized	O
by	O
sampling	O
from	O
the	O
Gaussian	O
distribution	O
of	O
mean	O
0	O
and	O
variance	O
0.01	O
2	O
.	O

In	O
this	O
section,	O
we	O
describe	O
in	O
detail	O
the	O
architecture	O
of	O
the	O
proposed	O
model	O
(RNNsearch)	B-MethodName
used	O
in	O
the	O
experiments	O
(see	O
.	O
From	O
here	O
on,	O
we	O
omit	O
all	O
bias	O
terms	O
in	O
order	O
to	O
increase	O
readability.	O
The	O
model	O
takes	O
a	O
source	O
sentence	O
of	O
1-of-K	O
coded	O
word	O
vectors	O
as	O
input	O
x	O
=	O
(x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
Tx	O
),	O
x	O
i	O
2	O
R	O
Kx	O
and	O
outputs	O
a	O
translated	O
sentence	O
of	O
1-of-K	O
coded	O
word	O
vectors	O
y	O
=	O
(y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
Ty	O
),	O
y	O
i	O
2	O
R	O
Ky	O
,	O
where	O
K	B-HyperparameterName
x	I-HyperparameterName
and	O
K	B-HyperparameterName
y	I-HyperparameterName
are	O
the	O
vocabulary	B-HyperparameterName
sizes	I-HyperparameterName
of	I-HyperparameterName
source	I-HyperparameterName
and	I-HyperparameterName
target	I-HyperparameterName
languages,	I-HyperparameterName
respectively.	B-HyperparameterName
T	I-HyperparameterName
x	I-HyperparameterName
and	O
T	B-HyperparameterName
y	I-HyperparameterName
respectively	O
denote	O
the	O
lengths	B-HyperparameterName
of	I-HyperparameterName
source	I-HyperparameterName
and	I-HyperparameterName
target	I-HyperparameterName
sentences.	I-HyperparameterName
First,	O
the	O
forward	O
states	O
of	O
the	O
bidirectional	O
recurrent	O
neural	O
network	O
(BiRNN)	B-MethodName
are	O
computed:	O
!	O
h	O
i	O
=	O
(	O
(1	O
!	O
z	O
i	O
)	O
!	O
h	O
i	O
1	O
+	O
!	O
z	O
i	O
!	O
h	O
i	O
,	O
if	O
i	O
>	O
0	O
0	O
,	O
if	O
i	O
=	O
0	O
where	O
!	O
h	O
i	O
=	O
tanh	O
⇣	O
!	O
W	O
Ex	O
i	O
+	O
!	O
U	O
h	O
!	O
r	O
i	O
!	O
h	O
i	O
1	O
i⌘	O
!	O
z	O
i	O
=	O
⇣	O
!	O
W	O
z	O
Ex	O
i	O
+	O
!	O
U	O
z	O
!	O
h	O
i	O
1	O
⌘	O
!	O
r	O
i	O
=	O
⇣	O
!	O
W	O
r	O
Ex	O
i	O
+	O
!	O
U	O
r	O
!	O
h	O
i	O
1	O
⌘	O
.	O
E	O
2	O
R	O
m⇥Kx	O
is	O
the	O
word	O
embedding	O
matrix.	O
!	O
W	O
,	O
!	O
W	O
z	O
,	O
!	O
W	O
r	O
2	O
R	O
n⇥m	O
,	O
!	O
U	O
,	O
!	O
U	O
z	O
,	O
!	O
U	O
r	O
2	O
R	O
n⇥n	O
are	O
weight	O
matrices.	O
m	O
and	O
n	O
are	O
the	O
word	O
embedding	O
dimensionality	O
and	O
the	O
number	O
of	O
hidden	O
units,	O
respectively.	O
(•)	O
is	O
as	O
usual	O
a	O
logistic	O
sigmoid	O
function.	O
The	O
backward	O
states	O
(	O
h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
Tx	O
)	O
are	O
computed	O
similarly.	O
We	O
share	O
the	O
word	O
embedding	O
matrix	O
E	O
between	O
the	O
forward	O
and	O
backward	O
RNNs,	B-MethodName
unlike	O
the	O
weight	O
matrices.	O
We	O
concatenate	O
the	O
forward	O
and	O
backward	O
states	O
to	O
to	O
obtain	O
the	O
annotations	O
(h	O
1	O
,	O
h	O
2	O
,	O
•	O
•	O
•	O
,	O
h	O
Tx	O
),	O
where	O
h	O
i	O
=	O
"	O
!	O
h	O
i	O
h	O
i	O
#	O
(7)	O
A.2.2	O
DECODER	O
The	O
hidden	O
state	O
s	O
i	O
of	O
the	O
decoder	O
given	O
the	O
annotations	O
from	O
the	O
encoder	O
is	O
computed	O
by	O
s	O
i	O
=(1	O
z	O
i	O
)	O
s	O
i	O
1	O
+	O
z	O
i	O
s	O
i	O
,	O
wheres	O
i	O
=	O
tanh	O
(W	O
Ey	O
i	O
1	O
+	O
U	O
[r	O
i	O
s	O
i	O
1	O
]	O
+	O
Cc	O
i	O
)	O
z	O
i	O
=	O
(W	O
z	O
Ey	O
i	O
1	O
+	O
U	O
z	O
s	O
i	O
1	O
+	O
C	O
z	O
c	O
i	O
)	O
r	O
i	O
=	O
(W	O
r	O
Ey	O
i	O
1	O
+	O
U	O
r	O
s	O
i	O
1	O
+	O
C	O
r	O
c	O
i	O
)	O
E	O
is	O
the	O
word	O
embedding	O
matrix	O
for	O
the	O
target	O
language.	O
W,	O
W	O
z	O
,	O
W	O
r	O
2	O
R	O
n⇥m	O
,	O
U,	O
U	O
z	O
,	O
U	O
r	O
2	O
R	O
n⇥n	O
,	O
and	O
C,	O
C	O
z	O
,	O
C	O
r	O
2	O
R	O
n⇥2n	O
are	O
weights.	O
Again,	O
m	O
and	O
n	O
are	O
the	O
word	O
embedding	O
dimensionality	O
and	O
the	O
number	O
of	O
hidden	O
units,	O
respectively.	O
The	O
initial	O
hidden	O
state	O
s	O
0	O
is	O
computed	O
by	O
s	O
0	O
=	O
tanh	O
⇣	O
W	O
s	O
h	O
1	O
⌘	O
,	O
where	O
W	O
s	O
2	O
R	O
n⇥n	O
.	O
The	O
context	O
vector	O
c	O
i	O
are	O
recomputed	O
at	O
each	O
step	O
by	O
the	O
alignment	O
model:	O
c	O
i	O
=	O
Tx	O
X	O
j=1	O
↵	O
ij	O
h	O
j	O
,	O
s	O
i	O
1	O
+	O
U	O
a	O
h	O
j	O
)	O
,	O
and	O
h	O
j	O
is	O
the	O
j-th	O
annotation	O
in	O
the	O
source	O
sentence	O
(see	O
Eq.	O
(	O
7)).	O
v	O
a	O
2	O
R	O
n	O
0	O
,	O
W	O
a	O
2	O
R	O
n	O
0	O
⇥n	O
and	O
U	O
a	O
2	O
R	O
n	O
0	O
⇥2n	O
are	O
weight	O
matrices.	O
Note	O
that	O
the	O
model	O
becomes	O
RNN	O
Encoder-Decoder	O
(Cho	O
et	O
al.,	O
2014a),	O
if	O
we	O
fix	O
c	O
i	O
to	O
!	O
h	O
Tx	O
.	O
With	O
the	O
decoder	O
state	O
s	O
i	O
1	O
,	O
the	O
context	O
c	O
i	O
and	O
the	O
last	O
generated	O
word	O
y	O
i	O
1	O
,	O
we	O
define	O
the	O
probability	O
of	O
a	O
target	O
word	O
y	O
i	O
as	O
p(y	O
i	O
|s	O
i	O
,	O
y	O
i	O
1	O
,	O
c	O
i	O
)	O
/	O
exp	O
y	O
>	O
i	O
W	O
o	O
t	O
i	O
,	O
where	O
t	O
i	O
=	O
⇥	O
max	O
t	O
i,2j	O
1	O
,t	O
i,2j	O
⇤	O
>	O
j=1,...,l	O
andt	O
i,k	O
is	O
the	O
k-th	O
element	O
of	O
a	O
vectort	O
i	O
which	O
is	O
computed	O
bỹ	O
t	O
i	O
=U	O
o	O
s	O
i	O
1	O
+	O
V	O
o	O
Ey	O
i	O
1	O
+	O
C	O
o	O
c	O
i	O
.	O
W	O
o	O
2	O
R	O
Ky⇥l	O
,	O
U	O
o	O
2	O
R	O
2l⇥n	O
,	O
V	O
o	O
2	O
R	O
2l⇥m	O
and	O
C	O
o	O
2	O
R	O
2l⇥2n	O
are	O
weight	O
matrices.	O
This	O
can	O
be	O
understood	O
as	O
having	O
a	O
deep	O
output	O
(Pascanu	O
et	O
al.,	O
2014)	O
with	O
a	O
single	B-HyperparameterValue
maxout	B-HyperparameterName
hidden	I-HyperparameterName
layer	I-HyperparameterName
(Goodfellow	O
et	O
al.,	O
2013).	O

The	O
alignment	O
model	O
should	O
be	O
designed	O
considering	O
that	O
the	O
model	O
needs	O
to	O
be	O
evaluated	O
T	O
x	O
⇥	O
T	O
y	O
times	O
for	O
each	O
sentence	O
pair	O
of	O
lengths	O
T	O
x	O
and	O
T	O
y	O
.	O
In	O
order	O
to	O
reduce	O
computation,	O
we	O
use	O
a	O
singlelayer	B-HyperparameterValue
multilayer	O
perceptron	O
such	O
that	O
a(s	O
i	O
1	O
,	O
h	O
j	O
)	O
=	O
v	O
>	O
a	O
tanh	O
(W	O
a	O
s	O
i	O
1	O
+	O
U	O
a	O
h	O
j	O
)	O
,	O
where	O
W	O
a	O
2	O
R	O
n⇥n	O
,	O
U	O
a	O
2	O
R	O
n⇥2n	O
and	O
v	O
a	O
2	O
R	O
n	O
are	O
the	O
weight	O
matrices.	O
Since	O
U	O
a	O
h	O
j	O
does	O
not	O
depend	O
on	O
i,	O
we	O
can	O
pre-compute	O
it	O
in	O
advance	O
to	O
minimize	O
the	O
computational	O
cost.	O

For	O
the	O
activation	O
function	O
f	O
of	O
an	O
RNN,	O
we	O
use	O
the	O
gated	O
hidden	O
unit	O
recently	O
proposed	O
by	O
Cho	O
et	O
al.	O
(2014a).	O
The	O
gated	O
hidden	O
unit	O
is	O
an	O
alternative	O
to	O
the	O
conventional	O
simple	O
units	O
such	O
as	O
an	O
element-wise	O
tanh.	O
This	O
gated	O
unit	O
is	O
similar	O
to	O
a	O
long	O
short-term	O
memory	O
(LSTM)	O
unit	O
proposed	O
earlier	O
by	O
Hochreiter	O
and	O
Schmidhuber	O
(1997),	O
sharing	O
with	O
it	O
the	O
ability	O
to	O
better	O
model	O
and	O
learn	O
long-term	O
dependencies.	O
This	O
is	O
made	O
possible	O
by	O
having	O
computation	O
paths	O
in	O
the	O
unfolded	O
RNN	O
for	O
which	O
the	O
product	O
of	O
derivatives	O
is	O
close	O
to	O
1.	O
These	O
paths	O
allow	O
gradients	O
to	O
flow	O
backward	O
easily	O
without	O
suffering	O
too	O
much	O
from	O
the	O
vanishing	O
effect	O
(Hochreiter,	O
1991;Bengio	O
et	O
al.,	O
1994;Pascanu	O
et	O
al.,	O
2013a).	O
It	O
is	O
therefore	O
possible	O
to	O
use	O
LSTM	O
units	O
instead	O
of	O
the	O
gated	O
hidden	O
unit	O
described	O
here,	O
as	O
was	O
done	O
in	O
a	O
similar	O
context	O
by	O
Sutskever	O
et	O
al.	O
(2014).	O
The	O
new	O
state	O
s	O
i	O
of	O
the	O
RNN	O
employing	O
n	B-HyperparameterName
gated	I-HyperparameterName
hidden	I-HyperparameterName
units	I-HyperparameterName
8	B-HyperparameterValue
is	O
computed	O
by	O
s	O
i	O
=	O
f	O
(s	O
i	O
1	O
,	O
y	O
i	O
1	O
,	O
c	O
i	O
)	O
=	O
(1	O
z	O
i	O
)	O
s	O
i	O
1	O
+	O
z	O
i	O
s	O
i	O
,	O
where	O
is	O
an	O
element-wise	O
multiplication,	O
and	O
z	O
i	O
is	O
the	O
output	O
of	O
the	O
update	O
gates	O
(see	O
below).	O
The	O
proposed	O
updated	O
states	O
i	O
is	O
computed	O
bỹ	O
s	O
i	O
=	O
tanh	O
(W	O
e(y	O
i	O
1	O
)	O
+	O
U	O
[r	O
i	O
s	O
i	O
1	O
]	O
+	O
Cc	O
i	O
)	O
,	O
where	O
e(y	O
i	O
1	O
)	O
2	O
R	O
m	O
is	O
an	O
m-dimensional	O
embedding	O
of	O
a	O
word	O
y	O
i	O
1	O
,	O
and	O
r	O
i	O
is	O
the	O
output	O
of	O
the	O
reset	O
gates	O
(see	O
below).	O
When	O
y	O
i	O
is	O
represented	O
as	O
a	O
1-of-K	O
vector,	O
e(y	O
i	O
)	O
is	O
simply	O
a	O
column	O
of	O
an	O
embedding	O
matrix	O
E	O
2	O
R	O
m⇥K	O
.	O
Whenever	O
possible,	O
we	O
omit	O
bias	O
terms	O
to	O
make	O
the	O
equations	O
less	O
cluttered.	O
The	O
update	O
gates	O
z	O
i	O
allow	O
each	O
hidden	O
unit	O
to	O
maintain	O
its	O
previous	O
activation,	O
and	O
the	O
reset	O
gates	O
r	O
i	O
control	O
how	O
much	O
and	O
what	O
information	O
from	O
the	O
previous	O
state	O
should	O
be	O
reset.	O
We	O
compute	O
them	O
by	O
z	O
i	O
=	O
(W	O
z	O
e(y	O
i	O
1	O
)	O
+	O
U	O
z	O
s	O
i	O
1	O
+	O
C	O
z	O
c	O
i	O
)	O
,	O
r	O
i	O
=	O
(W	O
r	O
e(y	O
i	O
1	O
)	O
+	O
U	O
r	O
s	O
i	O
1	O
+	O
C	O
r	O
c	O
i	O
)	O
,	O
where	O
(•)	O
is	O
a	O
logistic	O
sigmoid	O
function.	O
At	O
each	O
step	O
of	O
the	O
decoder,	O
we	O
compute	O
the	O
output	O
probability	O
(Eq.	O
(	O
4))	O
as	O
a	O
multi-layered	O
function	O
(Pascanu	O
et	O
al.,	O
2014).	O
We	O
use	O
a	O
single	B-HyperparameterValue
hidden	B-HyperparameterName
layer	I-HyperparameterName
of	O
maxout	O
units	O
(Goodfellow	O
et	O
al.,	O
2013)	O
and	O
normalize	O
the	O
output	O
probabilities	O
(one	O
for	O
each	O
word)	O
with	O
a	O
softmax	O
function	O
(see	O
Eq.	O
(	O
6)).	O

A.1	O
ARCHITECTURAL	O
CHOICES	O
The	O
proposed	O
scheme	O
in	O
Section	O
3	O
is	O
a	O
general	O
framework	O
where	O
one	O
can	O
freely	O
define,	O
for	O
instance,	O
the	O
activation	O
functions	O
f	O
of	O
recurrent	O
neural	O
networks	O
(RNN)	B-MethodName
and	O
the	O
alignment	O
model	O
a.	O
Here,	O
we	O
describe	O
the	O
choices	O
we	O
made	O
for	O
the	O
experiments	O
in	O
this	O
paper.	O

The	O
conventional	O
approach	O
to	O
neural	O
machine	O
translation,	O
called	O
an	O
encoder-decoder	B-MethodName
approach,	O
encodes	O
a	O
whole	O
input	O
sentence	O
into	O
a	O
fixed-length	O
vector	O
from	O
which	O
a	O
translation	O
will	O
be	O
decoded.	O
We	O
conjectured	O
that	O
the	O
use	O
of	O
a	O
fixed-length	O
context	O
vector	O
is	O
problematic	O
for	O
translating	O
long	O
sentences,	O
based	O
on	O
a	O
recent	O
empirical	O
study	O
reported	O
by	O
Cho	O
et	O
al.	O
(2014b)	O
and	O
Pouget-Abadie	O
et	O
al.	O
(2014).	O
In	O
this	O
paper,	O
we	O
proposed	O
a	O
novel	O
architecture	O
that	O
addresses	O
this	O
issue.	O
We	O
extended	O
the	O
basic	O
encoder-decoder	O
by	O
letting	O
a	O
model	O
(soft-)search	O
for	O
a	O
set	O
of	O
input	O
words,	O
or	O
their	O
annotations	O
computed	O
by	O
an	O
encoder,	O
when	O
generating	O
each	O
target	O
word.	O
This	O
frees	O
the	O
model	O
from	O
having	O
to	O
encode	O
a	O
whole	O
source	O
sentence	O
into	O
a	O
fixed-length	O
vector,	O
and	O
also	O
lets	O
the	O
model	O
focus	O
only	O
on	O
information	O
relevant	O
to	O
the	O
generation	O
of	O
the	O
next	O
target	O
word.	O
This	O
has	O
a	O
major	O
positive	O
impact	O
on	O
the	O
ability	O
of	O
the	O
neural	O
machine	O
translation	O
system	O
to	O
yield	O
good	O
results	O
on	O
longer	O
sentences.	O
Unlike	O
with	O
the	O
traditional	O
machine	O
translation	O
systems,	O
all	O
of	O
the	O
pieces	O
of	O
the	O
translation	O
system,	O
including	O
the	O
alignment	O
mechanism,	O
are	O
jointly	O
trained	O
towards	O
a	O
better	O
log-probability	O
of	O
producing	O
correct	O
translations.	O
We	O
tested	O
the	O
proposed	O
model,	O
called	O
RNNsearch,	B-MethodName
on	O
the	O
task	O
of	O
English-to-French	B-TaskName
translation.	I-TaskName
The	O
experiment	O
revealed	O
that	O
the	O
proposed	O
RNNsearch	B-MethodName
outperforms	O
the	O
conventional	O
encoder-decoder	O
model	O
(RNNencdec)	B-MethodName
significantly,	O
regardless	O
of	O
the	O
sentence	O
length	O
and	O
that	O
it	O
is	O
much	O
more	O
robust	O
to	O
the	O
length	O
of	O
a	O
source	O
sentence.	O
From	O
the	O
qualitative	O
analysis	O
where	O
we	O
investigated	O
the	O
(soft-)alignment	O
generated	O
by	O
the	O
RNNsearch,	B-MethodName
we	O
were	O
able	O
to	O
conclude	O
that	O
the	O
model	O
can	O
correctly	O
align	O
each	O
target	O
word	O
with	O
the	O
relevant	O
words,	O
or	O
their	O
annotations,	O
in	O
the	O
source	O
sentence	O
as	O
it	O
generated	O
a	O
correct	O
translation.	O
Perhaps	O
more	O
importantly,	O
the	O
proposed	O
approach	O
achieved	O
a	O
translation	O
performance	O
comparable	O
to	O
the	O
existing	O
phrase-based	O
statistical	O
machine	O
translation.	O
It	O
is	O
a	O
striking	O
result,	O
considering	O
that	O
the	O
proposed	O
architecture,	O
or	O
the	O
whole	O
family	O
of	O
neural	O
machine	O
translation,	O
has	O
only	O
been	O
proposed	O
as	O
recently	O
as	O
this	O
year.	O
We	O
believe	O
the	O
architecture	O
proposed	O
here	O
is	O
a	O
promising	O
step	O
toward	O
better	O
machine	O
translation	O
and	O
a	O
better	O
understanding	O
of	O
natural	O
languages	O
in	O
general.	O
One	O
of	O
challenges	O
left	O
for	O
the	O
future	O
is	O
to	O
better	O
handle	O
unknown,	O
or	O
rare	O
words.	O
This	O
will	O
be	O
required	O
for	O
the	O
model	O
to	O
be	O
more	O
widely	O
used	O
and	O
to	O
match	O
the	O
performance	O
of	O
current	O
state-of-the-art	O
machine	O
translation	O
systems	O
in	O
all	O
contexts.	O

Since	O
Bengio	O
et	O
al.	O
(2003)	O
introduced	O
a	O
neural	O
probabilistic	O
language	O
model	O
which	O
uses	O
a	O
neural	O
network	O
to	O
model	O
the	O
conditional	O
probability	O
of	O
a	O
word	O
given	O
a	O
fixed	O
number	O
of	O
the	O
preceding	O
words,	O
neural	O
networks	O
have	O
widely	O
been	O
used	O
in	O
machine	O
translation.	O
However,	O
the	O
role	O
of	O
neural	O
networks	O
has	O
been	O
largely	O
limited	O
to	O
simply	O
providing	O
a	O
single	O
feature	O
to	O
an	O
existing	O
statistical	O
machine	O
translation	O
system	O
or	O
to	O
re-rank	O
a	O
list	O
of	O
candidate	O
translations	O
provided	O
by	O
an	O
existing	O
system.	O
For	O
instance,	O
Schwenk	O
(2012)	O
proposed	O
using	O
a	O
feedforward	O
neural	O
network	O
to	O
compute	O
the	O
score	O
of	O
a	O
pair	O
of	O
source	O
and	O
target	O
phrases	O
and	O
to	O
use	O
the	O
score	O
as	O
an	O
additional	O
feature	O
in	O
the	O
phrase-based	O
statistical	O
machine	O
translation	O
system.	O
More	O
recently,	O
Kalchbrenner	O
and	O
Blunsom	O
(2013)	O
and	O
Devlin	O
et	O
al.	O
(2014)	O
reported	O
the	O
successful	O
use	O
of	O
the	O
neural	O
networks	O
as	O
a	O
sub-component	O
of	O
the	O
existing	O
translation	O
system.	O
Traditionally,	O
a	O
neural	O
network	O
trained	O
as	O
a	O
target-side	O
language	O
model	O
has	O
been	O
used	O
to	O
rescore	O
or	O
rerank	O
a	O
list	O
of	O
candidate	O
translations	O
(see,	O
e.g.,	O
Schwenk	O
et	O
al.,	O
2006).	O
Although	O
the	O
above	O
approaches	O
were	O
shown	O
to	O
improve	O
the	O
translation	O
performance	O
over	O
the	O
stateof-the-art	O
machine	O
translation	O
systems,	O
we	O
are	O
more	O
interested	O
in	O
a	O
more	O
ambitious	O
objective	O
of	O
designing	O
a	O
completely	O
new	O
translation	O
system	O
based	O
on	O
neural	O
networks.	O
The	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
approach	O
we	O
consider	O
in	O
this	O
paper	O
is	O
therefore	O
a	O
radical	O
departure	O
from	O
these	O
earlier	O
works.	O
Rather	O
than	O
using	O
a	O
neural	O
network	O
as	O
a	O
part	O
of	O
the	O
existing	O
system,	O
our	O
model	O
works	O
on	O
its	O
own	O
and	O
generates	O
a	O
translation	O
from	O
a	O
source	O
sentence	O
directly.	O

A	O
similar	O
approach	O
of	O
aligning	O
an	O
output	O
symbol	O
with	O
an	O
input	O
symbol	O
was	O
proposed	O
recently	O
by	O
Graves	O
(2013)	O
in	O
the	O
context	O
of	O
handwriting	O
synthesis.	O
Handwriting	O
synthesis	O
is	O
a	O
task	O
where	O
the	O
model	O
is	O
asked	O
to	O
generate	O
handwriting	O
of	O
a	O
given	O
sequence	O
of	O
characters.	O
In	O
his	O
work,	O
he	O
used	O
a	O
mixture	O
of	O
Gaussian	O
kernels	O
to	O
compute	O
the	O
weights	O
of	O
the	O
annotations,	O
where	O
the	O
location,	O
width	O
and	O
mixture	O
coefficient	O
of	O
each	O
kernel	O
was	O
predicted	O
from	O
an	O
alignment	O
model.	O
More	O
specifically,	O
his	O
alignment	O
was	O
restricted	O
to	O
predict	O
the	O
location	O
such	O
that	O
the	O
location	O
increases	O
monotonically.	O
The	O
main	O
difference	O
from	O
our	O
approach	O
is	O
that,	O
in	O
(Graves,	O
2013),	O
the	O
modes	O
of	O
the	O
weights	O
of	O
the	O
annotations	O
only	O
move	O
in	O
one	O
direction.	O
In	O
the	O
context	O
of	O
machine	O
translation,	O
this	O
is	O
a	O
severe	O
limitation,	O
as	O
(long-distance)	O
reordering	O
is	O
often	O
needed	O
to	O
generate	O
a	O
grammatically	O
correct	O
translation	O
(for	O
instance,	O
English-to-German).	O
Our	O
approach,	O
on	O
the	O
other	O
hand,	O
requires	O
computing	O
the	O
annotation	O
weight	O
of	O
every	O
word	O
in	O
the	O
source	O
sentence	O
for	O
each	O
word	O
in	O
the	O
translation.	O
This	O
drawback	O
is	O
not	O
severe	O
with	O
the	O
task	O
of	O
translation	O
in	O
which	O
most	O
of	O
input	O
and	O
output	O
sentences	O
are	O
only	O
15-40	O
words.	O
However,	O
this	O
may	O
limit	O
the	O
applicability	O
of	O
the	O
proposed	O
scheme	O
to	O
other	O
tasks.	O

Ce	O
type	O
d'expérience	O
fait	O
partie	O
des	O
initiatives	O
du	O
Disney	O
pour	O
"prolonger	O
la	O
durée	O
de	O
vie	O
de	O
ses	O
nouvelles	O
et	O
de	O
développer	O
des	O
liens	O
avec	O
les	O
lecteurs	O
numériques	O
qui	O
deviennent	O
plus	O
complexes.	O
As	O
with	O
the	O
previous	O
example,	O
the	O
RNNencdec	B-MethodName
began	O
deviating	O
from	O
the	O
actual	O
meaning	O
of	O
the	O
source	O
sentence	O
after	O
generating	O
approximately	O
30	O
words	O
(see	O
the	O
underlined	O
phrase).	O
After	O
that	O
point,	O
the	O
quality	O
of	O
the	O
translation	O
deteriorates,	O
with	O
basic	O
mistakes	O
such	O
as	O
the	O
lack	O
of	O
a	O
closing	O
quotation	O
mark.	O
Again,	O
the	O
RNNsearch-50	B-MethodName
was	O
able	O
to	O
translate	O
this	O
long	O
sentence	O
correctly:	O
Ce	O
genre	O
d'expérience	O
fait	O
partie	O
des	O
efforts	O
de	O
Disney	O
pour	O
"prolonger	O
la	O
durée	O
de	O
vie	O
de	O
ses	O
séries	O
et	O
créer	O
de	O
nouvelles	O
relations	O
avec	O
des	O
publics	O
via	O
des	O
plateformes	O
numériques	O
de	O
plus	O
en	O
plus	O
importantes",	O
a-t-il	O
ajouté.	O
In	O
conjunction	O
with	O
the	O
quantitative	O
results	O
presented	O
already,	O
these	O
qualitative	O
observations	O
confirm	O
our	O
hypotheses	O
that	O
the	O
RNNsearch	B-MethodName
architecture	O
enables	O
far	O
more	O
reliable	O
translation	O
of	O
long	O
sentences	O
than	O
the	O
standard	O
RNNencdec	B-MethodName
model.	O
In	O
Appendix	O
C,	O
we	O
provide	O
a	O
few	O
more	O
sample	O
translations	O
of	O
long	O
source	O
sentences	O
generated	O
by	O
the	O
RNNencdec-50,	B-MethodName
RNNsearch-50	B-MethodName
and	O
Google	O
Translate	O
along	O
with	O
the	O
reference	O
translations.	O
6	O
RELATED	O
WORK	O

As	O
clearly	O
visible	O
from	O
Fig.	O
2	O
the	O
proposed	O
model	O
(RNNsearch)	B-MethodName
is	O
much	O
better	O
than	O
the	O
conventional	O
model	O
(RNNencdec)	B-MethodName
at	O
translating	B-TaskName
long	I-TaskName
sentences.	I-TaskName
This	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
the	O
RNNsearch	B-MethodName
does	O
not	O
require	O
encoding	O
a	O
long	O
sentence	O
into	O
a	O
fixed-length	O
vector	O
perfectly,	O
but	O
only	O
accurately	O
encoding	O
the	O
parts	O
of	O
the	O
input	O
sentence	O
that	O
surround	O
a	O
particular	O
word.	O
As	O
an	O
example,	O
consider	O
this	O
source	O
sentence	O
from	O
the	O
test	O
set:	O
An	O
admitting	O
privilege	O
is	O
the	O
right	O
of	O
a	O
doctor	O
to	O
admit	O
a	O
patient	O
to	O
a	O
hospital	O
or	O
a	O
medical	O
centre	O
to	O
carry	O
out	O
a	O
diagnosis	O
or	O
a	O
procedure,	O
based	O
on	O
his	O
status	O
as	O
a	O
health	O
care	O
worker	O
at	O
a	O
hospital.	O
The	O
RNNencdec-50	B-MethodName
translated	O
this	O
sentence	O
into:	O
Un	O
privilège	O
d'admission	O
est	O
le	O
droit	O
d'un	O
médecin	O
de	O
reconnaître	O
un	O
patientà	O
l'hôpital	O
ou	O
un	O
centre	O
médical	O
d'un	O
diagnostic	O
ou	O
de	O
prendre	O
un	O
diagnostic	O
en	O
fonction	O
de	O
sonétat	O
de	O
santé.	O
The	O
RNNencdec-50	B-MethodName
correctly	O
translated	O
the	O
source	O
sentence	O
until	O
[a	O
medical	O
center].	O
However,	O
from	O
there	O
on	O
(underlined),	O
it	O
deviated	O
from	O
the	O
original	O
meaning	O
of	O
the	O
source	O
sentence.	O
For	O
instance,	O
it	O
replaced	O
[based	O
on	O
his	O
status	O
as	O
a	O
health	O
care	O
worker	O
at	O
a	O
hospital]	O
in	O
the	O
source	O
sentence	O
with	O
[en	O
fonction	O
de	O
sonétat	O
de	O
santé]	O
("based	O
on	O
his	O
state	O
of	O
health").	O
On	O
the	O
other	O
hand,	O
the	O
RNNsearch-50	B-MethodName
generated	O
the	O
following	O
correct	O
translation,	O
preserving	O
the	O
whole	O
meaning	O
of	O
the	O
input	O
sentence	O
without	O
omitting	O
any	O
details:	O
Un	O
privilège	O
d'admission	O
est	O
le	O
droit	O
d'un	O
médecin	O
d'admettre	O
un	O
patientà	O
un	O
hôpital	O
ou	O
un	O
centre	O
médical	O
pour	O
effectuer	O
un	O
diagnostic	O
ou	O
une	O
procédure,	O
selon	O
son	O
statut	O
de	O
travailleur	O
des	O
soins	O
de	O
santéà	O
l'hôpital.	O
Let	O
us	O
consider	O
another	O
sentence	O
from	O
the	O
test	O
set:	O
This	O
kind	O
of	O
experience	O
is	O
part	O
of	O
Disney's	O
efforts	O
to	O
"extend	O
the	O
lifetime	O
of	O
its	O
series	O
and	O
build	O
new	O
relationships	O
with	O
audiences	O
via	O
digital	O
platforms	O
that	O
are	O
becoming	O
ever	O
more	O
important,"	O
he	O
added.	O

The	O
proposed	O
approach	O
provides	O
an	O
intuitive	O
way	O
to	O
inspect	O
the	O
(soft-)alignment	O
between	O
the	O
words	O
in	O
a	O
generated	O
translation	O
and	O
those	O
in	O
a	O
source	O
sentence.	O
This	O
is	O
done	O
by	O
visualizing	O
the	O
annotation	O
weights	O
↵	O
ij	O
from	O
Eq.	O
(6),	O
as	O
in	O
Fig.	O
3.	O
Each	O
row	O
of	O
a	O
matrix	O
in	O
each	O
plot	O
indicates	O
the	O
weights	O
associated	O
with	O
the	O
annotations.	O
From	O
this	O
we	O
see	O
which	O
positions	O
in	O
the	O
source	O
sentence	O
were	O
considered	O
more	O
important	O
when	O
generating	O
the	O
target	O
word.	O
We	O
can	O
see	O
from	O
the	O
alignments	O
in	O
Fig.	O
3	O
that	O
the	O
alignment	O
of	O
words	O
between	O
English	O
and	O
French	O
is	O
largely	O
monotonic.	O
We	O
see	O
strong	O
weights	O
along	O
the	O
diagonal	O
of	O
each	O
matrix.	O
However,	O
we	O
also	O
observe	O
a	O
number	O
of	O
non-trivial,	O
non-monotonic	O
alignments.	O
Adjectives	O
and	O
nouns	O
are	O
typically	O
ordered	O
differently	O
between	O
French	O
and	O
English,	O
and	O
we	O
see	O
an	O
example	O
in	O
Fig.	O
3	O
We	O
observe	O
similar	O
behaviors	O
in	O
all	O
the	O
presented	O
cases	O
in	O
Fig.	O
3.	O
An	O
additional	O
benefit	O
of	O
the	O
soft	O
alignment	O
is	O
that	O
it	O
naturally	O
deals	O
with	O
source	O
and	O
target	O
phrases	O
of	O
different	O
lengths,	O
without	O
requiring	O
a	O
counter-intuitive	O
way	O
of	O
mapping	O
some	O
words	O
to	O
or	O
from	O
nowhere	O
([NULL])	O
(see,	O
e.g.,	O
Chapters	O
4	O
and	O
5	O
of	O
Koehn,	O
2010).	O

Here,	O
we	O
describe	O
briefly	O
the	O
underlying	O
framework,	O
called	O
RNN	B-MethodName
Encoder-Decoder,	I-MethodName
proposed	O
by	O
Cho	O
et	O
al.	O
(2014a)	O
and	O
Sutskever	O
et	O
al.	O
(2014)	O
upon	O
which	O
we	O
build	O
a	O
novel	O
architecture	O
that	O
learns	O
to	O
align	O
and	O
translate	O
simultaneously.	O
In	O
the	O
Encoder-Decoder	B-MethodName
framework,	O
an	O
encoder	O
reads	O
the	O
input	O
sentence,	O
a	O
sequence	O
of	O
vectors	O
x	O
=	O
(x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
Tx	O
),	O
into	O
a	O
vector	O
c.	O
2	O
The	O
most	O
common	O
approach	O
is	O
to	O
use	O
an	O
RNN	B-MethodName
such	O
that	O
h	O
t	O
=	O
f	O
(x	O
t	O
,	O
h	O
t	O
1	O
)	O
(1)	O
and	O
c	O
=	O
q	O
({h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
Tx	O
})	O
,	O
where	O
h	O
t	O
2	O
R	O
n	O
is	O
a	O
hidden	O
state	O
at	O
time	O
t,	O
and	O
c	O
is	O
a	O
vector	O
generated	O
from	O
the	O
sequence	O
of	O
the	O
hidden	O
states.	O
f	O
and	O
q	O
are	O
some	O
nonlinear	O
functions.	O
Sutskever	O
et	O
al.	O
(2014)	O
used	O
an	O
LSTM	O
as	O
f	O
and	O
q	O
({h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
T	O
})	O
=	O
h	O
T	O
,	O
for	O
instance.	O
The	O
decoder	O
is	O
often	O
trained	O
to	O
predict	O
the	O
next	O
word	O
y	O
t	O
0	O
given	O
the	O
context	O
vector	O
c	O
and	O
all	O
the	O
previously	O
predicted	O
words	O
{y	O
1	O
,	O
•	O
•	O
•	O
,	O
y	O
t	O
0	O
1	O
}.	O
In	O
other	O
words,	O
the	O
decoder	O
defines	O
a	O
probability	O
over	O
the	O
translation	O
y	O
by	O
decomposing	O
the	O
joint	O
probability	O
into	O
the	O
ordered	O
conditionals:	O
p(y)	O
=	O
T	O
Y	O
t=1	O
p(y	O
t	O
|	O
{y	O
1	O
,	O
•	O
•	O
•	O
,	O
y	O
t	O
1	O
}	O
,	O
c),(2)	O
where	O
y	O
=	O
y	O
1	O
,	O
•	O
•	O
•	O
,	O
y	O
Ty	O
.	O
With	O
an	O
RNN,	B-MethodName
each	O
conditional	O
probability	O
is	O
modeled	O
as	O
p(y	O
t	O
|	O
{y	O
1	O
,	O
•	O
•	O
•	O
,	O
y	O
t	O
1	O
}	O
,	O
c)	O
=	O
g(y	O
t	O
1	O
,	O
s	O
t	O
,	O
c),	O
(3	O
)	O
where	O
g	O
is	O
a	O
nonlinear,	O
potentially	O
multi-layered,	O
function	O
that	O
outputs	O
the	O
probability	O
of	O
y	O
t	O
,	O
and	O
s	O
t	O
is	O
the	O
hidden	O
state	O
of	O
the	O
RNN.	B-MethodName
It	O
should	O
be	O
noted	O
that	O
other	O
architectures	O
such	O
as	O
a	O
hybrid	O
of	O
an	O
RNN	B-MethodName
and	O
a	O
de-convolutional	O
neural	O
network	O
can	O
be	O
used	O
(Kalchbrenner	O
and	O
Blunsom,	O
2013).	O

Byte-Pair	B-MethodName
Encoding	I-MethodName
(BPE)	I-MethodName
(Sennrich	O
et	O
al.,	O
2016)	O
is	O
a	O
hybrid	O
between	O
character-and	O
word-level	O
representations	O
that	O
allows	O
handling	O
the	O
large	O
vocabularies	O
common	O
in	O
natural	O
language	O
corpora.	O
Instead	O
of	O
full	O
words,	O
BPE	O
relies	O
on	O
subwords	O
units,	O
which	O
are	O
extracted	O
by	O
performing	O
statistical	O
analysis	O
of	O
the	O
training	O
corpus.	O
BPE	O
vocabulary	O
sizes	O
typically	O
range	O
from	O
10K-100K	O
subword	O
units.	O
However,	O
unicode	O
characters	O
can	O
account	O
for	O
a	O
sizeable	O
portion	O
of	O
this	O
vocabulary	O
when	O
modeling	O
large	O
and	O
diverse	O
corpora,	O
such	O
as	O
the	O
ones	O
considered	O
in	O
this	O
work.	O
Radford	O
et	O
al.	O
(2019)	O
introduce	O
a	O
clever	O
implementation	O
of	O
BPE	O
that	O
uses	O
bytes	O
instead	O
of	O
unicode	O
characters	O
as	O
the	O
base	O
subword	O
units.	O
Using	O
bytes	O
makes	O
it	O
possible	O
to	O
learn	O
a	O
subword	O
vocabulary	O
of	O
a	O
modest	O
size	O
(50K	O
units)	O
that	O
can	O
still	O
encode	O
any	O
input	O
text	O
without	O
introducing	O
any	O
"unknown"	O
tokens.	O
8	O
Large	O
batch	O
training	O
can	O
improve	O
training	O
efficiency	O
even	O
without	O
large	O
scale	O
parallel	O
hardware	O
through	O
gradient	O
accumulation,	O
whereby	O
gradients	O
from	O
multiple	O
mini-batches	O
are	O
accumulated	O
locally	O
before	O
each	O
optimization	O
step.	O
This	O
functionality	O
is	O
supported	O
natively	O
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
The	O
original	O
BERT	B-MethodName
implementation	O
(Devlin	O
et	O
al.,	O
2019)	O
uses	O
a	O
character-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
of	O
size	O
30K,	B-HyperparameterValue
which	O
is	O
learned	O
after	O
preprocessing	O
the	O
input	O
with	O
heuristic	O
tokenization	O
rules.	O
Following	O
Radford	O
et	O
al.	O
(2019),	O
we	O
instead	O
consider	O
training	O
BERT	B-MethodName
with	O
a	O
larger	O
byte-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
containing	O
50K	B-HyperparameterValue
subword	I-HyperparameterValue
units,	I-HyperparameterValue
without	O
any	O
additional	O
preprocessing	O
or	O
tokenization	O
of	O
the	O
input.	O
This	O
adds	O
approximately	O
15M	O
and	O
20M	O
additional	O
parameters	O
for	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
respectively.	O
Early	O
experiments	O
revealed	O
only	O
slight	O
differences	O
between	O
these	O
encodings,	O
with	O
the	O
Radford	O
et	O
al.	O
(	O
2019)	O
BPE	B-MethodName
achieving	O
slightly	O
worse	O
end-task	O
performance	O
on	O
some	O
tasks.	O
Nevertheless,	O
we	O
believe	O
the	O
advantages	O
of	O
a	O
universal	O
encoding	O
scheme	O
outweighs	O
the	O
minor	O
degredation	O
in	O
performance	O
and	O
use	O
this	O
encoding	O
in	O
the	O
remainder	O
of	O
our	O
experiments.	O
A	O
more	O
detailed	O
comparison	O
of	O
these	O
encodings	O
is	O
left	O
to	O
future	O
work.	O

In	O
the	O
previous	O
section	O
we	O
propose	O
modifications	O
to	O
the	O
BERT	B-MethodName
pretraining	O
procedure	O
that	O
improve	O
end-task	O
performance.	O
We	O
now	O
aggregate	O
these	O
improvements	O
and	O
evaluate	O
their	O
combined	O
impact.	O
We	O
call	O
this	O
configuration	O
RoBERTa	B-MethodName
for	O
Robustly	B-MethodName
optimized	I-MethodName
BERT	I-MethodName
approach.	O
Specifically,	O
RoBERTa	B-MethodName
is	O
trained	O
with	O
dynamic	O
masking	O
(Section	O
4.1),	O
FULL-SENTENCES	O
without	O
NSP	O
loss	O
(Section	O
4.2),	O
large	O
mini-batches	O
(Section	O
4.3)	O
and	O
a	O
larger	O
byte-level	O
BPE	O
(Section	O
4.4).	O
Additionally,	O
we	O
investigate	O
two	O
other	O
important	O
factors	O
that	O
have	O
been	O
under-emphasized	O
in	O
previous	O
work:	O
(1)	O
the	O
data	O
used	O
for	O
pretraining,	O
and	O
(2)	O
the	O
number	O
of	O
training	O
passes	O
through	O
the	O
data.	O
For	O
example,	O
the	O
recently	O
proposed	O
XLNet	B-MethodName
architecture	O
(Yang	O
et	O
al.,	O
2019)	O
is	O
pretrained	O
using	O
nearly	O
10	O
times	O
more	O
data	O
than	O
the	O
original	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
It	O
is	O
also	O
trained	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
eight	O
times	O
larger	O
for	O
half	O
as	O
many	O
optimization	O
steps,	O
thus	O
seeing	O
four	O
times	O
as	O
many	O
sequences	O
in	O
pretraining	O
compared	O
to	O
BERT.	B-MethodName
To	O
help	O
disentangle	O
the	O
importance	O
of	O
these	O
factors	O
from	O
other	O
modeling	O
choices	O
(e.g.,	O
the	O
pretraining	O
objective),	O
we	O
begin	O
by	O
training	O
RoBERTa	B-MethodName
following	O
the	O
BERT	B-MethodName
LARGE	I-MethodName
architecture	O
(L	B-HyperparameterName
=	O
24,	B-HyperparameterValue
H	B-HyperparameterName
=	O
1024,	B-HyperparameterValue
A	B-HyperparameterName
=	O
16,	B-HyperparameterValue
355M	O
parameters).	O
We	O
pretrain	O
for	O
100K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
a	O
comparable	O
BOOK-CORPUS	B-DatasetName
plus	O
WIKIPEDIA	B-DatasetName
dataset	O
as	O
was	O
used	O
in	O
Yang	O
et	O
al.	O
(2019),	O
respectively.	O
Complete	O
results	O
on	O
all	O
GLUE	B-DatasetName
tasks	O
can	O
be	O
found	O
in	O
the	O
Appendix.	O
Devlin	O
et	O
al.	O
(2019).	O
We	O
pretrain	O
our	O
model	O
using	O
1024	O
V100	O
GPUs	O
for	O
approximately	O
one	O
day.	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
4.	O
When	O
controlling	O
for	O
training	O
data,	O
we	O
observe	O
that	O
RoBERTa	B-MethodName
provides	O
a	O
large	O
improvement	O
over	O
the	O
originally	O
reported	O
BERT	B-MethodName
LARGE	I-MethodName
results,	O
reaffirming	O
the	O
importance	O
of	O
the	O
design	O
choices	O
we	O
explored	O
in	O
Section	O
4.	O
Next,	O
we	O
combine	O
this	O
data	O
with	O
the	O
three	O
additional	O
datasets	O
described	O
in	O
Section	O
3.2.	O
We	O
train	O
RoBERTa	B-MethodName
over	O
the	O
combined	O
data	O
with	O
the	O
same	O
number	O
of	O
training	B-HyperparameterName
steps	I-HyperparameterName
as	O
before	O
(100K).	B-HyperparameterValue
In	O
total,	O
we	O
pretrain	O
over	O
160GB	O
of	O
text.	O
We	O
observe	O
further	O
improvements	O
in	O
performance	O
across	O
all	O
downstream	O
tasks,	O
validating	O
the	O
importance	O
of	O
data	O
size	O
and	O
diversity	O
in	O
pretraining.	O
9	O
Finally,	O
we	O
pretrain	O
RoBERTa	B-MethodName
for	O
significantly	O
longer,	O
increasing	O
the	O
number	O
of	O
pretraining	B-HyperparameterName
steps	I-HyperparameterName
from	O
100K	B-HyperparameterValue
to	I-HyperparameterValue
300K,	I-HyperparameterValue
and	O
then	O
further	O
to	O
500K.	B-HyperparameterValue
We	O
again	O
observe	O
significant	O
gains	O
in	O
downstream	O
task	O
performance,	O
and	O
the	O
300K	O
and	O
500K	O
step	O
models	O
outperform	O
XLNet	B-MethodName
LARGE	I-MethodName
across	O
most	O
tasks.	O
We	O
note	O
that	O
even	O
our	O
longest-trained	O
model	O
does	O
not	O
appear	O
to	O
overfit	O
our	O
data	O
and	O
would	O
likely	O
benefit	O
from	O
additional	O
training.	O
In	O
the	O
rest	O
of	O
the	O
paper,	O
we	O
evaluate	O
our	O
best	O
RoBERTa	B-MethodName
model	O
on	O
the	O
three	O
different	O
benchmarks:	O
GLUE,	B-DatasetName
SQuaD	B-DatasetName
and	O
RACE.	B-DatasetName
Specifically	O
we	O
consider	O
RoBERTa	B-MethodName
trained	O
for	O
500K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
all	O
five	O
of	O
the	O
datasets	O
introduced	O
in	O
Section	O
3.2.	O

For	O
GLUE	B-DatasetName
we	O
consider	O
two	O
finetuning	O
settings.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev)	O
we	O
finetune	O
RoBERTa	B-MethodName
separately	O
for	O
each	O
of	O
the	O
GLUE	B-DatasetName
tasks,	O
using	O
only	O
the	O
training	O
data	O
for	O
the	O
corresponding	O
task.	O
We	O
consider	O
a	O
limited	O
hyperparameter	O
sweep	O
for	O
each	O
task,	O
with	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
∈	O
{16,	B-HyperparameterValue
32}	I-HyperparameterValue
and	O
learning	B-HyperparameterName
rates	I-HyperparameterName
∈	O
{1e−5,	B-HyperparameterValue
2e−5,	I-HyperparameterValue
3e−5},	I-HyperparameterValue
with	O
a	O
linear	B-HyperparameterName
warmup	I-HyperparameterName
for	O
the	O
first	O
6%	B-HyperparameterValue
of	O
steps	O
followed	O
by	O
a	O
linear	B-HyperparameterName
decay	I-HyperparameterName
to	O
0.	O
We	O
finetune	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
perform	O
early	O
stopping	O
based	O
on	O
each	O
task's	O
evaluation	O
metric	O
on	O
the	O
dev	O
set.	O
The	O
rest	O
of	O
the	O
hyperparameters	O
remain	O
the	O
same	O
as	O
during	O
pretraining.	O
In	O
this	O
setting,	O
we	O
report	O
the	O
median	O
development	O
set	O
results	O
for	O
each	O
task	O
over	O
five	O
random	O
initializations,	O
without	O
model	O
ensembling.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
compare	O
RoBERTa	B-MethodName
to	O
other	O
approaches	O
on	O
the	O
test	O
set	O
via	O
the	O
GLUE	B-DatasetName
leaderboard.	O
While	O
many	O
submissions	O
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
depend	O
on	O
multitask	O
finetuning,	O
our	O
submission	O
depends	O
only	O
on	O
single-task	O
finetuning.	O
For	O
RTE,	B-DatasetName
STS	B-DatasetName
and	O
MRPC	B-DatasetName
we	O
found	O
it	O
helpful	O
to	O
finetune	O
starting	O
from	O
the	O
MNLI	O
single-task	O
model,	O
rather	O
than	O
the	O
baseline	O
pretrained	O
RoBERTa.	O
We	O
explore	O
a	O
slightly	O
wider	O
hyperparameter	O
space,	O
described	O
in	O
the	O
Appendix,	O
and	O
ensemble	O
between	O
5	O
and	O
7	O
models	O
per	O
task.	O
Task-specific	O
modifications	O
Two	O
of	O
the	O
GLUE	B-DatasetName
tasks	O
require	O
task-specific	O
finetuning	O
approaches	O
to	O
achieve	O
competitive	O
leaderboard	O
results.	O
QNLI:	B-DatasetName
Recent	O
submissions	O
on	O
the	O
GLUE	B-DatasetName
leaderboard	O
adopt	O
a	O
pairwise	O
ranking	O
formulation	O
for	O
the	O
QNLI	B-DatasetName
task,	O
in	O
which	O
candidate	O
answers	O
are	O
mined	O
from	O
the	O
training	O
set	O
and	O
compared	O
to	O
one	O
another,	O
and	O
a	O
single	O
(question,	O
candidate)	O
pair	O
is	O
classified	O
as	O
positive	O
(Liu	O
et	O
al.,	O
2019b,a;Yang	O
et	O
al.,	O
2019).	O
This	O
formulation	O
significantly	O
simplifies	O
the	O
task,	O
but	O
is	O
not	O
directly	O
comparable	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
Following	O
recent	O
work,	O
we	O
adopt	O
the	O
ranking	O
approach	O
for	O
our	O
test	O
submission,	O
but	O
for	O
direct	O
comparison	O
with	O
BERT	B-MethodName
we	O
report	O
development	O
set	O
results	O
based	O
on	O
a	O
pure	O
classification	O
approach.	O
WNLI:	B-DatasetName
We	O
found	O
the	O
provided	O
NLI-format	O
data	O
to	O
be	O
challenging	O
to	O
work	O
with.	O
Instead	O
we	O
use	O
the	O
reformatted	O
WNLI	B-DatasetName
data	O
from	O
Super-GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2019a),	O
which	O
indicates	O
the	O
span	O
of	O
the	O
query	O
pronoun	O
and	O
referent.	O
We	O
finetune	O
RoBERTa	B-MethodName
using	O
the	O
margin	O
ranking	O
loss	O
from	O
Kocijan	O
et	O
al.	O
(2019).	O
For	O
a	O
given	O
input	O
sentence,	O
we	O
use	O
spaCy	O
(Honnibal	O
and	O
Montani,	O
2017)	O
to	O
extract	O
additional	O
candidate	O
noun	O
phrases	O
from	O
the	O
sentence	O
and	O
finetune	O
our	O
model	O
so	O
that	O
it	O
assigns	O
higher	O
scores	O
to	O
positive	O
referent	O
phrases	O
than	O
for	O
any	O
of	O
the	O
generated	O
negative	O
candidate	O
phrases.	O
One	O
unfortunate	O
consequence	O
of	O
this	O
formulation	O
is	O
that	O
we	O
can	O
only	O
make	O
use	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
5.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev),	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
all	O
9	O
of	O
the	O
GLUE	O
task	O
development	O
sets.	O
Crucially,	O
RoBERTa	B-MethodName
uses	O
the	O
same	O
masked	O
language	O
modeling	O
pretraining	O
objective	O
and	O
architecture	O
as	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
yet	O
consistently	O
outperforms	O
both	O
BERT	B-MethodName
LARGE	I-MethodName
and	O
XLNet	B-MethodName
LARGE	I-MethodName
.	O
This	O
raises	O
questions	O
about	O
the	O
relative	O
importance	O
of	O
model	O
architecture	O
and	O
pretraining	O
objective,	O
compared	O
to	O
more	O
mundane	O
details	O
like	O
dataset	O
size	O
and	O
training	O
time	O
that	O
we	O
explore	O
in	O
this	O
work.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
and	O
achieve	O
state-of-the-art	O
results	O
on	O
4	O
out	O
of	O
9	O
tasks	O
and	O
the	O
highest	O
average	O
score	O
to	O
date.	O
This	O
is	O
especially	O
exciting	O
because	O
RoBERTa	B-MethodName
does	O
not	O
depend	O
on	O
multi-task	O
finetuning,	O
unlike	O
most	O
of	O
the	O
other	O
top	O
submissions.	O
We	O
expect	O
future	O
work	O
may	O
further	O
improve	O
these	O
results	O
by	O
incorporating	O
more	O
sophisticated	O
multi-task	O
finetuning	O
procedures.	O

We	O
adopt	O
a	O
much	O
simpler	O
approach	O
for	O
SQuAD	B-DatasetName
compared	O
to	O
past	O
work.	O
In	O
particular,	O
while	O
both	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
XLNet,	B-MethodName
while	O
we	O
use	O
the	O
same	O
learning	O
rate	O
for	O
all	O
layers.	O
For	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
we	O
follow	O
the	O
same	O
finetuning	O
procedure	O
as	O
Devlin	O
et	O
al.	O
(2019).	O
For	O
SQuAD	B-DatasetName
v2.0,	I-DatasetName
we	O
additionally	O
classify	O
whether	O
a	O
given	O
question	O
is	O
answerable;	O
we	O
train	O
this	O
classifier	O
jointly	O
with	O
the	O
span	O
predictor	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O

We	O
present	O
our	O
results	O
in	O
Table	O
6.	O
On	O
the	O
SQuAD	O
v1.1	O
development	O
set,	O
RoBERTa	B-MethodName
matches	O
the	O
state-of-the-art	O
set	O
by	O
XLNet.	B-MethodName
On	O
the	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
development	O
set,	O
RoBERTa	B-DatasetName
sets	O
a	O
new	O
state-of-the-art,	O
improving	O
over	O
XLNet	B-DatasetName
by	B-MetricValue
0.4	I-MetricValue
points	I-MetricValue
(EM)	B-MetricName
and	O
0.6	B-MetricValue
points	I-MetricValue
(F1).	B-MetricName
We	O
also	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
public	O
SQuAD	B-DatasetName
2.0	I-DatasetName
leaderboard	O
and	O
evaluate	O
its	O
performance	O
relative	O
to	O
other	O
systems.	O
Most	O
of	O
the	O
top	O
systems	O
build	O
upon	O
either	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
or	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
both	O
of	O
which	O
rely	O
on	O
additional	O
external	O
training	O
data.	O
In	O
contrast,	O
our	O
submission	O
does	O
not	O
use	O
any	O
additional	O
data.	O
Our	O
single	B-MethodName
RoBERTa	I-MethodName
model	O
outperforms	O
all	O
but	O
one	O
of	O
the	O
single	O
model	O
submissions,	O
and	O
is	O
the	O
top	O
scoring	O
system	O
among	O
those	O
that	O
do	O
not	O
rely	O
on	O
data	O
augmentation.	O

In	O
RACE,	B-DatasetName
systems	O
are	O
provided	O
with	O
a	O
passage	O
of	O
text,	O
an	O
associated	O
question,	O
and	O
four	O
candidate	O
answers.	O
Systems	O
are	O
required	O
to	O
classify	O
which	O
of	O
the	O
four	O
candidate	O
answers	O
is	O
correct.	O
We	O
modify	O
RoBERTa	B-MethodName
for	O
this	O
task	O
by	O
concate-	O
Yang	O
et	O
al.	O
(2019).	O
nating	O
each	O
candidate	O
answer	O
with	O
the	O
corresponding	O
question	O
and	O
passage.	O
We	O
then	O
encode	O
each	O
of	O
these	O
four	O
sequences	O
and	O
pass	O
the	O
resulting	O
[CLS]	O
representations	O
through	O
a	O
fully-connected	O
layer,	O
which	O
is	O
used	O
to	O
predict	O
the	O
correct	O
answer.	O
We	O
truncate	O
question-answer	O
pairs	O
that	O
are	O
longer	O
than	O
128	O
tokens	O
and,	O
if	O
needed,	O
the	O
passage	O
so	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Results	O
on	O
the	O
RACE	B-DatasetName
test	O
sets	O
are	O
presented	O
in	O
Table	O
7.	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
both	O
middle-school	O
and	O
high-school	O
settings.	O

Past	O
work	O
in	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
has	O
shown	O
that	O
training	O
with	O
very	O
large	O
mini-batches	B-HyperparameterName
can	O
both	O
improve	O
optimization	O
speed	O
and	O
end-task	O
performance	O
when	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
increased	O
appropriately	O
(Ott	O
et	O
al.,	O
2018).	O
Recent	O
work	O
has	O
shown	O
that	O
BERT	B-MethodName
is	O
also	O
amenable	O
to	O
large	O
batch	O
training	O
(You	O
et	O
al.,	O
2019	O

In	O
the	O
original	O
BERT	B-MethodName
pretraining	O
procedure,	O
the	O
model	O
observes	O
two	O
concatenated	O
document	O
segments,	O
which	O
are	O
either	O
sampled	O
contiguously	O
from	O
the	O
same	O
document	O
(with	O
p	B-MetricName
=	O
0.5)	B-MetricValue
or	O
from	O
distinct	O
documents.	O
In	O
addition	O
to	O
the	O
masked	O
language	O
modeling	O
objective,	O
the	O
model	O
is	O
trained	O
to	O
predict	O
whether	O
the	O
observed	O
document	O
segments	O
come	O
from	O
the	O
same	O
or	O
distinct	O
documents	O
via	O
an	O
auxiliary	O
Next	O
Sentence	O
Prediction	O
(NSP)	O
loss.	O
The	O
NSP	O
loss	O
was	O
hypothesized	O
to	O
be	O
an	O
important	O
factor	O
in	O
training	O
the	O
original	O
BERT	B-MethodName
model.	O
Devlin	O
et	O
al.	O
(2019)	O
observe	O
that	O
removing	O
NSP	O
hurts	O
performance,	O
with	O
significant	O
performance	O
degradation	O
on	O
QNLI,	B-DatasetName
MNLI,	B-DatasetName
and	O
SQuAD	B-DatasetName
1.1.	I-DatasetName
However,	O
some	O
recent	O
work	O
has	O
questioned	O
the	O
necessity	O
of	O
the	O
NSP	O
loss	O
(Lample	O
and	O
Conneau,	O
2019;Yang	O
et	O
al.,	O
2019;Joshi	O
et	O
al.,	O
2019).	O
To	O
better	O
understand	O
this	O
discrepancy,	O
we	O
compare	O
several	O
alternative	O
training	O
formats:	O
•	O
SEGMENT-PAIR+NSP:	O
This	O
follows	O
the	O
original	O
input	O
format	O
used	O
in	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
with	O
the	O
NSP	O
loss.	O
Each	O
input	O
has	O
a	O
pair	O
of	O
segments,	O
which	O
can	O
each	O
contain	O
multiple	O
natural	O
sentences,	O
but	O
the	O
total	O
combined	O
length	O
must	O
be	O
less	O
than	O
512	O
tokens.	O
•	O
SENTENCE-PAIR+NSP:	O
Each	O
input	O
contains	O
a	O
pair	O
of	O
natural	O
sentences,	O
either	O
sampled	O
from	O
a	O
contiguous	O
portion	O
of	O
one	O
document	O
or	O
from	O
separate	O
documents.	O
Since	O
these	O
inputs	O
are	O
significantly	O
shorter	O
than	O
512	O
tokens,	O
we	O
increase	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
so	O
that	O
the	O
total	O
number	O
of	O
tokens	O
remains	O
similar	O
to	O
SEGMENT-PAIR+NSP.	O
We	O
retain	O
the	O
NSP	O
loss.	O
•	O
FULL-SENTENCES:	O
Each	O
input	O
is	O
packed	O
with	O
full	O
sentences	O
sampled	O
contiguously	O
from	O
one	O
or	O
more	O
documents,	O
such	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Inputs	O
may	O
cross	O
document	O
boundaries.	O
When	O
we	O
reach	O
the	O
end	O
of	O
one	O
document,	O
we	O
begin	O
sampling	O
sentences	O
from	O
the	O
next	O
document	O
and	O
add	O
an	O
extra	O
separator	O
token	O
between	O
documents.	O
We	O
remove	O
the	O
NSP	O
loss.	O
•	O
DOC-SENTENCES:	O
Inputs	O
are	O
constructed	O
similarly	O
to	O
FULL-SENTENCES,	O
except	O
that	O
they	O
may	O
not	O
cross	O
document	O
boundaries.	O
Inputs	O
sampled	O
near	O
the	O
end	O
of	O
a	O
document	O
may	O
be	O
shorter	O
than	O
512	O
tokens,	O
so	O
we	O
dynamically	O
increase	O
the	O
batch	O
size	O
in	O
these	O
cases	O
to	O
achieve	O
a	O
similar	O
number	O
of	O
total	O
tokens	O
as	O
FULL-SENTENCES.	O
We	O
remove	O
the	O
NSP	O
loss.	O
Results	O
Table	O
2	O
shows	O
results	O
for	O
the	O
four	O
different	O
settings.	O
We	O
first	O
compare	O
the	O
original	O
SEGMENT-PAIR	O
input	O
format	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
the	O
SENTENCE-PAIR	O
format;	O
both	O
formats	O
retain	O
the	O
NSP	O
loss,	O
but	O
the	O
latter	O
uses	O
single	O
sentences.	O
We	O
find	O
that	O
using	O
individual	O
sentences	O
hurts	O
performance	O
on	O
downstream	O
tasks,	O
which	O
we	O
hypothesize	O
is	O
because	O
the	O
model	O
is	O
not	O
able	O
to	O
learn	O
long-range	O
dependencies.	O
We	O
next	O
compare	O
training	O
without	O
the	O
NSP	O
loss	O
and	O
training	O
with	O
blocks	O
of	O
text	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES).	O
We	O
find	O
that	O
this	O
setting	O
outperforms	O
the	O
originally	O
published	O
BERT	B-MethodName
BASE	I-MethodName
results	O
and	O
that	O
removing	O
the	O
NSP	O
loss	O
matches	O
or	O
slightly	O
improves	O
downstream	O
task	O
performance,	O
in	O
contrast	O
to	O
Devlin	O
et	O
al.	O
(2019).	O
It	O
is	O
possible	O
that	O
the	O
original	O
BERT	B-MethodName
implementation	O
may	O
only	O
have	O
removed	O
the	O
loss	O
term	O
while	O
still	O
retaining	O
the	O
SEGMENT-PAIR	O
input	O
format.	O
Finally	O
we	O
find	O
that	O
restricting	O
sequences	O
to	O
come	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES)	O
performs	O
slightly	O
better	O
than	O
packing	O
sequences	O
from	O
multiple	O
documents	O
(FULL-SENTENCES).	O
However,	O
because	O
the	O
DOC-SENTENCES	O
format	O
results	O
in	O
variable	O
batch	O
sizes,	O
we	O
use	O
FULL-SENTENCES	O
in	O
the	O
remainder	O
of	O
our	O
experiments	O
for	O
easier	O
comparison	O
with	O
related	O
work.	O

As	O
discussed	O
in	O
Section	O
2,	O
BERT	B-MethodName
relies	O
on	O
randomly	O
masking	O
and	O
predicting	O
tokens.	O
The	O
original	O
BERT	B-MethodName
implementation	O
performed	O
masking	O
once	O
during	O
data	O
preprocessing,	O
resulting	O
in	O
a	O
single	O
static	O
mask.	O
To	O
avoid	O
using	O
the	O
same	O
mask	O
for	O
each	O
training	O
instance	O
in	O
every	O
epoch,	O
training	O
data	O
was	O
duplicated	O
10	O
times	O
so	O
that	O
each	O
sequence	O
is	O
masked	O
in	O
10	O
different	O
ways	O
over	O
the	O
40	B-HyperparameterValue
epochs	B-HyperparameterName
of	O
training.	O
Thus,	O
each	O
training	O
sequence	O
was	O
seen	O
with	O
the	O
same	O
mask	O
four	O
times	O
during	O
training.	O
We	O
compare	O
this	O
strategy	O
with	O
dynamic	O
masking	O
where	O
we	O
generate	O
the	O
masking	O
pattern	O
every	O
time	O
we	O
feed	O
a	O
sequence	O
to	O
the	O
model.	O
This	O
becomes	O
crucial	O
when	O
pretraining	O
for	O
more	O
steps	O
or	O
with	O
larger	O
datasets.	O
Results	O
Table	O
1	O
compares	O
the	O
published	O
BERT	O
BASE	O
results	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
our	O
reimplementation	O
with	O
either	O
static	O
or	O
dynamic	O
masking.	O
We	O
find	O
that	O
our	O
reimplementation	O
with	O
static	O
masking	O
performs	O
similar	O
to	O
the	O
original	O
BERT	B-MethodName
model,	O
and	O
dynamic	O
masking	O
is	O
comparable	O
or	O
slightly	O
better	O
than	O
static	O
masking.	O
Given	O
these	O
results	O
and	O
the	O
additional	O
efficiency	O
benefits	O
of	O
dynamic	O
masking,	O
we	O
use	O
dynamic	O
masking	O
in	O
the	O
remainder	O
of	O
the	O
experiments.	O

Pretraining	O
methods	O
have	O
been	O
designed	O
with	O
different	O
training	O
objectives,	O
including	O
language	O
modeling	O
(Dai	O
and	O
Le,	O
2015;Peters	O
et	O
al.,	O
2018;Howard	O
and	O
Ruder,	O
2018),	O
machine	B-TaskName
translation	I-TaskName
(McCann	O
et	O
al.,	O
2017),	O
and	O
masked	O
language	O
modeling	O
(Devlin	O
et	O
al.,	O
2019;Lample	O
and	O
Conneau,	O
2019).	O
Many	O
recent	O
papers	O
have	O
used	O
a	O
basic	O
recipe	O
of	O
finetuning	O
models	O
for	O
each	O
end	O
task	O
(Howard	O
and	O
Ruder,	O
2018;Radford	O
et	O
al.,	O
2018),	O
and	O
pretraining	O
with	O
some	O
variant	O
of	O
a	O
masked	O
language	O
model	O
objective.	O
However,	O
newer	O
methods	O
have	O
improved	O
performance	O
by	O
multi-task	O
fine	O
tuning	O
(Dong	O
et	O
al.,	O
2019),	O
incorporating	O
entity	O
embeddings	O
(Sun	O
et	O
al.,	O
2019),	O
span	O
prediction	O
(Joshi	O
et	O
al.,	O
2019),	O
and	O
multiple	O
variants	O
of	O
autoregressive	O
pretraining	O
Chan	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019).	O
Performance	O
is	O
also	O
typically	O
improved	O
by	O
training	O
bigger	O
models	O
on	O
more	O
data	O
(Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Radford	O
et	O
al.,	O
2019).	O
Our	O
goal	O
was	O
to	O
replicate,	O
simplify,	O
and	O
better	O
tune	O
the	O
training	O
of	O
BERT,	B-MethodName
as	O
a	O
reference	O
point	O
for	O
better	O
understanding	O
the	O
relative	O
performance	O
of	O
all	O
of	O
these	O
methods.	O
We	O
carefully	O
evaluate	O
a	O
number	O
of	O
design	O
decisions	O
when	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
find	O
that	O
performance	O
can	O
be	O
substantially	O
improved	O
by	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches	O
over	O
more	O
data;	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
training	O
on	O
longer	O
sequences;	O
and	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
Our	O
improved	O
pretraining	O
procedure,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD,	B-DatasetName
without	O
multi-task	O
finetuning	O
for	O
GLUE	B-DatasetName
or	O
additional	O
data	O
for	O
SQuAD.	B-DatasetName
These	O
results	O
illustrate	O
the	O
importance	O
of	O
these	O
previously	O
overlooked	O
design	O
decisions	O
and	O
suggest	O
that	O
BERT's	B-MethodName
pretraining	O
objective	O
remains	O
competitive	O
with	O
recently	O
proposed	O
alternatives.	O
We	O
additionally	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
release	O
our	O
models	O
and	O
code	O
for	O
pretraining	O
and	O
finetuning	O
at:	O

This	O
section	O
explores	O
and	O
quantifies	O
which	O
choices	O
are	O
important	O
for	O
successfully	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
keep	O
the	O
model	O
architecture	O
fixed.	O
7	O
Specifically,	O
we	O
begin	O
by	O
training	O
BERT	B-MethodName
models	O
with	O
the	O
same	O
configuration	O
as	O
BERT	B-MethodName
BASE	I-MethodName
(L	B-HyperparameterName
=	O
12,	B-HyperparameterValue
H	B-HyperparameterName
=	O
768,	B-HyperparameterValue
A	B-HyperparameterName
=	O
12,	B-HyperparameterValue
110M	B-MetricValue
params).	I-MetricValue

Following	O
previous	O
work,	O
we	O
evaluate	O
our	O
pretrained	O
models	O
on	O
downstream	O
tasks	O
using	O
the	O
following	O
three	O
benchmarks.	O
GLUE	B-DatasetName
The	B-DatasetName
General	I-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	O
(Wang	O
et	O
al.,	O
2019b)	O
is	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
6	O
Tasks	O
are	O
framed	O
as	O
either	O
single-sentence	O
classification	O
or	O
sentence-pair	O
classification	O
tasks.	O
The	O
GLUE	B-DatasetName
organizers	O
provide	O
training	O
and	O
development	O
data	O
splits	O
as	O
well	O
as	O
a	O
submission	O
server	O
and	O
leaderboard	O
that	O
allows	O
participants	O
to	O
evaluate	O
and	O
compare	O
their	O
systems	O
on	O
private	O
held-out	O
test	O
data.	O
For	O
the	O
replication	O
study	O
in	O
Section	O
4,	O
we	O
report	O
results	O
on	O
the	O
development	O
sets	O
after	O
finetuning	O
the	O
pretrained	O
models	O
on	O
the	O
corresponding	O
singletask	O
training	O
data	O
(i.e.,	O
without	O
multi-task	O
training	O
or	O
ensembling).	O
Our	O
finetuning	O
procedure	O
follows	O
the	O
original	O
BERT	B-MethodName
paper	O
(Devlin	O
et	O
al.,	O
2019).	O
In	O
Section	O
5	O
we	O
additionally	O
report	O
test	O
set	O
results	O
obtained	O
from	O
the	O
public	O
leaderboard.	O
These	O
results	O
depend	O
on	O
a	O
several	O
task-specific	O
modifications,	O
which	O
we	O
describe	O
in	O
Section	O
5.1.	O
SQuAD	B-DatasetName
The	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(SQuAD)	I-DatasetName
provides	O
a	O
paragraph	O
of	O
context	O
and	O
a	O
question.	O
The	O
task	O
is	O
to	O
answer	O
the	O
question	O
by	O
extracting	O
the	O
relevant	O
span	O
from	O
the	O
context.	O
We	O
evaluate	O
on	O
two	O
versions	O
of	O
SQuAD:	B-DatasetName
V1.1	O
and	O
V2.0	O
(Rajpurkar	O
et	O
al.,	O
2016(Rajpurkar	O
et	O
al.,	O
,	O
2018.	O
In	O
V1.1	O
the	O
context	O
always	O
contains	O
an	O
answer,	O
whereas	O
in	O
V2.0	O
some	O
questions	O
are	O
not	O
answered	O
in	O
the	O
provided	O
context,	O
making	O
the	O
task	O
more	O
challenging.	O
For	O
SQuAD	B-DatasetName
V1.1	I-DatasetName
we	O
adopt	O
the	O
same	O
span	O
prediction	O
method	O
as	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
For	O
SQuAD	B-DatasetName
V2.0,	I-DatasetName
we	O
add	O
an	O
additional	O
binary	O
classifier	O
to	O
predict	O
whether	O
the	O
question	O
is	O
answerable,	O
which	O
we	O
train	O
jointly	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O
During	O
evaluation,	O
we	O
only	O
predict	O
span	O
indices	O
on	O
pairs	O
that	O
are	O
classified	O
as	O
answerable.	O
RACE	B-DatasetName
The	B-DatasetName
ReAding	I-DatasetName
Comprehension	I-DatasetName
from	I-DatasetName
Examinations	I-DatasetName
(RACE)	I-DatasetName
(Lai	O
et	O
al.,	O
2017)	O
task	O
is	O
a	O
large-scale	O
reading	O
comprehension	O
dataset	O
with	O
more	O
than	O
28,000	O
passages	O
and	O
nearly	O
100,000	O
questions.	O
The	O
dataset	O
is	O
collected	O
from	O
English	O
examinations	O
in	O
China,	O
which	O
are	O
designed	O
for	O
middle	O
and	O
high	O
school	O
students.	O
In	O
RACE,	B-DatasetName
each	O
passage	O
is	O
associated	O
with	O
multiple	O
questions.	O
For	O
every	O
question,	O
the	O
task	O
is	O
to	O
select	O
one	O
correct	O
answer	O
from	O
four	O
options.	O
RACE	B-DatasetName
has	O
significantly	O
longer	O
context	O
than	O
other	O
popular	O
reading	O
comprehension	O
datasets	O
and	O
the	O
proportion	O
of	O
questions	O
that	O
requires	O
reasoning	O
is	O
very	O
large.	O

BERT-style	B-MethodName
pretraining	O
crucially	O
relies	O
on	O
large	O
quantities	O
of	O
text.	O
demonstrate	O
that	O
increasing	O
data	O
size	O
can	O
result	O
in	O
improved	O
end-task	O
performance.	O
Several	O
efforts	O
have	O
trained	O
on	O
datasets	O
larger	O
and	O
more	O
diverse	O
than	O
the	O
original	O
BERT	B-MethodName
(Radford	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Zellers	O
et	O
al.,	O
2019).	O
Unfortunately,	O
not	O
all	O
of	O
the	O
additional	O
datasets	O
can	O
be	O
publicly	O
released.	O
For	O
our	O
study,	O
we	O
focus	O
on	O
gathering	O
as	O
much	O
data	O
as	O
possible	O
for	O
experimentation,	O
allowing	O
us	O
to	O
match	O
the	O
overall	O
quality	O
and	O
quantity	O
of	O
data	O
as	O
appropriate	O
for	O
each	O
comparison.	O
We	O
consider	O
five	O
English-language	O
corpora	O
of	O
varying	O
sizes	O
and	O
domains,	O
totaling	O
over	O
160GB	O
of	O
uncompressed	O
text.	O
We	O
use	O
the	O
following	O
text	O
corpora:	O
•	O
BOOKCORPUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA.	I-DatasetName
This	O
is	O
the	O
original	O
data	O
used	O
to	O
train	O
BERT.	B-MethodName
(16GB).	O
•	O
CC-NEWS,	B-DatasetName
which	O
we	O
collected	O
from	O
the	O
English	O
portion	O
of	O
the	O
CommonCrawl	B-DatasetName
News	I-DatasetName
dataset	I-DatasetName
(Nagel,	O
2016).	O
The	O
data	O
contains	O
63	B-HyperparameterName
million	I-HyperparameterName
English	I-HyperparameterName
news	I-HyperparameterName
articles	I-HyperparameterName
crawled	O
between	O
September	O
2016	O
and	O
February	O
2019.	O
(76GB	O
after	O
filtering).	O
4	O
•	O
OPENWEBTEXT	B-DatasetName
(Gokaslan	O
and	O
Cohen,	O
2019),	O
an	O
open-source	O
recreation	O
of	O
the	O
WebText	O
cor-pus	O
described	O
in	O
Radford	O
et	O
al.	O
(2019).	O
The	O
text	O
is	O
web	O
content	O
extracted	O
from	O
URLs	O
shared	O
on	O
Reddit	O
with	O
at	O
least	O
three	O
upvotes.	O
(38GB).	O
5	O
•	O
STORIES,	B-DatasetName
a	O
dataset	O
introduced	O
in	O
Trinh	O
and	O
Le	O
(2018)	O
containing	O
a	O
subset	O
of	O
CommonCrawl	B-DatasetName
data	O
filtered	O
to	O
match	O
the	O
story-like	O
style	O
of	O
Winograd	O
schemas.	O
(31GB).	O

We	O
reimplement	O
BERT	B-MethodName
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
We	O
primarily	O
follow	O
the	O
original	O
BERT	B-MethodName
optimization	O
hyperparameters,	O
given	O
in	O
Section	O
2,	O
except	O
for	O
the	O
peak	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
number	B-HyperparameterName
of	I-HyperparameterName
warmup	I-HyperparameterName
steps,	I-HyperparameterName
which	O
are	O
tuned	O
separately	O
for	O
each	O
setting.	O
We	O
additionally	O
found	O
training	O
to	O
be	O
very	O
sensitive	O
to	O
the	O
Adam	B-HyperparameterValue
epsilon	O
term,	O
and	O
in	O
some	O
cases	O
we	O
obtained	O
better	O
performance	O
or	O
improved	O
stability	O
after	O
tuning	O
it.	O
Similarly,	O
we	O
found	O
setting	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.98	B-HyperparameterValue
to	O
improve	O
stability	O
when	O
training	O
with	O
large	O
batch	O
sizes.	O
We	O
pretrain	O
with	O
sequences	O
of	O
at	O
most	O
T	O
=	O
512	O
tokens.	O
Unlike	O
Devlin	O
et	O
al.	O
(2019),	O
we	O
do	O
not	O
randomly	O
inject	O
short	O
sequences,	O
and	O
we	O
do	O
not	O
train	O
with	O
a	O
reduced	O
sequence	O
length	O
for	O
the	O
first	O
90%	B-MetricValue
of	O
updates.	O
We	O
train	O
only	O
with	O
full-length	O
sequences.	O
We	O
train	O
with	O
mixed	O
precision	O
floating	O
point	O
arithmetic	O
on	O
DGX-1	O
machines,	O
each	O
with	O
8	O
×	O
32GB	O
Nvidia	O
V100	O
GPUs	O
interconnected	O
by	O
Infiniband	O
(Micikevicius	O
et	O
al.,	O
2018).	O

In	O
this	O
section,	O
we	O
describe	O
the	O
experimental	O
setup	O
for	O
our	O
replication	O
study	O
of	O
BERT.	B-MethodName

BERT	B-MethodName
is	O
trained	O
on	O
a	O
combination	O
of	O
BOOKCOR-PUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA,	I-DatasetName
which	O
totals	O
16GB	O
of	O
uncompressed	O
text.	O
3	O

BERT	B-MethodName
is	O
optimized	O
with	O
Adam	B-HyperparameterValue
(Kingma	O
and	O
Ba,	O
2015)	O
using	O
the	O
following	O
parameters:	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9,	B-HyperparameterValue
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999,	B-HyperparameterValue
ǫ	B-HyperparameterName
=	O
1e-6	B-HyperparameterValue
and	O
L	B-HyperparameterName
2	I-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01.	B-HyperparameterValue
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
warmed	O
up	O
over	O
the	O
first	O
10,000	O
steps	O
to	O
a	O
peak	O
value	O
of	O
1e-4,	B-HyperparameterValue
and	O
then	O
linearly	O
decayed.	O
BERT	B-MethodName
trains	O
with	O
a	O
dropout	B-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers	O
and	O
attention	O
weights,	O
and	O
a	O
GELU	B-HyperparameterValue
activation	B-HyperparameterName
function	I-HyperparameterName
(Hendrycks	O
and	O
Gimpel,	O
2016).	O
Models	O
are	O
pretrained	O
for	O
S	B-HyperparameterName
=	O
1,000,000	B-HyperparameterValue
updates,	O
with	O
minibatches	B-HyperparameterName
containing	O
B	O
=	O
256	B-HyperparameterValue
sequences	O
of	O
maximum	O
length	O
T	O
=	O
512	O
tokens.	O

During	O
pretraining,	O
BERT	B-MethodName
uses	O
two	O
objectives:	O
masked	O
language	O
modeling	O
and	O
next	B-TaskName
sentence	I-TaskName
prediction.	I-TaskName
Masked	O
Language	O
Model	O
(MLM)	O
A	O
random	O
sample	O
of	O
the	O
tokens	O
in	O
the	O
input	O
sequence	O
is	O
selected	O
and	O
replaced	O
with	O
the	O
special	O
token	O
[MASK	O
].	O
The	O
MLM	O
objective	O
is	O
a	O
cross-entropy	B-MetricName
loss	O
on	O
predicting	O
the	O
masked	O
tokens.	O
BERT	B-MethodName
uniformly	O
selects	O
15%	B-MetricValue
of	O
the	O
input	O
tokens	O
for	O
possible	O
replacement.	O
Of	O
the	O
selected	O
tokens,	O
80%	B-MetricValue
are	O
replaced	O
with	O
[MASK	O
],	O
10%	B-MetricValue
are	O
left	O
unchanged,	O
and	O
10%	B-MetricValue
are	O
replaced	O
by	O
a	O
randomly	O
selected	O
vocabulary	O
token.	O
In	O
the	O
original	O
implementation,	O
random	O
masking	O
and	O
replacement	O
is	O
performed	O
once	O
in	O
the	O
beginning	O
and	O
saved	O
for	O
the	O
duration	O
of	O
training,	O
although	O
in	O
practice,	O
data	O
is	O
duplicated	O
so	O
the	O
mask	O
is	O
not	O
always	O
the	O
same	O
for	O
every	O
training	O
sentence	O
(see	O
Section	O
4.1).	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
(NSP)	O
NSP	B-TaskName
is	O
a	O
binary	O
classification	O
loss	O
for	O
predicting	O
whether	O
two	O
segments	O
follow	O
each	O
other	O
in	O
the	O
original	O
text.	O
Positive	O
examples	O
are	O
created	O
by	O
taking	O
consecutive	O
sentences	O
from	O
the	O
text	O
corpus.	O
Negative	O
examples	O
are	O
created	O
by	O
pairing	O
segments	O
from	O
different	O
documents.	O
Positive	O
and	O
negative	O
examples	O
are	O
sampled	O
with	O
equal	O
probability.	O
The	O
NSP	B-TaskName
objective	O
was	O
designed	O
to	O
improve	O
performance	O
on	O
downstream	O
tasks,	O
such	O
as	O
Natural	O
Language	O
Inference	O
(Bowman	O
et	O
al.,	O
2015),	O
which	O
require	O
reasoning	O
about	O
the	O
relationships	O
between	O
pairs	O
of	O
sentences.	O

BERT	B-MethodName
uses	O
the	O
now	O
ubiquitous	O
transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017),	O
which	O
we	O
will	O
not	O
review	O
in	O
detail.	O
We	O
use	O
a	O
transformer	O
architecture	O
with	O
L	O
layers.	O
Each	O
block	O
uses	O
A	O
self-attention	O
heads	O
and	O
hidden	O
dimension	O
H.	O

BERT	B-MethodName
takes	O
as	O
input	O
a	O
concatenation	O
of	O
two	O
segments	O
(sequences	O
of	O
tokens),	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
and	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
.	O
Segments	O
usually	O
consist	O
of	O
more	O
than	O
one	O
natural	O
sentence.	O
The	O
two	O
segments	O
are	O
presented	O
as	O
a	O
single	O
input	O
sequence	O
to	O
BERT	B-MethodName
with	O
special	O
tokens	O
delimiting	O
them:	O
[CLS	O
],	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
,	O
[SEP	O
],	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
,	O
[EOS	O
].	O
M	O
and	O
N	O
are	O
constrained	O
such	O
that	O
M	O
+	O
N	O
<	O
T	O
,	O
where	O
T	O
is	O
a	O
parameter	O
that	O
controls	O
the	O
maximum	O
sequence	O
length	O
during	O
training.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
a	O
large	O
unlabeled	O
text	O
corpus	O
and	O
subsequently	O
finetuned	O
using	O
end-task	O
labeled	O
data.	O

In	O
this	O
section,	O
we	O
give	O
a	O
brief	O
overview	O
of	O
the	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
pretraining	O
approach	O
and	O
some	O
of	O
the	O
training	O
choices	O
that	O
we	O
will	O
examine	O
experimentally	O
in	O
the	O
following	O
section.	O

Self-training	O
methods	O
such	O
as	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018),	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
XLM	B-MethodName
(Lample	O
and	O
Conneau,	O
2019),	O
and	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
have	O
brought	O
significant	O
performance	O
gains,	O
but	O
it	O
can	O
be	O
challenging	O
to	O
determine	O
which	O
aspects	O
of	O
the	O
methods	O
contribute	O
the	O
most.	O
Training	O
is	O
computationally	O
expensive,	O
limiting	O
the	O
amount	O
of	O
tuning	O
that	O
can	O
be	O
done,	O
and	O
is	O
often	O
done	O
with	O
private	O
training	O
data	O
of	O
varying	O
sizes,	O
limiting	O
our	O
ability	O
to	O
measure	O
the	O
effects	O
of	O
the	O
modeling	O
advances.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019),	O
which	O
includes	O
a	O
careful	O
evaluation	O
of	O
the	O
effects	O
of	O
hyperparmeter	O
tuning	O
and	O
training	O
set	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained	O
and	O
propose	O
an	O
improved	O
recipe	O
for	O
training	O
BERT	B-MethodName
models,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
that	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
all	O
of	O
the	O
post-BERT	O
methods.	O
Our	O
modifications	O
are	O
simple,	O
they	O
include:	O
(1)	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches,	O
over	O
more	O
data;	O
(2)	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
(3)	O
training	O
on	O
longer	O
sequences;	O
and	O
(4)	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
We	O
also	O
collect	O
a	O
large	O
new	O
dataset	O
(CC-NEWS)	B-DatasetName
of	O
comparable	O
size	O
to	O
other	O
privately	O
used	O
datasets,	O
to	O
better	O
control	O
for	O
training	O
set	O
size	O
effects.	O
When	O
controlling	O
for	O
training	O
data,	O
our	O
improved	O
training	O
procedure	O
improves	O
upon	O
the	O
published	O
BERT	B-MethodName
results	O
on	O
both	O
GLUE	B-DatasetName
and	O
SQuAD.	B-DatasetName
When	O
trained	O
for	O
longer	O
over	O
additional	O
data,	O
our	O
model	O
achieves	O
a	O
score	O
of	O
88.5	B-MetricValue
on	O
the	O
public	O
GLUE	B-DatasetName
leaderboard,	O
matching	O
the	O
88.4	B-MetricValue
reported	O
by	O
Yang	O
et	O
al.	O
(2019).	O
Our	O
model	O
establishes	O
a	O
new	O
state-of-the-art	O
on	O
4/9	O
of	O
the	O
GLUE	B-DatasetName
tasks:	O
MNLI,	B-DatasetName
QNLI,	B-DatasetName
RTE	B-DatasetName
and	O
STS-B.	B-DatasetName
We	O
also	O
match	O
state-of-the-art	O
results	O
on	O
SQuAD	B-DatasetName
and	O
RACE.	B-DatasetName
Overall,	O
we	O
re-establish	O
that	O
BERT's	B-MethodName
masked	O
language	O
model	O
training	O
objective	O
is	O
competitive	O
with	O
other	O
recently	O
proposed	O
training	O
objectives	O
such	O
as	O
perturbed	O
autoregressive	O
language	O
modeling	O
(Yang	O
et	O
al.,	O
2019).	O
2	O
In	O
summary,	O
the	O
contributions	O
of	O
this	O
paper	O
are:	O
(1)	O
We	O
present	O
a	O
set	O
of	O
important	O
BERT	B-MethodName
design	O
choices	O
and	O
training	O
strategies	O
and	O
introduce	O
alternatives	O
that	O
lead	O
to	O
better	O
downstream	O
task	O
performance;	O
(2)	O
We	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
confirm	O
that	O
using	O
more	O
data	O
for	O
pretraining	O
further	O
improves	O
performance	O
on	O
downstream	O
tasks;	O
(3)	O
Our	O
training	O
improvements	O
show	O
that	O
masked	O
language	O
model	O
pretraining,	O
under	O
the	O
right	O
design	O
choices,	O
is	O
competitive	O
with	O
all	O
other	O
recently	O
published	O
methods.	O
We	O
release	O
our	O
model,	O
pretraining	O
and	O
fine-tuning	O
code	O
implemented	O
in	O
PyTorch	O
(Paszke	O
et	O
al.,	O
2017).	O

Language	O
model	O
pretraining	O
has	O
led	O
to	O
significant	O
performance	O
gains	O
but	O
careful	O
comparison	O
between	O
different	O
approaches	O
is	O
challenging.	O
Training	O
is	O
computationally	O
expensive,	O
often	O
done	O
on	O
private	O
datasets	O
of	O
different	O
sizes,	O
and,	O
as	O
we	O
will	O
show,	O
hyperparameter	O
choices	O
have	O
significant	O
impact	O
on	O
the	O
final	O
results.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019)	O
that	O
carefully	O
measures	O
the	O
impact	O
of	O
many	O
key	O
hyperparameters	O
and	O
training	O
data	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained,	O
and	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
every	O
model	O
published	O
after	O
it.	O
Our	O
best	O
model	O
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD.	B-DatasetName
These	O
results	O
highlight	O
the	O
importance	O
of	O
previously	O
overlooked	O
design	O
choices,	O
and	O
raise	O
questions	O
about	O
the	O
source	O
of	O
recently	O
reported	O
improvements.	O
We	O
release	O
our	O
models	O
and	O
code.	O
1	O

In	O
Table	O
8	O
we	O
present	O
the	O
full	O
set	O
of	O
development	O
set	O
results	O
for	O
RoBERTa.	B-MethodName
We	O
present	O
results	O
for	O
a	O
LARGE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
as	O
well	O
as	O
a	O
BASE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
BASE	I-MethodName
.	O

Appendix	O
for	O
"RoBERTa:	B-MethodName
A	O
Robustly	O
Optimized	O
BERT	O
Pretraining	O
Approach"	O

According	O
to	O
the	O
comparison	O
above,	O
AR	O
language	O
modeling	O
and	O
BERT	B-MethodName
possess	O
their	O
unique	O
advantages	O
over	O
the	O
other.	O
A	O
natural	O
question	O
to	O
ask	O
is	O
whether	O
there	O
exists	O
a	O
pretraining	O
objective	O
that	O
brings	O
the	O
advantages	O
of	O
both	O
while	O
avoiding	O
their	O
weaknesses.	O
Borrowing	O
ideas	O
from	O
orderless	O
NADE	O
,	O
we	O
propose	O
the	O
permutation	B-TaskName
language	I-TaskName
modeling	I-TaskName
objective	O
that	O
not	O
only	O
retains	O
the	O
benefits	O
of	O
AR	O
models	O
but	O
also	O
allows	O
models	O
to	O
capture	O
bidirectional	O
contexts.	O
Specifically,	O
for	O
a	O
sequence	O
x	O
of	O
length	O
T	O
,	O
there	O
are	O
T	O
!	O
different	O
orders	O
to	O
perform	O
a	O
valid	O
autoregressive	O
factorization.	O
Intuitively,	O
if	O
model	O
parameters	O
are	O
shared	O
across	O
all	O
factorization	O
orders,	O
in	O
expectation,	O
the	O
model	O
will	O
learn	O
to	O
gather	O
information	O
from	O
all	O
positions	O
on	O
both	O
sides.	O
To	O
formalize	O
the	O
idea,	O
let	O
Z	O
T	O
be	O
the	O
set	O
of	O
all	O
possible	O
permutations	O
of	O
the	O
length-T	O
index	O
sequence	O
[1,	O
2,	O
.	O
.	O
.	O
,	O
T	O
].	O
We	O
use	O
z	O
t	O
and	O
z	O
<t	O
to	O
denote	O
the	O
t-th	O
element	O
and	O
the	O
first	O
t−1	O
elements	O
of	O
a	O
permutation	O
z	O
∈	O
Z	O
T	O
.	O
Then,	O
our	O
proposed	O
permutation	B-TaskName
language	I-TaskName
modeling	I-TaskName
objective	O
can	O
be	O
expressed	O
as	O
follows:	O
max	O
θ	O
E	O
z∼Z	O
T	O
T	O
t=1	O
log	O
p	O
θ	O
(x	O
zt	O
|	O
x	O
z<t	O
)	O
.(3)	O
Essentially,	O
for	O
a	O
text	O
sequence	O
x,	O
we	O
sample	O
a	O
factorization	O
order	O
z	O
at	O
a	O
time	O
and	O
decompose	O
the	O
likelihood	O
p	O
θ	O
(x)	O
according	O
to	O
factorization	O
order.	O
Since	O
the	O
same	O
model	O
parameter	O
θ	O
is	O
shared	O
across	O
all	O
factorization	O
orders	O
during	O
training,	O
in	O
expectation,	O
x	O
t	O
has	O
seen	O
every	O
possible	O
element	O
x	O
i	O
=	O
x	O
t	O
in	O
the	O
sequence,	O
hence	O
being	O
able	O
to	O
capture	O
the	O
bidirectional	O
context.	O
Moreover,	O
as	O
this	O
objective	O
fits	O
into	O
the	O
AR	O
framework,	O
it	O
naturally	O
avoids	O
the	O
independence	O
assumption	O
and	O
the	O
pretrain-finetune	O
discrepancy	O
discussed	O
in	O
Section	O
2.1.	O

The	O
hyperparameters	O
used	O
for	O
finetuning	O
XLNet	B-MethodName
on	O
various	O
tasks	O
are	O
shown	O
in	O
Table	O
8.	O
"Layer-wise	B-HyperparameterName
decay"	I-HyperparameterName
means	O
exponentially	O
decaying	O
the	O
learning	O
rates	O
of	O
individual	O
layers	O
in	O
a	O
top-down	O
manner.	O
For	O
example,	O
suppose	O
the	O
24-th	O
layer	O
uses	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
l,	B-HyperparameterName
and	O
the	O
Layer-wise	B-HyperparameterName
decay	I-HyperparameterName
rate	I-HyperparameterName
is	O
α,	B-HyperparameterName
then	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
layer	O
m	O
is	O
lα	B-HyperparameterName
24−m	O
.	O
A.5	O
Discussion	O
and	O
Analysis	O

To	O
prove	O
a	O
general	O
point	O
beyond	O
one	O
example,	O
we	O
now	O
turn	O
to	O
more	O
formal	O
expressions.	O
Inspired	O
by	O
previous	O
work	O
,	O
given	O
a	O
sequence	O
x	O
=	O
[x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
T	O
],	O
we	O
define	O
a	O
set	O
of	O
target-context	O
pairs	O
of	O
interest,	O
I	O
=	O
{(x,	O
U)},	O
where	O
U	O
is	O
a	O
set	O
of	O
tokens	O
in	O
x	O
that	O
form	O
a	O
context	O
of	O
x.	O
Intuitively,	O
we	O
want	O
the	O
model	O
to	O
learn	O
the	O
dependency	O
of	O
x	O
on	O
U	O
through	O
a	O
pretraining	O
loss	O
term	O
log	O
p(x	O
|	O
U).	O
For	O
example,	O
given	O
the	O
above	O
sentence,	O
the	O
pairs	O
of	O
interest	O
I	O
could	O
be	O
instantiated	O
as:	O
I	O
=	O
x	O
=	O
York,	O
U	O
=	O
{New}	O
,	O
x	O
=	O
York,	O
U	O
=	O
{city}	O
,	O
x	O
=	O
York,	O
U	O
=	O
{New,	O
city}	O
,	O
•	O
•	O
•	O
.	O
Note	O
that	O
I	O
is	O
merely	O
a	O
virtual	O
notion	O
without	O
unique	O
ground	O
truth,	O
and	O
our	O
analysis	O
will	O
hold	O
regardless	O
of	O
how	O
I	O
is	O
instantiated.	O
Given	O
the	O
definition,	O
let's	O
consider	O
two	O
cases:	O
•	O
If	O
U	O
⊆	O
N	O
,	O
the	O
dependency	O
(x,	O
U)	O
is	O
covered	O
by	O
both	O
BERT	B-MethodName
and	O
XLNet.	B-MethodName
•	O
If	O
U	O
⊆	O
N	O
∪	O
T	O
<x	O
and	O
U	O
∩	O
T	O
<x	O
=	O
∅,	O
the	O
dependency	O
can	O
only	O
be	O
covered	O
by	O
XLNet	B-MethodName
but	O
not	O
BERT.	B-MethodName
As	O
a	O
result,	O
XLNet	B-MethodName
is	O
able	O
to	O
cover	O
more	O
dependencies	O
than	O
BERT.	B-MethodName
In	O
other	O
words,	O
the	O
XLNet	B-MethodName
objective	O
contains	O
more	O
effective	O
training	O
signals,	O
which	O
empirically	O
leads	O
to	O
better	O
performance	O
in	O
Section	O
3.	O

In	O
this	O
section,	O
we	O
provide	O
a	O
detailed	O
visualization	O
of	O
the	O
proposed	O
permutation	O
language	O
modeling	O
objective,	O
including	O
the	O
mechanism	O
of	O
reusing	O
memory	O
(aka	O
the	O
recurrence	O
mechanism),	O
how	O
we	O
use	O
attention	O
masks	O
to	O
permute	O
the	O
factorization	O
order,	O
and	O
the	O
difference	O
of	O
the	O
two	O
attention	O
streams.	O
As	O
shown	O
in	O
Figure	O
5	O
and	O
6,	O
given	O
the	O
current	O
position	O
z	O
t	O
,	O
the	O
attention	O
mask	O
is	O
decided	O
by	O
the	O
permutation	O
(or	O
factorization	O
order)	O
z	O
such	O
that	O
only	O
tokens	O
the	O
occur	O
before	O
z	O
t	O
in	O
the	O
permutation	O
can	O
be	O
attended;	O
i.e.,	O
positions	O
z	O
i	O
with	O
i	O
<	O
t.	O
Moreover,	O
comparing	O
Figure	O
5	O
and	O
6,	O
we	O
can	O
see	O
how	O
the	O
query	O
stream	O
and	O
the	O
content	O
stream	O
work	O
differently	O
with	O
a	O
specific	O
permutation	O
through	O
attention	O
masks.	O
The	O
main	O
difference	O
is	O
that	O
the	O
query	O
stream	O
cannot	O
do	O
self-attention	O
and	O
does	O
not	O
have	O
access	O
to	O
the	O
token	O
at	O
the	O
position,	O
while	O
the	O
content	O
stream	O
performs	O
normal	O
self-attention.	O
mem	O
(+)	O
mem	O
(%)	O
x	O
mem	O
(+)	O
mem	O
(%)	O
x	O
mem	O
(+)	O
mem	O
(%)	O
x	O
mem	O
(+)	O
mem	O
(%)	O
x	O
Note	O
that	O
if	O
we	O
ignore	O
the	O
query	O
representation,	O
the	O
computation	O
in	O
this	O
figure	O
is	O
simply	O
the	O
standard	O
self-attention,	O
though	O
with	O
a	O
particular	O
attention	O
mask.	O
mem	O
(+)	O
mem	O
(%)	O
x	O
w	O
w	O
w	O
w	O
mem	O
(+)	O
mem	O
(%)	O
x	O
mem	O
(+)	O
mem	O
(%)	O
x	O
mem	O
(+)	O
mem	O
(%)	O
x	O

Factorization	O
order:	O
3	O
à	O
2	O
à	O
4	O
à	O
1	O
mem	O
(+)	O
mem	O
(+)	O
x	O
"	O
x	O
#	O
x	O
$	O
x	O
%	O
h	O
"	O
Factorization	O
order:	O
4	O
à	O
3	O
à	O
1	O
à	O
2	O
mem	O
(+)	O
mem	O
(#)	O
mem	O
(#)	O
mem	O
(#)	O
mem	O
(+)	O
x	O
%	O
x	O
%	O
x	O
%	O
x	O
%	O
Figure	O
4:	O
Illustration	O
of	O
the	O
permutation	O
language	O
modeling	O
objective	O
for	O
predicting	O
x	O
3	O
given	O
the	O
same	O
input	O
sequence	O
x	O
but	O
with	O
different	O
factorization	O
orders.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Qizhe	O
Xie	O
and	O
Adams	O
Wei	O
Yu	O
for	O
providing	O
useful	O
feedback	O
on	O
the	O
project,	O
Jamie	O
Callan	O
for	O
providing	O
the	O
ClueWeb	O
dataset,	O
Youlong	O
Cheng,	O
Yanping	O
Huang	O
and	O
Shibo	O
Wang	O
for	O
providing	O
ideas	O
to	O
improve	O
our	O
TPU	O
implementation,	O
Chenyan	O
Xiong	O
and	O
Zhuyun	O
Dai	O
for	O
clarifying	O
the	O
setting	O
of	O
the	O
document	O
ranking	O
task.	O
ZY	O
and	O
RS	O
were	O
supported	O
by	O
the	O
Office	O
of	O
Naval	O
Research	O
grant	O
N000141812861,	O
the	O
National	O
Science	O
Foundation	O
(NSF)	O
grant	O
IIS1763562,	O
the	O
Nvidia	O
fellowship,	O
and	O
the	O
Siebel	O
scholarship.	O
ZD	O
and	O
YY	O
were	O
supported	O
in	O
part	O
by	O
NSF	O
under	O
the	O
grant	O
IIS-1546329	O
and	O
by	O
the	O
DOE-Office	O
of	O
Science	O
under	O
the	O
grant	O
ASCR	O
#KJ040201.	O

We	O
compare	O
the	O
attention	O
pattern	O
of	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
without	O
finetuning.	O
Firstly,	O
we	O
found	O
4	O
typical	O
patterns	O
shared	O
by	O
both,	O
as	O
shown	O
in	O
Fig.	O
2.	O
More	O
interestingly,	O
in	O
Fig.	O
3,	O
we	O
present	O
3	O
patterns	O
that	O
only	O
appear	O
in	O
XLNet	B-MethodName
but	O
not	O
BERT:	B-MethodName
(a)	O
The	O
self-exclusion	O
pattern	O
attends	O
to	O
all	O
other	O
tokens	O
but	O
itself,	O
probably	O
offering	O
a	O
fast	O
way	O
to	O
gather	O
global	O
information;	O
(b)	O
The	O
relative-stride	O
pattern	O
attends	O
to	O
positions	O
every	O
a	O
few	O
stride	O
apart	O
relative	O
to	O
the	O
query	O
position;	O
(c)	O
The	O
one-side	O
masked	O
pattern	O
is	O
very	O
similar	O
to	O
the	O
lower-left	O
part	O
of	O
Fig.	O
1-(d),	O
with	O
the	O
upper-right	O
triangle	O
masked	O
out.	O
It	O
seems	O
that	O
the	O
model	O
learns	O
not	O
to	O
attend	O
the	O
relative	O
right	O
half.	O
Note	O
that	O
all	O
these	O
three	O
unique	O
patterns	O
involve	O
the	O
relative	O
positions	O
rather	O
than	O
absolute	O
ones,	O
and	O
hence	O
are	O
likely	O
enabled	O
by	O
the	O
"relative	O
attention"	O
mechanism	O
in	O
XLNet.	B-MethodName
We	O
conjecture	O
these	O
unique	O
patterns	O
contribute	O
to	O
the	O
performance	O
advantage	O
of	O
XLNet.	B-MethodName
On	O
the	O
other	O
hand,	O
the	O
proposed	O
permutation	B-TaskName
LM	I-TaskName
objective	O
mostly	O
contributes	O
to	O
a	O
better	O
data	O
efficiency,	O
whose	O
effects	O
may	O
not	O
be	O
obvious	O
from	O
qualitative	O
visualization.	O

With	O
a	O
deep	O
root	O
in	O
density	O
estimation	O
4	O
,	O
language	O
modeling	O
has	O
been	O
a	O
rapidly-developing	O
research	O
area	O
.	O
However,	O
there	O
has	O
been	O
a	O
gap	O
between	O
language	O
modeling	O
and	O
pretraining	O
due	O
to	O
the	O
lack	O
of	O
the	O
capability	O
of	O
bidirectional	O
context	O
modeling,	O
as	O
analyzed	O
in	O
Section	O
A.5.2.	O
It	O
has	O
even	O
been	O
challenged	O
by	O
some	O
machine	O
learning	O
practitioners	O
whether	O
language	O
modeling	O
is	O
a	O
meaningful	O
pursuit	O
if	O
it	O
does	O
not	O
directly	O
improve	O
downstream	O
tasks	O
5	O
.	O
XLNet	B-MethodName
generalizes	O
language	O
modeling	O
and	O
bridges	O
such	O
a	O
gap.	O
As	O
a	O
result,	O
it	O
further	O
"justifies"	O
language	O
modeling	O
research.	O
Moreover,	O
it	O
becomes	O
possible	O
to	O
leverage	O
the	O
rapid	O
progress	O
of	O
language	O
modeling	O
research	O
for	O
pretraining.	O
As	O
an	O
example,	O
we	O
integrate	O
Transformer-XL	O
into	O
XLNet	B-MethodName
to	O
demonstrate	O
the	O
usefulness	O
of	O
the	O
latest	O
language	O
modeling	O
progress.	O

Borrowing	O
examples	O
and	O
notations	O
from	O
Section	O
A.5.1,	O
a	O
standard	O
AR	O
language	O
model	O
like	O
GPT	O
is	O
only	O
able	O
to	O
cover	O
the	O
dependency	O
(x	O
=	O
York,	O
U	O
=	O
{New})	O
but	O
not	O
(x	O
=	O
New,	O
U	O
=	O
{York}).	O
XLNet,	B-MethodName
on	O
the	O
other	O
hand,	O
is	O
able	O
to	O
cover	O
both	O
in	O
expectation	O
over	O
all	O
factorization	O
orders.	O
Such	O
a	O
limitation	O
of	O
AR	O
language	O
modeling	O
can	O
be	O
critical	O
in	O
real-world	O
applications.	O
For	O
example,	O
consider	O
a	O
span	O
extraction	O
question	O
answering	O
task	O
with	O
the	O
context	O
"Thom	O
Yorke	O
is	O
the	O
singer	O
of	O
Radiohead"	O
and	O
the	O
question	O
"Who	O
is	O
the	O
singer	O
of	O
Radiohead".	O
The	O
representations	O
of	O
"Thom	O
Yorke"	O
are	O
not	O
dependent	O
on	O
"Radiohead"	O
with	O
AR	O
language	O
modeling	O
and	O
thus	O
they	O
will	O
not	O
be	O
chosen	O
as	O
the	O
answer	O
by	O
the	O
standard	O
approach	O
that	O
employs	O
softmax	O
over	O
all	O
token	O
representations.	O
More	O
formally,	O
consider	O
a	O
context-target	O
pair	O
(x,	O
U):	O
•	O
If	O
U	O
⊆	O
T	O
<x	O
,	O
where	O
T	O
<x	O
denotes	O
the	O
tokens	O
prior	O
to	O
x	O
in	O
the	O
original	O
sequence,	O
AR	O
language	O
modeling	O
is	O
not	O
able	O
to	O
cover	O
the	O
dependency.	O
•	O
In	O
comparison,	O
XLNet	B-MethodName
is	O
able	O
to	O
cover	O
all	O
dependencies	O
in	O
expectation.	O
Approaches	O
like	O
ELMo	O
concatenate	O
forward	O
and	O
backward	O
language	O
models	O
in	O
a	O
shallow	O
manner,	O
which	O
is	O
not	O
sufficient	O
for	O
modeling	O
deep	O
interactions	O
between	O
the	O
two	O
directions.	O

Following	O
the	O
setting	O
in	O
previous	O
work	O
,	O
we	O
use	O
the	O
ClueWeb09-B	B-DatasetName
dataset	O
to	O
evaluate	O
the	O
performance	O
on	O
document	O
ranking.	O
The	O
queries	O
were	O
created	O
by	O
the	O
TREC	O
2009-2012	O
Web	O
Tracks	O
based	O
on	O
50M	O
documents	O
and	O
the	O
task	O
is	O
to	O
rerank	O
the	O
top	O
100	O
documents	O
retrieved	O
using	O
a	O
standard	O
retrieval	O
method.	O
Since	O
document	O
ranking,	O
or	O
ad-hoc	O
retrieval,	O
mainly	O
concerns	O
the	O
low-level	O
representations	O
instead	O
of	O
high-level	O
semantics,	O
this	O
dataset	O
serves	O
as	O
a	O
testbed	O
for	O
evaluating	O
the	O
quality	O
of	O
word	O
embeddings.	O
We	O
use	O
a	O
pretrained	O
XLNet	B-MethodName
to	O
extract	O
word	O
embeddings	O
for	O
the	O
documents	O
and	O
queries	O
without	O
finetuning,	O
and	O
employ	O
a	O
kernel	O
pooling	O
network	O
to	O
rank	O
the	O
documents.	O
A.	O
The	O
hyperparameters	O
used	O
for	O
pretraining	O
XLNet	O
are	O
shown	O
in	O
Table	O
7.	O

The	O
GLUE	B-DatasetName
dataset	O
is	O
a	O
collection	O
of	O
9	O
natural	O
language	O
understanding	O
tasks.	O
The	O
test	O
set	O
labels	O
are	O
removed	O
from	O
the	O
publicly	O
released	O
version,	O
and	O
all	O
the	O
practitioners	O
must	O
submit	O
their	O
predictions	O
on	O
the	O
evaluation	O
server	O
to	O
obtain	O
test	O
set	O
results.	O
In	O
Table	O
5,	O
we	O
present	O
results	O
of	O
multiple	O
settings,	O
including	O
single-task	O
and	O
multi-task,	O
as	O
well	O
as	O
single	O
models	O
and	O
ensembles.	O
In	O
the	O
multi-task	O
setting,	O
we	O
jointly	O
train	O
an	O
XLNet	B-MethodName
on	O
the	O
four	O
largest	O
datasets-MNLI,	B-DatasetName
SST-2,	B-DatasetName
QNLI,	B-DatasetName
and	O
QQP-and	B-DatasetName
finetune	O
the	O
network	O
on	O
the	O
other	O
datasets.	O
Only	O
single-task	O
training	O
is	O
employed	O
for	O
the	O
four	O
large	O
datasets.	O
For	O
QNLI,	B-DatasetName
we	O
employed	O
a	O
pairwise	O
relevance	O
ranking	O
scheme	O
as	O
in	O
for	O
our	O
test	O
set	O
submission.	O
However,	O
for	O
fair	O
comparison	O
with	O
BERT,	B-MethodName
our	O
result	O
on	O
the	O
QNLI	B-DatasetName
dev	O
set	O
is	O
based	O
on	O
a	O
standard	O
classification	O
paradigm.	O
For	O
WNLI,	B-DatasetName
we	O
use	O
the	O
loss	O
described	O
in	O
.	O

Following	O
previous	O
work	O
on	O
text	O
classification	O
,	O
we	O
evaluate	O
XLNet	B-MethodName
on	O
the	O
following	O
benchmarks:	O
IMDB,	B-DatasetName
Yelp-2,	B-DatasetName
Yelp-5,	B-DatasetName
DBpedia,	B-DatasetName
AG,	B-DatasetName
Amazon-2,	B-DatasetName
and	O
Amazon-5.	B-DatasetName

SQuAD	B-DatasetName
is	O
a	O
large-scale	O
reading	O
comprehension	O
dataset	O
with	O
two	O
tasks.	O
SQuAD1.1	B-DatasetName
contains	O
questions	O
that	O
always	O
have	O
a	O
corresponding	O
answer	O
in	O
the	O
given	O
passages,	O
while	O
SQuAD2.0	B-DatasetName
introduces	O
unanswerable	O
questions.	O
To	O
finetune	O
an	O
XLNet	B-MethodName
on	O
SQuAD2.0,	B-DatasetName
we	O
jointly	O
apply	O
a	O
logistic	O
regression	O
loss	O
for	O
answerability	O
prediction	O
similar	O
to	O
classification	O
tasks	O
and	O
a	O
standard	O
span	O
extraction	O
loss	O
for	O
question	O
answering	O
.	O

Here,	O
we	O
provide	O
the	O
implementation	O
details	O
of	O
the	O
two-stream	O
attention	O
with	O
a	O
Transformer-XL	O
backbone.	O
Initial	O
represetation:	O
∀t	O
=	O
1,	O
.	O
.	O
.	O
,	O
T	O
:	O
h	O
t	O
=	O
e(x	O
t	O
)	O
and	O
g	O
t	O
=	O
w	O
Cached	O
layer-m	O
content	O
represetation	O
(memory)	O
from	O
previous	O
segment:h	O
(m)	O
For	O
the	O
Transformer-XL	O
layer	O
m	O
=	O
1,	O
•	O
•	O
•	O
,	O
M	O
,	O
attention	O
with	O
relative	O
positional	O
encoding	O
and	O
position-wise	O
feed-forward	O
are	O
consecutively	O
employed	O
to	O
update	O
the	O
represetntations:	O
∀t	O
=	O
1,	O
.	O
.	O
.	O
,	O
T	O
:ĥ	O
(m)	O
zt	O
=	O
LayerNorm	O
h	O
(m−1)	O
zt	O
+	O
RelAttn	O
h	O
(m−1)	O
zt	O
,	O
h	O
(m−1)	O
,	O
h	O
(m−1)	O
z	O
≤t	O
h	O
(m)	O
zt	O
=	O
LayerNorm	O
ĥ	O
(m)	O
zt	O
+	O
PosFF	O
ĥ	O
(m)	O
zt	O
ĝ	O
(m)	O
zt	O
=	O
LayerNorm	O
g	O
(m−1)	O
zt	O
+	O
RelAttn	O
g	O
(m−1)	O
zt	O
,	O
h	O
(m−1)	O
,	O
h	O
(m−1)	O
z<t	O
g	O
(m)	O
zt	O
=	O
LayerNorm	O
ĝ	O
(m)	O
zt	O
+	O
PosFF	O
ĝ	O
(m)	O
zt	O
Target-aware	O
prediction	O
distribution:	O
p	O
θ	O
(X	O
zt	O
=	O
x	O
|	O
x	O
z<t	O
)	O
=	O
exp	O
e(x)	O
g	O
(M	O
)	O
zt	O
x	O
exp	O
e(x	O
)	O
g	O
(M	O
)	O
zt	O
,	O
A.3	O
Datasets	O
A.3.1	O
RACE	B-DatasetName
Dataset	O
The	O
RACE	B-DatasetName
dataset	O
contains	O
near	O
100K	O
questions	O
taken	O
from	O
the	O
English	O
exams	O
for	O
middle	O
and	O
high	O
school	O
Chinese	O
students	O
in	O
the	O
age	O
range	O
between	O
12	O
to	O
18,	O
with	O
the	O
answers	O
generated	O
by	O
human	O
experts.	O
This	O
is	O
one	O
of	O
the	O
most	O
difficult	O
reading	O
comprehension	O
datasets	O
that	O
involve	O
challenging	O
reasoning	O
questions.	O
Moreover,	O
the	O
average	O
length	O
of	O
the	O
passages	O
in	O
RACE	B-DatasetName
are	O
longer	O
than	O
300,	O
which	O
is	O
significantly	O
longer	O
than	O
other	O
popular	O
reading	O
comprehension	O
datasets	O
such	O
as	O
SQuAD	O
.	O
As	O
a	O
result,	O
this	O
dataset	O
serves	O
as	O
a	O
challenging	O
benchmark	O
for	O
long	O
text	O
understanding.	O
We	O
use	O
a	O
sequence	B-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
during	O
finetuning.	O

In	O
this	O
section,	O
we	O
provide	O
a	O
concrete	O
example	O
to	O
show	O
how	O
the	O
standard	O
language	O
model	O
parameterization	O
fails	O
under	O
the	O
permutation	O
objective,	O
as	O
discussed	O
in	O
Section	O
2.3.	O
Specifically,	O
let's	O
consider	O
two	O
different	O
permutations	O
z	O
(1)	O
and	O
z	O
(2)	O
satisfying	O
the	O
following	O
relationship	O
z	O
(1)	O
<t	O
=	O
z	O
(2)	O
<t	O
=	O
z	O
<t	O
but	O
z	O
(1)	O
t	O
=	O
i	O
=	O
j	O
=	O
z(2)	O
t	O
.	O
Then,	O
substituting	O
the	O
two	O
permutations	O
respectively	O
into	O
the	O
naive	O
parameterization,	O
we	O
have	O
p	O
θ	O
(X	O
i	O
=	O
x	O
|	O
x	O
z<t	O
)	O
z	O
(1)	O
t	O
=i,	O
z	O
(1)	O
<t	O
=z<t	O
=	O
p	O
θ	O
(X	O
j	O
=	O
x	O
|	O
x	O
z<t	O
)	O
z	O
(1)	O
t	O
=j,	O
z(2)	O
<t	O
=z<t	O
=	O
exp	O
e(x)	O
h(x	O
z<t	O
)	O
x	O
exp	O
(e(x	O
)	O
h(x	O
z<t	O
))	O
.	O
Effectively,	O
two	O
different	O
target	O
positions	O
i	O
and	O
j	O
share	O
exactly	O
the	O
same	O
model	O
prediction.	O
However,	O
the	O
ground-truth	O
distribution	O
of	O
two	O
positions	O
should	O
certainly	O
be	O
different.	O

XLNet	B-MethodName
is	O
a	O
generalized	O
AR	O
pretraining	O
method	O
that	O
uses	O
a	O
permutation	O
language	O
modeling	O
objective	O
to	O
combine	O
the	O
advantages	O
of	O
AR	O
and	O
AE	O
methods.	O
The	O
neural	O
architecture	O
of	O
XLNet	B-MethodName
is	O
developed	O
to	O
work	O
seamlessly	O
with	O
the	O
AR	O
objective,	O
including	O
integrating	O
Transformer-XL	O
and	O
the	O
careful	O
design	O
of	O
the	O
two-stream	O
attention	O
mechanism.	O
XLNet	B-MethodName
achieves	O
substantial	O
improvement	O
over	O
previous	O
pretraining	O
objectives	O
on	O
various	O
tasks.	O
A	O
Target-Aware	O
Representation	O
via	O
Two-Stream	O
Self-Attention	O

We	O
perform	O
an	O
ablation	O
study	O
to	O
understand	O
the	O
importance	O
of	O
each	O
design	O
choice	O
based	O
on	O
four	O
datasets	O
with	O
diverse	O
characteristics.	O
Specifically,	O
there	O
are	O
three	O
main	O
aspects	O
we	O
hope	O
to	O
study:	O
•	O
The	O
effectiveness	O
of	O
the	O
permutation	O
language	O
modeling	O
objective	O
alone,	O
especially	O
compared	O
to	O
the	O
denoising	O
auto-encoding	O
objective	O
used	O
by	O
BERT.	B-MethodName
•	O
The	O
importance	O
of	O
using	O
Transformer-XL	O
as	O
the	O
backbone	O
neural	O
architecture.	O
•	O
The	O
necessity	O
of	O
some	O
implementation	O
details	O
including	O
span-based	O
prediction,	O
the	O
bidirectional	O
input	O
pipeline,	O
and	O
next-sentence	O
prediction.	O
With	O
these	O
purposes	O
in	O
mind,	O
in	O
Table	O
6,	O
we	O
compare	O
6	O
XLNet-Base	B-MethodName
variants	O
with	O
different	O
implementation	O
details	O
(rows	O
3	O
-8),	O
the	O
original	O
BERT-Base	O
model	O
(row	O
1),	O
and	O
an	O
additional	O
Transformer-XL	O
baseline	O
trained	O
with	O
the	O
denoising	O
auto-encoding	O
(DAE)	O
objective	O
used	O
in	O
BERT	B-MethodName
but	O
with	O
the	O
bidirectional	O
input	O
pipeline	O
(row	O
2).	O
For	O
fair	O
comparison,	O
all	O
models	O
are	O
based	O
on	O
a	O
12-layer	B-HyperparameterValue
architecture	O
with	O
the	O
same	O
model	O
hyper-parameters	O
as	O
BERT-Base	O
and	O
are	O
trained	O
on	O
only	O
Wikipedia	B-DatasetName
and	O
the	O
BooksCorpus.	B-DatasetName
All	O
results	O
reported	O
are	O
the	O
median	O
of	O
5	O
runs.	O
Table	O
6:	O
The	O
results	O
of	O
BERT	B-MethodName
on	O
RACE	O
are	O
taken	O
from	O
.	O
We	O
run	O
BERT	B-MethodName
on	O
the	O
other	O
datasets	O
using	O
the	O
official	O
implementation	O
and	O
the	O
same	O
hyperparameter	O
search	O
space	O
as	O
XLNet.	B-MethodName
K	B-HyperparameterName
is	O
a	O
hyperparameter	O
to	O
control	O
the	O
optimization	O
difficulty	O
(see	O
Section	O
2.3).	O
Examining	O
rows	O
1	O
-4	O
of	O
Table	O
6,	O
we	O
can	O
see	O
both	O
Transformer-XL	O
and	O
the	O
permutation	O
LM	O
clearly	O
contribute	O
the	O
superior	O
performance	O
of	O
XLNet	B-MethodName
over	O
BERT.	B-MethodName
Moreover,	O
if	O
we	O
remove	O
the	O
memory	O
caching	O
mechanism	O
(row	O
5),	O
the	O
performance	O
clearly	O
drops,	O
especially	O
for	O
RACE	O
which	O
involves	O
the	O
longest	O
context	O
among	O
the	O
4	O
tasks.	O
In	O
addition,	O
rows	O
6	O
-7	O
show	O
that	O
both	O
span-based	O
prediction	O
and	O
the	O
bidirectional	O
input	O
pipeline	O
play	O
important	O
roles	O
in	O
XLNet.	B-MethodName
Finally,	O
we	O
unexpectedly	O
find	O
the	O
the	O
next-sentence	O
prediction	O
objective	O
proposed	O
in	O
the	O
original	O
BERT	B-MethodName
does	O
not	O
necessarily	O
lead	O
to	O
an	O
improvement	O
in	O
our	O
setting.	O
Hence,	O
we	O
exclude	O
the	O
next-sentence	O
prediction	O
objective	O
from	O
XLNet.	B-MethodName
Finally,	O
we	O
also	O
perform	O
a	O
qualitative	O
study	O
of	O
the	O
attention	O
patterns,	O
which	O
is	O
included	O
in	O
Appendix	O
A.6	O
due	O
to	O
page	O
limit.	O

BERT.	B-MethodName
We	O
use	O
the	O
best	O
of	O
3	O
BERT	B-MethodName
variants	O
for	O
comparison;	O
i.e.,	O
the	O
original	O
BERT,	B-MethodName
BERT	B-MethodName
with	O
whole	O
word	O
masking,	O
and	O
BERT	B-MethodName
without	O
next	O
sentence	O
prediction.	O
Here,	O
we	O
first	O
compare	O
the	O
performance	O
of	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
in	O
a	O
fair	O
setting	O
to	O
decouple	O
the	O
effects	O
of	O
using	O
more	O
data	O
and	O
the	O
improvement	O
from	O
BERT	B-MethodName
to	O
XLNet.	B-MethodName
In	O
After	O
the	O
initial	O
publication	O
of	O
our	O
manuscript,	O
a	O
few	O
other	O
pretrained	O
models	O
were	O
released	O
such	O
as	O
RoBERTa	B-MethodName
and	O
ALBERT	O
.	O
Since	O
ALBERT	O
involves	O
increasing	O
the	O
model	O
hidden	O
size	O
from	O
1024	O
to	O
2048/4096	O
and	O
thus	O
substantially	O
increases	O
the	O
amount	O
of	O
computation	O
in	O
terms	O
of	O
FLOPs,	O
we	O
exclude	O
ALBERT	O
from	O
the	O
following	O
results	O
as	O
it	O
is	O
hard	O
to	O
lead	O
to	O
scientific	O
conclusions.	O
To	O
obtain	O
relatively	O
fair	O
comparison	O
with	O
RoBERTa,	O
the	O
experiment	O
in	O
this	O
section	O
is	O
based	O
on	O
full	O
data	O
and	O
reuses	O
the	O
hyper-parameters	O
of	O
RoBERTa,	B-MethodName
as	O
described	O
in	O
section	O
3.1.	O
The	O
results	O
are	O
presented	O
in	O
Tables	O
2	O
(reading	B-TaskName
comprehension	I-TaskName
&	O
document	B-TaskName
ranking),	I-TaskName
3	O
(question	B-TaskName
answering),	I-TaskName
4	O
(text	B-TaskName
classification)	I-TaskName
and	O
5	O
(natural	B-TaskName
language	I-TaskName
understanding),	I-TaskName
where	O
XLNet	B-MethodName
generally	O
outperforms	O
BERT	B-MethodName
and	O
RoBERTa.	B-MethodName
In	O
addition,	O
we	O
make	O
two	O
more	O
interesting	O
observations:	O
All	O
dev	O
results	O
are	O
the	O
median	O
of	O
10	O
runs.	O
The	O
upper	O
section	O
shows	O
direct	O
comparison	O
on	O
dev	O
data	O
and	O
the	O
lower	O
section	O
shows	O
comparison	O
with	O
state-of-the-art	O
results	O
on	O
the	O
public	O
leaderboard.	O
SQuAD2.0	B-DatasetName
EM	O
F1	B-MetricName
SQuAD1.1	B-DatasetName
EM	O
F1	B-MetricName
•	O
For	O
explicit	O
reasoning	O
tasks	O
like	O
SQuAD	B-DatasetName
and	O
RACE	B-DatasetName
that	O
involve	O
longer	O
context,	O
the	O
performance	O
gain	O
of	O
XLNet	B-MethodName
is	O
usually	O
larger.	O
This	O
superiority	O
at	O
dealing	O
with	O
longer	O
context	O
could	O
come	O
from	O
the	O
Transformer-XL	O
backbone	O
in	O
XLNet.	B-MethodName
•	O
For	O
classification	O
tasks	O
that	O
already	O
have	O
abundant	O
supervised	O
examples	O
such	O
as	O
MNLI	B-DatasetName
(>390K),	O
Yelp	B-DatasetName
(>560K)	O
and	O
Amazon	B-DatasetName
(>3M),	O
XLNet	B-MethodName
still	O
lead	O
to	O
substantial	O
gains.	O

Following	O
BERT	B-MethodName
,	O
we	O
use	O
the	O
BooksCorpus	B-DatasetName
and	O
English	B-DatasetName
Wikipedia	I-DatasetName
as	O
part	O
of	O
our	O
pretraining	O
data,	O
which	O
have	O
13GB	O
plain	O
text	O
combined.	O
In	O
addition,	O
we	O
include	O
Giga5	B-DatasetName
(16GB	O
text)	O
,	O
ClueWeb	B-DatasetName
2012-B	I-DatasetName
(extended	O
from	O
),	O
and	O
Common	B-DatasetName
Crawl	I-DatasetName
for	O
pretraining.	O
We	O
use	O
heuristics	O
to	O
aggressively	O
filter	O
out	O
short	O
or	O
low-quality	O
articles	O
for	O
ClueWeb	B-DatasetName
2012-B	I-DatasetName
and	O
Common	B-DatasetName
Crawl,	I-DatasetName
which	O
results	O
in	O
19GB	O
and	O
110GB	O
text	O
respectively.	O
After	O
tokenization	O
with	O
SentencePiece	O
,	O
we	O
obtain	O
2.78B,	O
1.09B,	O
4.75B,	O
4.30B,	O
and	O
19.97B	O
subword	O
pieces	O
for	O
Wikipedia,	B-DatasetName
BooksCorpus,	B-DatasetName
Giga5,	B-DatasetName
ClueWeb,	B-DatasetName
and	O
Common	B-DatasetName
Crawl	I-DatasetName
respectively,	O
which	O
are	O
32.89B	O
in	O
total.	O
Our	O
largest	O
model	O
XLNet-Large	B-MethodName
has	O
the	O
same	O
architecture	O
hyperparameters	O
as	O
BERT-Large,	O
which	O
results	O
in	O
a	O
similar	O
model	O
size.	O
During	O
pretraining,	O
we	O
always	O
use	O
a	O
full	O
sequence	B-HyperparameterName
length	I-HyperparameterName
of	O
512.	B-HyperparameterValue
Firstly,	O
to	O
provide	O
a	O
fair	O
comparison	O
with	O
BERT	B-MethodName
(section	O
3.2),	O
we	O
also	O
trained	O
XLNet-Large-wikibooks	B-MethodName
on	O
BooksCorpus	B-DatasetName
and	O
Wikipedia	B-DatasetName
only,	O
where	O
we	O
reuse	O
all	O
pretraining	O
hyper-parameters	O
as	O
in	O
the	O
original	O
BERT.	B-MethodName
Then,	O
we	O
scale	O
up	O
the	O
training	O
of	O
XLNet-Large	B-MethodName
by	O
using	O
all	O
the	O
datasets	O
described	O
above.	O
Specifically,	O
we	O
train	O
on	O
512	O
TPU	O
v3	O
chips	O
for	O
500K	O
steps	O
with	O
an	O
Adam	O
weight	O
decay	O
optimizer,	O
linear	O
learning	O
rate	O
decay,	O
and	O
a	O
batch	O
size	O
of	O
8192,	O
which	O
takes	O
about	O
5.5	O
days.	O
It	O
was	O
observed	O
that	O
the	O
model	O
still	O
underfits	O
the	O
data	O
at	O
the	O
end	O
of	O
training.	O
Finally,	O
we	O
perform	O
ablation	O
study	O
(section	O
3.4)	O
based	O
on	O
the	O
XLNet-Base-wikibooks.	B-MethodName
Since	O
the	O
recurrence	O
mechanism	O
is	O
introduced,	O
we	O
use	O
a	O
bidirectional	O
data	O
input	O
pipeline	O
where	O
each	O
of	O
the	O
forward	O
and	O
backward	O
directions	O
takes	O
half	O
of	O
the	O
batch	O
size.	O
For	O
training	O
XLNet-Large,	B-MethodName
we	O
set	O
the	O
partial	O
prediction	O
constant	O
K	B-HyperparameterName
as	O
6	B-HyperparameterValue
(see	O
Section	O
2.3).	O
Our	O
finetuning	O
procedure	O
follows	O
BERT	B-MethodName
except	O
otherwise	O
specified	O
3	O
.	O
We	O
employ	O
an	O
idea	O
of	O
span-based	O
prediction,	O
where	O
we	O
first	O
sample	O
a	O
length	O
L	O
∈	O
[1,	O
•	O
•	O
•	O
,	O
5],	O
and	O
then	O
randomly	O
select	O
a	O
consecutive	O
span	O
of	O
L	O
tokens	O
as	O
prediction	O
targets	O
within	O
a	O
context	O
of	O
(KL)	O
tokens.	O
We	O
use	O
a	O
variety	O
of	O
natural	O
language	O
understanding	O
datasets	O
to	O
evaluate	O
the	O
performance	O
of	O
our	O
method.	O
Detailed	O
descriptions	O
of	O
the	O
settings	O
for	O
all	O
the	O
datasets	O
can	O
be	O
found	O
in	O
Appendix	O
A.3.	O

Comparing	O
Eq.	O
(2)	O
and	O
(	O
5),	O
we	O
observe	O
that	O
both	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
perform	O
partial	O
prediction,	O
i.e.,	O
only	O
predicting	O
a	O
subset	O
of	O
tokens	O
in	O
the	O
sequence.	O
This	O
is	O
a	O
necessary	O
choice	O
for	O
BERT	B-MethodName
because	O
if	O
all	O
tokens	O
are	O
masked,	O
it	O
is	O
impossible	O
to	O
make	O
any	O
meaningful	O
predictions.	O
In	O
addition,	O
for	O
both	O
BERT	B-MethodName
and	O
XLNet,	B-MethodName
partial	O
prediction	O
plays	O
a	O
role	O
of	O
reducing	O
optimization	O
difficulty	O
by	O
only	O
predicting	O
tokens	O
with	O
sufficient	O
context.	O
However,	O
the	O
independence	O
assumption	O
discussed	O
in	O
Section	O
2.1	O
disables	O
BERT	B-MethodName
to	O
model	O
dependency	O
between	O
targets.	O
To	O
better	O
understand	O
the	O
difference,	O
let's	O
consider	O
a	O
concrete	O
example	O
[New,	O
York,	O
is,	O
a,	O
city].	O
Suppose	O
both	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
select	O
the	O
two	O
tokens	O
[New,	O
York]	O
as	O
the	O
prediction	O
targets	O
and	O
maximize	O
log	O
p(New	O
York	O
|	O
is	O
a	O
city).	O
Also	O
suppose	O
that	O
XLNet	B-MethodName
samples	O
the	O
factorization	O
order	O
[is,	O
a,	O
city,	O
New,	O
York].	O
In	O
this	O
case,	O
BERT	B-MethodName
and	O
XLNet	B-MethodName
respectively	O
reduce	O
to	O
the	O
following	O
objectives:	O
J	O
BERT	O
=	O
log	O
p(New	O
|	O
is	O
a	O
city)	O
+	O
log	O
p(York	O
|	O
is	O
a	O
city),	O
J	O
XLNet	O
=	O
log	O
p(New	O
|	O
is	O
a	O
city)	O
+	O
log	O
p(York	O
|	O
New,	O
is	O
a	O
city).	O
Notice	O
that	O
XLNet	B-MethodName
is	O
able	O
to	O
capture	O
the	O
dependency	O
between	O
the	O
pair	O
(New,	O
York),	O
which	O
is	O
omitted	O
by	O
BERT.	O
Although	O
in	O
this	O
example,	O
BERT	B-MethodName
learns	O
some	O
dependency	O
pairs	O
such	O
as	O
(New,	O
city)	O
and	O
(York,	O
city),	O
it	O
is	O
obvious	O
that	O
XLNet	B-MethodName
always	O
learns	O
more	O
dependency	O
pairs	O
given	O
the	O
same	O
target	O
and	O
contains	O
"denser"	O
effective	O
training	O
signals.	O
For	O
more	O
formal	O
analysis	O
and	O
further	O
discussion,	O
please	O
refer	O
to	O
Appendix	O
A.5.	O
3	O
Experiments	O

Many	O
downstream	O
tasks	O
have	O
multiple	O
input	O
segments,	O
e.g.,	O
a	O
question	O
and	O
a	O
context	O
paragraph	O
in	O
question	O
answering.	O
We	O
now	O
discuss	O
how	O
we	O
pretrain	O
XLNet	B-MethodName
to	O
model	O
multiple	O
segments	O
in	O
the	O
autoregressive	O
framework.	O
During	O
the	O
pretraining	O
phase,	O
following	O
BERT,	B-MethodName
we	O
randomly	O
sample	O
two	O
segments	O
(either	O
from	O
the	O
same	O
context	O
or	O
not)	O
and	O
treat	O
the	O
concatenation	O
of	O
two	O
segments	O
as	O
one	O
sequence	O
to	O
perform	O
permutation	O
language	B-TaskName
modeling.	I-TaskName
We	O
only	O
reuse	O
the	O
memory	O
that	O
belongs	O
to	O
the	O
same	O
context.	O
Specifically,	O
the	O
input	O
to	O
our	O
model	O
is	O
the	O
same	O
as	O
BERT:	B-MethodName
[CLS,	O
A,	O
SEP,	O
B,	O
SEP],	O
where	O
"SEP"	O
and	O
"CLS"	O
are	O
two	O
special	O
symbols	O
and	O
"A"	O
and	O
"B"	O
are	O
the	O
two	O
segments.	O
Although	O
we	O
follow	O
the	O
two-segment	O
data	O
format,	O
XLNet-Large	B-MethodName
does	O
not	O
use	O
the	O
objective	O
of	O
next	O
sentence	O
prediction	O
as	O
it	O
does	O
not	O
show	O
consistent	O
improvement	O
in	O
our	O
ablation	O
study	O
(see	O
Section	O
3.4).	O
Relative	O
Segment	O
Encodings	O
Architecturally,	O
different	O
from	O
BERT	B-MethodName
that	O
adds	O
an	O
absolute	O
segment	O
embedding	O
to	O
the	O
word	O
embedding	O
at	O
each	O
position,	O
we	O
extend	O
the	O
idea	O
of	O
relative	O
encodings	O
from	O
Transformer-XL	O
to	O
also	O
encode	O
the	O
segments.	O
Given	O
a	O
pair	O
of	O
positions	O
i	O
and	O
j	O
in	O
the	O
sequence,	O
if	O
i	O
and	O
j	O
are	O
from	O
the	O
same	O
segment,	O
we	O
use	O
a	O
segment	O
encoding	O
s	O
ij	O
=	O
s	O
+	O
or	O
otherwise	O
s	O
ij	O
=	O
s	O
−	O
,	O
where	O
s	O
+	O
and	O
s	O
−	O
are	O
learnable	O
model	O
parameters	O
for	O
each	O
attention	O
head.	O
In	O
other	O
words,	O
we	O
only	O
consider	O
whether	O
the	O
two	O
positions	O
are	O
within	O
the	O
same	O
segment,	O
as	O
opposed	O
to	O
considering	O
which	O
specific	O
segments	O
they	O
are	O
from.	O
This	O
is	O
consistent	O
with	O
the	O
core	O
idea	O
of	O
relative	O
encodings;	O
i.e.,	O
only	O
modeling	O
the	O
relationships	O
between	O
positions.	O
When	O
i	O
attends	O
to	O
j,	O
the	O
segment	O
encoding	O
s	O
ij	O
is	O
used	O
to	O
compute	O
an	O
attention	O
weight	O
a	O
ij	O
=	O
(q	O
i	O
+	O
b)	O
s	O
ij	O
,	O
where	O
q	O
i	O
is	O
the	O
query	O
vector	O
as	O
in	O
a	O
standard	O
attention	O
operation	O
and	O
b	O
is	O
a	O
learnable	O
head-specific	O
bias	O
vector.	O
Finally,	O
the	O
value	O
a	O
ij	O
is	O
added	O
to	O
the	O
normal	O
attention	O
weight.	O
There	O
are	O
two	O
benefits	O
of	O
using	O
relative	O
segment	O
encodings.	O
First,	O
the	O
inductive	O
bias	O
of	O
relative	O
encodings	O
improves	O
generalization	O
.	O
Second,	O
it	O
opens	O
the	O
possibility	O
of	O
finetuning	O
on	O
tasks	O
that	O
have	O
more	O
than	O
two	O
input	O
segments,	O
which	O
is	O
not	O
possible	O
using	O
absolute	O
segment	O
encodings.	O

Since	O
our	O
objective	O
function	O
fits	O
in	O
the	O
AR	O
framework,	O
we	O
incorporate	O
the	O
state-of-the-art	O
AR	O
language	O
model,	O
Transformer-XL	O
,	O
into	O
our	O
pretraining	O
framework,	O
and	O
name	O
our	O
method	O
after	O
it.	O
We	O
integrate	O
two	O
important	O
techniques	O
in	O
Transformer-XL,	O
namely	O
the	O
relative	O
positional	O
encoding	O
scheme	O
and	O
the	O
segment	O
recurrence	O
mechanism.	O
We	O
apply	O
relative	O
positional	O
encodings	O
based	O
on	O
the	O
original	O
sequence	O
as	O
discussed	O
earlier,	O
which	O
is	O
straightforward.	O
Now	O
we	O
discuss	O
how	O
to	O
integrate	O
the	O
recurrence	O
mechanism	O
into	O
the	O
proposed	O
permutation	O
setting	O
and	O
enable	O
the	O
model	O
to	O
reuse	O
hidden	O
states	O
from	O
previous	O
segments.	O
Without	O
loss	O
of	O
generality,	O
suppose	O
we	O
have	O
two	O
segments	O
taken	O
from	O
a	O
long	O
sequence	O
s;	O
i.e.,x	O
=	O
s	O
1:T	O
and	O
x	O
=	O
s	O
T	O
+1:2T	O
.	O
Letz	O
and	O
z	O
be	O
permutations	O
of	O
[1	O
•	O
•	O
•	O
T	O
]	O
and	O
[T	O
+	O
1	O
•	O
•	O
•	O
2T	O
]	O
respectively.	O
Then,	O
based	O
on	O
the	O
permutationz,	O
we	O
process	O
the	O
first	O
segment,	O
and	O
then	O
cache	O
the	O
obtained	O
content	O
representationsh	O
(m)	O
for	O
each	O
layer	O
m.	O
Then,	O
for	O
the	O
next	O
segment	O
x,	O
the	O
attention	O
update	O
with	O
memory	O
can	O
be	O
written	O
as	O
h	O
(m)	O
zt	O
←	O
Attention(Q	O
=	O
h	O
(m−1)	O
zt	O
,	O
KV	O
=	O
h	O
(m−1)	O
,	O
h	O
(m−1)	O
z	O
≤t	O
;	O
θ)	O
where	O
[.,	O
.]	O
denotes	O
concatenation	O
along	O
the	O
sequence	O
dimension.	O
Notice	O
that	O
positional	O
encodings	O
only	O
depend	O
on	O
the	O
actual	O
positions	O
in	O
the	O
original	O
sequence.	O
Thus,	O
the	O
above	O
attention	O
update	O
is	O
independent	O
ofz	O
once	O
the	O
representationsh	O
(m)	O
are	O
obtained.	O
This	O
allows	O
caching	O
and	O
reusing	O
the	O
memory	O
without	O
knowing	O
the	O
factorization	O
order	O
of	O
the	O
previous	O
segment.	O
In	O
expectation,	O
the	O
model	O
learns	O
to	O
utilize	O
the	O
memory	O
over	O
all	O
factorization	O
orders	O
of	O
the	O
last	O
segment.	O
The	O
query	O
stream	O
can	O
be	O
computed	O
in	O
the	O
same	O
way.	O
Finally,	O
Figure	O
1	O
(c)	O
presents	O
an	O
overview	O
of	O
the	O
proposed	O
permutation	O
language	O
modeling	O
with	O
two-stream	O
attention	O
(see	O
Appendix	O
A.7	O
for	O
more	O
detailed	O
illustration).	O

In	O
this	O
section,	O
we	O
first	O
review	O
and	O
compare	O
the	O
conventional	O
AR	O
language	O
modeling	O
and	O
BERT	O
for	O
language	O
pretraining.	O
Given	O
a	O
text	O
sequence	O
x	O
=	O
[x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
T	O
],	O
AR	O
language	O
modeling	O
performs	O
pretraining	O
by	O
maximizing	O
the	O
likelihood	O
under	O
the	O
forward	O
autoregressive	O
factorization:	O
max	O
θ	O
log	O
p	O
θ	O
(x)	O
=	O
T	O
t=1	O
log	O
p	O
θ	O
(x	O
t	O
|	O
x	O
<t	O
)	O
=	O
T	O
t=1	O
log	O
exp	O
h	O
θ	O
(x	O
1:t−1	O
)	O
e(x	O
t	O
)	O
x	O
exp	O
(h	O
θ	O
(x	O
1:t−1	O
)	O
e(x	O
))	O
,(1)	O
where	O
h	O
θ	O
(x	O
1:t−1	O
)	O
is	O
a	O
context	O
representation	O
produced	O
by	O
neural	O
models,	O
such	O
as	O
RNNs	O
or	O
Transformers,	O
and	O
e(x)	O
denotes	O
the	O
embedding	O
of	O
x.	O
In	O
comparison,	O
BERT	O
is	O
based	O
on	O
denoising	O
auto-encoding.	O
Specifically,	O
for	O
a	O
text	O
sequence	O
x,	O
BERT	O
first	O
constructs	O
a	O
corrupted	O
versionx	O
by	O
randomly	O
setting	O
a	O
portion	O
(e.g.	O
15%)	O
of	O
tokens	O
in	O
x	O
to	O
a	O
special	O
symbol	O
[MASK].	O
Let	O
the	O
masked	O
tokens	O
bex.	O
The	O
training	O
objective	O
is	O
to	O
reconstructx	O
fromx:	O
max	O
θ	O
log	O
p	O
θ	O
(x	O
|x)	O
≈	O
T	O
t=1	O
m	O
t	O
log	O
p	O
θ	O
(x	O
t	O
|x)	O
=	O
T	O
t=1	O
m	O
t	O
log	O
exp	O
H	O
θ	O
(x)	O
t	O
e(x	O
t	O
)	O
x	O
exp	O
H	O
θ	O
(x)	O
t	O
e(x	O
)	O
,(2)	O
where	O
m	O
t	O
=	O
1	O
indicates	O
x	O
t	O
is	O
masked,	O
and	O
H	O
θ	O
is	O
a	O
Transformer	O
that	O
maps	O
a	O
length-T	O
text	O
sequence	O
x	O
into	O
a	O
sequence	O
of	O
hidden	O
vectors	O
H	O
θ	O
(x)	O
=	O
[H	O
θ	O
(x)	O
1	O
,	O
H	O
θ	O
(x)	O
2	O
,	O
•	O
•	O
•	O
,	O
H	O
θ	O
(x)	O
T	O
].	O
The	O
pros	O
and	O
cons	O
of	O
the	O
two	O
pretraining	O
objectives	O
are	O
compared	O
in	O
the	O
following	O
aspects:	O
•	O
Independence	O
Assumption:	O
As	O
emphasized	O
by	O
the	O
≈	O
sign	O
in	O
Eq.	O
(2),	O
BERT	O
factorizes	O
the	O
joint	O
conditional	O
probability	O
p(x	O
|x)	O
based	O
on	O
an	O
independence	O
assumption	O
that	O
all	O
masked	O
tokensx	O
are	O
separately	O
reconstructed.	O
In	O
comparison,	O
the	O
AR	O
language	O
modeling	O
objective	O
(1)	O
factorizes	O
p	O
θ	O
(x)	O
using	O
the	O
product	O
rule	O
that	O
holds	O
universally	O
without	O
such	O
an	O
independence	O
assumption.	O
•	O
Input	O
noise:	O
The	O
input	O
to	O
BERT	O
contains	O
artificial	O
symbols	O
like	O
[MASK]	O
that	O
never	O
occur	O
in	O
downstream	O
tasks,	O
which	O
creates	O
a	O
pretrain-finetune	O
discrepancy.	O
Replacing	O
[MASK]	O
with	O
original	O
tokens	O
as	O
in	O
does	O
not	O
solve	O
the	O
problem	O
because	O
original	O
tokens	O
can	O
be	O
only	O
used	O
with	O
a	O
small	O
probability	O
-otherwise	O
Eq.	O
(2)	O
will	O
be	O
trivial	O
to	O
optimize.	O
In	O
comparison,	O
AR	O
language	O
modeling	O
does	O
not	O
rely	O
on	O
any	O
input	O
corruption	O
and	O
does	O
not	O
suffer	O
from	O
this	O
issue.	O
•	O
Context	O
dependency:	O
The	O
AR	O
representation	O
h	O
θ	O
(x	O
1:t−1	O
)	O
is	O
only	O
conditioned	O
on	O
the	O
tokens	O
up	O
to	O
position	O
t	O
(i.e.	O
tokens	O
to	O
the	O
left),	O
while	O
the	O
BERT	O
representation	O
H	O
θ	O
(x)	O
t	O
has	O
access	O
to	O
the	O
contextual	O
information	O
on	O
both	O
sides.	O
As	O
a	O
result,	O
the	O
BERT	O
objective	O
allows	O
the	O
model	O
to	O
be	O
pretrained	O
to	O
better	O
capture	O
bidirectional	O
context.	O

The	O
idea	O
of	O
permutation-based	O
AR	B-TaskName
modeling	I-TaskName
has	O
been	O
explored	O
in	O
,	O
but	O
there	O
are	O
several	O
key	O
differences.	O
Firstly,	O
previous	O
models	O
aim	O
to	O
improve	O
density	O
estimation	O
by	O
baking	O
an	O
"orderless"	O
inductive	O
bias	O
into	O
the	O
model	O
while	O
XLNet	B-MethodName
is	O
motivated	O
by	O
enabling	O
AR	O
language	O
models	O
to	O
learn	O
bidirectional	O
contexts.	O
Technically,	O
to	O
construct	O
a	O
valid	O
target-aware	O
prediction	O
distribution,	O
XLNet	B-MethodName
incorporates	O
the	O
target	O
position	O
into	O
the	O
hidden	O
state	O
via	O
two-stream	O
attention	O
while	O
previous	O
permutation-based	O
AR	O
models	O
relied	O
on	O
implicit	O
position	O
awareness	O
inherent	O
to	O
their	O
MLP	O
architectures.	O
Finally,	O
for	O
both	O
orderless	O
NADE	O
and	O
XLNet,	B-MethodName
we	O
would	O
like	O
to	O
emphasize	O
that	O
"orderless"	O
does	O
not	O
mean	O
that	O
the	O
input	O
sequence	O
can	O
be	O
randomly	O
permuted	O
but	O
that	O
the	O
model	O
allows	O
for	O
different	O
factorization	O
orders	O
of	O
the	O
distribution.	O
Another	O
related	O
idea	O
is	O
to	O
perform	O
autoregressive	O
denoising	O
in	O
the	O
context	O
of	O
text	O
generation	O
,	O
which	O
only	O
considers	O
a	O
fixed	O
order	O
though.	O
2	O
Proposed	O
Method	O

Unsupervised	O
representation	O
learning	O
has	O
been	O
highly	O
successful	O
in	O
the	O
domain	O
of	O
natural	O
language	O
processing	O
.	O
Typically,	O
these	O
methods	O
first	O
pretrain	O
neural	O
networks	O
on	O
large-scale	O
unlabeled	O
text	O
corpora,	O
and	O
then	O
finetune	O
the	O
models	O
or	O
representations	O
on	O
downstream	O
tasks.	O
Under	O
this	O
shared	O
high-level	O
idea,	O
different	O
unsupervised	O
pretraining	O
objectives	O
have	O
been	O
explored	O
in	O
literature.	O
Among	O
them,	O
autoregressive	B-TaskName
(AR)	B-TaskName
language	B-TaskName
modeling	I-TaskName
and	O
autoencoding	B-TaskName
(AE)	B-TaskName
have	O
been	O
the	O
two	O
most	O
successful	O
pretraining	O
objectives.	O
AR	B-TaskName
language	I-TaskName
modeling	I-TaskName
seeks	O
to	O
estimate	O
the	O
probability	O
distribution	O
of	O
a	O
text	O
corpus	O
with	O
an	O
autoregressive	O
model	O
.	O
Specifically,	O
given	O
a	O
text	O
sequence	O
x	O
=	O
(x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
T	O
),	O
AR	B-TaskName
language	I-TaskName
modeling	I-TaskName
factorizes	O
the	O
likelihood	O
into	O
a	O
forward	O
product	O
p(x)	O
=	O
T	O
t=1	O
p(x	O
t	O
|	O
x	O
<t	O
)	O
or	O
a	O
backward	O
one	O
p(x)	O
=	O
1	O
t=T	O
p(x	O
t	O
|	O
x	O
>t	O
).	O
A	O
parametric	O
model	O
(e.g.	O
a	O
neural	O
network)	O
is	O
trained	O
to	O
model	O
each	O
conditional	O
distribution.	O
Since	O
an	O
AR	O
language	O
model	O
is	O
only	O
trained	O
to	O
encode	O
a	O
uni-directional	O
context	O
(either	O
forward	O
or	O
backward),	O
it	O
is	O
not	O
effective	O
at	O
modeling	O
deep	O
bidirectional	O
contexts.	O
On	O
the	O
contrary,	O
downstream	O
language	O
understanding	O
tasks	O
often	O
require	O
bidirectional	O
context	O
information.	O
This	O
results	O
in	O
a	O
gap	O
between	O
AR	O
language	O
modeling	O
and	O
effective	O
pretraining.	O
In	O
comparison,	O
AE	O
based	O
pretraining	O
does	O
not	O
perform	O
explicit	O
density	O
estimation	O
but	O
instead	O
aims	O
to	O
reconstruct	O
the	O
original	O
data	O
from	O
corrupted	O
input.	O
A	O
notable	O
example	O
is	O
BERT	B-MethodName
,	O
which	O
has	O
been	O
the	O
state-of-the-art	O
pretraining	O
approach.	O
Given	O
the	O
input	O
token	O
sequence,	O
a	O
certain	O
portion	O
of	O
tokens	O
are	O
replaced	O
by	O
a	O
special	O
symbol	O
[MASK],	O
and	O
the	O
model	O
is	O
trained	O
to	O
recover	O
the	O
original	O
tokens	O
from	O
the	O
corrupted	O
version.	O
Since	O
density	O
estimation	O
is	O
not	O
part	O
of	O
the	O
objective,	O
BERT	B-MethodName
is	O
allowed	O
to	O
utilize	O
bidirectional	O
contexts	O
for	O
reconstruction.	O
As	O
an	O
immediate	O
benefit,	O
this	O
closes	O
the	O
aforementioned	O
bidirectional	O
information	O
gap	O
in	O
AR	O
language	O
modeling,	O
leading	O
to	O
improved	O
performance.	O
However,	O
the	O
artificial	O
symbols	O
like	O
[MASK]	O
used	O
by	O
BERT	B-MethodName
during	O
pretraining	O
are	O
absent	O
from	O
real	O
data	O
at	O
finetuning	O
time,	O
resulting	O
in	O
a	O
pretrain-finetune	O
discrepancy.	O
Moreover,	O
since	O
the	O
predicted	O
tokens	O
are	O
masked	O
in	O
the	O
input,	O
BERT	B-MethodName
is	O
not	O
able	O
to	O
model	O
the	O
joint	O
probability	O
using	O
the	O
product	O
rule	O
as	O
in	O
AR	O
language	O
modeling.	O
In	O
other	O
words,	O
BERT	B-MethodName
assumes	O
the	O
predicted	O
tokens	O
are	O
independent	O
of	O
each	O
other	O
given	O
the	O
unmasked	O
tokens,	O
which	O
is	O
oversimplified	O
as	O
high-order,	O
long-range	O
dependency	O
is	O
prevalent	O
in	O
natural	O
language	O
.	O
Faced	O
with	O
the	O
pros	O
and	O
cons	O
of	O
existing	O
language	O
pretraining	O
objectives,	O
in	O
this	O
work,	O
we	O
propose	O
XLNet,	B-MethodName
a	O
generalized	O
autoregressive	O
method	O
that	O
leverages	O
the	O
best	O
of	O
both	O
AR	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
AE	B-TaskName
while	O
avoiding	O
their	O
limitations.	O
•	O
Firstly,	O
instead	O
of	O
using	O
a	O
fixed	O
forward	O
or	O
backward	O
factorization	O
order	O
as	O
in	O
conventional	O
AR	O
models,	O
XLNet	B-MethodName
maximizes	O
the	O
expected	O
log	O
likelihood	O
of	O
a	O
sequence	O
w.r.t.	O
all	O
possible	O
permutations	O
of	O
the	O
factorization	O
order.	O
Thanks	O
to	O
the	O
permutation	O
operation,	O
the	O
context	O
for	O
each	O
position	O
can	O
consist	O
of	O
tokens	O
from	O
both	O
left	O
and	O
right.	O
In	O
expectation,	O
each	O
position	O
learns	O
to	O
utilize	O
contextual	O
information	O
from	O
all	O
positions,	O
i.e.,	O
capturing	O
bidirectional	O
context.	O
•	O
Secondly,	O
as	O
a	O
generalized	O
AR	O
language	O
model,	O
XLNet	B-MethodName
does	O
not	O
rely	O
on	O
data	O
corruption.	O
Hence,	O
XLNet	B-MethodName
does	O
not	O
suffer	O
from	O
the	O
pretrain-finetune	O
discrepancy	O
that	O
BERT	B-MethodName
is	O
subject	O
to.	O
Meanwhile,	O
the	O
autoregressive	O
objective	O
also	O
provides	O
a	O
natural	O
way	O
to	O
use	O
the	O
product	O
rule	O
for	O
factorizing	O
the	O
joint	O
probability	O
of	O
the	O
predicted	O
tokens,	O
eliminating	O
the	O
independence	O
assumption	O
made	O
in	O
BERT.	B-MethodName
In	O
addition	O
to	O
a	O
novel	O
pretraining	O
objective,	O
XLNet	B-MethodName
improves	O
architectural	O
designs	O
for	O
pretraining.	O
•	O
Inspired	O
by	O
the	O
latest	O
advancements	O
in	O
AR	O
language	O
modeling,	O
XLNet	B-MethodName
integrates	O
the	O
segment	O
recurrence	O
mechanism	O
and	O
relative	O
encoding	O
scheme	O
of	O
Transformer-XL	O
into	O
pretraining,	O
which	O
empirically	O
improves	O
the	O
performance	O
especially	O
for	O
tasks	O
involving	O
a	O
longer	O
text	O
sequence.	O
•	O
Naively	O
applying	O
a	O
Transformer(-XL)	O
architecture	O
to	O
permutation-based	O
language	O
modeling	O
does	O
not	O
work	O
because	O
the	O
factorization	O
order	O
is	O
arbitrary	O
and	O
the	O
target	O
is	O
ambiguous.	O
As	O
a	O
solution,	O
we	O
propose	O
to	O
reparameterize	O
the	O
Transformer(-XL)	O
network	O
to	O
remove	O
the	O
ambiguity.	O
Empirically,	O
under	O
comparable	O
experiment	O
setting,	O
XLNet	B-MethodName
consistently	O
outperforms	O
BERT	B-MethodName
on	O
a	O
wide	O
spectrum	O
of	O
problems	O
including	O
GLUE	B-DatasetName
language	B-TaskName
understanding	I-TaskName
tasks,	O
reading	B-TaskName
comprehension	I-TaskName
tasks	O
like	O
SQuAD	B-DatasetName
and	O
RACE,	B-DatasetName
text	B-TaskName
classification	I-TaskName
tasks	O
such	O
as	O
Yelp	B-DatasetName
and	O
IMDB,	B-DatasetName
and	O
the	O
ClueWeb09-B	B-DatasetName
document	B-TaskName
ranking	I-TaskName
task.	O

With	O
the	O
capability	O
of	O
modeling	O
bidirectional	O
contexts,	O
denoising	O
autoencoding	O
based	O
pretraining	O
like	O
BERT	B-MethodName
achieves	O
better	O
performance	O
than	O
pretraining	O
approaches	O
based	O
on	O
autoregressive	O
language	O
modeling.	O
However,	O
relying	O
on	O
corrupting	O
the	O
input	O
with	O
masks,	O
BERT	B-MethodName
neglects	O
dependency	O
between	O
the	O
masked	O
positions	O
and	O
suffers	O
from	O
a	O
pretrain-finetune	O
discrepancy.	O
In	O
light	O
of	O
these	O
pros	O
and	O
cons,	O
we	O
propose	O
XLNet,	B-MethodName
a	O
generalized	O
autoregressive	O
pretraining	O
method	O
that	O
(1)	O
enables	O
learning	O
bidirectional	O
contexts	O
by	O
maximizing	O
the	O
expected	O
likelihood	O
over	O
all	O
permutations	O
of	O
the	O
factorization	O
order	O
and	O
(2)	O
overcomes	O
the	O
limitations	O
of	O
BERT	O
thanks	O
to	O
its	O
autoregressive	O
formulation.	O
Furthermore,	O
XLNet	B-MethodName
integrates	O
ideas	O
from	O
Transformer-XL,	O
the	O
state-of-the-art	O
autoregressive	O
model,	O
into	O
pretraining.	O
Empirically,	O
under	O
comparable	O
experiment	O
settings,	O
XLNet	B-MethodName
outperforms	O
BERT	B-MethodName
on	O
20	O
tasks,	O
often	O
by	O
a	O
large	O
margin,	O
including	O
question	B-TaskName
answering,	I-TaskName
natural	B-TaskName
language	I-TaskName
inference,	I-TaskName
sentiment	B-TaskName
analysis,	I-TaskName
and	O
document	B-TaskName
ranking.	I-TaskName
1	O
.	O