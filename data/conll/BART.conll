-DOCSTART- -X- O
We -X- _ O
also -X- _ O
evaluated -X- _ O
performance -X- _ O
on -X- _ O
WMT16 -X- _ B-DatasetName
Romanian-English, -X- _ I-DatasetName
augmented -X- _ O
with -X- _ O
back-translation -X- _ O
data -X- _ O
from -X- _ O
Sennrich -X- _ O
et -X- _ O
al. -X- _ O
(2016). -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
6-layer -X- _ O
transformer -X- _ O
source -X- _ O
encoder -X- _ O
to -X- _ O
map -X- _ O
Romanian -X- _ O
into -X- _ O
a -X- _ O
representation -X- _ O
that -X- _ O
BART -X- _ B-MethodName
is -X- _ O
able -X- _ O
to -X- _ O
de-noise -X- _ O
into -X- _ O
English, -X- _ O
following -X- _ O
the -X- _ O
approach -X- _ O
introduced -X- _ O
in -X- _ O
ยง3.4. -X- _ O
Experiment -X- _ O
results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
6. -X- _ O
We -X- _ O
compare -X- _ O
our -X- _ O
results -X- _ O
against -X- _ O
a -X- _ O
baseline -X- _ O
Transformer -X- _ O
architecture -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017) -X- _ O
with -X- _ O
Transformerlarge -X- _ O
settings -X- _ O
(the -X- _ O
baseline -X- _ O
row). -X- _ O
We -X- _ O
show -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
both -X- _ O
steps -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
in -X- _ O
the -X- _ O
fixed -X- _ O
BART -X- _ B-MethodName
and -X- _ O
tuned -X- _ O
BART -X- _ B-MethodName
rows. -X- _ O
For -X- _ O
each -X- _ O
row -X- _ O
we -X- _ O
experiment -X- _ O
on -X- _ O
the -X- _ O
original -X- _ O
WMT16 -X- _ B-DatasetName
Romanian-English -X- _ I-DatasetName
augmented -X- _ O
with -X- _ O
back-translation -X- _ O
data. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
beam -X- _ B-HyperparameterName
width -X- _ I-HyperparameterName
of -X- _ O
5 -X- _ B-HyperparameterValue
and -X- _ O
a -X- _ O
length -X- _ B-HyperparameterName
penalty -X- _ I-HyperparameterName
of -X- _ O
ฮฑ -X- _ B-HyperparameterValue
= -X- _ I-HyperparameterValue
1. -X- _ I-HyperparameterValue
Preliminary -X- _ O
results -X- _ O
suggested -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
was -X- _ O
less -X- _ O
effective -X- _ O
without -X- _ O
back-translation -X- _ O
data, -X- _ O
and -X- _ O
prone -X- _ O
to -X- _ O
overfitting-future -X- _ O
work -X- _ O
should -X- _ O
explore -X- _ O
additional -X- _ O
regularization -X- _ O
techniques. -X- _ O

We -X- _ O
also -X- _ O
experiment -X- _ O
with -X- _ O
several -X- _ O
text -X- _ O
generation -X- _ O
tasks. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
fine-tuned -X- _ O
as -X- _ O
a -X- _ O
standard -X- _ O
sequence-to-sequence -X- _ O
model -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
text. -X- _ O
During -X- _ O
finetuning -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
label -X- _ O
smoothed -X- _ O
cross -X- _ O
entropy -X- _ O
loss -X- _ O
(Pereyra -X- _ O
et -X- _ O
al., -X- _ O
2017), -X- _ O
with -X- _ O
the -X- _ O
smoothing -X- _ B-HyperparameterName
parameter -X- _ I-HyperparameterName
set -X- _ O
to -X- _ O
0.1. -X- _ B-HyperparameterValue
During -X- _ O
generation, -X- _ O
we -X- _ O
set -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
5, -X- _ B-HyperparameterValue
remove -X- _ O
duplicated -X- _ O
trigrams -X- _ O
in -X- _ O
beam -X- _ O
search, -X- _ O
and -X- _ O
tuned -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
min-len, -X- _ O
max-len, -X- _ O
length -X- _ O
penalty -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
(Fan -X- _ O
et -X- _ O
al., -X- _ O
2017 -X- _ O
Summarization -X- _ O
To -X- _ O
provide -X- _ O
a -X- _ O
comparison -X- _ O
with -X- _ O
the -X- _ O
state-of-the-art -X- _ O
in -X- _ O
summarization, -X- _ O
we -X- _ O
present -X- _ O
results -X- _ O
on -X- _ O
two -X- _ O
summarization -X- _ O
datasets, -X- _ O
CNN/DailyMail -X- _ B-DatasetName
and -X- _ O
XSum, -X- _ B-DatasetName
which -X- _ O
have -X- _ O
distinct -X- _ O
properties. -X- _ O
Summaries -X- _ O
in -X- _ O
the -X- _ O
CNN/DailyMail -X- _ B-DatasetName
tend -X- _ O
to -X- _ O
resemble -X- _ O
source -X- _ O
sentences. -X- _ O
Extractive -X- _ O
models -X- _ O
do -X- _ O
well -X- _ O
here, -X- _ O
and -X- _ O
even -X- _ O
the -X- _ O
baseline -X- _ O
of -X- _ O
the -X- _ O
first-three -X- _ O
source -X- _ O
sentences -X- _ O
is -X- _ O
highly -X- _ O
competitive. -X- _ O
Nevertheless, -X- _ O
BART -X- _ O
outperforms -X- _ O
all -X- _ O
existing -X- _ O
work. -X- _ O
In -X- _ O
contrast, -X- _ O
XSum -X- _ B-MethodName
is -X- _ O
highly -X- _ O
abstractive, -X- _ O
and -X- _ O
extractive -X- _ O
models -X- _ O
perform -X- _ O
poorly. -X- _ O
BART -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
best -X- _ O
previous -X- _ O
work, -X- _ O
which -X- _ O
leverages -X- _ O
BERT, -X- _ B-MethodName
by -X- _ O
roughly -X- _ O
6.0 -X- _ B-MetricValue
points -X- _ I-MetricValue
on -X- _ O
all -X- _ O
ROUGE -X- _ B-MetricName
metrics-representing -X- _ O
a -X- _ O
significant -X- _ O
advance -X- _ O
in -X- _ O
performance -X- _ O
on -X- _ O
this -X- _ O
problem. -X- _ O
Qualitatively, -X- _ O
sample -X- _ O
quality -X- _ O
is -X- _ O
high -X- _ O
(see -X- _ O
ยง6). -X- _ O
Dialogue -X- _ O
We -X- _ O
evaluate -X- _ O
dialogue -X- _ O
response -X- _ O
generation -X- _ O
on -X- _ O
CONVAI2 -X- _ B-DatasetName
(Dinan -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
in -X- _ O
which -X- _ O
agents -X- _ O
must -X- _ O
generate -X- _ O
responses -X- _ O
conditioned -X- _ O
on -X- _ O
both -X- _ O
the -X- _ O
previous -X- _ O
context -X- _ O
and -X- _ O
a -X- _ O
textually-specified -X- _ O
persona. -X- _ O
BART -X- _ B-MethodName
outperforms -X- _ O
previous -X- _ O
work -X- _ O
on -X- _ O
two -X- _ O
automated -X- _ O
metrics. -X- _ O
Abstractive -X- _ B-TaskName
QA -X- _ I-TaskName
We -X- _ O
use -X- _ O
the -X- _ O
recently -X- _ O
proposed -X- _ O
ELI5 -X- _ B-DatasetName
dataset -X- _ O
to -X- _ O
test -X- _ O
the -X- _ O
model's -X- _ O
ability -X- _ O
to -X- _ O
generate -X- _ O
long -X- _ O
freeform -X- _ O
answers. -X- _ O
We -X- _ O
find -X- _ O
BART -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
best -X- _ O
previous -X- _ O
work -X- _ O
by -X- _ O
1.2 -X- _ B-MetricValue
ROUGE-L, -X- _ B-MetricName
but -X- _ O
the -X- _ O
dataset -X- _ O
remains -X- _ O
a -X- _ O
challenging, -X- _ O
because -X- _ O
answers -X- _ O
are -X- _ O
only -X- _ O
weakly -X- _ O
specified -X- _ O
by -X- _ O
the -X- _ O
question. -X- _ O

Table -X- _ O
2 -X- _ O
compares -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
BART -X- _ B-MethodName
with -X- _ O
several -X- _ O
recent -X- _ O
approaches -X- _ O
on -X- _ O
the -X- _ O
well-studied -X- _ O
SQuAD -X- _ B-TaskName
and -X- _ O
GLUE -X- _ B-TaskName
tasks -X- _ O
(Warstadt -X- _ O
et -X- _ O
al., -X- _ O
2018;Socher -X- _ O
et -X- _ O
al., -X- _ O
2013;Dolan -X- _ O
& -X- _ O
Brockett, -X- _ O
2005;Agirre -X- _ O
et -X- _ O
al., -X- _ O
2007;Williams -X- _ O
et -X- _ O
al., -X- _ O
2018;Dagan -X- _ O
et -X- _ O
al., -X- _ O
2006;Levesque -X- _ O
et -X- _ O
al., -X- _ O
2011). -X- _ O
The -X- _ O
most -X- _ O
directly -X- _ O
comparable -X- _ O
baseline -X- _ O
is -X- _ O
RoBERTa, -X- _ B-MethodName
which -X- _ O
was -X- _ O
pre-trained -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
resources, -X- _ O
but -X- _ O
a -X- _ O
different -X- _ O
objective. -X- _ O
Overall, -X- _ O
BART -X- _ B-MethodName
performs -X- _ O
similarly, -X- _ O
with -X- _ O
only -X- _ O
small -X- _ O
differences -X- _ O
between -X- _ O
the -X- _ O
models -X- _ O
on -X- _ O
most -X- _ O
tasks. -X- _ O
suggesting -X- _ O
that -X- _ O
BART's -X- _ B-MethodName
improvements -X- _ O
on -X- _ O
generation -X- _ O
tasks -X- _ O
do -X- _ O
not -X- _ O
come -X- _ O
at -X- _ O
the -X- _ O
expense -X- _ O
of -X- _ O
classification -X- _ O
performance. -X- _ O

We -X- _ O
pre-train -X- _ O
a -X- _ O
large -X- _ O
model -X- _ O
with -X- _ O
12 -X- _ O
layers -X- _ O
in -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder, -X- _ O
and -X- _ O
a -X- _ O
hidden -X- _ O
size -X- _ O
of -X- _ O
1024. -X- _ O
Following -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
8000, -X- _ B-HyperparameterValue
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
500000 -X- _ B-HyperparameterValue
steps. -X- _ B-HyperparameterName
Documents -X- _ O
are -X- _ O
tokenized -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
byte-pair -X- _ O
encoding -X- _ O
as -X- _ O
GPT-2 -X- _ B-MethodName
(Radford -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Section -X- _ O
ยง4, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
combination -X- _ O
of -X- _ O
text -X- _ O
infilling -X- _ O
and -X- _ O
sentence -X- _ O
permutation. -X- _ O
We -X- _ O
mask -X- _ O
30% -X- _ B-MetricValue
of -X- _ O
tokens -X- _ O
in -X- _ O
each -X- _ O
document, -X- _ O
and -X- _ O
permute -X- _ O
all -X- _ O
sentences. -X- _ O
Although -X- _ O
sentence -X- _ O
permutation -X- _ O
only -X- _ O
shows -X- _ O
significant -X- _ O
additive -X- _ O
gains -X- _ O
on -X- _ O
the -X- _ O
CNN/DM -X- _ B-DatasetName
summarization -X- _ I-DatasetName
dataset, -X- _ O
we -X- _ O
hypothesised -X- _ O
that -X- _ O
larger -X- _ O
pre-trained -X- _ O
models -X- _ O
may -X- _ O
be -X- _ O
better -X- _ O
able -X- _ O
to -X- _ O
learn -X- _ O
from -X- _ O
this -X- _ O
task. -X- _ O
To -X- _ O
help -X- _ O
the -X- _ O
model -X- _ O
better -X- _ O
fit -X- _ O
the -X- _ O
data, -X- _ O
we -X- _ O
disabled -X- _ O
dropout -X- _ B-HyperparameterName
for -X- _ O
the -X- _ O
final -X- _ O
10% -X- _ B-MetricValue
of -X- _ O
training -X- _ O
steps. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
pre-training -X- _ O
data -X- _ O
as -X- _ O
, -X- _ O
consisting -X- _ O
of -X- _ O
160Gb -X- _ O
of -X- _ O
news, -X- _ O
books, -X- _ O
stories, -X- _ O
and -X- _ O
web -X- _ O
text. -X- _ O

Recent -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
downstream -X- _ O
performance -X- _ O
can -X- _ O
dramatically -X- _ O
improve -X- _ O
when -X- _ O
pre-training -X- _ O
is -X- _ O
scaled -X- _ O
to -X- _ O
large -X- _ O
batch -X- _ O
sizes -X- _ O
(Yang -X- _ O
et -X- _ O
al., -X- _ O
2019; -X- _ O
and -X- _ O
corpora. -X- _ O
To -X- _ O
test -X- _ O
how -X- _ O
well -X- _ O
BART -X- _ B-MethodName
performs -X- _ O
in -X- _ O
this -X- _ O
regime, -X- _ O
and -X- _ O
to -X- _ O
create -X- _ O
a -X- _ O
useful -X- _ O
model -X- _ O
for -X- _ O
downstream -X- _ O
tasks, -X- _ O
we -X- _ O
trained -X- _ O
BART -X- _ B-MethodName
using -X- _ O
the -X- _ O
same -X- _ O
scale -X- _ O
as -X- _ O
the -X- _ O
RoBERTa -X- _ B-MethodName
model. -X- _ O

BART -X- _ B-MethodName
shows -X- _ O
large -X- _ O
improvements -X- _ O
on -X- _ O
summarization -X- _ O
metrics, -X- _ O
of -X- _ O
up -X- _ O
to -X- _ O
6 -X- _ O
points -X- _ O
over -X- _ O
the -X- _ O
prior -X- _ O
state-of-the-art. -X- _ O
To -X- _ O
understand -X- _ O
BART's -X- _ B-MethodName
performance -X- _ O
beyond -X- _ O
automated -X- _ O
metrics, -X- _ O
we -X- _ O
analyse -X- _ O
its -X- _ O
generations -X- _ O
qualitatively. -X- _ O
Table -X- _ O
7 -X- _ O
shows -X- _ O
example -X- _ O
summaries -X- _ O
generated -X- _ O
by -X- _ O
BART. -X- _ B-MethodName
Examples -X- _ O
are -X- _ O
taken -X- _ O
from -X- _ O
WikiNews -X- _ B-DatasetName
articles -X- _ I-DatasetName
published -X- _ O
after -X- _ O
the -X- _ O
creation -X- _ O
of -X- _ O
the -X- _ O
pre-training -X- _ O
corpus, -X- _ O
to -X- _ O
eliminate -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
the -X- _ O
events -X- _ O
described -X- _ O
being -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
model's -X- _ O
training -X- _ O
data. -X- _ O
Following -X- _ O
Narayan -X- _ O
et -X- _ O
al. -X- _ O
(2018), -X- _ O
we -X- _ O
remove -X- _ O
the -X- _ O
first -X- _ O
sentence -X- _ O
of -X- _ O
the -X- _ O
article -X- _ O
prior -X- _ O
to -X- _ O
summarizing -X- _ O
it, -X- _ O
so -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
easy -X- _ O
extractive -X- _ O
summary -X- _ O
of -X- _ O
the -X- _ O
document. -X- _ O
Unsurprisingly, -X- _ O
model -X- _ O
output -X- _ O
is -X- _ O
fluent -X- _ O
and -X- _ O
grammatical -X- _ O
English. -X- _ O
However, -X- _ O
model -X- _ O
output -X- _ O
is -X- _ O
also -X- _ O
highly -X- _ O
abstractive, -X- _ O
with -X- _ O
few -X- _ O
phrases -X- _ O
copied -X- _ O
from -X- _ O
the -X- _ O
input. -X- _ O
The -X- _ O
output -X- _ O
is -X- _ O
also -X- _ O
generally -X- _ O
factually -X- _ O
accurate, -X- _ O
and -X- _ O
integrates -X- _ O
supporting -X- _ O
evidence -X- _ O
from -X- _ O
across -X- _ O
the -X- _ O
input -X- _ O
document -X- _ O
with -X- _ O
background -X- _ O
knowledge -X- _ O
(for -X- _ O
example, -X- _ O
correctly -X- _ O
completing -X- _ O
names, -X- _ O
or -X- _ O
inferring -X- _ O
that -X- _ O
PG&E -X- _ O
operates -X- _ O
in -X- _ O
California). -X- _ O
In -X- _ O
the -X- _ O
first -X- _ O
example, -X- _ O
inferring -X- _ O
that -X- _ O
fish -X- _ O
are -X- _ O
protecting -X- _ O
reefs -X- _ O
from -X- _ O
global -X- _ O
warming -X- _ O
requires -X- _ O
non-trivial -X- _ O
inference -X- _ O
from -X- _ O
the -X- _ O
text. -X- _ O
However, -X- _ O
the -X- _ O
claim -X- _ O
that -X- _ O
the -X- _ O
work -X- _ O
was -X- _ O
published -X- _ O
in -X- _ O
Science -X- _ O
is -X- _ O
not -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
source. -X- _ O
These -X- _ O
samples -X- _ O
demonstrate -X- _ O
that -X- _ O
the -X- _ O
BART -X- _ B-MethodName
pretraining -X- _ O
has -X- _ O
learned -X- _ O
a -X- _ O
strong -X- _ O
combination -X- _ O
of -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
and -X- _ I-TaskName
generation. -X- _ I-TaskName

Early -X- _ O
methods -X- _ O
for -X- _ O
pretraining -X- _ O
were -X- _ O
based -X- _ O
on -X- _ O
language -X- _ O
models. -X- _ O
GPT -X- _ B-MethodName
(Radford -X- _ O
et -X- _ O
al., -X- _ O
2018) -X- _ O
only -X- _ O
models -X- _ O
leftward -X- _ O
context, -X- _ O
which -X- _ O
is -X- _ O
problematic -X- _ O
for -X- _ O
some -X- _ O
tasks. -X- _ O
ELMo -X- _ B-MethodName
(Peters -X- _ O
et -X- _ O
al., -X- _ O
2018) -X- _ O
concatenates -X- _ O
left-only -X- _ O
and -X- _ O
right-only -X- _ O
representations, -X- _ O
but -X- _ O
does -X- _ O
not -X- _ O
pre-train -X- _ O
interactions -X- _ O
between -X- _ O
these -X- _ O
features. -X- _ O
Radford -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
demonstrated -X- _ O
that -X- _ O
very -X- _ O
large -X- _ O
language -X- _ O
models -X- _ O
can -X- _ O
act -X- _ O
as -X- _ O
unsupervised -X- _ O
multitask -X- _ O
models. -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
introduced -X- _ O
masked -X- _ O
language -X- _ O
modelling, -X- _ O
which -X- _ O
allows -X- _ O
pre-training -X- _ O
to -X- _ O
learn -X- _ O
interactions -X- _ O
between -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
context -X- _ O
words. -X- _ O
Recent -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
very -X- _ O
strong -X- _ O
performance -X- _ O
can -X- _ O
be -X- _ O
achieved -X- _ O
by -X- _ O
training -X- _ O
for -X- _ O
longer -X- _ O
, -X- _ O
by -X- _ O
tying -X- _ O
parameters -X- _ O
across -X- _ O
layers -X- _ O
(Lan -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
and -X- _ O
by -X- _ O
masking -X- _ O
spans -X- _ O
instead -X- _ O
of -X- _ O
words -X- _ O
. -X- _ O
Predictions -X- _ O
are -X- _ O
not -X- _ O
made -X- _ O
auto-regressively, -X- _ O
reducing -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
for -X- _ O
generation -X- _ O
tasks. -X- _ O
UniLM -X- _ B-MethodName
(Dong -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
fine-tunes -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
an -X- _ O
ensemble -X- _ O
of -X- _ O
masks, -X- _ O
some -X- _ O
of -X- _ O
which -X- _ O
allow -X- _ O
only -X- _ O
leftward -X- _ O
context. -X- _ O
Like -X- _ O
BART, -X- _ B-MethodName
this -X- _ O
allows -X- _ O
UniLM -X- _ B-MethodName
to -X- _ O
be -X- _ O
used -X- _ O
for -X- _ O
both -X- _ O
generative -X- _ O
and -X- _ O
discriminative -X- _ O
tasks. -X- _ O
A -X- _ O
difference -X- _ O
is -X- _ O
that -X- _ O
UniLM -X- _ B-MethodName
predictions -X- _ O
are -X- _ O
conditionally -X- _ O
independent, -X- _ O
whereas -X- _ O
BART's -X- _ B-MethodName
are -X- _ O
autoregressive. -X- _ O
BART -X- _ B-MethodName
reduces -X- _ O
the -X- _ O
mismatch -X- _ O
between -X- _ O
pre-training -X- _ O
and -X- _ O
generation -X- _ O
tasks, -X- _ O
because -X- _ O
the -X- _ O
decoder -X- _ O
is -X- _ O
always -X- _ O
trained -X- _ O
on -X- _ O
uncorrupted -X- _ O
context. -X- _ O
MASS -X- _ B-MethodName
(Song -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
is -X- _ O
perhaps -X- _ O
the -X- _ O
most -X- _ O
similar -X- _ O
model -X- _ O
to -X- _ O
BART. -X- _ B-MethodName
An -X- _ O
input -X- _ O
sequence -X- _ O
where -X- _ O
a -X- _ O
contiguous -X- _ O
span -X- _ O
of -X- _ O
tokens -X- _ O
is -X- _ O
masked -X- _ O
is -X- _ O
mapped -X- _ O
to -X- _ O
a -X- _ O
sequence -X- _ O
consisting -X- _ O
of -X- _ O
the -X- _ O
missing -X- _ O
tokens. -X- _ O
MASS -X- _ B-MethodName
is -X- _ O
less -X- _ O
effective -X- _ O
for -X- _ O
discriminative -X- _ O
tasks, -X- _ O
because -X- _ O
disjoint -X- _ O
sets -X- _ O
of -X- _ O
tokens -X- _ O
are -X- _ O
fed -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder. -X- _ O
XL-Net -X- _ B-MethodName
(Yang -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
extends -X- _ O
BERT -X- _ B-MethodName
by -X- _ O
pre-Source -X- _ O
Document -X- _ O
(abbreviated) -X- _ O

The -X- _ O
researchers -X- _ O
examined -X- _ O
three -X- _ O
types -X- _ O
of -X- _ O
coral -X- _ O
in -X- _ O
reefs -X- _ O
off -X- _ O
the -X- _ O
coast -X- _ O
of -X- _ O
Fiji -X- _ O
... -X- _ O
The -X- _ O
researchers -X- _ O
found -X- _ O
when -X- _ O
fish -X- _ O
were -X- _ O
plentiful, -X- _ O
they -X- _ O
would -X- _ O
eat -X- _ O
algae -X- _ O
and -X- _ O
seaweed -X- _ O
off -X- _ O
the -X- _ O
corals, -X- _ O
which -X- _ O
appeared -X- _ O
to -X- _ O
leave -X- _ O
them -X- _ O
more -X- _ O
resistant -X- _ O
to -X- _ O
the -X- _ O
bacterium -X- _ O
Vibrio -X- _ O
coralliilyticus, -X- _ O
a -X- _ O
bacterium -X- _ O
associated -X- _ O
with -X- _ O
bleaching. -X- _ O
The -X- _ O
researchers -X- _ O
suggested -X- _ O
the -X- _ O
algae, -X- _ O
like -X- _ O
warming -X- _ O
temperatures, -X- _ O
might -X- _ O
render -X- _ O
the -X- _ O
corals' -X- _ O
chemical -X- _ O
defenses -X- _ O
less -X- _ O
effective, -X- _ O
and -X- _ O
the -X- _ O
fish -X- _ O
were -X- _ O
protecting -X- _ O
the -X- _ O
coral -X- _ O
by -X- _ O
removing -X- _ O
the -X- _ O
algae. -X- _ O
Fisheries -X- _ O
off -X- _ O
the -X- _ O
coast -X- _ O
of -X- _ O
Fiji -X- _ O
are -X- _ O
protecting -X- _ O
coral -X- _ O
reefs -X- _ O
from -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
global -X- _ O
warming, -X- _ O
according -X- _ O
to -X- _ O
a -X- _ O
study -X- _ O
in -X- _ O
the -X- _ O
journal -X- _ O
Science. -X- _ O
Sacoolas, -X- _ O
who -X- _ O
has -X- _ O
immunity -X- _ O
as -X- _ O
a -X- _ O
diplomat's -X- _ O
wife, -X- _ O
was -X- _ O
involved -X- _ O
in -X- _ O
a -X- _ O
traffic -X- _ O
collision -X- _ O
... -X- _ O
Prime -X- _ O
Minister -X- _ O
Johnson -X- _ O
was -X- _ O
questioned -X- _ O
about -X- _ O
the -X- _ O
case -X- _ O
while -X- _ O
speaking -X- _ O
to -X- _ O
the -X- _ O
press -X- _ O
at -X- _ O
a -X- _ O
hospital -X- _ O
in -X- _ O
Watford. -X- _ O
He -X- _ O
said, -X- _ O
"I -X- _ O
hope -X- _ O
that -X- _ O
Anne -X- _ O
Sacoolas -X- _ O
will -X- _ O
come -X- _ O
back -X- _ O
... -X- _ O
if -X- _ O
we -X- _ O
can't -X- _ O
resolve -X- _ O
it -X- _ O
then -X- _ O
of -X- _ O
course -X- _ O
I -X- _ O
will -X- _ O
be -X- _ O
raising -X- _ O
it -X- _ O
myself -X- _ O
personally -X- _ O
with -X- _ O
the -X- _ O
White -X- _ O
House." -X- _ O
Boris -X- _ O
Johnson -X- _ O
has -X- _ O
said -X- _ O
he -X- _ O
will -X- _ O
raise -X- _ O
the -X- _ O
issue -X- _ O
of -X- _ O
US -X- _ O
diplomat -X- _ O
Anne -X- _ O
Sacoolas' -X- _ O
diplomatic -X- _ O
immunity -X- _ O
with -X- _ O
the -X- _ O
White -X- _ O
House. -X- _ O
PG&E -X- _ O
stated -X- _ O
it -X- _ O
scheduled -X- _ O
the -X- _ O
blackouts -X- _ O
in -X- _ O
response -X- _ O
to -X- _ O
forecasts -X- _ O
for -X- _ O
high -X- _ O
winds -X- _ O
amid -X- _ O
dry -X- _ O
conditions. -X- _ O
The -X- _ O
aim -X- _ O
is -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
risk -X- _ O
of -X- _ O
wildfires. -X- _ O
Nearly -X- _ O
800 -X- _ O
thousand -X- _ O
customers -X- _ O
were -X- _ O
scheduled -X- _ O
to -X- _ O
be -X- _ O
affected -X- _ O
by -X- _ O
the -X- _ O
shutoffs -X- _ O
which -X- _ O
were -X- _ O
expected -X- _ O
to -X- _ O
last -X- _ O
through -X- _ O
at -X- _ O
least -X- _ O
midday -X- _ O
tomorrow. -X- _ O
Power -X- _ O
has -X- _ O
been -X- _ O
turned -X- _ O
off -X- _ O
to -X- _ O
millions -X- _ O
of -X- _ O
customers -X- _ O
in -X- _ O
California -X- _ O
as -X- _ O
part -X- _ O
of -X- _ O
a -X- _ O
power -X- _ O
shutoff -X- _ O
plan. -X- _ O
dicting -X- _ O
masked -X- _ O
tokens -X- _ O
auto-regressively -X- _ O
in -X- _ O
a -X- _ O
permuted -X- _ O
order. -X- _ O
This -X- _ O
objective -X- _ O
allows -X- _ O
predictions -X- _ O
to -X- _ O
condition -X- _ O
on -X- _ O
both -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
context. -X- _ O
In -X- _ O
contrast, -X- _ O
the -X- _ O
BART -X- _ B-MethodName
decoder -X- _ O
works -X- _ O
left-to-right -X- _ O
during -X- _ O
pre-training, -X- _ O
matching -X- _ O
the -X- _ O
setting -X- _ O
during -X- _ O
generation. -X- _ O
Several -X- _ O
papers -X- _ O
have -X- _ O
explored -X- _ O
using -X- _ O
pre-trained -X- _ O
representations -X- _ O
to -X- _ O
improve -X- _ O
machine -X- _ B-TaskName
translation. -X- _ I-TaskName
The -X- _ O
largest -X- _ O
improvements -X- _ O
have -X- _ O
come -X- _ O
from -X- _ O
pre-training -X- _ O
on -X- _ O
both -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
languages -X- _ O
(Song -X- _ O
et -X- _ O
al., -X- _ O
2019;Lample -X- _ O
& -X- _ O
Conneau, -X- _ O
2019), -X- _ O
but -X- _ O
this -X- _ O
requires -X- _ O
pretraining -X- _ O
on -X- _ O
all -X- _ O
languages -X- _ O
of -X- _ O
interest. -X- _ O
Other -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
encoders -X- _ O
can -X- _ O
be -X- _ O
improved -X- _ O
using -X- _ O
pre-trained -X- _ O
representations -X- _ O
(Edunov -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
but -X- _ O
gains -X- _ O
in -X- _ O
decoders -X- _ O
are -X- _ O
more -X- _ O
limited. -X- _ O
We -X- _ O
show -X- _ O
how -X- _ O
BART -X- _ B-MethodName
can -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
improve -X- _ O
machine -X- _ O
translation -X- _ O
decoders. -X- _ O

We -X- _ O
introduced -X- _ O
BART, -X- _ B-MethodName
a -X- _ O
pre-training -X- _ O
approach -X- _ O
that -X- _ O
learns -X- _ O
to -X- _ O
map -X- _ O
corrupted -X- _ O
documents -X- _ O
to -X- _ O
the -X- _ O
original. -X- _ O
BART -X- _ B-MethodName
achieves -X- _ O
similar -X- _ O
performance -X- _ O
to -X- _ O
RoBERTa -X- _ B-MethodName
on -X- _ O
discriminative -X- _ O
tasks, -X- _ O
while -X- _ O
achieving -X- _ O
new -X- _ O
state-of-theart -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
tasks. -X- _ O
Future -X- _ O
work -X- _ O
should -X- _ O
explore -X- _ O
new -X- _ O
methods -X- _ O
for -X- _ O
corrupting -X- _ O
documents -X- _ O
for -X- _ O
pre-training, -X- _ O
perhaps -X- _ O
tailoring -X- _ O
them -X- _ O
to -X- _ O
specific -X- _ O
end -X- _ O
tasks. -X- _ O

The -X- _ O
Masked -X- _ O
Language -X- _ O
Model -X- _ O
and -X- _ O
the -X- _ O
Permuted -X- _ O
Language -X- _ O
Model -X- _ O
perform -X- _ O
less -X- _ O
well -X- _ O
than -X- _ O
others -X- _ O
on -X- _ O
generation, -X- _ O
and -X- _ O
are -X- _ O
the -X- _ O
only -X- _ O
models -X- _ O
we -X- _ O
consider -X- _ O
that -X- _ O
do -X- _ O
not -X- _ O
include -X- _ O
left-to-right -X- _ O
auto-regressive -X- _ O
language -X- _ O
modelling -X- _ O
during -X- _ O
pre-training. -X- _ O
Bidirectional -X- _ O
encoders -X- _ O
are -X- _ O
crucial -X- _ O
for -X- _ O
SQuAD -X- _ B-DatasetName
As -X- _ O
noted -X- _ O
in -X- _ O
previous -X- _ O
work -X- _ O
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
just -X- _ O
left-to-right -X- _ O
decoder -X- _ O
performs -X- _ O
poorly -X- _ O
on -X- _ O
SQuAD, -X- _ B-DatasetName
because -X- _ O
future -X- _ O
context -X- _ O
is -X- _ O
crucial -X- _ O
in -X- _ O
classification -X- _ O
decisions. -X- _ O
However, -X- _ O
BART -X- _ B-MethodName
achieves -X- _ O
similar -X- _ O
performance -X- _ O
with -X- _ O
only -X- _ O
half -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
bidirectional -X- _ O
layers. -X- _ O
The -X- _ O
pre-training -X- _ O
objective -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
only -X- _ O
important -X- _ O
factor -X- _ O
Our -X- _ O
Permuted -X- _ O
Language -X- _ O
Model -X- _ O
performs -X- _ O
less -X- _ O
well -X- _ O
than -X- _ O
XLNet -X- _ B-MethodName
(Yang -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
Some -X- _ O
of -X- _ O
this -X- _ O
difference -X- _ O
is -X- _ O
likely -X- _ O
due -X- _ O
to -X- _ O
not -X- _ O
including -X- _ O
other -X- _ O
architectural -X- _ O
improvements, -X- _ O
such -X- _ O
as -X- _ O
relative-position -X- _ O
embeddings -X- _ O
or -X- _ O
segment-level -X- _ O
recurrence. -X- _ O
Pure -X- _ O
language -X- _ O
models -X- _ O
perform -X- _ O
best -X- _ O
on -X- _ O
ELI5 -X- _ B-DatasetName
The -X- _ O
ELI5 -X- _ B-DatasetName
dataset -X- _ O
is -X- _ O
an -X- _ O
outlier, -X- _ O
with -X- _ O
much -X- _ O
higher -X- _ O
perplexities -X- _ B-MetricName
than -X- _ O
other -X- _ O
tasks, -X- _ O
and -X- _ O
is -X- _ O
the -X- _ O
only -X- _ O
generation -X- _ O
task -X- _ O
where -X- _ O
other -X- _ O
models -X- _ O
outperform -X- _ O
BART. -X- _ B-MethodName
A -X- _ O
pure -X- _ O
language -X- _ O
model -X- _ O
performs -X- _ O
best, -X- _ O
suggesting -X- _ O
that -X- _ O
BART -X- _ B-MethodName
is -X- _ O
less -X- _ O
effective -X- _ O
when -X- _ O
the -X- _ O
output -X- _ O
is -X- _ O
only -X- _ O
loosely -X- _ O
constrained -X- _ O
by -X- _ O
the -X- _ O
input. -X- _ O
BART -X- _ B-MethodName
achieves -X- _ O
the -X- _ O
most -X- _ O
consistently -X- _ O
strong -X- _ O
performance. -X- _ O
With -X- _ O
the -X- _ O
exception -X- _ O
of -X- _ O
ELI5, -X- _ B-DatasetName
BART -X- _ B-MethodName
models -X- _ O
using -X- _ O
text-infilling -X- _ O
perform -X- _ O
well -X- _ O
on -X- _ O
all -X- _ O
tasks. -X- _ O

SQuAD -X- _ B-MethodName
(Rajpurkar -X- _ O
et -X- _ O
al., -X- _ O
2016)a -X- _ O
an -X- _ O
extractive -X- _ O
question -X- _ O
answering -X- _ O
task -X- _ O
on -X- _ O
Wikipedia -X- _ O
paragraphs. -X- _ O
Answers -X- _ O
are -X- _ O
text -X- _ O
spans -X- _ O
extracted -X- _ O
from -X- _ O
a -X- _ O
given -X- _ O
document -X- _ O
context. -X- _ O
Similar -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
we -X- _ O
use -X- _ O
concatenated -X- _ O
question -X- _ O
and -X- _ O
context -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
of -X- _ O
BART, -X- _ B-MethodName
and -X- _ O
additionally -X- _ O
pass -X- _ O
them -X- _ O
to -X- _ O
the -X- _ O
decoder. -X- _ O
The -X- _ O
model -X- _ O
includes -X- _ O
classifiers -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
start -X- _ O
and -X- _ O
end -X- _ O
indices -X- _ O
of -X- _ O
each -X- _ O
token. -X- _ O
MNLI -X- _ B-DatasetName
(Williams -X- _ O
et -X- _ O
al., -X- _ O
2017), -X- _ O
a -X- _ O
bitext -X- _ B-TaskName
classification -X- _ I-TaskName
task -X- _ I-TaskName
to -X- _ O
predict -X- _ O
whether -X- _ O
one -X- _ O
sentence -X- _ O
entails -X- _ O
another. -X- _ O
The -X- _ O
fine-tuned -X- _ O
model -X- _ O
concatenates -X- _ O
the -X- _ O
two -X- _ O
sentences -X- _ O
with -X- _ O
appended -X- _ O
an -X- _ O
EOS -X- _ O
token, -X- _ O
and -X- _ O
passes -X- _ O
them -X- _ O
to -X- _ O
both -X- _ O
the -X- _ O
BART -X- _ B-MethodName
encoder -X- _ O
and -X- _ O
decoder. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
BERT, -X- _ B-MethodName
the -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
EOS -X- _ O
token -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
classify -X- _ O
the -X- _ O
sentences -X- _ O
relations. -X- _ O
ELI5 -X- _ B-DatasetName
(Fan -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
a -X- _ O
long-form -X- _ O
abstractive -X- _ B-TaskName
question -X- _ I-TaskName
answering -X- _ I-TaskName
dataset. -X- _ O
Models -X- _ O
generate -X- _ O
answers -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
a -X- _ O
question -X- _ O
and -X- _ O
supporting -X- _ O
documents. -X- _ O
XSum -X- _ B-DatasetName
(Narayan -X- _ O
et -X- _ O
al., -X- _ O
2018), -X- _ O
a -X- _ O
news -X- _ O
summarization -X- _ O
dataset -X- _ O
with -X- _ O
highly -X- _ O
abstractive -X- _ O
summaries. -X- _ O
ConvAI2 -X- _ B-DatasetName
(Dinan -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
a -X- _ O
dialogue -X- _ O
response -X- _ O
generation -X- _ O
task, -X- _ O
conditioned -X- _ O
on -X- _ O
context -X- _ O
and -X- _ O
a -X- _ O
persona. -X- _ O
(Hermann -X- _ O
et -X- _ O
al., -X- _ O
2015), -X- _ O
a -X- _ O
news -X- _ O
summarization -X- _ O
dataset. -X- _ O
Summaries -X- _ O
here -X- _ O
are -X- _ O
typically -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
source -X- _ O
sentences. -X- _ O

While -X- _ O
many -X- _ O
pre-training -X- _ O
objectives -X- _ O
have -X- _ O
been -X- _ O
proposed, -X- _ O
fair -X- _ O
comparisons -X- _ O
between -X- _ O
these -X- _ O
have -X- _ O
been -X- _ O
difficult -X- _ O
to -X- _ O
perform, -X- _ O
at -X- _ O
least -X- _ O
in -X- _ O
part -X- _ O
due -X- _ O
to -X- _ O
differences -X- _ O
in -X- _ O
training -X- _ O
data, -X- _ O
training -X- _ O
resources, -X- _ O
architectural -X- _ O
differences -X- _ O
between -X- _ O
models, -X- _ O
and -X- _ O
fine-tuning -X- _ O
procedures. -X- _ O
We -X- _ O
re-implement -X- _ O
strong -X- _ O
pre-training -X- _ O
approaches -X- _ O
recently -X- _ O
proposed -X- _ O
for -X- _ O
discriminative -X- _ B-TaskName
and -X- _ I-TaskName
generation -X- _ I-TaskName
tasks. -X- _ I-TaskName
We -X- _ O
aim, -X- _ O
as -X- _ O
much -X- _ O
as -X- _ O
possible, -X- _ O
to -X- _ O
control -X- _ O
for -X- _ O
differences -X- _ O
unrelated -X- _ O
to -X- _ O
the -X- _ O
pre-training -X- _ O
objective. -X- _ O
However, -X- _ O
we -X- _ O
do -X- _ O
make -X- _ O
minor -X- _ O
changes -X- _ O
to -X- _ O
the -X- _ O
learning -X- _ O
rate -X- _ O
and -X- _ O
usage -X- _ O
of -X- _ O
layer -X- _ O
normalisation -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
improve -X- _ O
performance -X- _ O
(tuning -X- _ O
these -X- _ O
separately -X- _ O
for -X- _ O
each -X- _ O
objective). -X- _ O
For -X- _ O
reference, -X- _ O
we -X- _ O
compare -X- _ O
our -X- _ O
implementations -X- _ O
with -X- _ O
published -X- _ O
numbers -X- _ O
from -X- _ O
BERT, -X- _ B-MethodName
which -X- _ O
was -X- _ O
also -X- _ O
trained -X- _ O
for -X- _ O
1M -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
on -X- _ O
a -X- _ O
combination -X- _ O
of -X- _ O
books -X- _ O
and -X- _ O
Wikipedia -X- _ O
data. -X- _ O
We -X- _ O
compare -X- _ O
the -X- _ O
following -X- _ O
approaches: -X- _ O
Language -X- _ O
Model -X- _ O
Similarly -X- _ O
to -X- _ O
GPT -X- _ B-MethodName
(Radford -X- _ O
et -X- _ O
al., -X- _ O
2018), -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
left-to-right -X- _ O
Transformer -X- _ O
language -X- _ O
model. -X- _ O
This -X- _ O
model -X- _ O
is -X- _ O
equivalent -X- _ O
to -X- _ O
the -X- _ O
BART -X- _ B-MethodName
decoder, -X- _ O
without -X- _ O
cross-attention. -X- _ O
Permuted -X- _ O
Language -X- _ O
Model -X- _ O
Based -X- _ O
on -X- _ O
XLNet -X- _ B-MethodName
(Yang -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
we -X- _ O
sample -X- _ O
1/6 -X- _ O
of -X- _ O
the -X- _ O
tokens, -X- _ O
and -X- _ O
generate -X- _ O
them -X- _ O
in -X- _ O
a -X- _ O
random -X- _ O
order -X- _ O
autoregressively. -X- _ O
For -X- _ O
consistency -X- _ O
with -X- _ O
other -X- _ O
models, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
implement -X- _ O
the -X- _ O
relative -X- _ O
positional -X- _ O
embeddings -X- _ O
or -X- _ O
attention -X- _ O
across -X- _ O
segments -X- _ O
from -X- _ O
XLNet. -X- _ B-MethodName
Masked -X- _ O
Language -X- _ O
Model -X- _ O
Following -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
we -X- _ O
replace -X- _ O
15% -X- _ B-MetricValue
of -X- _ O
tokens -X- _ O
with -X- _ O
[MASK] -X- _ O
symbols, -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
independently -X- _ O
predict -X- _ O
the -X- _ O
original -X- _ O
tokens. -X- _ O
Multitask -X- _ O
Masked -X- _ O
Language -X- _ O
Model -X- _ O
As -X- _ O
in -X- _ O
UniLM -X- _ B-MethodName
(Dong -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
Masked -X- _ O
Language -X- _ O
Model -X- _ O
with -X- _ O
additional -X- _ O
self-attention -X- _ O
masks. -X- _ O
Self -X- _ O
attention -X- _ O
masks -X- _ O
are -X- _ O
chosen -X- _ O
randomly -X- _ O
in -X- _ O
with -X- _ O
the -X- _ O
follow -X- _ O
proportions: -X- _ O
1/6 -X- _ O
left-to-right, -X- _ O
1/6 -X- _ O
right-to-left, -X- _ O
1/3 -X- _ O
unmasked, -X- _ O
and -X- _ O
1/3 -X- _ O
with -X- _ O
the -X- _ O
first -X- _ O
50% -X- _ B-MetricValue
of -X- _ O
tokens -X- _ O
unmasked -X- _ O
and -X- _ O
a -X- _ O
left-to-right -X- _ O
mask -X- _ O
for -X- _ O
the -X- _ O
remainder. -X- _ O
Masked -X- _ B-MethodName
Seq-to-Seq -X- _ I-MethodName
Inspired -X- _ I-MethodName
by -X- _ I-MethodName
MASS -X- _ I-MethodName
(Song -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
we -X- _ O
mask -X- _ O
a -X- _ O
span -X- _ O
containing -X- _ O
50% -X- _ B-MetricValue
of -X- _ O
tokens, -X- _ O
and -X- _ O
train -X- _ O
a -X- _ O
sequence -X- _ O
to -X- _ O
sequence -X- _ O
model -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
masked -X- _ O
tokens. -X- _ O
For -X- _ O
the -X- _ O
Permuted -X- _ O
LM, -X- _ O
Masked -X- _ B-MethodName
LM -X- _ I-MethodName
and -X- _ O
Multitask -X- _ B-MethodName
Masked -X- _ I-MethodName
LM, -X- _ I-MethodName
we -X- _ O
use -X- _ O
two-stream -X- _ O
attention -X- _ O
(Yang -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
to -X- _ O
efficiently -X- _ O
compute -X- _ O
likelihoods -X- _ O
of -X- _ O
the -X- _ O
output -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
(using -X- _ O
a -X- _ O
diagonal -X- _ O
self-attention -X- _ O
mask -X- _ O
on -X- _ O
the -X- _ O
output -X- _ O
to -X- _ O
predict -X- _ O
words -X- _ O
left-to-right). -X- _ O
We -X- _ O
experiment -X- _ O
with -X- _ O
(1) -X- _ O
treating -X- _ O
the -X- _ O
task -X- _ O
as -X- _ O
a -X- _ O
standard -X- _ O
sequence-to-sequence -X- _ O
problem, -X- _ O
where -X- _ O
the -X- _ O
source -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
the -X- _ O
target -X- _ O
is -X- _ O
the -X- _ O
decoder -X- _ O
output, -X- _ O
or -X- _ O
(2) -X- _ O
adding -X- _ O
the -X- _ O
source -X- _ O
as -X- _ O
prefix -X- _ O
to -X- _ O
the -X- _ O
target -X- _ O
in -X- _ O
the -X- _ O
decoder, -X- _ O
with -X- _ O
a -X- _ O
loss -X- _ O
only -X- _ O
on -X- _ O
the -X- _ O
target -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
sequence. -X- _ O
We -X- _ O
find -X- _ O
the -X- _ O
former -X- _ O
works -X- _ O
better -X- _ O
for -X- _ O
BART -X- _ B-MethodName
models, -X- _ O
and -X- _ O
the -X- _ O
latter -X- _ O
for -X- _ O
other -X- _ O
models. -X- _ O
To -X- _ O
most -X- _ O
directly -X- _ O
compare -X- _ O
our -X- _ O
models -X- _ O
on -X- _ O
their -X- _ O
ability -X- _ O
to -X- _ O
model -X- _ O
their -X- _ O
fine-tuning -X- _ O
objective -X- _ O
(the -X- _ O
log -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
human -X- _ O
text), -X- _ O
we -X- _ O
report -X- _ O
perplexity -X- _ B-MetricName
in -X- _ O
Table -X- _ O
1. -X- _ O

BART -X- _ B-MethodName
supports -X- _ O
a -X- _ O
much -X- _ O
wider -X- _ O
range -X- _ O
of -X- _ O
noising -X- _ O
schemes -X- _ O
during -X- _ O
pre-training -X- _ O
than -X- _ O
previous -X- _ O
work. -X- _ O
We -X- _ O
compare -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
options -X- _ O
using -X- _ O
base-size -X- _ O
models -X- _ O
(6 -X- _ O
encoder -X- _ O
and -X- _ O
6 -X- _ B-HyperparameterValue
decoder -X- _ B-HyperparameterName
layers, -X- _ I-HyperparameterName
with -X- _ O
a -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
768), -X- _ B-HyperparameterValue
evaluated -X- _ O
on -X- _ O
a -X- _ O
representative -X- _ O
subset -X- _ O
of -X- _ O
the -X- _ O
tasks -X- _ O
we -X- _ O
will -X- _ O
consider -X- _ O
for -X- _ O
the -X- _ O
full -X- _ O
large -X- _ O
scale -X- _ O
experiments -X- _ O
in -X- _ O
ยง5. -X- _ O

We -X- _ O
also -X- _ O
explore -X- _ O
using -X- _ O
BART -X- _ B-MethodName
to -X- _ O
improve -X- _ O
machine -X- _ O
translation -X- _ O
decoders -X- _ O
for -X- _ O
translating -X- _ O
into -X- _ O
English. -X- _ O
Previous -X- _ O
work -X- _ O
Edunov -X- _ O
et -X- _ O
al. -X- _ O
(2019) -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
improved -X- _ O
by -X- _ O
incorporating -X- _ O
pre-trained -X- _ O
encoders, -X- _ O
but -X- _ O
gains -X- _ O
from -X- _ O
using -X- _ O
pre-trained -X- _ O
language -X- _ O
models -X- _ O
in -X- _ O
decoders -X- _ O
have -X- _ O
been -X- _ O
limited. -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
entire -X- _ O
BART -X- _ B-MethodName
model -X- _ O
(both -X- _ O
encoder -X- _ O
and -X- _ O
decoder) -X- _ O
as -X- _ O
a -X- _ O
single -X- _ O
pretrained -X- _ O
decoder -X- _ O
for -X- _ O
machine -X- _ B-TaskName
translation, -X- _ I-TaskName
by -X- _ O
adding -X- _ O
a -X- _ O
new -X- _ O
set -X- _ O
of -X- _ O
encoder -X- _ O
parameters -X- _ O
that -X- _ O
are -X- _ O
learned -X- _ O
from -X- _ O
bitext -X- _ O
(see -X- _ O
Figure -X- _ O
3b). -X- _ O
More -X- _ O
precisely, -X- _ O
we -X- _ O
replace -X- _ O
BART's -X- _ B-MethodName
encoder -X- _ O
embedding -X- _ O
layer -X- _ O
with -X- _ O
a -X- _ O
new -X- _ O
randomly -X- _ O
initialized -X- _ O
encoder. -X- _ O
The -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
end-to-end, -X- _ O
which -X- _ O
trains -X- _ O
the -X- _ O
new -X- _ O
encoder -X- _ O
to -X- _ O
map -X- _ O
foreign -X- _ O
words -X- _ O
into -X- _ O
an -X- _ O
input -X- _ O
that -X- _ O
BART -X- _ B-MethodName
can -X- _ O
de-noise -X- _ O
to -X- _ O
English. -X- _ O
The -X- _ O
new -X- _ O
encoder -X- _ O
can -X- _ O
use -X- _ O
a -X- _ O
separate -X- _ O
vocabulary -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
BART -X- _ B-MethodName
model. -X- _ O
We -X- _ O
train -X- _ O
the -X- _ O
source -X- _ O
encoder -X- _ O
in -X- _ O
two -X- _ O
steps, -X- _ O
in -X- _ O
both -X- _ O
cases -X- _ O
backpropagating -X- _ O
the -X- _ O
cross-entropy -X- _ B-MetricName
loss -X- _ O
from -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
BART -X- _ B-MethodName
model. -X- _ O
In -X- _ O
the -X- _ O
first -X- _ O
step, -X- _ O
we -X- _ O
freeze -X- _ O
most -X- _ O
of -X- _ O
BART -X- _ B-MethodName
parameters -X- _ O
and -X- _ O
only -X- _ O
update -X- _ O
the -X- _ O
randomly -X- _ O
initialized -X- _ O
source -X- _ O
encoder, -X- _ O
the -X- _ O
BART -X- _ B-MethodName
positional -X- _ O
embeddings, -X- _ O
and -X- _ O
the -X- _ O
self-attention -X- _ O
input -X- _ O
projection -X- _ O
matrix -X- _ O
of -X- _ O
BART's -X- _ B-MethodName
encoder -X- _ O
first -X- _ O
layer. -X- _ O
In -X- _ O
the -X- _ O
second -X- _ O
step, -X- _ O
we -X- _ O
train -X- _ O
all -X- _ O
model -X- _ O
parameters -X- _ O
for -X- _ O
a -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
iterations. -X- _ O

Because -X- _ O
BART -X- _ B-MethodName
has -X- _ O
an -X- _ O
autoregressive -X- _ O
decoder, -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
fine -X- _ O
tuned -X- _ O
for -X- _ O
sequence -X- _ O
generation -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
abstractive -X- _ B-TaskName
question -X- _ I-TaskName
answering -X- _ I-TaskName
and -X- _ I-TaskName
summarization. -X- _ I-TaskName
In -X- _ O
both -X- _ O
of -X- _ O
these -X- _ O
tasks, -X- _ O
information -X- _ O
is -X- _ O
copied -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
but -X- _ O
manipulated, -X- _ O
which -X- _ O
is -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
denoising -X- _ O
pre-training -X- _ O
objective. -X- _ O
Here, -X- _ O
the -X- _ O
encoder -X- _ O
input -X- _ O
is -X- _ O
the -X- _ O
input -X- _ O
sequence, -X- _ O
and -X- _ O
the -X- _ O
decoder -X- _ O
generates -X- _ O
outputs -X- _ O
autoregressively. -X- _ O

For -X- _ O
token -X- _ B-TaskName
classification -X- _ I-TaskName
tasks, -X- _ I-TaskName
such -X- _ O
as -X- _ O
answer -X- _ O
endpoint -X- _ B-TaskName
classification -X- _ I-TaskName
for -X- _ O
SQuAD, -X- _ B-DatasetName
we -X- _ O
feed -X- _ O
the -X- _ O
complete -X- _ O
document -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder, -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
top -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
decoder -X- _ O
as -X- _ O
a -X- _ O
representation -X- _ O
for -X- _ O
each -X- _ O
word. -X- _ O
This -X- _ O
representation -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
classify -X- _ O
the -X- _ O
token. -X- _ O

For -X- _ O
sequence -X- _ B-TaskName
classification -X- _ I-TaskName
tasks, -X- _ O
the -X- _ O
same -X- _ O
input -X- _ O
is -X- _ O
fed -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder, -X- _ O
and -X- _ O
the -X- _ O
final -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
final -X- _ O
decoder -X- _ O
token -X- _ O
is -X- _ O
fed -X- _ O
into -X- _ O
new -X- _ O
multi-class -X- _ O
linear -X- _ O
classifier. -X- _ O
This -X- _ O
approach -X- _ O
is -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
CLS -X- _ O
token -X- _ O
in -X- _ O
BERT; -X- _ B-MethodName
however -X- _ O
we -X- _ O
add -X- _ O
the -X- _ O
additional -X- _ O
token -X- _ O
to -X- _ O
the -X- _ O
end -X- _ O
so -X- _ O
that -X- _ O
representation -X- _ O
for -X- _ O
the -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
can -X- _ O
attend -X- _ O
to -X- _ O
decoder -X- _ O
states -X- _ O
from -X- _ O
the -X- _ O
complete -X- _ O
input -X- _ O
(Figure -X- _ O
3a). -X- _ O

The -X- _ O
representations -X- _ O
produced -X- _ O
by -X- _ O
BART -X- _ B-MethodName
can -X- _ O
be -X- _ O
used -X- _ O
in -X- _ O
several -X- _ O
ways -X- _ O
for -X- _ O
downstream -X- _ O
applications. -X- _ O

BART -X- _ B-MethodName
is -X- _ O
trained -X- _ O
by -X- _ O
corrupting -X- _ O
documents -X- _ O
and -X- _ O
then -X- _ O
optimizing -X- _ O
a -X- _ O
reconstruction -X- _ B-MetricName
loss-the -X- _ I-MetricName
cross-entropy -X- _ B-MetricName
between -X- _ O
the -X- _ O
decoder's -X- _ O
output -X- _ O
and -X- _ O
the -X- _ O
original -X- _ O
document. -X- _ O
Unlike -X- _ O
existing -X- _ O
denoising -X- _ O
autoencoders, -X- _ O
which -X- _ O
are -X- _ O
tailored -X- _ O
to -X- _ O
specific -X- _ O
noising -X- _ O
schemes, -X- _ O
BART -X- _ B-MethodName
allows -X- _ O
us -X- _ O
to -X- _ O
apply -X- _ O
any -X- _ O
type -X- _ O
of -X- _ O
document -X- _ O
corruption. -X- _ O
In -X- _ O
the -X- _ O
extreme -X- _ O
case, -X- _ O
where -X- _ O
all -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
source -X- _ O
is -X- _ O
lost, -X- _ O
BART -X- _ B-MethodName
is -X- _ O
equivalent -X- _ O
to -X- _ O
a -X- _ O
language -X- _ O
model. -X- _ O
We -X- _ O
experiment -X- _ O
with -X- _ O
several -X- _ O
previously -X- _ O
proposed -X- _ O
and -X- _ O
novel -X- _ O
transformations, -X- _ O
but -X- _ O
we -X- _ O
believe -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
significant -X- _ O
potential -X- _ O
for -X- _ O
development -X- _ O
of -X- _ O
other -X- _ O
new -X- _ O
alternatives. -X- _ O
The -X- _ O
transformations -X- _ O
we -X- _ O
used -X- _ O
are -X- _ O
summarized -X- _ O
below, -X- _ O
and -X- _ O
examples -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2. -X- _ O
Token -X- _ O
Masking -X- _ O
Following -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
random -X- _ O
tokens -X- _ O
are -X- _ O
sampled -X- _ O
and -X- _ O
replaced -X- _ O
with -X- _ O
[MASK] -X- _ O
elements. -X- _ O
Token -X- _ O
Deletion -X- _ O
Random -X- _ O
tokens -X- _ O
are -X- _ O
deleted -X- _ O
from -X- _ O
the -X- _ O
input. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
token -X- _ O
masking, -X- _ O
the -X- _ O
model -X- _ O
must -X- _ O
decide -X- _ O
which -X- _ O
positions -X- _ O
are -X- _ O
missing -X- _ O
inputs. -X- _ O
Text -X- _ O
Infilling -X- _ O
A -X- _ O
number -X- _ O
of -X- _ O
text -X- _ O
spans -X- _ O
are -X- _ O
sampled, -X- _ O
with -X- _ O
span -X- _ O
lengths -X- _ O
drawn -X- _ O
from -X- _ O
a -X- _ O
Poisson -X- _ O
distribution -X- _ O
(ฮป -X- _ O
= -X- _ O
3). -X- _ O
Each -X- _ O
span -X- _ O
is -X- _ O
replaced -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
[MASK] -X- _ O
token. -X- _ O
0-length -X- _ O
spans -X- _ O
correspond -X- _ O
to -X- _ O
the -X- _ O
insertion -X- _ O
of -X- _ O
[MASK] -X- _ O
tokens. -X- _ O
Text -X- _ O
infilling -X- _ O
is -X- _ O
inspired -X- _ O
by -X- _ O
Span-BERT -X- _ B-MethodName
, -X- _ O
but -X- _ O
SpanBERT -X- _ B-MethodName
samples -X- _ O
span -X- _ O
lengths -X- _ O
from -X- _ O
a -X- _ O
different -X- _ O
(clamped -X- _ O
geometric) -X- _ O
distribution, -X- _ O
and -X- _ O
replaces -X- _ O
each -X- _ O
span -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
[MASK] -X- _ O
tokens -X- _ O
of -X- _ O
exactly -X- _ O
the -X- _ O
same -X- _ O
length. -X- _ O
Text -X- _ O
infilling -X- _ O
teaches -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
predict -X- _ O
how -X- _ O
many -X- _ O
tokens -X- _ O
are -X- _ O
missing -X- _ O
from -X- _ O
a -X- _ O
span. -X- _ O
Sentence -X- _ O
Permutation -X- _ O
A -X- _ O
document -X- _ O
is -X- _ O
divided -X- _ O
into -X- _ O
sentences -X- _ O
based -X- _ O
on -X- _ O
full -X- _ O
stops, -X- _ O
and -X- _ O
these -X- _ O
sentences -X- _ O
are -X- _ O
shuffled -X- _ O
in -X- _ O
a -X- _ O
random -X- _ O
order. -X- _ O
Document -X- _ O
Rotation -X- _ O
A -X- _ O
token -X- _ O
is -X- _ O
chosen -X- _ O
uniformly -X- _ O
at -X- _ O
random, -X- _ O
and -X- _ O
the -X- _ O
document -X- _ O
is -X- _ O
rotated -X- _ O
so -X- _ O
that -X- _ O
it -X- _ O
begins -X- _ O
with -X- _ O
that -X- _ O
token. -X- _ O
This -X- _ O
task -X- _ O
trains -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
start -X- _ O
of -X- _ O
the -X- _ O
document. -X- _ O

BART -X- _ B-MethodName
uses -X- _ O
the -X- _ O
standard -X- _ O
sequence-to-sequence -X- _ O
Transformer -X- _ O
architecture -X- _ O
from -X- _ O
(Vaswani -X- _ O
et -X- _ O
al., -X- _ O
2017), -X- _ O
except, -X- _ O
following -X- _ O
GPT, -X- _ B-MethodName
that -X- _ O
we -X- _ O
modify -X- _ O
ReLU -X- _ B-HyperparameterValue
activation -X- _ B-HyperparameterName
functions -X- _ I-HyperparameterName
to -X- _ O
GeLUs -X- _ B-HyperparameterValue
(Hendrycks -X- _ O
& -X- _ O
Gimpel, -X- _ O
2016) -X- _ O
and -X- _ O
initialise -X- _ O
parameters -X- _ O
from -X- _ O
N -X- _ O
(0, -X- _ O
0.02). -X- _ O
For -X- _ O
our -X- _ O
base -X- _ O
model, -X- _ O
we -X- _ O
use -X- _ O
6 -X- _ O
layers -X- _ O
in -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
de-coder, -X- _ O
and -X- _ O
for -X- _ O
our -X- _ O
large -X- _ O
model -X- _ O
we -X- _ O
use -X- _ O
12 -X- _ O
layers -X- _ O
in -X- _ O
each. -X- _ O
The -X- _ O
architecture -X- _ O
is -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
that -X- _ O
used -X- _ O
in -X- _ O
BERT, -X- _ B-MethodName
with -X- _ O
the -X- _ O
following -X- _ O
differences: -X- _ O
(1) -X- _ O
each -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
decoder -X- _ O
additionally -X- _ O
performs -X- _ O
cross-attention -X- _ O
over -X- _ O
the -X- _ O
final -X- _ O
hidden -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
(as -X- _ O
in -X- _ O
the -X- _ O
transformer -X- _ O
sequence-to-sequence -X- _ O
model); -X- _ O
and -X- _ O
(2) -X- _ O
BERT -X- _ B-MethodName
uses -X- _ O
an -X- _ O
additional -X- _ O
feed-forward -X- _ O
network -X- _ O
before -X- _ O
wordprediction, -X- _ O
which -X- _ O
BART -X- _ B-MethodName
does -X- _ O
not. -X- _ O
In -X- _ O
total, -X- _ O
BART -X- _ B-MethodName
contains -X- _ O
roughly -X- _ O
10% -X- _ B-MetricValue
more -X- _ O
parameters -X- _ O
than -X- _ O
the -X- _ O
equivalently -X- _ O
sized -X- _ O
BERT -X- _ B-MethodName
model. -X- _ O

BART -X- _ B-MethodName
is -X- _ O
a -X- _ O
denoising -X- _ O
autoencoder -X- _ O
that -X- _ O
maps -X- _ O
a -X- _ O
corrupted -X- _ O
document -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
document -X- _ O
it -X- _ O
was -X- _ O
derived -X- _ O
from. -X- _ O
It -X- _ O
is -X- _ O
implemented -X- _ O
as -X- _ O
a -X- _ O
sequence-to-sequence -X- _ O
model -X- _ O
with -X- _ O
a -X- _ O
bidirectional -X- _ O
encoder -X- _ O
over -X- _ O
corrupted -X- _ O
text -X- _ O
and -X- _ O
a -X- _ O
left-to-right -X- _ O
autoregressive -X- _ O
decoder. -X- _ O
For -X- _ O
pre-training, -X- _ O
we -X- _ O
optimize -X- _ O
the -X- _ O
negative -X- _ B-MetricName
log -X- _ I-MetricName
likelihood -X- _ I-MetricName
of -X- _ O
the -X- _ O
original -X- _ O
document. -X- _ O

Bidirectional -X- _ O
Encoder -X- _ O
A -X- _ O
B -X- _ O
C -X- _ O
D -X- _ O
E -X- _ O
A -X- _ O
_ -X- _ O
B -X- _ O
_ -X- _ O
E -X- _ O
<s> -X- _ O
A -X- _ O
B -X- _ O
C -X- _ O
D( -X- _ O
c) -X- _ O
BART: -X- _ O
Inputs -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
need -X- _ O
not -X- _ O
be -X- _ O
aligned -X- _ O
with -X- _ O
decoder -X- _ O
outputs, -X- _ O
allowing -X- _ O
arbitary -X- _ O
noise -X- _ O
transformations. -X- _ O
Here, -X- _ O
a -X- _ O
document -X- _ O
has -X- _ O
been -X- _ O
corrupted -X- _ O
by -X- _ O
replacing -X- _ O
spans -X- _ O
of -X- _ O
text -X- _ O
with -X- _ O
mask -X- _ O
symbols. -X- _ O
The -X- _ O
corrupted -X- _ O
document -X- _ O
(left) -X- _ O
is -X- _ O
encoded -X- _ O
with -X- _ O
a -X- _ O
bidirectional -X- _ O
model, -X- _ O
and -X- _ O
then -X- _ O
the -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
document -X- _ O
(right) -X- _ O
is -X- _ O
calculated -X- _ O
with -X- _ O
an -X- _ O
autoregressive -X- _ O
decoder. -X- _ O
For -X- _ O
fine-tuning, -X- _ O
an -X- _ O
uncorrupted -X- _ O
document -X- _ O
is -X- _ O
input -X- _ O
to -X- _ O
both -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder, -X- _ O
and -X- _ O
we -X- _ O
use -X- _ O
representations -X- _ O
from -X- _ O
the -X- _ O
final -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
decoder. -X- _ O
Figure -X- _ O
1: -X- _ O
A -X- _ O
schematic -X- _ O
comparison -X- _ O
of -X- _ O
BART -X- _ B-MethodName
with -X- _ O
BERT -X- _ B-MethodName
(Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019) -X- _ O
and -X- _ O
GPT -X- _ B-MethodName
(Radford -X- _ O
et -X- _ O
al., -X- _ O
2018). -X- _ O
English, -X- _ O
by -X- _ O
propagation -X- _ O
through -X- _ O
BART, -X- _ B-MethodName
thereby -X- _ O
using -X- _ O
BART -X- _ B-MethodName
as -X- _ O
a -X- _ O
pre-trained -X- _ O
target-side -X- _ O
language -X- _ O
model. -X- _ O
This -X- _ O
approach -X- _ O
improves -X- _ O
performance -X- _ O
over -X- _ O
a -X- _ O
strong -X- _ O
back-translation -X- _ O
MT -X- _ O
baseline -X- _ O
by -X- _ O
1.1 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
on -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
Romanian-English -X- _ I-DatasetName
benchmark. -X- _ O
To -X- _ O
better -X- _ O
understand -X- _ O
these -X- _ O
effects, -X- _ O
we -X- _ O
also -X- _ O
report -X- _ O
an -X- _ O
ablation -X- _ O
analysis -X- _ O
that -X- _ O
replicates -X- _ O
other -X- _ O
recently -X- _ O
proposed -X- _ O
training -X- _ O
objectives. -X- _ O
This -X- _ O
study -X- _ O
allows -X- _ O
us -X- _ O
to -X- _ O
carefully -X- _ O
control -X- _ O
for -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
factors, -X- _ O
including -X- _ O
data -X- _ O
and -X- _ O
optimization -X- _ O
parameters, -X- _ O
which -X- _ O
have -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
as -X- _ O
important -X- _ O
for -X- _ O
overall -X- _ O
performance -X- _ O
as -X- _ O
the -X- _ O
selection -X- _ O
of -X- _ O
training -X- _ O
objectives -X- _ O
. -X- _ O
We -X- _ O
find -X- _ O
that -X- _ O
BART -X- _ B-MethodName
exhibits -X- _ O
the -X- _ O
most -X- _ O
consistently -X- _ O
strong -X- _ O
performance -X- _ O
across -X- _ O
the -X- _ O
full -X- _ O
range -X- _ O
of -X- _ O
tasks -X- _ O
we -X- _ O
consider. -X- _ O

A -X- _ O
B -X- _ O
C -X- _ O
D -X- _ O
E -X- _ O
<s> -X- _ O
A -X- _ O
B -X- _ O
C -X- _ O
D -X- _ O
(b) -X- _ O
GPT: -X- _ B-MethodName
Tokens -X- _ O
are -X- _ O
predicted -X- _ O
auto-regressively, -X- _ O
meaning -X- _ O
GPT -X- _ B-MethodName
can -X- _ O
be -X- _ O
used -X- _ O
for -X- _ O
generation. -X- _ O
However -X- _ O
words -X- _ O
can -X- _ O
only -X- _ O
condition -X- _ O
on -X- _ O
leftward -X- _ O
context, -X- _ O
so -X- _ O
it -X- _ O
cannot -X- _ O
learn -X- _ O
bidirectional -X- _ O
interactions. -X- _ O

Self-supervised -X- _ O
methods -X- _ O
have -X- _ O
achieved -X- _ O
remarkable -X- _ O
success -X- _ O
in -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
NLP -X- _ O
tasks -X- _ O
(Mikolov -X- _ O
et -X- _ O
al., -X- _ O
2013;Peters -X- _ O
et -X- _ O
al., -X- _ O
2018;Devlin -X- _ O
et -X- _ O
al., -X- _ O
2019;Yang -X- _ O
et -X- _ O
al., -X- _ O
2019;. -X- _ O
The -X- _ O
most -X- _ O
successful -X- _ O
approaches -X- _ O
have -X- _ O
been -X- _ O
variants -X- _ O
of -X- _ O
masked -X- _ O
language -X- _ O
models, -X- _ O
which -X- _ O
are -X- _ O
denoising -X- _ O
autoencoders -X- _ O
that -X- _ O
are -X- _ O
trained -X- _ O
to -X- _ O
reconstruct -X- _ O
text -X- _ O
where -X- _ O
a -X- _ O
random -X- _ O
subset -X- _ O
of -X- _ O
the -X- _ O
words -X- _ O
has -X- _ O
been -X- _ O
masked -X- _ O
out. -X- _ O
Recent -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
gains -X- _ O
by -X- _ O
improving -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
masked -X- _ O
tokens -X- _ O
, -X- _ O
the -X- _ O
order -X- _ O
in -X- _ O
which -X- _ O
masked -X- _ O
tokens -X- _ O
are -X- _ O
predicted -X- _ O
(Yang -X- _ O
et -X- _ O
al., -X- _ O
2019), -X- _ O
and -X- _ O
the -X- _ O
available -X- _ O
context -X- _ O
for -X- _ O
replacing -X- _ O
masked -X- _ O
tokens -X- _ O
(Dong -X- _ O
et -X- _ O
al., -X- _ O
2019). -X- _ O
However, -X- _ O
these -X- _ O
methods -X- _ O
typically -X- _ O
focus -X- _ O
on -X- _ O
particular -X- _ O
types -X- _ O
of -X- _ O
end -X- _ O
tasks -X- _ O
(e.g. -X- _ O
span -X- _ B-TaskName
prediction, -X- _ I-TaskName
generation, -X- _ B-TaskName
etc.), -X- _ O
limiting -X- _ O
their -X- _ O
applicability. -X- _ O
In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
present -X- _ O
BART, -X- _ B-MethodName
which -X- _ O
pre-trains -X- _ O
a -X- _ O
model -X- _ O
combining -X- _ O
Bidirectional -X- _ O
and -X- _ O
Auto-Regressive -X- _ O
Transformers. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
a -X- _ O
denoising -X- _ O
autoencoder -X- _ O
built -X- _ O
with -X- _ O
a -X- _ O
sequence-to-sequence -X- _ O
model -X- _ O
that -X- _ O
is -X- _ O
applicable -X- _ O
to -X- _ O
a -X- _ O
very -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
end -X- _ O
tasks. -X- _ O
Pretraining -X- _ O
has -X- _ O
two -X- _ O
stages -X- _ O
(1) -X- _ O
text -X- _ O
is -X- _ O
corrupted -X- _ O
with -X- _ O
an -X- _ O
arbitrary -X- _ O
noising -X- _ O
function, -X- _ O
and -X- _ O
(2) -X- _ O
a -X- _ O
sequence-to-sequence -X- _ O
model -X- _ O
is -X- _ O
learned -X- _ O
to -X- _ O
reconstruct -X- _ O
the -X- _ O
original -X- _ O
text. -X- _ O
BART -X- _ B-MethodName
uses -X- _ O
a -X- _ O
standard -X- _ O
Tranformer-based -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
architecture -X- _ O
which, -X- _ O
despite -X- _ O
its -X- _ O
simplicity, -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
generalizing -X- _ O
BERT -X- _ B-MethodName
(due -X- _ O
to -X- _ O
the -X- _ O
bidirectional -X- _ O
encoder), -X- _ O
GPT -X- _ B-MethodName
(with -X- _ O
the -X- _ O
left-to-right -X- _ O
decoder), -X- _ O
and -X- _ O
many -X- _ O
other -X- _ O
more -X- _ O
recent -X- _ O
pretraining -X- _ O
schemes -X- _ O
(see -X- _ O
Figure -X- _ O
1). -X- _ O
A -X- _ O
key -X- _ O
advantage -X- _ O
of -X- _ O
this -X- _ O
setup -X- _ O
is -X- _ O
the -X- _ O
noising -X- _ O
flexibility; -X- _ O
arbitrary -X- _ O
transformations -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
text, -X- _ O
including -X- _ O
changing -X- _ O
its -X- _ O
length. -X- _ O
We -X- _ O
evaluate -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
noising -X- _ O
approaches, -X- _ O
finding -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
by -X- _ O
both -X- _ O
randomly -X- _ O
shuffling -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
sentences -X- _ O
and -X- _ O
using -X- _ O
a -X- _ O
novel -X- _ O
in-filling -X- _ O
scheme, -X- _ O
where -X- _ O
arbitrary -X- _ O
length -X- _ O
spans -X- _ O
of -X- _ O
text -X- _ O
(including -X- _ O
zero -X- _ O
length) -X- _ O
are -X- _ O
replaced -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
mask -X- _ O
token. -X- _ O
This -X- _ O
approach -X- _ O
generalizes -X- _ O
the -X- _ O
original -X- _ O
word -X- _ O
masking -X- _ O
and -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
objectives -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
by -X- _ O
forcing -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
reason -X- _ O
more -X- _ O
about -X- _ O
overall -X- _ O
sentence -X- _ O
length -X- _ O
and -X- _ O
make -X- _ O
longer -X- _ O
range -X- _ O
transformations -X- _ O
to -X- _ O
the -X- _ O
input. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
particularly -X- _ O
effective -X- _ O
when -X- _ O
fine -X- _ O
tuned -X- _ O
for -X- _ O
text -X- _ O
generation -X- _ O
but -X- _ O
also -X- _ O
works -X- _ O
well -X- _ O
for -X- _ O
comprehension -X- _ O
tasks. -X- _ O
It -X- _ O
matches -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
with -X- _ O
comparable -X- _ O
training -X- _ O
resources -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
(Wang -X- _ O
et -X- _ O
al., -X- _ O
2018) -X- _ O
and -X- _ O
SQuAD -X- _ B-DatasetName
(Rajpurkar -X- _ O
et -X- _ O
al., -X- _ O
2016), -X- _ O
and -X- _ O
achieves -X- _ O
new -X- _ O
state-of-the-art -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
abstractive -X- _ O
dialogue, -X- _ O
question -X- _ O
answering, -X- _ O
and -X- _ O
summarization -X- _ O
tasks. -X- _ O
For -X- _ O
example, -X- _ O
it -X- _ O
improves -X- _ O
performance -X- _ O
by -X- _ O
6 -X- _ B-MetricValue
ROUGE -X- _ B-MetricName
over -X- _ O
previous -X- _ O
work -X- _ O
on -X- _ O
XSum -X- _ O
(Narayan -X- _ O
et -X- _ O
al., -X- _ O
2018). -X- _ O
BART -X- _ B-MethodName
also -X- _ O
opens -X- _ O
up -X- _ O
new -X- _ O
ways -X- _ O
of -X- _ O
thinking -X- _ O
about -X- _ O
fine -X- _ O
tuning. -X- _ O
We -X- _ O
present -X- _ O
a -X- _ O
new -X- _ O
scheme -X- _ O
for -X- _ O
machine -X- _ O
translation -X- _ O
where -X- _ O
a -X- _ O
BART -X- _ B-MethodName
model -X- _ O
is -X- _ O
stacked -X- _ O
above -X- _ O
a -X- _ O
few -X- _ O
additional -X- _ O
transformer -X- _ O
layers. -X- _ O
These -X- _ O
layers -X- _ O
are -X- _ O
trained -X- _ O
to -X- _ O
essentially -X- _ O
translate -X- _ O
the -X- _ O
foreign -X- _ O
language -X- _ O
to -X- _ O
noised -X- _ O
Bidirectional -X- _ O
Encoder -X- _ O
A -X- _ O
_ -X- _ O
C -X- _ O
_ -X- _ O
E -X- _ O
B -X- _ O
D -X- _ O
(a) -X- _ O
BERT: -X- _ B-MethodName
Random -X- _ O
tokens -X- _ O
are -X- _ O
replaced -X- _ O
with -X- _ O
masks, -X- _ O
and -X- _ O
the -X- _ O
document -X- _ O
is -X- _ O
encoded -X- _ O
bidirectionally. -X- _ O
Missing -X- _ O
tokens -X- _ O
are -X- _ O
predicted -X- _ O
independently, -X- _ O
so -X- _ O
BERT -X- _ B-MethodName
cannot -X- _ O
easily -X- _ O
be -X- _ O
used -X- _ O
for -X- _ O
generation. -X- _ O

We -X- _ O
present -X- _ O
BART, -X- _ B-MethodName
a -X- _ O
denoising -X- _ O
autoencoder -X- _ O
for -X- _ O
pretraining -X- _ O
sequence-to-sequence -X- _ O
models. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
trained -X- _ O
by -X- _ O
(1) -X- _ O
corrupting -X- _ O
text -X- _ O
with -X- _ O
an -X- _ O
arbitrary -X- _ O
noising -X- _ O
function, -X- _ O
and -X- _ O
(2) -X- _ O
learning -X- _ O
a -X- _ O
model -X- _ O
to -X- _ O
reconstruct -X- _ O
the -X- _ O
original -X- _ O
text. -X- _ O
It -X- _ O
uses -X- _ O
a -X- _ O
standard -X- _ O
Tranformer-based -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
architecture -X- _ O
which, -X- _ O
despite -X- _ O
its -X- _ O
simplicity, -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
generalizing -X- _ O
BERT -X- _ B-MethodName
(due -X- _ O
to -X- _ O
the -X- _ O
bidirectional -X- _ O
encoder), -X- _ O
GPT -X- _ B-MethodName
(with -X- _ O
the -X- _ O
left-to-right -X- _ O
decoder), -X- _ O
and -X- _ O
many -X- _ O
other -X- _ O
more -X- _ O
recent -X- _ O
pretraining -X- _ O
schemes. -X- _ O
We -X- _ O
evaluate -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
noising -X- _ O
approaches, -X- _ O
finding -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
by -X- _ O
both -X- _ O
randomly -X- _ O
shuffling -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
sentences -X- _ O
and -X- _ O
using -X- _ O
a -X- _ O
novel -X- _ O
in-filling -X- _ O
scheme, -X- _ O
where -X- _ O
spans -X- _ O
of -X- _ O
text -X- _ O
are -X- _ O
replaced -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
mask -X- _ O
token. -X- _ O
BART -X- _ O
is -X- _ O
particularly -X- _ O
effective -X- _ O
when -X- _ O
fine -X- _ O
tuned -X- _ O
for -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
but -X- _ O
also -X- _ O
works -X- _ O
well -X- _ O
for -X- _ O
comprehension -X- _ B-TaskName
tasks. -X- _ O
It -X- _ O
matches -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
RoBERTa -X- _ O
with -X- _ O
comparable -X- _ O
training -X- _ O
resources -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
and -X- _ O
SQuAD, -X- _ B-DatasetName
achieves -X- _ O
new -X- _ O
stateof-the-art -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
abstractive -X- _ B-TaskName
dialogue, -X- _ I-TaskName
question -X- _ B-TaskName
answering, -X- _ I-TaskName
and -X- _ O
summarization -X- _ O
tasks, -X- _ O
with -X- _ O
gains -X- _ O
of -X- _ O
up -X- _ O
to -X- _ O
6 -X- _ B-MetricValue
ROUGE. -X- _ B-MetricName
BART -X- _ B-MethodName
also -X- _ O
provides -X- _ O
a -X- _ O
1.1 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
increase -X- _ O
over -X- _ O
a -X- _ O
back-translation -X- _ O
system -X- _ O
for -X- _ O
machine -X- _ O
translation, -X- _ B-TaskName
with -X- _ O
only -X- _ O
target -X- _ O
language -X- _ O
pretraining. -X- _ O
We -X- _ O
also -X- _ O
report -X- _ O
ablation -X- _ O
experiments -X- _ O
that -X- _ O
replicate -X- _ O
other -X- _ O
pretraining -X- _ O
schemes -X- _ O
within -X- _ O
the -X- _ O
BART -X- _ B-MethodName
framework, -X- _ O
to -X- _ O
better -X- _ O
measure -X- _ O
which -X- _ O
factors -X- _ O
most -X- _ O
influence -X- _ O
end-task -X- _ O
performance. -X- _ O

