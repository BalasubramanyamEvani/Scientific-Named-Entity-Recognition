Acknowledgements	O
We	O
are	O
grateful	O
to	O
Nal	O
Kalchbrenner	O
and	O
Stephan	O
Gouws	O
for	O
their	O
fruitful	O
comments,	O
corrections	O
and	O
inspiration.	O

In	O
this	O
work,	O
we	O
presented	O
the	O
Transformer,	B-MethodName
the	O
first	O
sequence	O
transduction	O
model	O
based	O
entirely	O
on	O
attention,	O
replacing	O
the	O
recurrent	O
layers	O
most	O
commonly	O
used	O
in	O
encoder-decoder	O
architectures	O
with	O
multi-headed	O
self-attention.	O
For	O
translation	O
tasks,	O
the	O
Transformer	B-MethodName
can	O
be	O
trained	O
significantly	O
faster	O
than	O
architectures	O
based	O
on	O
recurrent	O
or	O
convolutional	O
layers.	O
On	O
both	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
and	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
tasks,	O
we	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art.	O
In	O
the	O
former	O
task	O
our	O
best	O
model	O
outperforms	O
even	O
all	O
previously	O
reported	O
ensembles.	O
We	O
are	O
excited	O
about	O
the	O
future	O
of	O
attention-based	O
models	O
and	O
plan	O
to	O
apply	O
them	O
to	O
other	O
tasks.	O
We	O
plan	O
to	O
extend	O
the	O
Transformer	B-MethodName
to	O
problems	O
involving	O
input	O
and	O
output	O
modalities	O
other	O
than	O
text	O
and	O
to	O
investigate	O
local,	O
restricted	O
attention	O
mechanisms	O
to	O
efficiently	O
handle	O
large	O
inputs	O
and	O
outputs	O
such	O
as	O
images,	O
audio	O
and	O
video.	O
Making	O
generation	O
less	O
sequential	O
is	O
another	O
research	O
goals	O
of	O
ours.	O
The	O
code	O
we	O
used	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
is	O
available	O
at	O
tensorflow/tensor2tensor.	O

To	O
evaluate	O
if	O
the	O
Transformer	B-MethodName
can	O
generalize	O
to	O
other	O
tasks	O
we	O
performed	O
experiments	O
on	O
English	B-TaskName
constituency	I-TaskName
parsing.	I-TaskName
This	O
task	O
presents	O
specific	O
challenges:	O
the	O
output	O
is	O
subject	O
to	O
strong	O
structural	O
constraints	O
and	O
is	O
significantly	O
longer	O
than	O
the	O
input.	O
Furthermore,	O
RNN	O
sequence-to-sequence	O
models	O
have	O
not	O
been	O
able	O
to	O
attain	O
state-of-the-art	O
results	O
in	O
small-data	O
regimes	O
.	O
We	O
trained	O
a	O
4-layer	O
transformer	O
with	O
d	O
model	O
=	O
1024	O
on	O
the	O
Wall	B-DatasetName
Street	I-DatasetName
Journal	I-DatasetName
(WSJ)	I-DatasetName
portion	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
,	O
about	O
40K	O
training	O
sentences.	O
We	O
also	O
trained	O
it	O
in	O
a	O
semi-supervised	O
setting,	O
using	O
the	O
larger	O
high-confidence	O
and	O
BerkleyParser	B-DatasetName
corpora	I-DatasetName
from	O
with	O
approximately	O
17M	O
sentences	O
.	O
We	O
used	O
a	O
vocabulary	B-HyperparameterName
of	O
16K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
WSJ	O
only	O
setting	O
and	O
a	O
vocabulary	B-HyperparameterName
of	O
32K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
semi-supervised	O
setting.	O
We	O
performed	O
only	O
a	O
small	O
number	O
of	O
experiments	O
to	O
select	O
the	O
dropout,	B-HyperparameterName
both	O
attention	O
and	O
residual	O
(section	O
5.4),	O
learning	B-HyperparameterName
rates	I-HyperparameterName
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
on	O
the	O
Section	O
22	O
development	O
set,	O
all	O
other	O
parameters	O
remained	O
unchanged	O
from	O
the	O
English-to-German	O
base	O
translation	O
model.	O
During	O
inference,	O
we	O
increased	O
the	O
maximum	O
output	O
length	O
to	O
input	O
length	O
+	O
300.	O
We	O
used	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
21	B-HyperparameterValue
and	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
for	O
both	O
WSJ	O
only	O
and	O
the	O
semi-supervised	O
setting.	O
Our	O
results	O
in	O
Table	O
4	O
show	O
that	O
despite	O
the	O
lack	O
of	O
task-specific	O
tuning	O
our	O
model	O
performs	O
surprisingly	O
well,	O
yielding	O
better	O
results	O
than	O
all	O
previously	O
reported	O
models	O
with	O
the	O
exception	O
of	O
the	O
Recurrent	O
Neural	O
Network	O
Grammar	O
.	O
In	O
contrast	O
to	O
RNN	O
sequence-to-sequence	O
models	O
,	O
the	O
Transformer	B-MethodName
outperforms	O
the	O
Berkeley-Parser	B-MethodName
even	O
when	O
training	O
only	O
on	O
the	O
WSJ	B-DatasetName
training	O
set	O
of	O
40K	O
sentences.	O

To	O
evaluate	O
the	O
importance	O
of	O
different	O
components	O
of	O
the	O
Transformer,	B-MethodName
we	O
varied	O
our	O
base	O
model	O
in	O
different	O
ways,	O
measuring	O
the	O
change	O
in	O
performance	O
on	O
English-to-German	B-TaskName
translation	I-TaskName
on	O
the	O
development	O
set,	O
newstest2013.	B-DatasetName
We	O
used	O
beam	B-MethodName
search	I-MethodName
as	O
described	O
in	O
the	O
previous	O
section,	O
but	O
no	O
checkpoint	O
averaging.	O
We	O
present	O
these	O
results	O
in	O
Table	O
3.	O
In	O
Table	O
3	O
rows	O
(A),	O
we	O
vary	O
the	O
number	O
of	O
attention	O
heads	O
and	O
the	O
attention	O
key	O
and	O
value	O
dimensions,	O
keeping	O
the	O
amount	O
of	O
computation	O
constant,	O
as	O
described	O
in	O
Section	O
3.2.2.	O
While	O
single-head	O
attention	O
is	O
0.9	B-MetricValue
BLEU	B-MetricName
worse	O
than	O
the	O
best	O
setting,	O
quality	O
also	O
drops	O
off	O
with	O
too	O
many	O
heads.	O
In	O
Table	O
3	O
rows	O
(B),	O
we	O
observe	O
that	O
reducing	O
the	O
attention	O
key	O
size	O
d	O
k	O
hurts	O
model	O
quality.	O
This	O
suggests	O
that	O
determining	O
compatibility	O
is	O
not	O
easy	O
and	O
that	O
a	O
more	O
sophisticated	O
compatibility	O
function	O
than	O
dot	O
product	O
may	O
be	O
beneficial.	O
We	O
further	O
observe	O
in	O
rows	O
(C)	O
and	O
(D)	O
that,	O
as	O
expected,	O
bigger	O
models	O
are	O
better,	O
and	O
dropout	O
is	O
very	O
helpful	O
in	O
avoiding	O
over-fitting.	O
In	O
row	O
(E)	O
we	O
replace	O
our	O
sinusoidal	O
positional	O
encoding	O
with	O
learned	O
positional	O
embeddings	O
,	O
and	O
observe	O
nearly	O
identical	O
results	O
to	O
the	O
base	O
model.	O

On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
translation	O
task,	O
the	O
big	O
transformer	B-MethodName
model	O
(Transformer	B-MethodName
(big)	I-MethodName
in	O
Table	O
2)	O
outperforms	O
the	O
best	O
previously	O
reported	O
models	O
(including	O
ensembles)	O
by	O
more	O
than	O
2.0	B-MetricValue
BLEU,	B-MetricName
establishing	O
a	O
new	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
28.4.	B-MetricValue
The	O
configuration	O
of	O
this	O
model	O
is	O
listed	O
in	O
the	O
bottom	O
line	O
of	O
Table	O
3.	O
Training	O
took	O
3.5	O
days	O
on	O
8	O
P100	O
GPUs.	O
Even	O
our	O
base	O
model	O
surpasses	O
all	O
previously	O
published	O
models	O
and	O
ensembles,	O
at	O
a	O
fraction	O
of	O
the	O
training	O
cost	O
of	O
any	O
of	O
the	O
competitive	O
models.	O
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
big	O
model	O
achieves	O
a	O
BLEU	B-MetricName
score	O
of	O
41.0,	B-MetricValue
outperforming	O
all	O
of	O
the	O
previously	O
published	O
single	O
models,	O
at	O
less	O
than	O
1/4	O
the	O
training	O
cost	O
of	O
the	O
previous	O
state-of-the-art	O
model.	O
The	O
Transformer	O
(big)	O
model	O
trained	O
for	O
English-to-French	O
used	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
P	O
drop	B-HyperparameterName
=	O
0.1,	B-HyperparameterValue
instead	O
of	O
0.3.	B-HyperparameterValue
For	O
the	O
base	O
models,	O
we	O
used	O
a	O
single	O
model	O
obtained	O
by	O
averaging	O
the	O
last	O
5	O
checkpoints,	O
which	O
were	O
written	O
at	O
10-minute	O
intervals.	O
For	O
the	O
big	O
models,	O
we	O
averaged	O
the	O
last	O
20	O
checkpoints.	O
We	O
used	O
beam	B-MethodName
search	I-MethodName
with	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
4	B-HyperparameterValue
and	O
length	B-HyperparameterName
penalty	I-HyperparameterName
α	B-HyperparameterValue
=	I-HyperparameterValue
0.6	I-HyperparameterValue
.	O
These	O
hyperparameters	O
were	O
chosen	O
after	O
experimentation	O
on	O
the	O
development	O
set.	O
We	O
set	O
the	O
maximum	O
output	O
length	O
during	O
inference	O
to	O
input	O
length	O
+	O
50,	O
but	O
terminate	O
early	O
when	O
possible	O
.	O

We	O
employ	O
three	O
types	O
of	O
regularization	O
during	O
training:	O
Residual	O
Dropout	B-HyperparameterName
We	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
output	O
of	O
each	O
sub-layer,	O
before	O
it	O
is	O
added	O
to	O
the	O
sub-layer	O
input	O
and	O
normalized.	O
In	O
addition,	O
we	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
sums	O
of	O
the	O
embeddings	O
and	O
the	O
positional	O
encodings	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
For	O
the	O
base	O
model,	O
we	O
use	O
a	O
rate	O
of	O
P	O
drop	B-HyperparameterName
=	O
0.1.	B-HyperparameterValue
Label	B-HyperparameterName
Smoothing	I-HyperparameterName
During	O
training,	O
we	O
employed	O
label	O
smoothing	O
of	O
value	O
ls	B-HyperparameterName
=	O
0.1	B-HyperparameterValue
.	O
This	O
hurts	O
perplexity,	B-MetricName
as	O
the	O
model	O
learns	O
to	O
be	O
more	O
unsure,	O
but	O
improves	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
score.	O
6	B-MetricValue
Results	O

We	O
used	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
β	O
1	O
=	O
0.9,	O
β	O
2	O
=	O
0.98	O
and	O
=	O
10	O
−9	O
.	O
We	O
varied	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
over	O
the	O
course	O
of	O
training,	O
according	O
to	O
the	O
formula:	O
lrate	O
=	O
d	O
−0.5	O
model	O
•	O
min(step_num	O
−0.5	O
,	O
step_num	O
•	O
warmup_steps	O
−1.5	O
)(3)	O
This	O
corresponds	O
to	O
increasing	O
the	O
learning	O
rate	O
linearly	O
for	O
the	O
first	O
warmup_steps	O
training	O
steps,	O
and	O
decreasing	O
it	O
thereafter	O
proportionally	O
to	O
the	O
inverse	O
square	O
root	O
of	O
the	O
step	O
number.	O
We	O
used	O
warmup_steps	B-HyperparameterName
=	O
4000.	B-HyperparameterValue

We	O
trained	O
our	O
models	O
on	O
one	O
machine	O
with	O
8	O
NVIDIA	O
P100	O
GPUs.	O
For	O
our	O
base	O
models	O
using	O
the	O
hyperparameters	O
described	O
throughout	O
the	O
paper,	O
each	O
training	O
step	O
took	O
about	O
0.4	O
seconds.	O
We	O
trained	O
the	O
base	O
models	O
for	O
a	O
total	O
of	O
100,000	B-HyperparameterValue
steps	B-HyperparameterName
or	O
12	O
hours.	O
For	O
our	O
big	O
models,(described	O
on	O
the	O
bottom	O
line	O
of	O
table	O
3),	O
step	O
time	O
was	O
1.0	O
seconds.	O
The	O
big	O
models	O
were	O
trained	O
for	O
300,000	B-HyperparameterValue
steps	B-HyperparameterName
(3.5	O
days).	O

We	O
trained	O
on	O
the	O
standard	O
WMT	B-DatasetName
2014	I-DatasetName
English-German	I-DatasetName
dataset	O
consisting	O
of	O
about	O
4.5	O
million	O
sentence	O
pairs.	O
Sentences	O
were	O
encoded	O
using	O
byte-pair	O
encoding	O
,	O
which	O
has	O
a	O
shared	O
sourcetarget	O
vocabulary	O
of	O
about	O
37000	O
tokens.	O
For	O
English-French,	O
we	O
used	O
the	O
significantly	O
larger	O
WMT	B-DatasetName
2014	I-DatasetName
English-French	I-DatasetName
dataset	O
consisting	O
of	O
36M	O
sentences	O
and	O
split	O
tokens	O
into	O
a	O
32000	O
word-piece	O
vocabulary	O
.	O
Sentence	O
pairs	O
were	O
batched	O
together	O
by	O
approximate	O
sequence	O
length.	O
Each	O
training	O
batch	O
contained	O
a	O
set	O
of	O
sentence	O
pairs	O
containing	O
approximately	O
25000	O
source	O
tokens	O
and	O
25000	O
target	O
tokens.	O

This	O
section	O
describes	O
the	O
training	O
regime	O
for	O
our	O
models.	O

In	O
this	O
section	O
we	O
compare	O
various	O
aspects	O
of	O
self-attention	O
layers	O
to	O
the	O
recurrent	O
and	O
convolutional	O
layers	O
commonly	O
used	O
for	O
mapping	O
one	O
variable-length	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
another	O
sequence	O
of	O
equal	O
length	O
(z	O
1	O
,	O
...,	O
z	O
n	O
),	O
with	O
x	O
i	O
,	O
z	O
i	O
∈	O
R	O
d	O
,	O
such	O
as	O
a	O
hidden	O
layer	O
in	O
a	O
typical	O
sequence	O
transduction	O
encoder	O
or	O
decoder.	O
Motivating	O
our	O
use	O
of	O
self-attention	O
we	O
consider	O
three	O
desiderata.	O
One	O
is	O
the	O
total	O
computational	O
complexity	O
per	O
layer.	O
Another	O
is	O
the	O
amount	O
of	O
computation	O
that	O
can	O
be	O
parallelized,	O
as	O
measured	O
by	O
the	O
minimum	O
number	O
of	O
sequential	O
operations	O
required.	O
The	O
third	O
is	O
the	O
path	O
length	O
between	O
long-range	O
dependencies	O
in	O
the	O
network.	O
Learning	O
long-range	O
dependencies	O
is	O
a	O
key	O
challenge	O
in	O
many	O
sequence	O
transduction	O
tasks.	O
One	O
key	O
factor	O
affecting	O
the	O
ability	O
to	O
learn	O
such	O
dependencies	O
is	O
the	O
length	O
of	O
the	O
paths	O
forward	O
and	O
backward	O
signals	O
have	O
to	O
traverse	O
in	O
the	O
network.	O
The	O
shorter	O
these	O
paths	O
between	O
any	O
combination	O
of	O
positions	O
in	O
the	O
input	O
and	O
output	O
sequences,	O
the	O
easier	O
it	O
is	O
to	O
learn	O
long-range	O
dependencies	O
.	O
Hence	O
we	O
also	O
compare	O
the	O
maximum	O
path	O
length	O
between	O
any	O
two	O
input	O
and	O
output	O
positions	O
in	O
networks	O
composed	O
of	O
the	O
different	O
layer	O
types.	O
As	O
noted	O
in	O
Table	O
1,	O
a	O
self-attention	O
layer	O
connects	O
all	O
positions	O
with	O
a	O
constant	O
number	O
of	O
sequentially	O
executed	O
operations,	O
whereas	O
a	O
recurrent	O
layer	O
requires	O
O(n)	O
sequential	O
operations.	O
In	O
terms	O
of	O
computational	O
complexity,	O
self-attention	O
layers	O
are	O
faster	O
than	O
recurrent	O
layers	O
when	O
the	O
sequence	O
length	O
n	O
is	O
smaller	O
than	O
the	O
representation	O
dimensionality	O
d,	O
which	O
is	O
most	O
often	O
the	O
case	O
with	O
sentence	O
representations	O
used	O
by	O
state-of-the-art	O
models	O
in	O
machine	O
translations,	O
such	O
as	O
word-piece	O
and	O
byte-pair	O
representations.	O
To	O
improve	O
computational	O
performance	O
for	O
tasks	O
involving	O
very	O
long	O
sequences,	O
self-attention	O
could	O
be	O
restricted	O
to	O
considering	O
only	O
a	O
neighborhood	O
of	O
size	O
r	O
in	O
the	O
input	O
sequence	O
centered	O
around	O
the	O
respective	O
output	O
position.	O
This	O
would	O
increase	O
the	O
maximum	O
path	O
length	O
to	O
O(n/r).	O
We	O
plan	O
to	O
investigate	O
this	O
approach	O
further	O
in	O
future	O
work.	O
A	O
single	O
convolutional	O
layer	O
with	O
kernel	O
width	O
k	O
<	O
n	O
does	O
not	O
connect	O
all	O
pairs	O
of	O
input	O
and	O
output	O
positions.	O
Doing	O
so	O
requires	O
a	O
stack	O
of	O
O(n/k)	O
convolutional	O
layers	O
in	O
the	O
case	O
of	O
contiguous	O
kernels,	O
or	O
O(log	O
k	O
(n))	O
in	O
the	O
case	O
of	O
dilated	O
convolutions	O
,	O
increasing	O
the	O
length	O
of	O
the	O
longest	O
paths	O
between	O
any	O
two	O
positions	O
in	O
the	O
network.	O
Convolutional	O
layers	O
are	O
generally	O
more	O
expensive	O
than	O
recurrent	O
layers,	O
by	O
a	O
factor	O
of	O
k.	O
Separable	O
convolutions	O
,	O
however,	O
decrease	O
the	O
complexity	O
considerably,	O
to	O
O(k	O
•	O
n	O
•	O
d	O
+	O
n	O
•	O
d	O
2	O
)	O
.	O
Even	O
with	O
k	O
=	O
n,	O
however,	O
the	O
complexity	O
of	O
a	O
separable	O
convolution	O
is	O
equal	O
to	O
the	O
combination	O
of	O
a	O
self-attention	O
layer	O
and	O
a	O
point-wise	O
feed-forward	O
layer,	O
the	O
approach	O
we	O
take	O
in	O
our	O
model.	O
As	O
side	O
benefit,	O
self-attention	O
could	O
yield	O
more	O
interpretable	O
models.	O
We	O
inspect	O
attention	O
distributions	O
from	O
our	O
models	O
and	O
present	O
and	O
discuss	O
examples	O
in	O
the	O
appendix.	O
Not	O
only	O
do	O
individual	O
attention	O
heads	O
clearly	O
learn	O
to	O
perform	O
different	O
tasks,	O
many	O
appear	O
to	O
exhibit	O
behavior	O
related	O
to	O
the	O
syntactic	O
and	O
semantic	O
structure	O
of	O
the	O
sentences.	O

Since	O
our	O
model	O
contains	O
no	O
recurrence	O
and	O
no	O
convolution,	O
in	O
order	O
for	O
the	O
model	O
to	O
make	O
use	O
of	O
the	O
order	O
of	O
the	O
sequence,	O
we	O
must	O
inject	O
some	O
information	O
about	O
the	O
relative	O
or	O
absolute	O
position	O
of	O
the	O
O(n	O
2	O
•	O
d)	O
O(1)	O
O(1)	O
Recurrent	O
O(n	O
•	O
d	O
2	O
)	O
O(n)	O
O(n)	O
Convolutional	O
O(k	O
•	O
n	O
•	O
d	O
2	O
)	O
O(1)	O
O(log	O
k	O
(n))	O
Self-Attention	O
(restricted)	O
O(r	O
•	O
n	O
•	O
d)	O
O(1)	O
O(n/r)	O
tokens	O
in	O
the	O
sequence.	O
To	O
this	O
end,	O
we	O
add	O
"positional	O
encodings"	O
to	O
the	O
input	O
embeddings	O
at	O
the	O
bottoms	O
of	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
The	O
positional	O
encodings	O
have	O
the	O
same	O
dimension	O
d	O
model	O
as	O
the	O
embeddings,	O
so	O
that	O
the	O
two	O
can	O
be	O
summed.	O
There	O
are	O
many	O
choices	O
of	O
positional	O
encodings,	O
learned	O
and	O
fixed	O
.	O
In	O
this	O
work,	O
we	O
use	O
sine	O
and	O
cosine	O
functions	O
of	O
different	O
frequencies:	O
P	O
E	O
(pos,2i)	O
=	O
sin(pos/10000	O
2i/dmodel	O
)	O
P	O
E	O
(pos,2i+1)	O
=	O
cos(pos/10000	O
2i/dmodel	O
)	O
where	O
pos	O
is	O
the	O
position	O
and	O
i	O
is	O
the	O
dimension.	O
That	O
is,	O
each	O
dimension	O
of	O
the	O
positional	O
encoding	O
corresponds	O
to	O
a	O
sinusoid.	O
The	O
wavelengths	O
form	O
a	O
geometric	O
progression	O
from	O
2π	O
to	O
10000	O
•	O
2π.	O
We	O
chose	O
this	O
function	O
because	O
we	O
hypothesized	O
it	O
would	O
allow	O
the	O
model	O
to	O
easily	O
learn	O
to	O
attend	O
by	O
relative	O
positions,	O
since	O
for	O
any	O
fixed	O
offset	O
k,	O
P	O
E	O
pos+k	O
can	O
be	O
represented	O
as	O
a	O
linear	O
function	O
of	O
P	O
E	O
pos	O
.	O
We	O
also	O
experimented	O
with	O
using	O
learned	O
positional	O
embeddings	O
instead,	O
and	O
found	O
that	O
the	O
two	O
versions	O
produced	O
nearly	O
identical	O
results	O
(see	O
Table	O
3	O
row	O
(E)).	O
We	O
chose	O
the	O
sinusoidal	O
version	O
because	O
it	O
may	O
allow	O
the	O
model	O
to	O
extrapolate	O
to	O
sequence	O
lengths	O
longer	O
than	O
the	O
ones	O
encountered	O
during	O
training.	O

Similarly	O
to	O
other	O
sequence	O
transduction	O
models,	O
we	O
use	O
learned	O
embeddings	O
to	O
convert	O
the	O
input	O
tokens	O
and	O
output	O
tokens	O
to	O
vectors	O
of	O
dimension	O
d	O
model	O
.	O
We	O
also	O
use	O
the	O
usual	O
learned	O
linear	O
transformation	O
and	O
softmax	O
function	O
to	O
convert	O
the	O
decoder	O
output	O
to	O
predicted	O
next-token	O
probabilities.	O
In	O
our	O
model,	O
we	O
share	O
the	O
same	O
weight	O
matrix	O
between	O
the	O
two	O
embedding	O
layers	O
and	O
the	O
pre-softmax	O
linear	O
transformation,	O
similar	O
to	O
.	O
In	O
the	O
embedding	O
layers,	O
we	O
multiply	O
those	O
weights	O
by	O
√	O
d	O
model	O
.	O

In	O
addition	O
to	O
attention	O
sub-layers,	O
each	O
of	O
the	O
layers	O
in	O
our	O
encoder	O
and	O
decoder	O
contains	O
a	O
fully	O
connected	O
feed-forward	O
network,	O
which	O
is	O
applied	O
to	O
each	O
position	O
separately	O
and	O
identically.	O
This	O
consists	O
of	O
two	O
linear	O
transformations	O
with	O
a	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
in	O
between.	O
FFN(x)	O
=	O
max(0,	O
xW	O
1	O
+	O
b	O
1	O
)W	O
2	O
+	O
b	O
2	O
(2)	O
While	O
the	O
linear	O
transformations	O
are	O
the	O
same	O
across	O
different	O
positions,	O
they	O
use	O
different	O
parameters	O
from	O
layer	O
to	O
layer.	O
Another	O
way	O
of	O
describing	O
this	O
is	O
as	O
two	O
convolutions	O
with	O
kernel	B-HyperparameterName
size	I-HyperparameterName
1.	B-HyperparameterValue
The	O
dimensionality	O
of	O
input	O
and	O
output	O
is	O
d	O
model	O
=	O
512,	O
and	O
the	O
inner-layer	O
has	O
dimensionality	O
d	O
f	O
f	O
=	O
2048.	O

The	O
Transformer	B-MethodName
uses	O
multi-head	O
attention	O
in	O
three	O
different	O
ways:	O
•	O
In	O
"encoder-decoder	O
attention"	O
layers,	O
the	O
queries	O
come	O
from	O
the	O
previous	O
decoder	O
layer,	O
and	O
the	O
memory	O
keys	O
and	O
values	O
come	O
from	O
the	O
output	O
of	O
the	O
encoder.	O
This	O
allows	O
every	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
over	O
all	O
positions	O
in	O
the	O
input	O
sequence.	O
This	O
mimics	O
the	O
typical	O
encoder-decoder	O
attention	O
mechanisms	O
in	O
sequence-to-sequence	O
models	O
such	O
as	O
.	O
•	O
The	O
encoder	O
contains	O
self-attention	O
layers.	O
In	O
a	O
self-attention	O
layer	O
all	O
of	O
the	O
keys,	O
values	O
and	O
queries	O
come	O
from	O
the	O
same	O
place,	O
in	O
this	O
case,	O
the	O
output	O
of	O
the	O
previous	O
layer	O
in	O
the	O
encoder.	O
Each	O
position	O
in	O
the	O
encoder	O
can	O
attend	O
to	O
all	O
positions	O
in	O
the	O
previous	O
layer	O
of	O
the	O
encoder.	O
•	O
Similarly,	O
self-attention	O
layers	O
in	O
the	O
decoder	O
allow	O
each	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
to	O
all	O
positions	O
in	O
the	O
decoder	O
up	O
to	O
and	O
including	O
that	O
position.	O
We	O
need	O
to	O
prevent	O
leftward	O
information	O
flow	O
in	O
the	O
decoder	O
to	O
preserve	O
the	O
auto-regressive	O
property.	O
We	O
implement	O
this	O
inside	O
of	O
scaled	O
dot-product	O
attention	O
by	O
masking	O
out	O
(setting	O
to	O
−∞)	O
all	O
values	O
in	O
the	O
input	O
of	O
the	O
softmax	O
which	O
correspond	O
to	O
illegal	O
connections.	O
See	O
Figure	O
2.	O

Instead	O
of	O
performing	O
a	O
single	O
attention	O
function	O
with	O
d	O
model	O
-dimensional	O
keys,	O
values	O
and	O
queries,	O
we	O
found	O
it	O
beneficial	O
to	O
linearly	O
project	O
the	O
queries,	O
keys	O
and	O
values	O
h	O
times	O
with	O
different,	O
learned	O
linear	O
projections	O
to	O
d	O
k	O
,	O
d	O
k	O
and	O
d	O
v	O
dimensions,	O
respectively.	O
On	O
each	O
of	O
these	O
projected	O
versions	O
of	O
queries,	O
keys	O
and	O
values	O
we	O
then	O
perform	O
the	O
attention	O
function	O
in	O
parallel,	O
yielding	O
d	O
v	O
-dimensional	O
output	O
values.	O
These	O
are	O
concatenated	O
and	O
once	O
again	O
projected,	O
resulting	O
in	O
the	O
final	O
values,	O
as	O
depicted	O
in	O
Figure	O
2.	O
Multi-head	O
attention	O
allows	O
the	O
model	O
to	O
jointly	O
attend	O
to	O
information	O
from	O
different	O
representation	O
subspaces	O
at	O
different	O
positions.	O
With	O
a	O
single	O
attention	O
head,	O
averaging	O
inhibits	O
this.	O
MultiHead(Q,	O
K,	O
V	O
)	O
=	O
Concat(head	O
1	O
,	O
...,	O
head	O
h	O
)W	O
O	O
where	O
head	O
i	O
=	O
Attention(QW	O
Q	O
i	O
,	O
KW	O
K	O
i	O
,	O
V	O
W	O
V	O
i	O
)	O
Where	O
the	O
projections	O
are	O
parameter	O
matrices	O
W	O
Q	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
K	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
V	O
i	O
∈	O
R	O
dmodel×dv	O
and	O
W	O
O	O
∈	O
R	O
hdv×dmodel	O
.	O
In	O
this	O
work	O
we	O
employ	O
h	O
=	O
8	O
parallel	O
attention	O
layers,	O
or	O
heads.	O
For	O
each	O
of	O
these	O
we	O
use	O
d	O
k	O
=	O
d	O
v	O
=	O
d	O
model	O
/h	O
=	O
64.	O
Due	O
to	O
the	O
reduced	O
dimension	O
of	O
each	O
head,	O
the	O
total	O
computational	O
cost	O
is	O
similar	O
to	O
that	O
of	O
single-head	O
attention	O
with	O
full	O
dimensionality.	O

We	O
call	O
our	O
particular	O
attention	O
"Scaled	O
Dot-Product	O
Attention"	O
(Figure	O
2).	O
The	O
input	O
consists	O
of	O
queries	O
and	O
keys	O
of	O
dimension	O
d	O
k	O
,	O
and	O
values	O
of	O
dimension	O
d	O
v	O
.	O
We	O
compute	O
the	O
dot	O
products	O
of	O
the	O
query	O
with	O
all	O
keys,	O
divide	O
each	O
by	O
√	O
d	O
k	O
,	O
and	O
apply	O
a	O
softmax	O
function	O
to	O
obtain	O
the	O
weights	O
on	O
the	O
values.	O
In	O
practice,	O
we	O
compute	O
the	O
attention	O
function	O
on	O
a	O
set	O
of	O
queries	O
simultaneously,	O
packed	O
together	O
into	O
a	O
matrix	O
Q.	O
The	O
keys	O
and	O
values	O
are	O
also	O
packed	O
together	O
into	O
matrices	O
K	O
and	O
V	O
.	O
We	O
compute	O
the	O
matrix	O
of	O
outputs	O
as:	O
Attention(Q,	O
K,	O
V	O
)	O
=	O
softmax(	O
QK	O
T	O
√	O
d	O
k	O
)V	O
(1)	O
The	O
two	O
most	O
commonly	O
used	O
attention	O
functions	O
are	O
additive	O
attention	O
,	O
and	O
dot-product	O
(multiplicative)	O
attention.	O
Dot-product	O
attention	O
is	O
identical	O
to	O
our	O
algorithm,	O
except	O
for	O
the	O
scaling	O
factor	O
of	O
1	O
√	O
d	O
k	O
.	O
Additive	O
attention	O
computes	O
the	O
compatibility	O
function	O
using	O
a	O
feed-forward	O
network	O
with	O
a	O
single	O
hidden	O
layer.	O
While	O
the	O
two	O
are	O
similar	O
in	O
theoretical	O
complexity,	O
dot-product	O
attention	O
is	O
much	O
faster	O
and	O
more	O
space-efficient	O
in	O
practice,	O
since	O
it	O
can	O
be	O
implemented	O
using	O
highly	O
optimized	O
matrix	O
multiplication	O
code.	O
While	O
for	O
small	O
values	O
of	O
d	O
k	O
the	O
two	O
mechanisms	O
perform	O
similarly,	O
additive	O
attention	O
outperforms	O
dot	O
product	O
attention	O
without	O
scaling	O
for	O
larger	O
values	O
of	O
d	O
k	O
.	O
We	O
suspect	O
that	O
for	O
large	O
values	O
of	O
d	O
k	O
,	O
the	O
dot	O
products	O
grow	O
large	O
in	O
magnitude,	O
pushing	O
the	O
softmax	O
function	O
into	O
regions	O
where	O
it	O
has	O
extremely	O
small	O
gradients	O
4	O
.	O
To	O
counteract	O
this	O
effect,	O
we	O
scale	O
the	O
dot	O
products	O
by	O
1	O
√	O
d	O
k	O
.	O

An	O
attention	O
function	O
can	O
be	O
described	O
as	O
mapping	O
a	O
query	O
and	O
a	O
set	O
of	O
key-value	O
pairs	O
to	O
an	O
output,	O
where	O
the	O
query,	O
keys,	O
values,	O
and	O
output	O
are	O
all	O
vectors.	O
The	O
output	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
values,	O
where	O
the	O
weight	O
assigned	O
to	O
each	O
value	O
is	O
computed	O
by	O
a	O
compatibility	O
function	O
of	O
the	O
query	O
with	O
the	O
corresponding	O
key.	O

Encoder:	O
The	O
encoder	O
is	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
Each	O
layer	O
has	O
two	O
sub-layers.	O
The	O
first	O
is	O
a	O
multi-head	O
self-attention	O
mechanism,	O
and	O
the	O
second	O
is	O
a	O
simple,	O
positionwise	O
fully	O
connected	O
feed-forward	O
network.	O
We	O
employ	O
a	O
residual	O
connection	O
around	O
each	O
of	O
the	O
two	O
sub-layers,	O
followed	O
by	O
layer	O
normalization	O
.	O
That	O
is,	O
the	O
output	O
of	O
each	O
sub-layer	O
is	O
LayerNorm(x	O
+	O
Sublayer(x)),	O
where	O
Sublayer(x)	O
is	O
the	O
function	O
implemented	O
by	O
the	O
sub-layer	O
itself.	O
To	O
facilitate	O
these	O
residual	O
connections,	O
all	O
sub-layers	O
in	O
the	O
model,	O
as	O
well	O
as	O
the	O
embedding	O
layers,	O
produce	O
outputs	O
of	O
dimension	O
d	O
model	O
=	O
512.	O
Decoder:	O
The	O
decoder	O
is	O
also	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
In	O
addition	O
to	O
the	O
two	O
sub-layers	O
in	O
each	O
encoder	O
layer,	O
the	O
decoder	O
inserts	O
a	O
third	O
sub-layer,	O
which	O
performs	O
multi-head	O
attention	O
over	O
the	O
output	O
of	O
the	O
encoder	O
stack.	O
Similar	O
to	O
the	O
encoder,	O
we	O
employ	O
residual	O
connections	O
around	O
each	O
of	O
the	O
sub-layers,	O
followed	O
by	O
layer	O
normalization.	O
We	O
also	O
modify	O
the	O
self-attention	O
sub-layer	O
in	O
the	O
decoder	O
stack	O
to	O
prevent	O
positions	O
from	O
attending	O
to	O
subsequent	O
positions.	O
This	O
masking,	O
combined	O
with	O
fact	O
that	O
the	O
output	O
embeddings	O
are	O
offset	O
by	O
one	O
position,	O
ensures	O
that	O
the	O
predictions	O
for	O
position	O
i	O
can	O
depend	O
only	O
on	O
the	O
known	O
outputs	O
at	O
positions	O
less	O
than	O
i.	O

Most	O
competitive	O
neural	O
sequence	O
transduction	O
models	O
have	O
an	O
encoder-decoder	O
structure	O
.	O
Here,	O
the	O
encoder	O
maps	O
an	O
input	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
a	O
sequence	O
of	O
continuous	O
representations	O
z	O
=	O
(z	O
1	O
,	O
...,	O
z	O
n	O
).	O
Given	O
z,	O
the	O
decoder	O
then	O
generates	O
an	O
output	O
sequence	O
(y	O
1	O
,	O
...,	O
y	O
m	O
)	O
of	O
symbols	O
one	O
element	O
at	O
a	O
time.	O
At	O
each	O
step	O
the	O
model	O
is	O
auto-regressive	O
,	O
consuming	O
the	O
previously	O
generated	O
symbols	O
as	O
additional	O
input	O
when	O
generating	O
the	O
next.	O
The	O
Transformer	B-MethodName
follows	O
this	O
overall	O
architecture	O
using	O
stacked	O
self-attention	O
and	O
point-wise,	O
fully	O
connected	O
layers	O
for	O
both	O
the	O
encoder	O
and	O
decoder,	O
shown	O
in	O
the	O
left	O
and	O
right	O
halves	O
of	O
Figure	O
1,	O
respectively.	O
Figure	O
1:	O
The	O
Transformer	B-MethodName
-model	O
architecture.	O

The	O
goal	O
of	O
reducing	O
sequential	O
computation	O
also	O
forms	O
the	O
foundation	O
of	O
the	O
Extended	O
Neural	O
GPU	O
,	O
ByteNet	B-MethodName
and	O
ConvS2S	B-MethodName
,	O
all	O
of	O
which	O
use	O
convolutional	O
neural	O
networks	O
as	O
basic	O
building	O
block,	O
computing	O
hidden	O
representations	O
in	O
parallel	O
for	O
all	O
input	O
and	O
output	O
positions.	O
In	O
these	O
models,	O
the	O
number	O
of	O
operations	O
required	O
to	O
relate	O
signals	O
from	O
two	O
arbitrary	O
input	O
or	O
output	O
positions	O
grows	O
in	O
the	O
distance	O
between	O
positions,	O
linearly	O
for	O
ConvS2S	B-MethodName
and	O
logarithmically	O
for	O
ByteNet.	B-MethodName
This	O
makes	O
it	O
more	O
difficult	O
to	O
learn	O
dependencies	O
between	O
distant	O
positions	O
.	O
In	O
the	O
Transformer	B-MethodName
this	O
is	O
reduced	O
to	O
a	O
constant	O
number	O
of	O
operations,	O
albeit	O
at	O
the	O
cost	O
of	O
reduced	O
effective	O
resolution	O
due	O
to	O
averaging	O
attention-weighted	O
positions,	O
an	O
effect	O
we	O
counteract	O
with	O
Multi-Head	O
Attention	O
as	O
described	O
in	O
section	O
3.2.	O
Self-attention,	O
sometimes	O
called	O
intra-attention	O
is	O
an	O
attention	O
mechanism	O
relating	O
different	O
positions	O
of	O
a	O
single	O
sequence	O
in	O
order	O
to	O
compute	O
a	O
representation	O
of	O
the	O
sequence.	O
Self-attention	O
has	O
been	O
used	O
successfully	O
in	O
a	O
variety	O
of	O
tasks	O
including	O
reading	O
comprehension,	O
abstractive	O
summarization,	O
textual	O
entailment	O
and	O
learning	O
task-independent	O
sentence	O
representations	O
.	O
End-to-end	O
memory	O
networks	O
are	O
based	O
on	O
a	O
recurrent	O
attention	O
mechanism	O
instead	O
of	O
sequencealigned	O
recurrence	O
and	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
simple-language	B-TaskName
question	I-TaskName
answering	I-TaskName
and	O
language	B-TaskName
modeling	I-TaskName
tasks	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge,	O
however,	O
the	O
Transformer	B-MethodName
is	O
the	O
first	O
transduction	O
model	O
relying	O
entirely	O
on	O
self-attention	O
to	O
compute	O
representations	O
of	O
its	O
input	O
and	O
output	O
without	O
using	O
sequencealigned	O
RNNs	O
or	O
convolution.	O
In	O
the	O
following	O
sections,	O
we	O
will	O
describe	O
the	O
Transformer,	B-MethodName
motivate	O
self-attention	O
and	O
discuss	O
its	O
advantages	O
over	O
models	O
such	O
as	O
and	O
.	O

Recurrent	O
neural	O
networks,	O
long	O
short-term	O
memory	O
and	O
gated	O
recurrent	O
neural	O
networks	O
in	O
particular,	O
have	O
been	O
firmly	O
established	O
as	O
state	O
of	O
the	O
art	O
approaches	O
in	O
sequence	O
modeling	O
and	O
transduction	O
problems	O
such	O
as	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
Numerous	O
efforts	O
have	O
since	O
continued	O
to	O
push	O
the	O
boundaries	O
of	O
recurrent	O
language	O
models	O
and	O
encoder-decoder	O
architectures	O
.	O
Recurrent	O
models	O
typically	O
factor	O
computation	O
along	O
the	O
symbol	O
positions	O
of	O
the	O
input	O
and	O
output	O
sequences.	O
Aligning	O
the	O
positions	O
to	O
steps	O
in	O
computation	O
time,	O
they	O
generate	O
a	O
sequence	O
of	O
hidden	O
states	O
h	O
t	O
,	O
as	O
a	O
function	O
of	O
the	O
previous	O
hidden	O
state	O
h	O
t−1	O
and	O
the	O
input	O
for	O
position	O
t.	O
This	O
inherently	O
sequential	O
nature	O
precludes	O
parallelization	O
within	O
training	O
examples,	O
which	O
becomes	O
critical	O
at	O
longer	O
sequence	O
lengths,	O
as	O
memory	O
constraints	O
limit	O
batching	O
across	O
examples.	O
Recent	O
work	O
has	O
achieved	O
significant	O
improvements	O
in	O
computational	O
efficiency	O
through	O
factorization	O
tricks	O
and	O
conditional	O
computation	O
,	O
while	O
also	O
improving	O
model	O
performance	O
in	O
case	O
of	O
the	O
latter.	O
The	O
fundamental	O
constraint	O
of	O
sequential	O
computation,	O
however,	O
remains.	O
Attention	O
mechanisms	O
have	O
become	O
an	O
integral	O
part	O
of	O
compelling	O
sequence	O
modeling	O
and	O
transduction	O
models	O
in	O
various	O
tasks,	O
allowing	O
modeling	O
of	O
dependencies	O
without	O
regard	O
to	O
their	O
distance	O
in	O
the	O
input	O
or	O
output	O
sequences	O
.	O
In	O
all	O
but	O
a	O
few	O
cases	O
,	O
however,	O
such	O
attention	O
mechanisms	O
are	O
used	O
in	O
conjunction	O
with	O
a	O
recurrent	O
network.	O
In	O
this	O
work	O
we	O
propose	O
the	O
Transformer,	B-MethodName
a	O
model	O
architecture	O
eschewing	O
recurrence	O
and	O
instead	O
relying	O
entirely	O
on	O
an	O
attention	O
mechanism	O
to	O
draw	O
global	O
dependencies	O
between	O
input	O
and	O
output.	O
The	O
Transformer	B-MethodName
allows	O
for	O
significantly	O
more	O
parallelization	O
and	O
can	O
reach	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
translation	O
quality	O
after	O
being	O
trained	O
for	O
as	O
little	O
as	O
twelve	O
hours	O
on	O
eight	O
P100	O
GPUs.	O

The	O
dominant	O
sequence	O
transduction	O
models	O
are	O
based	O
on	O
complex	O
recurrent	O
or	O
convolutional	O
neural	O
networks	O
that	O
include	O
an	O
encoder	O
and	O
a	O
decoder.	O
The	O
best	O
performing	O
models	O
also	O
connect	O
the	O
encoder	O
and	O
decoder	O
through	O
an	O
attention	O
mechanism.	O
We	O
propose	O
a	O
new	O
simple	O
network	O
architecture,	O
the	O
Transformer,	B-MethodName
based	O
solely	O
on	O
attention	O
mechanisms,	O
dispensing	O
with	O
recurrence	O
and	O
convolutions	O
entirely.	O
Experiments	O
on	O
two	O
machine	B-TaskName
translation	I-TaskName
tasks	O
show	O
these	O
models	O
to	O
be	O
superior	O
in	O
quality	O
while	O
being	O
more	O
parallelizable	O
and	O
requiring	O
significantly	O
less	O
time	O
to	O
train.	O
Our	O
model	O
achieves	O
28.4	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
Englishto-German	I-DatasetName
translation	O
task,	O
improving	O
over	O
the	O
existing	O
best	O
results,	O
including	O
ensembles,	O
by	O
over	O
2	B-MetricValue
BLEU.	B-MetricName
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
model	O
establishes	O
a	O
new	O
single-model	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
41.8	B-MetricValue
after	O
training	O
for	O
3.5	O
days	O
on	O
eight	O
GPUs,	O
a	O
small	O
fraction	O
of	O
the	O
training	O
costs	O
of	O
the	O
best	O
models	O
from	O
the	O
literature.	O
We	O
show	O
that	O
the	O
Transformer	B-MethodName
generalizes	O
well	O
to	O
other	O
tasks	O
by	O
applying	O
it	O
successfully	O
to	O
English	B-TaskName
constituency	I-TaskName
parsing	I-TaskName
both	O
with	O
large	O
and	O
limited	O
training	O
data.	O
*	O
Equal	O
contribution.	O
Listing	O
order	O
is	O
random.	O
Jakob	O
proposed	O
replacing	O
RNNs	O
with	O
self-attention	O
and	O
started	O
the	O
effort	O
to	O
evaluate	O
this	O
idea.	O
Ashish,	O
with	O
Illia,	O
designed	O
and	O
implemented	O
the	O
first	O
Transformer	B-MethodName
models	O
and	O
has	O
been	O
crucially	O
involved	O
in	O
every	O
aspect	O
of	O
this	O
work.	O
Noam	O
proposed	O
scaled	O
dot-product	O
attention,	O
multi-head	O
attention	O
and	O
the	O
parameter-free	O
position	O
representation	O
and	O
became	O
the	O
other	O
person	O
involved	O
in	O
nearly	O
every	O
detail.	O
Niki	O
designed,	O
implemented,	O
tuned	O
and	O
evaluated	O
countless	O
model	O
variants	O
in	O
our	O
original	O
codebase	O
and	O
tensor2tensor.	O
Llion	O
also	O
experimented	O
with	O
novel	O
model	O
variants,	O
was	O
responsible	O
for	O
our	O
initial	O
codebase,	O
and	O
efficient	O
inference	O
and	O
visualizations.	O
Lukasz	O
and	O
Aidan	O
spent	O
countless	O
long	O
days	O
designing	O
various	O
parts	O
of	O
and	O
implementing	O
tensor2tensor,	O
replacing	O
our	O
earlier	O
codebase,	O
greatly	O
improving	O
results	O
and	O
massively	O
accelerating	O
our	O
research.	O
†	O
Work	O
performed	O
while	O
at	O
Google	O
Brain.	O
‡	O
Work	O
performed	O
while	O
at	O
Google	O
Research.	O

We	O
also	O
evaluated	O
performance	O
on	O
WMT16	B-DatasetName
Romanian-English,	I-DatasetName
augmented	O
with	O
back-translation	O
data	O
from	O
Sennrich	O
et	O
al.	O
(2016).	O
We	O
use	O
a	O
6-layer	O
transformer	O
source	O
encoder	O
to	O
map	O
Romanian	O
into	O
a	O
representation	O
that	O
BART	B-MethodName
is	O
able	O
to	O
de-noise	O
into	O
English,	O
following	O
the	O
approach	O
introduced	O
in	O
§3.4.	O
Experiment	O
results	O
are	O
presented	O
in	O
Table	O
6.	O
We	O
compare	O
our	O
results	O
against	O
a	O
baseline	O
Transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017)	O
with	O
Transformerlarge	O
settings	O
(the	O
baseline	O
row).	O
We	O
show	O
the	O
performance	O
of	O
both	O
steps	O
of	O
our	O
model	O
in	O
the	O
fixed	O
BART	B-MethodName
and	O
tuned	O
BART	B-MethodName
rows.	O
For	O
each	O
row	O
we	O
experiment	O
on	O
the	O
original	O
WMT16	B-DatasetName
Romanian-English	I-DatasetName
augmented	O
with	O
back-translation	O
data.	O
We	O
use	O
a	O
beam	B-HyperparameterName
width	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
α	B-HyperparameterValue
=	I-HyperparameterValue
1.	I-HyperparameterValue
Preliminary	O
results	O
suggested	O
that	O
our	O
approach	O
was	O
less	O
effective	O
without	O
back-translation	O
data,	O
and	O
prone	O
to	O
overfitting-future	O
work	O
should	O
explore	O
additional	O
regularization	O
techniques.	O

We	O
also	O
experiment	O
with	O
several	O
text	O
generation	O
tasks.	O
BART	B-MethodName
is	O
fine-tuned	O
as	O
a	O
standard	O
sequence-to-sequence	O
model	O
from	O
the	O
input	O
to	O
the	O
output	O
text.	O
During	O
finetuning	O
we	O
use	O
a	O
label	O
smoothed	O
cross	O
entropy	O
loss	O
(Pereyra	O
et	O
al.,	O
2017),	O
with	O
the	O
smoothing	B-HyperparameterName
parameter	I-HyperparameterName
set	O
to	O
0.1.	B-HyperparameterValue
During	O
generation,	O
we	O
set	O
beam	B-HyperparameterName
size	I-HyperparameterName
as	O
5,	B-HyperparameterValue
remove	O
duplicated	O
trigrams	O
in	O
beam	O
search,	O
and	O
tuned	O
the	O
model	O
with	O
min-len,	O
max-len,	O
length	O
penalty	O
on	O
the	O
validation	O
set	O
(Fan	O
et	O
al.,	O
2017	O
Summarization	O
To	O
provide	O
a	O
comparison	O
with	O
the	O
state-of-the-art	O
in	O
summarization,	O
we	O
present	O
results	O
on	O
two	O
summarization	O
datasets,	O
CNN/DailyMail	B-DatasetName
and	O
XSum,	B-DatasetName
which	O
have	O
distinct	O
properties.	O
Summaries	O
in	O
the	O
CNN/DailyMail	B-DatasetName
tend	O
to	O
resemble	O
source	O
sentences.	O
Extractive	O
models	O
do	O
well	O
here,	O
and	O
even	O
the	O
baseline	O
of	O
the	O
first-three	O
source	O
sentences	O
is	O
highly	O
competitive.	O
Nevertheless,	O
BART	O
outperforms	O
all	O
existing	O
work.	O
In	O
contrast,	O
XSum	B-MethodName
is	O
highly	O
abstractive,	O
and	O
extractive	O
models	O
perform	O
poorly.	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work,	O
which	O
leverages	O
BERT,	B-MethodName
by	O
roughly	O
6.0	B-MetricValue
points	I-MetricValue
on	O
all	O
ROUGE	B-MetricName
metrics-representing	O
a	O
significant	O
advance	O
in	O
performance	O
on	O
this	O
problem.	O
Qualitatively,	O
sample	O
quality	O
is	O
high	O
(see	O
§6).	O
Dialogue	O
We	O
evaluate	O
dialogue	O
response	O
generation	O
on	O
CONVAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
in	O
which	O
agents	O
must	O
generate	O
responses	O
conditioned	O
on	O
both	O
the	O
previous	O
context	O
and	O
a	O
textually-specified	O
persona.	O
BART	B-MethodName
outperforms	O
previous	O
work	O
on	O
two	O
automated	O
metrics.	O
Abstractive	B-TaskName
QA	I-TaskName
We	O
use	O
the	O
recently	O
proposed	O
ELI5	B-DatasetName
dataset	O
to	O
test	O
the	O
model's	O
ability	O
to	O
generate	O
long	O
freeform	O
answers.	O
We	O
find	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work	O
by	O
1.2	B-MetricValue
ROUGE-L,	B-MetricName
but	O
the	O
dataset	O
remains	O
a	O
challenging,	O
because	O
answers	O
are	O
only	O
weakly	O
specified	O
by	O
the	O
question.	O

Table	O
2	O
compares	O
the	O
performance	O
of	O
BART	B-MethodName
with	O
several	O
recent	O
approaches	O
on	O
the	O
well-studied	O
SQuAD	B-TaskName
and	O
GLUE	B-TaskName
tasks	O
(Warstadt	O
et	O
al.,	O
2018;Socher	O
et	O
al.,	O
2013;Dolan	O
&	O
Brockett,	O
2005;Agirre	O
et	O
al.,	O
2007;Williams	O
et	O
al.,	O
2018;Dagan	O
et	O
al.,	O
2006;Levesque	O
et	O
al.,	O
2011).	O
The	O
most	O
directly	O
comparable	O
baseline	O
is	O
RoBERTa,	B-MethodName
which	O
was	O
pre-trained	O
with	O
the	O
same	O
resources,	O
but	O
a	O
different	O
objective.	O
Overall,	O
BART	B-MethodName
performs	O
similarly,	O
with	O
only	O
small	O
differences	O
between	O
the	O
models	O
on	O
most	O
tasks.	O
suggesting	O
that	O
BART's	B-MethodName
improvements	O
on	O
generation	O
tasks	O
do	O
not	O
come	O
at	O
the	O
expense	O
of	O
classification	O
performance.	O

We	O
pre-train	O
a	O
large	O
model	O
with	O
12	O
layers	O
in	O
each	O
of	O
the	O
encoder	O
and	O
decoder,	O
and	O
a	O
hidden	O
size	O
of	O
1024.	O
Following	O
RoBERTa	B-MethodName
,	O
we	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8000,	B-HyperparameterValue
and	O
train	O
the	O
model	O
for	O
500000	B-HyperparameterValue
steps.	B-HyperparameterName
Documents	O
are	O
tokenized	O
with	O
the	O
same	O
byte-pair	O
encoding	O
as	O
GPT-2	B-MethodName
(Radford	O
et	O
al.,	O
2019).	O
Based	O
on	O
the	O
results	O
in	O
Section	O
§4,	O
we	O
use	O
a	O
combination	O
of	O
text	O
infilling	O
and	O
sentence	O
permutation.	O
We	O
mask	O
30%	B-MetricValue
of	O
tokens	O
in	O
each	O
document,	O
and	O
permute	O
all	O
sentences.	O
Although	O
sentence	O
permutation	O
only	O
shows	O
significant	O
additive	O
gains	O
on	O
the	O
CNN/DM	B-DatasetName
summarization	I-DatasetName
dataset,	O
we	O
hypothesised	O
that	O
larger	O
pre-trained	O
models	O
may	O
be	O
better	O
able	O
to	O
learn	O
from	O
this	O
task.	O
To	O
help	O
the	O
model	O
better	O
fit	O
the	O
data,	O
we	O
disabled	O
dropout	B-HyperparameterName
for	O
the	O
final	O
10%	B-MetricValue
of	O
training	O
steps.	O
We	O
use	O
the	O
same	O
pre-training	O
data	O
as	O
,	O
consisting	O
of	O
160Gb	O
of	O
news,	O
books,	O
stories,	O
and	O
web	O
text.	O

Recent	O
work	O
has	O
shown	O
that	O
downstream	O
performance	O
can	O
dramatically	O
improve	O
when	O
pre-training	O
is	O
scaled	O
to	O
large	O
batch	O
sizes	O
(Yang	O
et	O
al.,	O
2019;	O
and	O
corpora.	O
To	O
test	O
how	O
well	O
BART	B-MethodName
performs	O
in	O
this	O
regime,	O
and	O
to	O
create	O
a	O
useful	O
model	O
for	O
downstream	O
tasks,	O
we	O
trained	O
BART	B-MethodName
using	O
the	O
same	O
scale	O
as	O
the	O
RoBERTa	B-MethodName
model.	O

BART	B-MethodName
shows	O
large	O
improvements	O
on	O
summarization	O
metrics,	O
of	O
up	O
to	O
6	O
points	O
over	O
the	O
prior	O
state-of-the-art.	O
To	O
understand	O
BART's	B-MethodName
performance	O
beyond	O
automated	O
metrics,	O
we	O
analyse	O
its	O
generations	O
qualitatively.	O
Table	O
7	O
shows	O
example	O
summaries	O
generated	O
by	O
BART.	B-MethodName
Examples	O
are	O
taken	O
from	O
WikiNews	B-DatasetName
articles	I-DatasetName
published	O
after	O
the	O
creation	O
of	O
the	O
pre-training	O
corpus,	O
to	O
eliminate	O
the	O
possibility	O
of	O
the	O
events	O
described	O
being	O
present	O
in	O
the	O
model's	O
training	O
data.	O
Following	O
Narayan	O
et	O
al.	O
(2018),	O
we	O
remove	O
the	O
first	O
sentence	O
of	O
the	O
article	O
prior	O
to	O
summarizing	O
it,	O
so	O
there	O
is	O
no	O
easy	O
extractive	O
summary	O
of	O
the	O
document.	O
Unsurprisingly,	O
model	O
output	O
is	O
fluent	O
and	O
grammatical	O
English.	O
However,	O
model	O
output	O
is	O
also	O
highly	O
abstractive,	O
with	O
few	O
phrases	O
copied	O
from	O
the	O
input.	O
The	O
output	O
is	O
also	O
generally	O
factually	O
accurate,	O
and	O
integrates	O
supporting	O
evidence	O
from	O
across	O
the	O
input	O
document	O
with	O
background	O
knowledge	O
(for	O
example,	O
correctly	O
completing	O
names,	O
or	O
inferring	O
that	O
PG&E	O
operates	O
in	O
California).	O
In	O
the	O
first	O
example,	O
inferring	O
that	O
fish	O
are	O
protecting	O
reefs	O
from	O
global	O
warming	O
requires	O
non-trivial	O
inference	O
from	O
the	O
text.	O
However,	O
the	O
claim	O
that	O
the	O
work	O
was	O
published	O
in	O
Science	O
is	O
not	O
supported	O
by	O
the	O
source.	O
These	O
samples	O
demonstrate	O
that	O
the	O
BART	B-MethodName
pretraining	O
has	O
learned	O
a	O
strong	O
combination	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
and	I-TaskName
generation.	I-TaskName

Early	O
methods	O
for	O
pretraining	O
were	O
based	O
on	O
language	O
models.	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018)	O
only	O
models	O
leftward	O
context,	O
which	O
is	O
problematic	O
for	O
some	O
tasks.	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018)	O
concatenates	O
left-only	O
and	O
right-only	O
representations,	O
but	O
does	O
not	O
pre-train	O
interactions	O
between	O
these	O
features.	O
Radford	O
et	O
al.	O
(2019)	O
demonstrated	O
that	O
very	O
large	O
language	O
models	O
can	O
act	O
as	O
unsupervised	O
multitask	O
models.	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
introduced	O
masked	O
language	O
modelling,	O
which	O
allows	O
pre-training	O
to	O
learn	O
interactions	O
between	O
left	O
and	O
right	O
context	O
words.	O
Recent	O
work	O
has	O
shown	O
that	O
very	O
strong	O
performance	O
can	O
be	O
achieved	O
by	O
training	O
for	O
longer	O
,	O
by	O
tying	O
parameters	O
across	O
layers	O
(Lan	O
et	O
al.,	O
2019),	O
and	O
by	O
masking	O
spans	O
instead	O
of	O
words	O
.	O
Predictions	O
are	O
not	O
made	O
auto-regressively,	O
reducing	O
the	O
effectiveness	O
of	O
BERT	B-MethodName
for	O
generation	O
tasks.	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019)	O
fine-tunes	O
BERT	B-MethodName
with	O
an	O
ensemble	O
of	O
masks,	O
some	O
of	O
which	O
allow	O
only	O
leftward	O
context.	O
Like	O
BART,	B-MethodName
this	O
allows	O
UniLM	B-MethodName
to	O
be	O
used	O
for	O
both	O
generative	O
and	O
discriminative	O
tasks.	O
A	O
difference	O
is	O
that	O
UniLM	B-MethodName
predictions	O
are	O
conditionally	O
independent,	O
whereas	O
BART's	B-MethodName
are	O
autoregressive.	O
BART	B-MethodName
reduces	O
the	O
mismatch	O
between	O
pre-training	O
and	O
generation	O
tasks,	O
because	O
the	O
decoder	O
is	O
always	O
trained	O
on	O
uncorrupted	O
context.	O
MASS	B-MethodName
(Song	O
et	O
al.,	O
2019)	O
is	O
perhaps	O
the	O
most	O
similar	O
model	O
to	O
BART.	B-MethodName
An	O
input	O
sequence	O
where	O
a	O
contiguous	O
span	O
of	O
tokens	O
is	O
masked	O
is	O
mapped	O
to	O
a	O
sequence	O
consisting	O
of	O
the	O
missing	O
tokens.	O
MASS	B-MethodName
is	O
less	O
effective	O
for	O
discriminative	O
tasks,	O
because	O
disjoint	O
sets	O
of	O
tokens	O
are	O
fed	O
into	O
the	O
encoder	O
and	O
decoder.	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
extends	O
BERT	B-MethodName
by	O
pre-Source	O
Document	O
(abbreviated)	O

The	O
researchers	O
examined	O
three	O
types	O
of	O
coral	O
in	O
reefs	O
off	O
the	O
coast	O
of	O
Fiji	O
...	O
The	O
researchers	O
found	O
when	O
fish	O
were	O
plentiful,	O
they	O
would	O
eat	O
algae	O
and	O
seaweed	O
off	O
the	O
corals,	O
which	O
appeared	O
to	O
leave	O
them	O
more	O
resistant	O
to	O
the	O
bacterium	O
Vibrio	O
coralliilyticus,	O
a	O
bacterium	O
associated	O
with	O
bleaching.	O
The	O
researchers	O
suggested	O
the	O
algae,	O
like	O
warming	O
temperatures,	O
might	O
render	O
the	O
corals'	O
chemical	O
defenses	O
less	O
effective,	O
and	O
the	O
fish	O
were	O
protecting	O
the	O
coral	O
by	O
removing	O
the	O
algae.	O
Fisheries	O
off	O
the	O
coast	O
of	O
Fiji	O
are	O
protecting	O
coral	O
reefs	O
from	O
the	O
effects	O
of	O
global	O
warming,	O
according	O
to	O
a	O
study	O
in	O
the	O
journal	O
Science.	O
Sacoolas,	O
who	O
has	O
immunity	O
as	O
a	O
diplomat's	O
wife,	O
was	O
involved	O
in	O
a	O
traffic	O
collision	O
...	O
Prime	O
Minister	O
Johnson	O
was	O
questioned	O
about	O
the	O
case	O
while	O
speaking	O
to	O
the	O
press	O
at	O
a	O
hospital	O
in	O
Watford.	O
He	O
said,	O
"I	O
hope	O
that	O
Anne	O
Sacoolas	O
will	O
come	O
back	O
...	O
if	O
we	O
can't	O
resolve	O
it	O
then	O
of	O
course	O
I	O
will	O
be	O
raising	O
it	O
myself	O
personally	O
with	O
the	O
White	O
House."	O
Boris	O
Johnson	O
has	O
said	O
he	O
will	O
raise	O
the	O
issue	O
of	O
US	O
diplomat	O
Anne	O
Sacoolas'	O
diplomatic	O
immunity	O
with	O
the	O
White	O
House.	O
PG&E	O
stated	O
it	O
scheduled	O
the	O
blackouts	O
in	O
response	O
to	O
forecasts	O
for	O
high	O
winds	O
amid	O
dry	O
conditions.	O
The	O
aim	O
is	O
to	O
reduce	O
the	O
risk	O
of	O
wildfires.	O
Nearly	O
800	O
thousand	O
customers	O
were	O
scheduled	O
to	O
be	O
affected	O
by	O
the	O
shutoffs	O
which	O
were	O
expected	O
to	O
last	O
through	O
at	O
least	O
midday	O
tomorrow.	O
Power	O
has	O
been	O
turned	O
off	O
to	O
millions	O
of	O
customers	O
in	O
California	O
as	O
part	O
of	O
a	O
power	O
shutoff	O
plan.	O
dicting	O
masked	O
tokens	O
auto-regressively	O
in	O
a	O
permuted	O
order.	O
This	O
objective	O
allows	O
predictions	O
to	O
condition	O
on	O
both	O
left	O
and	O
right	O
context.	O
In	O
contrast,	O
the	O
BART	B-MethodName
decoder	O
works	O
left-to-right	O
during	O
pre-training,	O
matching	O
the	O
setting	O
during	O
generation.	O
Several	O
papers	O
have	O
explored	O
using	O
pre-trained	O
representations	O
to	O
improve	O
machine	B-TaskName
translation.	I-TaskName
The	O
largest	O
improvements	O
have	O
come	O
from	O
pre-training	O
on	O
both	O
source	O
and	O
target	O
languages	O
(Song	O
et	O
al.,	O
2019;Lample	O
&	O
Conneau,	O
2019),	O
but	O
this	O
requires	O
pretraining	O
on	O
all	O
languages	O
of	O
interest.	O
Other	O
work	O
has	O
shown	O
that	O
encoders	O
can	O
be	O
improved	O
using	O
pre-trained	O
representations	O
(Edunov	O
et	O
al.,	O
2019),	O
but	O
gains	O
in	O
decoders	O
are	O
more	O
limited.	O
We	O
show	O
how	O
BART	B-MethodName
can	O
be	O
used	O
to	O
improve	O
machine	O
translation	O
decoders.	O

We	O
introduced	O
BART,	B-MethodName
a	O
pre-training	O
approach	O
that	O
learns	O
to	O
map	O
corrupted	O
documents	O
to	O
the	O
original.	O
BART	B-MethodName
achieves	O
similar	O
performance	O
to	O
RoBERTa	B-MethodName
on	O
discriminative	O
tasks,	O
while	O
achieving	O
new	O
state-of-theart	O
results	O
on	O
a	O
number	O
of	O
text	B-TaskName
generation	I-TaskName
tasks.	O
Future	O
work	O
should	O
explore	O
new	O
methods	O
for	O
corrupting	O
documents	O
for	O
pre-training,	O
perhaps	O
tailoring	O
them	O
to	O
specific	O
end	O
tasks.	O

The	O
Masked	O
Language	O
Model	O
and	O
the	O
Permuted	O
Language	O
Model	O
perform	O
less	O
well	O
than	O
others	O
on	O
generation,	O
and	O
are	O
the	O
only	O
models	O
we	O
consider	O
that	O
do	O
not	O
include	O
left-to-right	O
auto-regressive	O
language	O
modelling	O
during	O
pre-training.	O
Bidirectional	O
encoders	O
are	O
crucial	O
for	O
SQuAD	B-DatasetName
As	O
noted	O
in	O
previous	O
work	O
(Devlin	O
et	O
al.,	O
2019),	O
just	O
left-to-right	O
decoder	O
performs	O
poorly	O
on	O
SQuAD,	B-DatasetName
because	O
future	O
context	O
is	O
crucial	O
in	O
classification	O
decisions.	O
However,	O
BART	B-MethodName
achieves	O
similar	O
performance	O
with	O
only	O
half	O
the	O
number	O
of	O
bidirectional	O
layers.	O
The	O
pre-training	O
objective	O
is	O
not	O
the	O
only	O
important	O
factor	O
Our	O
Permuted	O
Language	O
Model	O
performs	O
less	O
well	O
than	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019).	O
Some	O
of	O
this	O
difference	O
is	O
likely	O
due	O
to	O
not	O
including	O
other	O
architectural	O
improvements,	O
such	O
as	O
relative-position	O
embeddings	O
or	O
segment-level	O
recurrence.	O
Pure	O
language	O
models	O
perform	O
best	O
on	O
ELI5	B-DatasetName
The	O
ELI5	B-DatasetName
dataset	O
is	O
an	O
outlier,	O
with	O
much	O
higher	O
perplexities	B-MetricName
than	O
other	O
tasks,	O
and	O
is	O
the	O
only	O
generation	O
task	O
where	O
other	O
models	O
outperform	O
BART.	B-MethodName
A	O
pure	O
language	O
model	O
performs	O
best,	O
suggesting	O
that	O
BART	B-MethodName
is	O
less	O
effective	O
when	O
the	O
output	O
is	O
only	O
loosely	O
constrained	O
by	O
the	O
input.	O
BART	B-MethodName
achieves	O
the	O
most	O
consistently	O
strong	O
performance.	O
With	O
the	O
exception	O
of	O
ELI5,	B-DatasetName
BART	B-MethodName
models	O
using	O
text-infilling	O
perform	O
well	O
on	O
all	O
tasks.	O

SQuAD	B-MethodName
(Rajpurkar	O
et	O
al.,	O
2016)a	O
an	O
extractive	O
question	O
answering	O
task	O
on	O
Wikipedia	O
paragraphs.	O
Answers	O
are	O
text	O
spans	O
extracted	O
from	O
a	O
given	O
document	O
context.	O
Similar	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
use	O
concatenated	O
question	O
and	O
context	O
as	O
input	O
to	O
the	O
encoder	O
of	O
BART,	B-MethodName
and	O
additionally	O
pass	O
them	O
to	O
the	O
decoder.	O
The	O
model	O
includes	O
classifiers	O
to	O
predict	O
the	O
start	O
and	O
end	O
indices	O
of	O
each	O
token.	O
MNLI	B-DatasetName
(Williams	O
et	O
al.,	O
2017),	O
a	O
bitext	B-TaskName
classification	I-TaskName
task	I-TaskName
to	O
predict	O
whether	O
one	O
sentence	O
entails	O
another.	O
The	O
fine-tuned	O
model	O
concatenates	O
the	O
two	O
sentences	O
with	O
appended	O
an	O
EOS	O
token,	O
and	O
passes	O
them	O
to	O
both	O
the	O
BART	B-MethodName
encoder	O
and	O
decoder.	O
In	O
contrast	O
to	O
BERT,	B-MethodName
the	O
representation	O
of	O
the	O
EOS	O
token	O
is	O
used	O
to	O
classify	O
the	O
sentences	O
relations.	O
ELI5	B-DatasetName
(Fan	O
et	O
al.,	O
2019),	O
a	O
long-form	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
dataset.	O
Models	O
generate	O
answers	O
conditioned	O
on	O
the	O
concatenation	O
of	O
a	O
question	O
and	O
supporting	O
documents.	O
XSum	B-DatasetName
(Narayan	O
et	O
al.,	O
2018),	O
a	O
news	O
summarization	O
dataset	O
with	O
highly	O
abstractive	O
summaries.	O
ConvAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
a	O
dialogue	O
response	O
generation	O
task,	O
conditioned	O
on	O
context	O
and	O
a	O
persona.	O
(Hermann	O
et	O
al.,	O
2015),	O
a	O
news	O
summarization	O
dataset.	O
Summaries	O
here	O
are	O
typically	O
closely	O
related	O
to	O
source	O
sentences.	O

While	O
many	O
pre-training	O
objectives	O
have	O
been	O
proposed,	O
fair	O
comparisons	O
between	O
these	O
have	O
been	O
difficult	O
to	O
perform,	O
at	O
least	O
in	O
part	O
due	O
to	O
differences	O
in	O
training	O
data,	O
training	O
resources,	O
architectural	O
differences	O
between	O
models,	O
and	O
fine-tuning	O
procedures.	O
We	O
re-implement	O
strong	O
pre-training	O
approaches	O
recently	O
proposed	O
for	O
discriminative	B-TaskName
and	I-TaskName
generation	I-TaskName
tasks.	I-TaskName
We	O
aim,	O
as	O
much	O
as	O
possible,	O
to	O
control	O
for	O
differences	O
unrelated	O
to	O
the	O
pre-training	O
objective.	O
However,	O
we	O
do	O
make	O
minor	O
changes	O
to	O
the	O
learning	O
rate	O
and	O
usage	O
of	O
layer	O
normalisation	O
in	O
order	O
to	O
improve	O
performance	O
(tuning	O
these	O
separately	O
for	O
each	O
objective).	O
For	O
reference,	O
we	O
compare	O
our	O
implementations	O
with	O
published	O
numbers	O
from	O
BERT,	B-MethodName
which	O
was	O
also	O
trained	O
for	O
1M	B-HyperparameterValue
steps	B-HyperparameterName
on	O
a	O
combination	O
of	O
books	O
and	O
Wikipedia	O
data.	O
We	O
compare	O
the	O
following	O
approaches:	O
Language	O
Model	O
Similarly	O
to	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
we	O
train	O
a	O
left-to-right	O
Transformer	O
language	O
model.	O
This	O
model	O
is	O
equivalent	O
to	O
the	O
BART	B-MethodName
decoder,	O
without	O
cross-attention.	O
Permuted	O
Language	O
Model	O
Based	O
on	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
we	O
sample	O
1/6	O
of	O
the	O
tokens,	O
and	O
generate	O
them	O
in	O
a	O
random	O
order	O
autoregressively.	O
For	O
consistency	O
with	O
other	O
models,	O
we	O
do	O
not	O
implement	O
the	O
relative	O
positional	O
embeddings	O
or	O
attention	O
across	O
segments	O
from	O
XLNet.	B-MethodName
Masked	O
Language	O
Model	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
replace	O
15%	B-MetricValue
of	O
tokens	O
with	O
[MASK]	O
symbols,	O
and	O
train	O
the	O
model	O
to	O
independently	O
predict	O
the	O
original	O
tokens.	O
Multitask	O
Masked	O
Language	O
Model	O
As	O
in	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019),	O
we	O
train	O
a	O
Masked	O
Language	O
Model	O
with	O
additional	O
self-attention	O
masks.	O
Self	O
attention	O
masks	O
are	O
chosen	O
randomly	O
in	O
with	O
the	O
follow	O
proportions:	O
1/6	O
left-to-right,	O
1/6	O
right-to-left,	O
1/3	O
unmasked,	O
and	O
1/3	O
with	O
the	O
first	O
50%	B-MetricValue
of	O
tokens	O
unmasked	O
and	O
a	O
left-to-right	O
mask	O
for	O
the	O
remainder.	O
Masked	B-MethodName
Seq-to-Seq	I-MethodName
Inspired	I-MethodName
by	I-MethodName
MASS	I-MethodName
(Song	O
et	O
al.,	O
2019),	O
we	O
mask	O
a	O
span	O
containing	O
50%	B-MetricValue
of	O
tokens,	O
and	O
train	O
a	O
sequence	O
to	O
sequence	O
model	O
to	O
predict	O
the	O
masked	O
tokens.	O
For	O
the	O
Permuted	O
LM,	O
Masked	B-MethodName
LM	I-MethodName
and	O
Multitask	B-MethodName
Masked	I-MethodName
LM,	I-MethodName
we	O
use	O
two-stream	O
attention	O
(Yang	O
et	O
al.,	O
2019)	O
to	O
efficiently	O
compute	O
likelihoods	O
of	O
the	O
output	O
part	O
of	O
the	O
sequence	O
(using	O
a	O
diagonal	O
self-attention	O
mask	O
on	O
the	O
output	O
to	O
predict	O
words	O
left-to-right).	O
We	O
experiment	O
with	O
(1)	O
treating	O
the	O
task	O
as	O
a	O
standard	O
sequence-to-sequence	O
problem,	O
where	O
the	O
source	O
input	O
to	O
the	O
encoder	O
and	O
the	O
target	O
is	O
the	O
decoder	O
output,	O
or	O
(2)	O
adding	O
the	O
source	O
as	O
prefix	O
to	O
the	O
target	O
in	O
the	O
decoder,	O
with	O
a	O
loss	O
only	O
on	O
the	O
target	O
part	O
of	O
the	O
sequence.	O
We	O
find	O
the	O
former	O
works	O
better	O
for	O
BART	B-MethodName
models,	O
and	O
the	O
latter	O
for	O
other	O
models.	O
To	O
most	O
directly	O
compare	O
our	O
models	O
on	O
their	O
ability	O
to	O
model	O
their	O
fine-tuning	O
objective	O
(the	O
log	O
likelihood	O
of	O
the	O
human	O
text),	O
we	O
report	O
perplexity	B-MetricName
in	O
Table	O
1.	O

BART	B-MethodName
supports	O
a	O
much	O
wider	O
range	O
of	O
noising	O
schemes	O
during	O
pre-training	O
than	O
previous	O
work.	O
We	O
compare	O
a	O
range	O
of	O
options	O
using	O
base-size	O
models	O
(6	O
encoder	O
and	O
6	B-HyperparameterValue
decoder	B-HyperparameterName
layers,	I-HyperparameterName
with	O
a	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	O
768),	B-HyperparameterValue
evaluated	O
on	O
a	O
representative	O
subset	O
of	O
the	O
tasks	O
we	O
will	O
consider	O
for	O
the	O
full	O
large	O
scale	O
experiments	O
in	O
§5.	O

We	O
also	O
explore	O
using	O
BART	B-MethodName
to	O
improve	O
machine	O
translation	O
decoders	O
for	O
translating	O
into	O
English.	O
Previous	O
work	O
Edunov	O
et	O
al.	O
(2019)	O
has	O
shown	O
that	O
models	O
can	O
be	O
improved	O
by	O
incorporating	O
pre-trained	O
encoders,	O
but	O
gains	O
from	O
using	O
pre-trained	O
language	O
models	O
in	O
decoders	O
have	O
been	O
limited.	O
We	O
show	O
that	O
it	O
is	O
possible	O
to	O
use	O
the	O
entire	O
BART	B-MethodName
model	O
(both	O
encoder	O
and	O
decoder)	O
as	O
a	O
single	O
pretrained	O
decoder	O
for	O
machine	B-TaskName
translation,	I-TaskName
by	O
adding	O
a	O
new	O
set	O
of	O
encoder	O
parameters	O
that	O
are	O
learned	O
from	O
bitext	O
(see	O
Figure	O
3b).	O
More	O
precisely,	O
we	O
replace	O
BART's	B-MethodName
encoder	O
embedding	O
layer	O
with	O
a	O
new	O
randomly	O
initialized	O
encoder.	O
The	O
model	O
is	O
trained	O
end-to-end,	O
which	O
trains	O
the	O
new	O
encoder	O
to	O
map	O
foreign	O
words	O
into	O
an	O
input	O
that	O
BART	B-MethodName
can	O
de-noise	O
to	O
English.	O
The	O
new	O
encoder	O
can	O
use	O
a	O
separate	O
vocabulary	O
from	O
the	O
original	O
BART	B-MethodName
model.	O
We	O
train	O
the	O
source	O
encoder	O
in	O
two	O
steps,	O
in	O
both	O
cases	O
backpropagating	O
the	O
cross-entropy	B-MetricName
loss	O
from	O
the	O
output	O
of	O
the	O
BART	B-MethodName
model.	O
In	O
the	O
first	O
step,	O
we	O
freeze	O
most	O
of	O
BART	B-MethodName
parameters	O
and	O
only	O
update	O
the	O
randomly	O
initialized	O
source	O
encoder,	O
the	O
BART	B-MethodName
positional	O
embeddings,	O
and	O
the	O
self-attention	O
input	O
projection	O
matrix	O
of	O
BART's	B-MethodName
encoder	O
first	O
layer.	O
In	O
the	O
second	O
step,	O
we	O
train	O
all	O
model	O
parameters	O
for	O
a	O
small	O
number	O
of	O
iterations.	O

Because	O
BART	B-MethodName
has	O
an	O
autoregressive	O
decoder,	O
it	O
can	O
be	O
directly	O
fine	O
tuned	O
for	O
sequence	O
generation	O
tasks	O
such	O
as	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
and	I-TaskName
summarization.	I-TaskName
In	O
both	O
of	O
these	O
tasks,	O
information	O
is	O
copied	O
from	O
the	O
input	O
but	O
manipulated,	O
which	O
is	O
closely	O
related	O
to	O
the	O
denoising	O
pre-training	O
objective.	O
Here,	O
the	O
encoder	O
input	O
is	O
the	O
input	O
sequence,	O
and	O
the	O
decoder	O
generates	O
outputs	O
autoregressively.	O

For	O
token	B-TaskName
classification	I-TaskName
tasks,	I-TaskName
such	O
as	O
answer	O
endpoint	B-TaskName
classification	I-TaskName
for	O
SQuAD,	B-DatasetName
we	O
feed	O
the	O
complete	O
document	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
use	O
the	O
top	O
hidden	O
state	O
of	O
the	O
decoder	O
as	O
a	O
representation	O
for	O
each	O
word.	O
This	O
representation	O
is	O
used	O
to	O
classify	O
the	O
token.	O

For	O
sequence	B-TaskName
classification	I-TaskName
tasks,	O
the	O
same	O
input	O
is	O
fed	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
the	O
final	O
hidden	O
state	O
of	O
the	O
final	O
decoder	O
token	O
is	O
fed	O
into	O
new	O
multi-class	O
linear	O
classifier.	O
This	O
approach	O
is	O
related	O
to	O
the	O
CLS	O
token	O
in	O
BERT;	B-MethodName
however	O
we	O
add	O
the	O
additional	O
token	O
to	O
the	O
end	O
so	O
that	O
representation	O
for	O
the	O
token	O
in	O
the	O
decoder	O
can	O
attend	O
to	O
decoder	O
states	O
from	O
the	O
complete	O
input	O
(Figure	O
3a).	O

The	O
representations	O
produced	O
by	O
BART	B-MethodName
can	O
be	O
used	O
in	O
several	O
ways	O
for	O
downstream	O
applications.	O

BART	B-MethodName
is	O
trained	O
by	O
corrupting	O
documents	O
and	O
then	O
optimizing	O
a	O
reconstruction	B-MetricName
loss-the	I-MetricName
cross-entropy	B-MetricName
between	O
the	O
decoder's	O
output	O
and	O
the	O
original	O
document.	O
Unlike	O
existing	O
denoising	O
autoencoders,	O
which	O
are	O
tailored	O
to	O
specific	O
noising	O
schemes,	O
BART	B-MethodName
allows	O
us	O
to	O
apply	O
any	O
type	O
of	O
document	O
corruption.	O
In	O
the	O
extreme	O
case,	O
where	O
all	O
information	O
about	O
the	O
source	O
is	O
lost,	O
BART	B-MethodName
is	O
equivalent	O
to	O
a	O
language	O
model.	O
We	O
experiment	O
with	O
several	O
previously	O
proposed	O
and	O
novel	O
transformations,	O
but	O
we	O
believe	O
there	O
is	O
a	O
significant	O
potential	O
for	O
development	O
of	O
other	O
new	O
alternatives.	O
The	O
transformations	O
we	O
used	O
are	O
summarized	O
below,	O
and	O
examples	O
are	O
shown	O
in	O
Figure	O
2.	O
Token	O
Masking	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
random	O
tokens	O
are	O
sampled	O
and	O
replaced	O
with	O
[MASK]	O
elements.	O
Token	O
Deletion	O
Random	O
tokens	O
are	O
deleted	O
from	O
the	O
input.	O
In	O
contrast	O
to	O
token	O
masking,	O
the	O
model	O
must	O
decide	O
which	O
positions	O
are	O
missing	O
inputs.	O
Text	O
Infilling	O
A	O
number	O
of	O
text	O
spans	O
are	O
sampled,	O
with	O
span	O
lengths	O
drawn	O
from	O
a	O
Poisson	O
distribution	O
(λ	O
=	O
3).	O
Each	O
span	O
is	O
replaced	O
with	O
a	O
single	O
[MASK]	O
token.	O
0-length	O
spans	O
correspond	O
to	O
the	O
insertion	O
of	O
[MASK]	O
tokens.	O
Text	O
infilling	O
is	O
inspired	O
by	O
Span-BERT	B-MethodName
,	O
but	O
SpanBERT	B-MethodName
samples	O
span	O
lengths	O
from	O
a	O
different	O
(clamped	O
geometric)	O
distribution,	O
and	O
replaces	O
each	O
span	O
with	O
a	O
sequence	O
of	O
[MASK]	O
tokens	O
of	O
exactly	O
the	O
same	O
length.	O
Text	O
infilling	O
teaches	O
the	O
model	O
to	O
predict	O
how	O
many	O
tokens	O
are	O
missing	O
from	O
a	O
span.	O
Sentence	O
Permutation	O
A	O
document	O
is	O
divided	O
into	O
sentences	O
based	O
on	O
full	O
stops,	O
and	O
these	O
sentences	O
are	O
shuffled	O
in	O
a	O
random	O
order.	O
Document	O
Rotation	O
A	O
token	O
is	O
chosen	O
uniformly	O
at	O
random,	O
and	O
the	O
document	O
is	O
rotated	O
so	O
that	O
it	O
begins	O
with	O
that	O
token.	O
This	O
task	O
trains	O
the	O
model	O
to	O
identify	O
the	O
start	O
of	O
the	O
document.	O

BART	B-MethodName
uses	O
the	O
standard	O
sequence-to-sequence	O
Transformer	O
architecture	O
from	O
(Vaswani	O
et	O
al.,	O
2017),	O
except,	O
following	O
GPT,	B-MethodName
that	O
we	O
modify	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
functions	I-HyperparameterName
to	O
GeLUs	B-HyperparameterValue
(Hendrycks	O
&	O
Gimpel,	O
2016)	O
and	O
initialise	O
parameters	O
from	O
N	O
(0,	O
0.02).	O
For	O
our	O
base	O
model,	O
we	O
use	O
6	O
layers	O
in	O
the	O
encoder	O
and	O
de-coder,	O
and	O
for	O
our	O
large	O
model	O
we	O
use	O
12	O
layers	O
in	O
each.	O
The	O
architecture	O
is	O
closely	O
related	O
to	O
that	O
used	O
in	O
BERT,	B-MethodName
with	O
the	O
following	O
differences:	O
(1)	O
each	O
layer	O
of	O
the	O
decoder	O
additionally	O
performs	O
cross-attention	O
over	O
the	O
final	O
hidden	O
layer	O
of	O
the	O
encoder	O
(as	O
in	O
the	O
transformer	O
sequence-to-sequence	O
model);	O
and	O
(2)	O
BERT	B-MethodName
uses	O
an	O
additional	O
feed-forward	O
network	O
before	O
wordprediction,	O
which	O
BART	B-MethodName
does	O
not.	O
In	O
total,	O
BART	B-MethodName
contains	O
roughly	O
10%	B-MetricValue
more	O
parameters	O
than	O
the	O
equivalently	O
sized	O
BERT	B-MethodName
model.	O

BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
that	O
maps	O
a	O
corrupted	O
document	O
to	O
the	O
original	O
document	O
it	O
was	O
derived	O
from.	O
It	O
is	O
implemented	O
as	O
a	O
sequence-to-sequence	O
model	O
with	O
a	O
bidirectional	O
encoder	O
over	O
corrupted	O
text	O
and	O
a	O
left-to-right	O
autoregressive	O
decoder.	O
For	O
pre-training,	O
we	O
optimize	O
the	O
negative	B-MetricName
log	I-MetricName
likelihood	I-MetricName
of	O
the	O
original	O
document.	O

Bidirectional	O
Encoder	O
A	O
B	O
C	O
D	O
E	O
A	O
_	O
B	O
_	O
E	O
<s>	O
A	O
B	O
C	O
D(	O
c)	O
BART:	O
Inputs	O
to	O
the	O
encoder	O
need	O
not	O
be	O
aligned	O
with	O
decoder	O
outputs,	O
allowing	O
arbitary	O
noise	O
transformations.	O
Here,	O
a	O
document	O
has	O
been	O
corrupted	O
by	O
replacing	O
spans	O
of	O
text	O
with	O
mask	O
symbols.	O
The	O
corrupted	O
document	O
(left)	O
is	O
encoded	O
with	O
a	O
bidirectional	O
model,	O
and	O
then	O
the	O
likelihood	O
of	O
the	O
original	O
document	O
(right)	O
is	O
calculated	O
with	O
an	O
autoregressive	O
decoder.	O
For	O
fine-tuning,	O
an	O
uncorrupted	O
document	O
is	O
input	O
to	O
both	O
the	O
encoder	O
and	O
decoder,	O
and	O
we	O
use	O
representations	O
from	O
the	O
final	O
hidden	O
state	O
of	O
the	O
decoder.	O
Figure	O
1:	O
A	O
schematic	O
comparison	O
of	O
BART	B-MethodName
with	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018).	O
English,	O
by	O
propagation	O
through	O
BART,	B-MethodName
thereby	O
using	O
BART	B-MethodName
as	O
a	O
pre-trained	O
target-side	O
language	O
model.	O
This	O
approach	O
improves	O
performance	O
over	O
a	O
strong	O
back-translation	O
MT	O
baseline	O
by	O
1.1	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
Romanian-English	I-DatasetName
benchmark.	O
To	O
better	O
understand	O
these	O
effects,	O
we	O
also	O
report	O
an	O
ablation	O
analysis	O
that	O
replicates	O
other	O
recently	O
proposed	O
training	O
objectives.	O
This	O
study	O
allows	O
us	O
to	O
carefully	O
control	O
for	O
a	O
number	O
of	O
factors,	O
including	O
data	O
and	O
optimization	O
parameters,	O
which	O
have	O
been	O
shown	O
to	O
be	O
as	O
important	O
for	O
overall	O
performance	O
as	O
the	O
selection	O
of	O
training	O
objectives	O
.	O
We	O
find	O
that	O
BART	B-MethodName
exhibits	O
the	O
most	O
consistently	O
strong	O
performance	O
across	O
the	O
full	O
range	O
of	O
tasks	O
we	O
consider.	O

A	O
B	O
C	O
D	O
E	O
<s>	O
A	O
B	O
C	O
D	O
(b)	O
GPT:	B-MethodName
Tokens	O
are	O
predicted	O
auto-regressively,	O
meaning	O
GPT	B-MethodName
can	O
be	O
used	O
for	O
generation.	O
However	O
words	O
can	O
only	O
condition	O
on	O
leftward	O
context,	O
so	O
it	O
cannot	O
learn	O
bidirectional	O
interactions.	O

Self-supervised	O
methods	O
have	O
achieved	O
remarkable	O
success	O
in	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
(Mikolov	O
et	O
al.,	O
2013;Peters	O
et	O
al.,	O
2018;Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;.	O
The	O
most	O
successful	O
approaches	O
have	O
been	O
variants	O
of	O
masked	O
language	O
models,	O
which	O
are	O
denoising	O
autoencoders	O
that	O
are	O
trained	O
to	O
reconstruct	O
text	O
where	O
a	O
random	O
subset	O
of	O
the	O
words	O
has	O
been	O
masked	O
out.	O
Recent	O
work	O
has	O
shown	O
gains	O
by	O
improving	O
the	O
distribution	O
of	O
masked	O
tokens	O
,	O
the	O
order	O
in	O
which	O
masked	O
tokens	O
are	O
predicted	O
(Yang	O
et	O
al.,	O
2019),	O
and	O
the	O
available	O
context	O
for	O
replacing	O
masked	O
tokens	O
(Dong	O
et	O
al.,	O
2019).	O
However,	O
these	O
methods	O
typically	O
focus	O
on	O
particular	O
types	O
of	O
end	O
tasks	O
(e.g.	O
span	B-TaskName
prediction,	I-TaskName
generation,	B-TaskName
etc.),	O
limiting	O
their	O
applicability.	O
In	O
this	O
paper,	O
we	O
present	O
BART,	B-MethodName
which	O
pre-trains	O
a	O
model	O
combining	O
Bidirectional	O
and	O
Auto-Regressive	O
Transformers.	O
BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
built	O
with	O
a	O
sequence-to-sequence	O
model	O
that	O
is	O
applicable	O
to	O
a	O
very	O
wide	O
range	O
of	O
end	O
tasks.	O
Pretraining	O
has	O
two	O
stages	O
(1)	O
text	O
is	O
corrupted	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
a	O
sequence-to-sequence	O
model	O
is	O
learned	O
to	O
reconstruct	O
the	O
original	O
text.	O
BART	B-MethodName
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes	O
(see	O
Figure	O
1).	O
A	O
key	O
advantage	O
of	O
this	O
setup	O
is	O
the	O
noising	O
flexibility;	O
arbitrary	O
transformations	O
can	O
be	O
applied	O
to	O
the	O
original	O
text,	O
including	O
changing	O
its	O
length.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
arbitrary	O
length	O
spans	O
of	O
text	O
(including	O
zero	O
length)	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
This	O
approach	O
generalizes	O
the	O
original	O
word	O
masking	O
and	O
next	O
sentence	O
prediction	O
objectives	O
in	O
BERT	B-MethodName
by	O
forcing	O
the	O
model	O
to	O
reason	O
more	O
about	O
overall	O
sentence	O
length	O
and	O
make	O
longer	O
range	O
transformations	O
to	O
the	O
input.	O
BART	B-MethodName
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	O
generation	O
but	O
also	O
works	O
well	O
for	O
comprehension	O
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2018)	O
and	O
SQuAD	B-DatasetName
(Rajpurkar	O
et	O
al.,	O
2016),	O
and	O
achieves	O
new	O
state-of-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	O
dialogue,	O
question	O
answering,	O
and	O
summarization	O
tasks.	O
For	O
example,	O
it	O
improves	O
performance	O
by	O
6	B-MetricValue
ROUGE	B-MetricName
over	O
previous	O
work	O
on	O
XSum	O
(Narayan	O
et	O
al.,	O
2018).	O
BART	B-MethodName
also	O
opens	O
up	O
new	O
ways	O
of	O
thinking	O
about	O
fine	O
tuning.	O
We	O
present	O
a	O
new	O
scheme	O
for	O
machine	O
translation	O
where	O
a	O
BART	B-MethodName
model	O
is	O
stacked	O
above	O
a	O
few	O
additional	O
transformer	O
layers.	O
These	O
layers	O
are	O
trained	O
to	O
essentially	O
translate	O
the	O
foreign	O
language	O
to	O
noised	O
Bidirectional	O
Encoder	O
A	O
_	O
C	O
_	O
E	O
B	O
D	O
(a)	O
BERT:	B-MethodName
Random	O
tokens	O
are	O
replaced	O
with	O
masks,	O
and	O
the	O
document	O
is	O
encoded	O
bidirectionally.	O
Missing	O
tokens	O
are	O
predicted	O
independently,	O
so	O
BERT	B-MethodName
cannot	O
easily	O
be	O
used	O
for	O
generation.	O

We	O
present	O
BART,	B-MethodName
a	O
denoising	O
autoencoder	O
for	O
pretraining	O
sequence-to-sequence	O
models.	O
BART	B-MethodName
is	O
trained	O
by	O
(1)	O
corrupting	O
text	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
learning	O
a	O
model	O
to	O
reconstruct	O
the	O
original	O
text.	O
It	O
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
spans	O
of	O
text	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
BART	O
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	B-TaskName
generation	I-TaskName
but	O
also	O
works	O
well	O
for	O
comprehension	B-TaskName
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	O
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
and	O
SQuAD,	B-DatasetName
achieves	O
new	O
stateof-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	B-TaskName
dialogue,	I-TaskName
question	B-TaskName
answering,	I-TaskName
and	O
summarization	O
tasks,	O
with	O
gains	O
of	O
up	O
to	O
6	B-MetricValue
ROUGE.	B-MetricName
BART	B-MethodName
also	O
provides	O
a	O
1.1	B-MetricValue
BLEU	B-MetricName
increase	O
over	O
a	O
back-translation	O
system	O
for	O
machine	O
translation,	B-TaskName
with	O
only	O
target	O
language	O
pretraining.	O
We	O
also	O
report	O
ablation	O
experiments	O
that	O
replicate	O
other	O
pretraining	O
schemes	O
within	O
the	O
BART	B-MethodName
framework,	O
to	O
better	O
measure	O
which	O
factors	O
most	O
influence	O
end-task	O
performance.	O

Task-specific	O
distillation	O
Most	O
of	O
the	O
prior	O
works	O
focus	O
on	O
building	O
task-specific	O
distillation	O
setups.	O
Tang	O
et	O
al.	O
transfer	O
fine-tune	O
classification	O
model	O
BERT	B-MethodName
to	O
an	O
LSTM-based	B-MethodName
classifier.	O
Chatterjee	O
distill	B-MethodName
BERT	I-MethodName
model	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
in	O
a	O
smaller	O
Transformer	B-MethodName
model	O
previously	O
initialized	O
from	O
BERT.	B-MethodName
In	O
the	O
present	O
work,	O
we	O
found	O
it	O
beneficial	O
to	O
use	O
a	O
general-purpose	O
pre-training	O
distillation	O
rather	O
than	O
a	O
task-specific	O
distillation.	O
Turc	O
et	O
al.	O
use	O
the	O
original	O
pretraining	O
objective	O
to	O
train	O
smaller	O
student,	O
then	O
fine-tuned	O
via	O
distillation.	B-MethodName
As	O
shown	O
in	O
the	O
ablation	O
study,	O
we	O
found	O
it	O
beneficial	O
to	O
leverage	O
the	O
teacher's	O
knowledge	O
to	O
pre-train	O
with	O
additional	O
distillation	O
signal.	O
Multi-distillation	O
Yang	O
et	O
al.	O
combine	O
the	O
knowledge	O
of	O
an	O
ensemble	O
of	O
teachers	O
using	O
multi-task	O
learning	O
to	O
regularize	O
the	O
distillation.	O
The	O
authors	O
apply	O
Multi-Task	O
Knowledge	O
Distillation	O
to	O
learn	O
a	O
compact	O
question	O
answering	O
model	O
from	O
a	O
set	O
of	O
large	O
question	B-TaskName
answering	I-TaskName
models.	O
An	O
application	O
of	O
multi-distillation	O
is	O
multi-linguality:	B-TaskName
Tsai	O
et	O
al.	O
adopts	O
a	O
similar	O
approach	O
to	O
us	O
by	O
pre-training	O
a	O
multilingual	O
model	O
from	O
scratch	O
solely	O
through	O
distillation.	O
However,	O
as	O
shown	O
in	O
the	O
ablation	O
study,	O
leveraging	O
the	O
teacher's	O
knowledge	O
with	O
initialization	O
and	O
additional	O
losses	O
leads	O
to	O
substantial	O
gains.	O
Other	O
compression	O
techniques	O
have	O
been	O
studied	O
to	O
compress	O
large	O
models.	O
Recent	O
developments	O
in	O
weights	O
pruning	O
reveal	O
that	O
it	O
is	O
possible	O
to	O
remove	O
some	O
heads	O
in	O
the	O
self-attention	O
at	O
test	O
time	O
without	O
significantly	O
degrading	O
the	O
performance	O
Michel	O
et	O
al.	O
.	O
Some	O
layers	O
can	O
be	O
reduced	O
to	O
one	O
head.	O
A	O
separate	O
line	O
of	O
study	O
leverages	O
quantization	O
to	O
derive	O
smaller	O
models	O
(Gupta	O
et	O
al.	O
).	O
Pruning	O
and	O
quantization	O
are	O
orthogonal	O
to	O
the	O
present	O
work.	O

We	O
introduced	O
DistilBERT,	B-MethodName
a	O
general-purpose	O
pre-trained	O
version	O
of	O
BERT,	B-MethodName
40%	B-MetricValue
smaller,	O
60%	B-MetricValue
faster,	O
that	O
retains	O
97%	B-MetricValue
of	O
the	O
language	O
understanding	O
capabilities.	O
We	O
showed	O
that	O
a	O
general-purpose	O
language	O
model	O
can	O
be	O
successfully	O
trained	O
with	O
distillation	O
and	O
analyzed	O
the	O
various	O
components	O
with	O
an	O
ablation	O
study.	O
We	O
further	O
demonstrated	O
that	O
DistilBERT	B-MethodName
is	O
a	O
compelling	O
option	O
for	O
edge	O
applications.	O

In	O
this	O
section,	O
we	O
investigate	O
the	O
influence	O
of	O
various	O
components	O
of	O
the	O
triple	O
loss	O
and	O
the	O
student	O
initialization	O
on	O
the	O
performances	O
of	O
the	O
distilled	O
model.	O
We	O
report	O
the	O
macro-score	O
on	O
GLUE.	B-DatasetName
Table	O
4	O
presents	O
the	O
deltas	O
with	O
the	O
full	O
triple	O
loss:	O
removing	O
the	O
Masked	B-MetricName
Language	I-MetricName
Modeling	I-MetricName
loss	I-MetricName
has	O
little	O
impact	O
while	O
the	O
two	O
distillation	O
losses	O
account	O
for	O
a	O
large	O
portion	O
of	O
the	O
performance.	O

To	O
further	O
investigate	O
the	O
speed-up/size	O
trade-off	O
of	O
DistilBERT,	B-MethodName
we	O
compare	O
(in	O
Table	O
3)	O
the	O
number	O
of	O
parameters	O
of	O
each	O
model	O
along	O
with	O
the	O
inference	O
time	O
needed	O
to	O
do	O
a	O
full	O
pass	O
on	O
the	O
STS-B	B-DatasetName
development	O
set	O
on	O
CPU	O
(Intel	O
Xeon	O
E5-2690	O
v3	O
Haswell	O
@2.9GHz)	O
using	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1.	B-HyperparameterValue
DistilBERT	B-MethodName
has	O
40%	B-MetricValue
fewer	O
parameters	O
than	O
BERT	B-MethodName
and	O
is	O
60%	B-MetricValue
faster	O
than	O
BERT.	B-MethodName
On	O
device	O
computation	O
We	O
studied	O
whether	O
DistilBERT	B-MethodName
could	O
be	O
used	O
for	O
on-the-edge	O
applications	O
by	O
building	O
a	O
mobile	O
application	O
for	O
question	O
answering.	O
We	O
compare	O
the	O
average	O
inference	O
time	O
on	O
a	O
recent	O
smartphone	O
(iPhone	O
7	O
Plus)	O
against	O
our	O
previously	O
trained	O
question	O
answering	O
model	O
based	O
on	O
BERT-base.	B-MethodName
Excluding	O
the	O
tokenization	O
step,	O
DistilBERT	B-MethodName
is	O
71%	B-MetricValue
faster	O
than	O
BERT,	B-MethodName
and	O
the	O
whole	O
model	O
weighs	O
207	O
MB	O
(which	O
could	O
be	O
further	O
reduced	O
with	O
quantization).	O
Our	O
code	O
is	O
available	O
5	O
.	O

Downstream	O
tasks	O
We	O
further	O
study	O
the	O
performances	O
of	O
DistilBERT	B-MethodName
on	O
several	O
downstream	O
tasks	O
under	O
efficient	O
inference	O
constraints:	O
a	O
classification	O
task	O
(IMDb	O
sentiment	O
classification	O
-	O
Maas	O
et	O
al.	O
)	O
and	O
a	O
question	O
answering	O
task	O
(SQuAD	B-DatasetName
v1.1	O
-Rajpurkar	O
et	O
al.	O
).	O
As	O
shown	O
in	O
Table	O
2,	O
DistilBERT	B-MethodName
is	O
only	O
0.6%	B-MetricValue
point	O
behind	O
BERT	B-MethodName
in	O
test	O
accuracy	B-MetricName
on	O
the	O
IMDb	B-DatasetName
benchmark	O
while	O
being	O
40%	B-MetricValue
smaller.	O
On	O
SQuAD,	B-DatasetName
DistilBERT	B-MethodName
is	O
within	O
3.9	B-MetricValue
points	I-MetricValue
of	O
the	O
full	O
BERT.	B-MethodName
We	O
also	O
studied	O
whether	O
we	O
could	O
add	O
another	O
step	O
of	O
distillation	B-MethodName
during	O
the	O
adaptation	O
phase	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
on	O
SQuAD	B-DatasetName
using	O
a	O
BERT	B-MethodName
model	O
previously	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
as	O
a	O
∅	O
-L	O
cos	O
-L	O
mlm	O
-2.96	O
L	O
ce	O
-∅	O
-L	O
mlm	O
-1.46	O
L	O
ce	O
-L	O
cos	O
-∅	O
-0.	O
31	O
Triple	B-MetricName
loss	I-MetricName
+	O
random	O
weights	O
initialization	O
-3.69	O
teacher	O
for	O
an	O
additional	O
term	O
in	O
the	O
loss	O
(knowledge	O
distillation).	O
In	O
this	O
setting,	O
there	O
are	O
thus	O
two	O
successive	O
steps	O
of	O
distillation,	B-MethodName
one	O
during	O
the	O
pre-training	O
phase	O
and	O
one	O
during	O
the	O
adaptation	O
phase.	O
In	O
this	O
case,	O
we	O
were	O
able	O
to	O
reach	O
interesting	O
performances	O
given	O
the	O
size	O
of	O
the	O
model:	O
79.8	O
F1	O
and	O
70.4	O
EM,	O
i.e.	O
within	O
3	O
points	O
of	O
the	O
full	O
model.	O

General	O
Language	O
Understanding	O
We	O
assess	O
the	O
language	O
understanding	O
and	O
generalization	O
capabilities	O
of	B-MethodName
DistilBERT	I-MethodName
on	O
the	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	I-DatasetName
[Wang	O
et	O
al.,	O
2018],	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
We	O
report	O
scores	O
on	O
the	O
development	O
sets	O
for	O
each	O
task	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
without	O
the	O
use	O
of	O
ensembling	O
or	O
multi-tasking	O
scheme	O
for	O
fine-tuning	O
(which	O
are	O
mostly	O
orthogonal	O
to	O
the	O
present	O
work).	O
We	O
compare	O
the	O
results	O
to	O
the	O
baseline	O
provided	O
by	O
the	O
authors	O
of	O
GLUE:	B-DatasetName
an	O
ELMo	B-MethodName
(Peters	O
et	O
al.	O
)	O
encoder	O
followed	O
by	O
two	O
BiLSTMs.	B-MethodName
4	O
The	O
results	O
on	O
each	O
of	O
the	O
9	O
tasks	O
are	O
showed	O
on	O
Table	O
1	O
along	O
with	O
the	O
macro-score	O
(average	O
of	O
individual	O
scores).	O
Among	O
the	O
9	O
tasks,	O
DistilBERT	B-MethodName
is	O
always	O
on	O
par	O
or	O
improving	O
over	O
the	O
ELMo	B-MethodName
baseline	O
(up	O
to	O
19	B-MetricValue
points	I-MetricValue
of	O
accuracy	B-MetricName
on	O
STS-B).	B-DatasetName
DistilBERT	B-MethodName
also	O
compares	O
surprisingly	O
well	O
to	O
BERT,	B-MethodName
retaining	O
97%	B-MetricValue
of	O
the	O
performance	B-MetricName
with	O
40%	B-MetricValue
fewer	O
parameters.	O

In	O
addition	O
to	O
the	O
previously	O
described	O
optimization	O
and	O
architectural	O
choices,	O
an	O
important	O
element	O
in	O
our	O
training	O
procedure	O
is	O
to	O
find	O
the	O
right	O
initialization	O
for	O
the	O
sub-network	O
to	O
converge.	O
Taking	O
advantage	O
of	O
the	O
common	O
dimensionality	O
between	O
teacher	O
and	O
student	O
networks,	O
we	O
initialize	O
the	O
student	O
from	O
the	O
teacher	O
by	O
taking	O
one	O
layer	O
out	O
of	O
two.	O

Knowledge	B-MethodName
distillation	I-MethodName
[Bucila	O
et	O
al.,	O
2006,	O
Hinton	O
et	O
al.,	O
2015]	O
is	O
a	O
compression	O
technique	O
in	O
which	O
a	O
compact	O
model	O
-the	O
student	O
-is	O
trained	O
to	O
reproduce	O
the	O
behaviour	O
of	O
a	O
larger	O
model	O
-the	O
teacheror	O
an	O
ensemble	O
of	O
models.	O
In	O
supervised	O
learning,	O
a	O
classification	O
model	O
is	O
generally	O
trained	O
to	O
predict	O
an	O
instance	O
class	O
by	O
maximizing	O
the	O
estimated	O
probability	O
of	O
gold	O
labels.	O
A	O
standard	O
training	O
objective	O
thus	O
involves	O
minimizing	O
the	O
cross-entropy	O
between	O
the	O
model's	O
predicted	O
distribution	O
and	O
the	O
one-hot	O
empirical	O
distribution	O
of	O
training	O
labels.	O
A	O
model	O
performing	O
well	O
on	O
the	O
training	O
set	O
will	O
predict	O
an	O
output	O
distribution	O
with	O
high	O
probability	O
on	O
the	O
correct	O
class	O
and	O
with	O
near-zero	O
probabilities	O
on	O
other	O
classes.	O
But	O
some	O
of	O
these	O
"near-zero"	O
probabilities	O
are	O
larger	O
than	O
others	O
and	O
reflect,	O
in	O
part,	O
the	O
generalization	O
capabilities	O
of	O
the	O
model	O
and	O
how	O
well	O
it	O
will	O
perform	O
on	O
the	O
test	O
set	O
3	O
.	O
Training	O
loss	O
The	O
student	O
is	O
trained	O
with	O
a	O
distillation	B-MetricName
loss	I-MetricName
over	O
the	O
soft	O
target	O
probabilities	O
of	O
the	O
teacher:	O
L	O
ce	O
=	O
i	O
t	O
i	O
*	O
log(s	O
i	O
)	O
where	O
t	O
i	O
(resp.	O
s	O
i	O
)	O
is	O
a	O
probability	O
estimated	O
by	O
the	O
teacher	O
(resp.	O
the	O
student).	O
This	O
objective	O
results	O
in	O
a	O
rich	O
training	O
signal	O
by	O
leveraging	O
the	O
full	O
teacher	O
distribution.	O
Following	O
Hinton	O
et	O
al.	O
we	O
used	O
a	O
softmax-temperature:	B-HyperparameterName
p	O
i	O
=	O
exp(zi/T	O
)	O
j	O
exp(zj	O
/T	O
)	O
where	O
T	B-HyperparameterName
controls	O
the	O
smoothness	O
of	O
the	O
output	O
distribution	O
and	O
z	O
i	O
is	O
the	O
model	O
score	O
for	O
the	O
class	O
i.	O
The	O
same	O
temperature	O
T	O
is	O
applied	O
to	O
the	O
student	O
and	O
the	O
teacher	O
at	O
training	O
time,	O
while	O
at	O
inference,	O
T	B-HyperparameterName
is	O
set	O
to	O
1	B-HyperparameterValue
to	O
recover	O
a	O
standard	O
softmax.	O
The	O
final	O
training	O
objective	O
is	O
a	O
linear	O
combination	O
of	O
the	O
distillation	B-MetricName
loss	I-MetricName
L	I-MetricName
ce	O
with	O
the	O
supervised	O
training	O
loss,	O
in	O
our	O
case	O
the	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
loss	I-MetricName
L	O
mlm	O
[Devlin	O
et	O
al.,	O
2018].	O
We	O
found	O
it	O
beneficial	O
to	O
add	O
a	O
cosine	B-MetricName
embedding	I-MetricName
loss	I-MetricName
(L	I-MetricName
cos	I-MetricName
)	I-MetricName
which	O
will	O
tend	O
to	O
align	O
the	O
directions	O
of	O
the	O
student	O
and	O
teacher	O
hidden	O
states	O
vectors.	O
3	O
DistilBERT:	B-MethodName
a	O
distilled	O
version	O
of	O
BERT	B-MethodName
Student	O
architecture	O
In	O
the	O
present	O
work,	O
the	O
student	O
-DistilBERT	B-MethodName
-has	O
the	O
same	O
general	O
architecture	O
as	O
BERT.	B-MethodName
The	O
token-type	O
embeddings	O
and	O
the	O
pooler	O
are	O
removed	O
while	O
the	O
number	O
of	O
layers	O
is	O
reduced	O
by	O
a	O
factor	O
of	O
2.	O
Most	O
of	O
the	O
operations	O
used	O
in	O
the	O
Transformer	B-MethodName
architecture	O
(linear	O
layer	O
and	O
layer	O
normalisation)	O
are	O
highly	O
optimized	O
in	O
modern	O
linear	O
algebra	O
frameworks	O
and	O
our	O
investigations	O
showed	O
that	O
variations	O
on	O
the	O
last	O
dimension	O
of	O
the	O
tensor	O
(hidden	O
size	O
dimension)	O
have	O
a	O
smaller	O
impact	O
on	O
computation	O
efficiency	O
(for	O
a	O
fixed	O
parameters	O
budget)	O
than	O
variations	O
on	O
other	O
factors	O
like	O
the	O
number	O
of	O
layers.	O
Thus	O
we	O
focus	O
on	O
reducing	O
the	O
number	O
of	O
layers.	O

The	O
last	O
two	O
years	O
have	O
seen	O
the	O
rise	O
of	O
Transfer	B-MethodName
Learning	I-MethodName
approaches	O
in	O
Natural	O
Language	O
Processing	O
(NLP)	O
with	O
large-scale	O
pre-trained	O
language	O
models	O
becoming	O
a	O
basic	O
tool	O
in	O
many	O
NLP	O
tasks	O
[Devlin	O
et	O
al.,	O
2018,	O
Radford	O
et	O
al.,	O
2019.	O
While	O
these	O
models	O
lead	O
to	O
significant	O
improvement,	O
they	O
often	O
have	O
several	O
hundred	O
million	O
parameters	O
and	O
current	O
research	O
1	O
on	O
pre-trained	O
models	O
indicates	O
that	O
training	O
even	O
larger	O
models	O
still	O
leads	O
to	O
better	O
performances	O
on	O
downstream	O
tasks.	O
The	O
trend	O
toward	O
bigger	O
models	O
raises	O
several	O
concerns.	O
First	O
is	O
the	O
environmental	O
cost	O
of	O
exponentially	O
scaling	O
these	O
models'	O
computational	O
requirements	O
as	O
mentioned	O
in	O
Schwartz	O
et	O
al.	O
,	O
Strubell	O
et	O
al.	O
.	O
Second,	O
while	O
operating	O
these	O
models	O
on-device	O
in	O
real-time	O
has	O
the	O
potential	O
to	O
enable	O
novel	O
and	O
interesting	O
language	O
processing	O
applications,	O
the	O
growing	O
computational	O
and	O
memory	O
requirements	O
of	O
these	O
models	O
may	O
hamper	O
wide	O
adoption.	O
In	O
this	O
paper,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
reach	O
similar	O
performances	O
on	O
many	O
downstream-tasks	O
using	O
much	O
smaller	O
language	O
models	O
pre-trained	O
with	O
knowledge	O
distillation,	B-MethodName
resulting	O
in	O
models	O
that	O
are	O
lighter	O
and	O
faster	O
at	O
inference	O
time,	O
while	O
also	O
requiring	O
a	O
smaller	O
computational	O
training	O
budget.	O
Our	O
general-purpose	O
pre-trained	O
models	O
can	O
be	O
fine-tuned	O
with	O
good	O
performances	O
on	O
several	O
downstream	O
tasks,	O
keeping	O
the	O
flexibility	O
of	O
larger	O
models.	O
We	O
also	O
show	O
that	O
our	O
compressed	O
models	O
are	O
small	O
enough	O
to	O
run	O
on	O
the	O
edge,	O
e.g.	O
on	O
mobile	O
devices.	O
Using	O
a	O
triple	O
loss,	O
we	O
show	O
that	O
a	O
40%	B-MetricValue
smaller	O
Transformer	O
(Vaswani	O
et	O
al.	O
)	O
pre-trained	O
through	O
distillation	B-MethodName
via	O
the	O
supervision	O
of	O
a	O
bigger	O
Transformer	B-MethodName
language	O
model	O
can	O
achieve	O
similar	O
performance	O
on	O
a	O
variety	O
of	O
downstream	O
tasks,	O
while	O
being	O
60%	O
faster	O
at	O
inference	O
time.	O
Further	O
ablation	O
studies	O
indicate	O
that	O
all	O
the	O
components	O
of	O
the	O
triple	O
loss	O
are	O
important	O
for	O
best	O
performances.	O
We	O
have	O
made	O
the	O
trained	O
weights	O
available	O
along	O
with	O
the	O
training	O
code	O
in	O
the	O
Transformers	O
2	O
library	O
from	O
HuggingFace	O
[Wolf	O
et	O
al.,	O
2019].	O

As	O
Transfer	B-MethodName
Learning	I-MethodName
from	O
large-scale	O
pre-trained	O
models	O
becomes	O
more	O
prevalent	O
in	O
Natural	O
Language	O
Processing	O
(NLP),	O
operating	O
these	O
large	O
models	O
in	O
on-theedge	O
and/or	O
under	O
constrained	O
computational	O
training	O
or	O
inference	O
budgets	O
remains	O
challenging.	O
In	O
this	O
work,	O
we	O
propose	O
a	O
method	O
to	O
pre-train	O
a	O
smaller	O
generalpurpose	O
language	O
representation	O
model,	O
called	O
DistilBERT,	B-MethodName
which	O
can	O
then	O
be	O
finetuned	O
with	O
good	O
performances	O
on	O
a	O
wide	O
range	O
of	O
tasks	O
like	O
its	O
larger	O
counterparts.	O
While	O
most	O
prior	O
work	O
investigated	O
the	O
use	O
of	O
distillation	B-MethodName
for	O
building	O
task-specific	O
models,	O
we	O
leverage	O
knowledge	O
distillation	B-MethodName
during	O
the	O
pre-training	O
phase	O
and	O
show	O
that	O
it	O
is	O
possible	O
to	O
reduce	O
the	O
size	O
of	O
a	O
BERT	B-MethodName
model	O
by	O
40%,	B-MetricValue
while	O
retaining	O
97%	B-MetricValue
of	O
its	O
language	O
understanding	O
capabilities	O
and	O
being	O
60%	B-MetricValue
faster.	O
To	O
leverage	O
the	O
inductive	O
biases	O
learned	O
by	O
larger	O
models	O
during	O
pre-training,	O
we	O
introduce	O
a	O
triple	O
loss	O
combining	O
language	O
modeling,	O
distillation	O
and	O
cosine-distance	O
losses.	O
Our	O
smaller,	O
faster	O
and	O
lighter	O
model	O
is	O
cheaper	O
to	O
pre-train	O
and	O
we	O
demonstrate	O
its	O
capabilities	O
for	O
on-device	O
computations	O
in	O
a	O
proof-of-concept	O
experiment	O
and	O
a	O
comparative	O
on-device	O
study.	O

Byte-Pair	B-MethodName
Encoding	I-MethodName
(BPE)	I-MethodName
(Sennrich	O
et	O
al.,	O
2016)	O
is	O
a	O
hybrid	O
between	O
character-and	O
word-level	O
representations	O
that	O
allows	O
handling	O
the	O
large	O
vocabularies	O
common	O
in	O
natural	O
language	O
corpora.	O
Instead	O
of	O
full	O
words,	O
BPE	O
relies	O
on	O
subwords	O
units,	O
which	O
are	O
extracted	O
by	O
performing	O
statistical	O
analysis	O
of	O
the	O
training	O
corpus.	O
BPE	O
vocabulary	O
sizes	O
typically	O
range	O
from	O
10K-100K	O
subword	O
units.	O
However,	O
unicode	O
characters	O
can	O
account	O
for	O
a	O
sizeable	O
portion	O
of	O
this	O
vocabulary	O
when	O
modeling	O
large	O
and	O
diverse	O
corpora,	O
such	O
as	O
the	O
ones	O
considered	O
in	O
this	O
work.	O
Radford	O
et	O
al.	O
(2019)	O
introduce	O
a	O
clever	O
implementation	O
of	O
BPE	O
that	O
uses	O
bytes	O
instead	O
of	O
unicode	O
characters	O
as	O
the	O
base	O
subword	O
units.	O
Using	O
bytes	O
makes	O
it	O
possible	O
to	O
learn	O
a	O
subword	O
vocabulary	O
of	O
a	O
modest	O
size	O
(50K	O
units)	O
that	O
can	O
still	O
encode	O
any	O
input	O
text	O
without	O
introducing	O
any	O
"unknown"	O
tokens.	O
8	O
Large	O
batch	O
training	O
can	O
improve	O
training	O
efficiency	O
even	O
without	O
large	O
scale	O
parallel	O
hardware	O
through	O
gradient	O
accumulation,	O
whereby	O
gradients	O
from	O
multiple	O
mini-batches	O
are	O
accumulated	O
locally	O
before	O
each	O
optimization	O
step.	O
This	O
functionality	O
is	O
supported	O
natively	O
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
The	O
original	O
BERT	B-MethodName
implementation	O
(Devlin	O
et	O
al.,	O
2019)	O
uses	O
a	O
character-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
of	O
size	O
30K,	B-HyperparameterValue
which	O
is	O
learned	O
after	O
preprocessing	O
the	O
input	O
with	O
heuristic	O
tokenization	O
rules.	O
Following	O
Radford	O
et	O
al.	O
(2019),	O
we	O
instead	O
consider	O
training	O
BERT	B-MethodName
with	O
a	O
larger	O
byte-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
containing	O
50K	B-HyperparameterValue
subword	I-HyperparameterValue
units,	I-HyperparameterValue
without	O
any	O
additional	O
preprocessing	O
or	O
tokenization	O
of	O
the	O
input.	O
This	O
adds	O
approximately	O
15M	O
and	O
20M	O
additional	O
parameters	O
for	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
respectively.	O
Early	O
experiments	O
revealed	O
only	O
slight	O
differences	O
between	O
these	O
encodings,	O
with	O
the	O
Radford	O
et	O
al.	O
(	O
2019)	O
BPE	B-MethodName
achieving	O
slightly	O
worse	O
end-task	O
performance	O
on	O
some	O
tasks.	O
Nevertheless,	O
we	O
believe	O
the	O
advantages	O
of	O
a	O
universal	O
encoding	O
scheme	O
outweighs	O
the	O
minor	O
degredation	O
in	O
performance	O
and	O
use	O
this	O
encoding	O
in	O
the	O
remainder	O
of	O
our	O
experiments.	O
A	O
more	O
detailed	O
comparison	O
of	O
these	O
encodings	O
is	O
left	O
to	O
future	O
work.	O

In	O
the	O
previous	O
section	O
we	O
propose	O
modifications	O
to	O
the	O
BERT	B-MethodName
pretraining	O
procedure	O
that	O
improve	O
end-task	O
performance.	O
We	O
now	O
aggregate	O
these	O
improvements	O
and	O
evaluate	O
their	O
combined	O
impact.	O
We	O
call	O
this	O
configuration	O
RoBERTa	B-MethodName
for	O
Robustly	B-MethodName
optimized	I-MethodName
BERT	I-MethodName
approach.	O
Specifically,	O
RoBERTa	B-MethodName
is	O
trained	O
with	O
dynamic	O
masking	O
(Section	O
4.1),	O
FULL-SENTENCES	O
without	O
NSP	O
loss	O
(Section	O
4.2),	O
large	O
mini-batches	O
(Section	O
4.3)	O
and	O
a	O
larger	O
byte-level	O
BPE	O
(Section	O
4.4).	O
Additionally,	O
we	O
investigate	O
two	O
other	O
important	O
factors	O
that	O
have	O
been	O
under-emphasized	O
in	O
previous	O
work:	O
(1)	O
the	O
data	O
used	O
for	O
pretraining,	O
and	O
(2)	O
the	O
number	O
of	O
training	O
passes	O
through	O
the	O
data.	O
For	O
example,	O
the	O
recently	O
proposed	O
XLNet	B-MethodName
architecture	O
(Yang	O
et	O
al.,	O
2019)	O
is	O
pretrained	O
using	O
nearly	O
10	O
times	O
more	O
data	O
than	O
the	O
original	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
It	O
is	O
also	O
trained	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
eight	O
times	O
larger	O
for	O
half	O
as	O
many	O
optimization	O
steps,	O
thus	O
seeing	O
four	O
times	O
as	O
many	O
sequences	O
in	O
pretraining	O
compared	O
to	O
BERT.	B-MethodName
To	O
help	O
disentangle	O
the	O
importance	O
of	O
these	O
factors	O
from	O
other	O
modeling	O
choices	O
(e.g.,	O
the	O
pretraining	O
objective),	O
we	O
begin	O
by	O
training	O
RoBERTa	B-MethodName
following	O
the	O
BERT	B-MethodName
LARGE	I-MethodName
architecture	O
(L	B-HyperparameterName
=	O
24,	B-HyperparameterValue
H	B-HyperparameterName
=	O
1024,	B-HyperparameterValue
A	B-HyperparameterName
=	O
16,	B-HyperparameterValue
355M	O
parameters).	O
We	O
pretrain	O
for	O
100K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
a	O
comparable	O
BOOK-CORPUS	B-DatasetName
plus	O
WIKIPEDIA	B-DatasetName
dataset	O
as	O
was	O
used	O
in	O
Yang	O
et	O
al.	O
(2019),	O
respectively.	O
Complete	O
results	O
on	O
all	O
GLUE	B-DatasetName
tasks	O
can	O
be	O
found	O
in	O
the	O
Appendix.	O
Devlin	O
et	O
al.	O
(2019).	O
We	O
pretrain	O
our	O
model	O
using	O
1024	O
V100	O
GPUs	O
for	O
approximately	O
one	O
day.	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
4.	O
When	O
controlling	O
for	O
training	O
data,	O
we	O
observe	O
that	O
RoBERTa	B-MethodName
provides	O
a	O
large	O
improvement	O
over	O
the	O
originally	O
reported	O
BERT	B-MethodName
LARGE	I-MethodName
results,	O
reaffirming	O
the	O
importance	O
of	O
the	O
design	O
choices	O
we	O
explored	O
in	O
Section	O
4.	O
Next,	O
we	O
combine	O
this	O
data	O
with	O
the	O
three	O
additional	O
datasets	O
described	O
in	O
Section	O
3.2.	O
We	O
train	O
RoBERTa	B-MethodName
over	O
the	O
combined	O
data	O
with	O
the	O
same	O
number	O
of	O
training	B-HyperparameterName
steps	I-HyperparameterName
as	O
before	O
(100K).	B-HyperparameterValue
In	O
total,	O
we	O
pretrain	O
over	O
160GB	O
of	O
text.	O
We	O
observe	O
further	O
improvements	O
in	O
performance	O
across	O
all	O
downstream	O
tasks,	O
validating	O
the	O
importance	O
of	O
data	O
size	O
and	O
diversity	O
in	O
pretraining.	O
9	O
Finally,	O
we	O
pretrain	O
RoBERTa	B-MethodName
for	O
significantly	O
longer,	O
increasing	O
the	O
number	O
of	O
pretraining	B-HyperparameterName
steps	I-HyperparameterName
from	O
100K	B-HyperparameterValue
to	I-HyperparameterValue
300K,	I-HyperparameterValue
and	O
then	O
further	O
to	O
500K.	B-HyperparameterValue
We	O
again	O
observe	O
significant	O
gains	O
in	O
downstream	O
task	O
performance,	O
and	O
the	O
300K	O
and	O
500K	O
step	O
models	O
outperform	O
XLNet	B-MethodName
LARGE	I-MethodName
across	O
most	O
tasks.	O
We	O
note	O
that	O
even	O
our	O
longest-trained	O
model	O
does	O
not	O
appear	O
to	O
overfit	O
our	O
data	O
and	O
would	O
likely	O
benefit	O
from	O
additional	O
training.	O
In	O
the	O
rest	O
of	O
the	O
paper,	O
we	O
evaluate	O
our	O
best	O
RoBERTa	B-MethodName
model	O
on	O
the	O
three	O
different	O
benchmarks:	O
GLUE,	B-DatasetName
SQuaD	B-DatasetName
and	O
RACE.	B-DatasetName
Specifically	O
we	O
consider	O
RoBERTa	B-MethodName
trained	O
for	O
500K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
all	O
five	O
of	O
the	O
datasets	O
introduced	O
in	O
Section	O
3.2.	O

For	O
GLUE	B-DatasetName
we	O
consider	O
two	O
finetuning	O
settings.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev)	O
we	O
finetune	O
RoBERTa	B-MethodName
separately	O
for	O
each	O
of	O
the	O
GLUE	B-DatasetName
tasks,	O
using	O
only	O
the	O
training	O
data	O
for	O
the	O
corresponding	O
task.	O
We	O
consider	O
a	O
limited	O
hyperparameter	O
sweep	O
for	O
each	O
task,	O
with	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
∈	O
{16,	B-HyperparameterValue
32}	I-HyperparameterValue
and	O
learning	B-HyperparameterName
rates	I-HyperparameterName
∈	O
{1e−5,	B-HyperparameterValue
2e−5,	I-HyperparameterValue
3e−5},	I-HyperparameterValue
with	O
a	O
linear	B-HyperparameterName
warmup	I-HyperparameterName
for	O
the	O
first	O
6%	B-HyperparameterValue
of	O
steps	O
followed	O
by	O
a	O
linear	B-HyperparameterName
decay	I-HyperparameterName
to	O
0.	O
We	O
finetune	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
perform	O
early	O
stopping	O
based	O
on	O
each	O
task's	O
evaluation	O
metric	O
on	O
the	O
dev	O
set.	O
The	O
rest	O
of	O
the	O
hyperparameters	O
remain	O
the	O
same	O
as	O
during	O
pretraining.	O
In	O
this	O
setting,	O
we	O
report	O
the	O
median	O
development	O
set	O
results	O
for	O
each	O
task	O
over	O
five	O
random	O
initializations,	O
without	O
model	O
ensembling.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
compare	O
RoBERTa	B-MethodName
to	O
other	O
approaches	O
on	O
the	O
test	O
set	O
via	O
the	O
GLUE	B-DatasetName
leaderboard.	O
While	O
many	O
submissions	O
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
depend	O
on	O
multitask	O
finetuning,	O
our	O
submission	O
depends	O
only	O
on	O
single-task	O
finetuning.	O
For	O
RTE,	B-DatasetName
STS	B-DatasetName
and	O
MRPC	B-DatasetName
we	O
found	O
it	O
helpful	O
to	O
finetune	O
starting	O
from	O
the	O
MNLI	O
single-task	O
model,	O
rather	O
than	O
the	O
baseline	O
pretrained	O
RoBERTa.	O
We	O
explore	O
a	O
slightly	O
wider	O
hyperparameter	O
space,	O
described	O
in	O
the	O
Appendix,	O
and	O
ensemble	O
between	O
5	O
and	O
7	O
models	O
per	O
task.	O
Task-specific	O
modifications	O
Two	O
of	O
the	O
GLUE	B-DatasetName
tasks	O
require	O
task-specific	O
finetuning	O
approaches	O
to	O
achieve	O
competitive	O
leaderboard	O
results.	O
QNLI:	B-DatasetName
Recent	O
submissions	O
on	O
the	O
GLUE	B-DatasetName
leaderboard	O
adopt	O
a	O
pairwise	O
ranking	O
formulation	O
for	O
the	O
QNLI	B-DatasetName
task,	O
in	O
which	O
candidate	O
answers	O
are	O
mined	O
from	O
the	O
training	O
set	O
and	O
compared	O
to	O
one	O
another,	O
and	O
a	O
single	O
(question,	O
candidate)	O
pair	O
is	O
classified	O
as	O
positive	O
(Liu	O
et	O
al.,	O
2019b,a;Yang	O
et	O
al.,	O
2019).	O
This	O
formulation	O
significantly	O
simplifies	O
the	O
task,	O
but	O
is	O
not	O
directly	O
comparable	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
Following	O
recent	O
work,	O
we	O
adopt	O
the	O
ranking	O
approach	O
for	O
our	O
test	O
submission,	O
but	O
for	O
direct	O
comparison	O
with	O
BERT	B-MethodName
we	O
report	O
development	O
set	O
results	O
based	O
on	O
a	O
pure	O
classification	O
approach.	O
WNLI:	B-DatasetName
We	O
found	O
the	O
provided	O
NLI-format	O
data	O
to	O
be	O
challenging	O
to	O
work	O
with.	O
Instead	O
we	O
use	O
the	O
reformatted	O
WNLI	B-DatasetName
data	O
from	O
Super-GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2019a),	O
which	O
indicates	O
the	O
span	O
of	O
the	O
query	O
pronoun	O
and	O
referent.	O
We	O
finetune	O
RoBERTa	B-MethodName
using	O
the	O
margin	O
ranking	O
loss	O
from	O
Kocijan	O
et	O
al.	O
(2019).	O
For	O
a	O
given	O
input	O
sentence,	O
we	O
use	O
spaCy	O
(Honnibal	O
and	O
Montani,	O
2017)	O
to	O
extract	O
additional	O
candidate	O
noun	O
phrases	O
from	O
the	O
sentence	O
and	O
finetune	O
our	O
model	O
so	O
that	O
it	O
assigns	O
higher	O
scores	O
to	O
positive	O
referent	O
phrases	O
than	O
for	O
any	O
of	O
the	O
generated	O
negative	O
candidate	O
phrases.	O
One	O
unfortunate	O
consequence	O
of	O
this	O
formulation	O
is	O
that	O
we	O
can	O
only	O
make	O
use	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
5.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev),	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
all	O
9	O
of	O
the	O
GLUE	O
task	O
development	O
sets.	O
Crucially,	O
RoBERTa	B-MethodName
uses	O
the	O
same	O
masked	O
language	O
modeling	O
pretraining	O
objective	O
and	O
architecture	O
as	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
yet	O
consistently	O
outperforms	O
both	O
BERT	B-MethodName
LARGE	I-MethodName
and	O
XLNet	B-MethodName
LARGE	I-MethodName
.	O
This	O
raises	O
questions	O
about	O
the	O
relative	O
importance	O
of	O
model	O
architecture	O
and	O
pretraining	O
objective,	O
compared	O
to	O
more	O
mundane	O
details	O
like	O
dataset	O
size	O
and	O
training	O
time	O
that	O
we	O
explore	O
in	O
this	O
work.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
and	O
achieve	O
state-of-the-art	O
results	O
on	O
4	O
out	O
of	O
9	O
tasks	O
and	O
the	O
highest	O
average	O
score	O
to	O
date.	O
This	O
is	O
especially	O
exciting	O
because	O
RoBERTa	B-MethodName
does	O
not	O
depend	O
on	O
multi-task	O
finetuning,	O
unlike	O
most	O
of	O
the	O
other	O
top	O
submissions.	O
We	O
expect	O
future	O
work	O
may	O
further	O
improve	O
these	O
results	O
by	O
incorporating	O
more	O
sophisticated	O
multi-task	O
finetuning	O
procedures.	O

We	O
adopt	O
a	O
much	O
simpler	O
approach	O
for	O
SQuAD	B-DatasetName
compared	O
to	O
past	O
work.	O
In	O
particular,	O
while	O
both	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
XLNet,	B-MethodName
while	O
we	O
use	O
the	O
same	O
learning	O
rate	O
for	O
all	O
layers.	O
For	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
we	O
follow	O
the	O
same	O
finetuning	O
procedure	O
as	O
Devlin	O
et	O
al.	O
(2019).	O
For	O
SQuAD	B-DatasetName
v2.0,	I-DatasetName
we	O
additionally	O
classify	O
whether	O
a	O
given	O
question	O
is	O
answerable;	O
we	O
train	O
this	O
classifier	O
jointly	O
with	O
the	O
span	O
predictor	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O

We	O
present	O
our	O
results	O
in	O
Table	O
6.	O
On	O
the	O
SQuAD	O
v1.1	O
development	O
set,	O
RoBERTa	B-MethodName
matches	O
the	O
state-of-the-art	O
set	O
by	O
XLNet.	B-MethodName
On	O
the	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
development	O
set,	O
RoBERTa	B-DatasetName
sets	O
a	O
new	O
state-of-the-art,	O
improving	O
over	O
XLNet	B-DatasetName
by	B-MetricValue
0.4	I-MetricValue
points	I-MetricValue
(EM)	B-MetricName
and	O
0.6	B-MetricValue
points	I-MetricValue
(F1).	B-MetricName
We	O
also	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
public	O
SQuAD	B-DatasetName
2.0	I-DatasetName
leaderboard	O
and	O
evaluate	O
its	O
performance	O
relative	O
to	O
other	O
systems.	O
Most	O
of	O
the	O
top	O
systems	O
build	O
upon	O
either	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
or	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
both	O
of	O
which	O
rely	O
on	O
additional	O
external	O
training	O
data.	O
In	O
contrast,	O
our	O
submission	O
does	O
not	O
use	O
any	O
additional	O
data.	O
Our	O
single	B-MethodName
RoBERTa	I-MethodName
model	O
outperforms	O
all	O
but	O
one	O
of	O
the	O
single	O
model	O
submissions,	O
and	O
is	O
the	O
top	O
scoring	O
system	O
among	O
those	O
that	O
do	O
not	O
rely	O
on	O
data	O
augmentation.	O

In	O
RACE,	B-DatasetName
systems	O
are	O
provided	O
with	O
a	O
passage	O
of	O
text,	O
an	O
associated	O
question,	O
and	O
four	O
candidate	O
answers.	O
Systems	O
are	O
required	O
to	O
classify	O
which	O
of	O
the	O
four	O
candidate	O
answers	O
is	O
correct.	O
We	O
modify	O
RoBERTa	B-MethodName
for	O
this	O
task	O
by	O
concate-	O
Yang	O
et	O
al.	O
(2019).	O
nating	O
each	O
candidate	O
answer	O
with	O
the	O
corresponding	O
question	O
and	O
passage.	O
We	O
then	O
encode	O
each	O
of	O
these	O
four	O
sequences	O
and	O
pass	O
the	O
resulting	O
[CLS]	O
representations	O
through	O
a	O
fully-connected	O
layer,	O
which	O
is	O
used	O
to	O
predict	O
the	O
correct	O
answer.	O
We	O
truncate	O
question-answer	O
pairs	O
that	O
are	O
longer	O
than	O
128	O
tokens	O
and,	O
if	O
needed,	O
the	O
passage	O
so	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Results	O
on	O
the	O
RACE	B-DatasetName
test	O
sets	O
are	O
presented	O
in	O
Table	O
7.	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
both	O
middle-school	O
and	O
high-school	O
settings.	O

Past	O
work	O
in	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
has	O
shown	O
that	O
training	O
with	O
very	O
large	O
mini-batches	B-HyperparameterName
can	O
both	O
improve	O
optimization	O
speed	O
and	O
end-task	O
performance	O
when	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
increased	O
appropriately	O
(Ott	O
et	O
al.,	O
2018).	O
Recent	O
work	O
has	O
shown	O
that	O
BERT	B-MethodName
is	O
also	O
amenable	O
to	O
large	O
batch	O
training	O
(You	O
et	O
al.,	O
2019	O

In	O
the	O
original	O
BERT	B-MethodName
pretraining	O
procedure,	O
the	O
model	O
observes	O
two	O
concatenated	O
document	O
segments,	O
which	O
are	O
either	O
sampled	O
contiguously	O
from	O
the	O
same	O
document	O
(with	O
p	B-MetricName
=	O
0.5)	B-MetricValue
or	O
from	O
distinct	O
documents.	O
In	O
addition	O
to	O
the	O
masked	O
language	O
modeling	O
objective,	O
the	O
model	O
is	O
trained	O
to	O
predict	O
whether	O
the	O
observed	O
document	O
segments	O
come	O
from	O
the	O
same	O
or	O
distinct	O
documents	O
via	O
an	O
auxiliary	O
Next	O
Sentence	O
Prediction	O
(NSP)	O
loss.	O
The	O
NSP	O
loss	O
was	O
hypothesized	O
to	O
be	O
an	O
important	O
factor	O
in	O
training	O
the	O
original	O
BERT	B-MethodName
model.	O
Devlin	O
et	O
al.	O
(2019)	O
observe	O
that	O
removing	O
NSP	O
hurts	O
performance,	O
with	O
significant	O
performance	O
degradation	O
on	O
QNLI,	B-DatasetName
MNLI,	B-DatasetName
and	O
SQuAD	B-DatasetName
1.1.	I-DatasetName
However,	O
some	O
recent	O
work	O
has	O
questioned	O
the	O
necessity	O
of	O
the	O
NSP	O
loss	O
(Lample	O
and	O
Conneau,	O
2019;Yang	O
et	O
al.,	O
2019;Joshi	O
et	O
al.,	O
2019).	O
To	O
better	O
understand	O
this	O
discrepancy,	O
we	O
compare	O
several	O
alternative	O
training	O
formats:	O
•	O
SEGMENT-PAIR+NSP:	O
This	O
follows	O
the	O
original	O
input	O
format	O
used	O
in	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
with	O
the	O
NSP	O
loss.	O
Each	O
input	O
has	O
a	O
pair	O
of	O
segments,	O
which	O
can	O
each	O
contain	O
multiple	O
natural	O
sentences,	O
but	O
the	O
total	O
combined	O
length	O
must	O
be	O
less	O
than	O
512	O
tokens.	O
•	O
SENTENCE-PAIR+NSP:	O
Each	O
input	O
contains	O
a	O
pair	O
of	O
natural	O
sentences,	O
either	O
sampled	O
from	O
a	O
contiguous	O
portion	O
of	O
one	O
document	O
or	O
from	O
separate	O
documents.	O
Since	O
these	O
inputs	O
are	O
significantly	O
shorter	O
than	O
512	O
tokens,	O
we	O
increase	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
so	O
that	O
the	O
total	O
number	O
of	O
tokens	O
remains	O
similar	O
to	O
SEGMENT-PAIR+NSP.	O
We	O
retain	O
the	O
NSP	O
loss.	O
•	O
FULL-SENTENCES:	O
Each	O
input	O
is	O
packed	O
with	O
full	O
sentences	O
sampled	O
contiguously	O
from	O
one	O
or	O
more	O
documents,	O
such	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Inputs	O
may	O
cross	O
document	O
boundaries.	O
When	O
we	O
reach	O
the	O
end	O
of	O
one	O
document,	O
we	O
begin	O
sampling	O
sentences	O
from	O
the	O
next	O
document	O
and	O
add	O
an	O
extra	O
separator	O
token	O
between	O
documents.	O
We	O
remove	O
the	O
NSP	O
loss.	O
•	O
DOC-SENTENCES:	O
Inputs	O
are	O
constructed	O
similarly	O
to	O
FULL-SENTENCES,	O
except	O
that	O
they	O
may	O
not	O
cross	O
document	O
boundaries.	O
Inputs	O
sampled	O
near	O
the	O
end	O
of	O
a	O
document	O
may	O
be	O
shorter	O
than	O
512	O
tokens,	O
so	O
we	O
dynamically	O
increase	O
the	O
batch	O
size	O
in	O
these	O
cases	O
to	O
achieve	O
a	O
similar	O
number	O
of	O
total	O
tokens	O
as	O
FULL-SENTENCES.	O
We	O
remove	O
the	O
NSP	O
loss.	O
Results	O
Table	O
2	O
shows	O
results	O
for	O
the	O
four	O
different	O
settings.	O
We	O
first	O
compare	O
the	O
original	O
SEGMENT-PAIR	O
input	O
format	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
the	O
SENTENCE-PAIR	O
format;	O
both	O
formats	O
retain	O
the	O
NSP	O
loss,	O
but	O
the	O
latter	O
uses	O
single	O
sentences.	O
We	O
find	O
that	O
using	O
individual	O
sentences	O
hurts	O
performance	O
on	O
downstream	O
tasks,	O
which	O
we	O
hypothesize	O
is	O
because	O
the	O
model	O
is	O
not	O
able	O
to	O
learn	O
long-range	O
dependencies.	O
We	O
next	O
compare	O
training	O
without	O
the	O
NSP	O
loss	O
and	O
training	O
with	O
blocks	O
of	O
text	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES).	O
We	O
find	O
that	O
this	O
setting	O
outperforms	O
the	O
originally	O
published	O
BERT	B-MethodName
BASE	I-MethodName
results	O
and	O
that	O
removing	O
the	O
NSP	O
loss	O
matches	O
or	O
slightly	O
improves	O
downstream	O
task	O
performance,	O
in	O
contrast	O
to	O
Devlin	O
et	O
al.	O
(2019).	O
It	O
is	O
possible	O
that	O
the	O
original	O
BERT	B-MethodName
implementation	O
may	O
only	O
have	O
removed	O
the	O
loss	O
term	O
while	O
still	O
retaining	O
the	O
SEGMENT-PAIR	O
input	O
format.	O
Finally	O
we	O
find	O
that	O
restricting	O
sequences	O
to	O
come	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES)	O
performs	O
slightly	O
better	O
than	O
packing	O
sequences	O
from	O
multiple	O
documents	O
(FULL-SENTENCES).	O
However,	O
because	O
the	O
DOC-SENTENCES	O
format	O
results	O
in	O
variable	O
batch	O
sizes,	O
we	O
use	O
FULL-SENTENCES	O
in	O
the	O
remainder	O
of	O
our	O
experiments	O
for	O
easier	O
comparison	O
with	O
related	O
work.	O

As	O
discussed	O
in	O
Section	O
2,	O
BERT	B-MethodName
relies	O
on	O
randomly	O
masking	O
and	O
predicting	O
tokens.	O
The	O
original	O
BERT	B-MethodName
implementation	O
performed	O
masking	O
once	O
during	O
data	O
preprocessing,	O
resulting	O
in	O
a	O
single	O
static	O
mask.	O
To	O
avoid	O
using	O
the	O
same	O
mask	O
for	O
each	O
training	O
instance	O
in	O
every	O
epoch,	O
training	O
data	O
was	O
duplicated	O
10	O
times	O
so	O
that	O
each	O
sequence	O
is	O
masked	O
in	O
10	O
different	O
ways	O
over	O
the	O
40	B-HyperparameterValue
epochs	B-HyperparameterName
of	O
training.	O
Thus,	O
each	O
training	O
sequence	O
was	O
seen	O
with	O
the	O
same	O
mask	O
four	O
times	O
during	O
training.	O
We	O
compare	O
this	O
strategy	O
with	O
dynamic	O
masking	O
where	O
we	O
generate	O
the	O
masking	O
pattern	O
every	O
time	O
we	O
feed	O
a	O
sequence	O
to	O
the	O
model.	O
This	O
becomes	O
crucial	O
when	O
pretraining	O
for	O
more	O
steps	O
or	O
with	O
larger	O
datasets.	O
Results	O
Table	O
1	O
compares	O
the	O
published	O
BERT	O
BASE	O
results	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
our	O
reimplementation	O
with	O
either	O
static	O
or	O
dynamic	O
masking.	O
We	O
find	O
that	O
our	O
reimplementation	O
with	O
static	O
masking	O
performs	O
similar	O
to	O
the	O
original	O
BERT	B-MethodName
model,	O
and	O
dynamic	O
masking	O
is	O
comparable	O
or	O
slightly	O
better	O
than	O
static	O
masking.	O
Given	O
these	O
results	O
and	O
the	O
additional	O
efficiency	O
benefits	O
of	O
dynamic	O
masking,	O
we	O
use	O
dynamic	O
masking	O
in	O
the	O
remainder	O
of	O
the	O
experiments.	O

Pretraining	O
methods	O
have	O
been	O
designed	O
with	O
different	O
training	O
objectives,	O
including	O
language	O
modeling	O
(Dai	O
and	O
Le,	O
2015;Peters	O
et	O
al.,	O
2018;Howard	O
and	O
Ruder,	O
2018),	O
machine	B-TaskName
translation	I-TaskName
(McCann	O
et	O
al.,	O
2017),	O
and	O
masked	O
language	O
modeling	O
(Devlin	O
et	O
al.,	O
2019;Lample	O
and	O
Conneau,	O
2019).	O
Many	O
recent	O
papers	O
have	O
used	O
a	O
basic	O
recipe	O
of	O
finetuning	O
models	O
for	O
each	O
end	O
task	O
(Howard	O
and	O
Ruder,	O
2018;Radford	O
et	O
al.,	O
2018),	O
and	O
pretraining	O
with	O
some	O
variant	O
of	O
a	O
masked	O
language	O
model	O
objective.	O
However,	O
newer	O
methods	O
have	O
improved	O
performance	O
by	O
multi-task	O
fine	O
tuning	O
(Dong	O
et	O
al.,	O
2019),	O
incorporating	O
entity	O
embeddings	O
(Sun	O
et	O
al.,	O
2019),	O
span	O
prediction	O
(Joshi	O
et	O
al.,	O
2019),	O
and	O
multiple	O
variants	O
of	O
autoregressive	O
pretraining	O
Chan	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019).	O
Performance	O
is	O
also	O
typically	O
improved	O
by	O
training	O
bigger	O
models	O
on	O
more	O
data	O
(Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Radford	O
et	O
al.,	O
2019).	O
Our	O
goal	O
was	O
to	O
replicate,	O
simplify,	O
and	O
better	O
tune	O
the	O
training	O
of	O
BERT,	B-MethodName
as	O
a	O
reference	O
point	O
for	O
better	O
understanding	O
the	O
relative	O
performance	O
of	O
all	O
of	O
these	O
methods.	O
We	O
carefully	O
evaluate	O
a	O
number	O
of	O
design	O
decisions	O
when	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
find	O
that	O
performance	O
can	O
be	O
substantially	O
improved	O
by	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches	O
over	O
more	O
data;	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
training	O
on	O
longer	O
sequences;	O
and	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
Our	O
improved	O
pretraining	O
procedure,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD,	B-DatasetName
without	O
multi-task	O
finetuning	O
for	O
GLUE	B-DatasetName
or	O
additional	O
data	O
for	O
SQuAD.	B-DatasetName
These	O
results	O
illustrate	O
the	O
importance	O
of	O
these	O
previously	O
overlooked	O
design	O
decisions	O
and	O
suggest	O
that	O
BERT's	B-MethodName
pretraining	O
objective	O
remains	O
competitive	O
with	O
recently	O
proposed	O
alternatives.	O
We	O
additionally	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
release	O
our	O
models	O
and	O
code	O
for	O
pretraining	O
and	O
finetuning	O
at:	O

This	O
section	O
explores	O
and	O
quantifies	O
which	O
choices	O
are	O
important	O
for	O
successfully	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
keep	O
the	O
model	O
architecture	O
fixed.	O
7	O
Specifically,	O
we	O
begin	O
by	O
training	O
BERT	B-MethodName
models	O
with	O
the	O
same	O
configuration	O
as	O
BERT	B-MethodName
BASE	I-MethodName
(L	B-HyperparameterName
=	O
12,	B-HyperparameterValue
H	B-HyperparameterName
=	O
768,	B-HyperparameterValue
A	B-HyperparameterName
=	O
12,	B-HyperparameterValue
110M	B-MetricValue
params).	I-MetricValue

Following	O
previous	O
work,	O
we	O
evaluate	O
our	O
pretrained	O
models	O
on	O
downstream	O
tasks	O
using	O
the	O
following	O
three	O
benchmarks.	O
GLUE	B-DatasetName
The	B-DatasetName
General	I-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	O
(Wang	O
et	O
al.,	O
2019b)	O
is	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
6	O
Tasks	O
are	O
framed	O
as	O
either	O
single-sentence	O
classification	O
or	O
sentence-pair	O
classification	O
tasks.	O
The	O
GLUE	B-DatasetName
organizers	O
provide	O
training	O
and	O
development	O
data	O
splits	O
as	O
well	O
as	O
a	O
submission	O
server	O
and	O
leaderboard	O
that	O
allows	O
participants	O
to	O
evaluate	O
and	O
compare	O
their	O
systems	O
on	O
private	O
held-out	O
test	O
data.	O
For	O
the	O
replication	O
study	O
in	O
Section	O
4,	O
we	O
report	O
results	O
on	O
the	O
development	O
sets	O
after	O
finetuning	O
the	O
pretrained	O
models	O
on	O
the	O
corresponding	O
singletask	O
training	O
data	O
(i.e.,	O
without	O
multi-task	O
training	O
or	O
ensembling).	O
Our	O
finetuning	O
procedure	O
follows	O
the	O
original	O
BERT	B-MethodName
paper	O
(Devlin	O
et	O
al.,	O
2019).	O
In	O
Section	O
5	O
we	O
additionally	O
report	O
test	O
set	O
results	O
obtained	O
from	O
the	O
public	O
leaderboard.	O
These	O
results	O
depend	O
on	O
a	O
several	O
task-specific	O
modifications,	O
which	O
we	O
describe	O
in	O
Section	O
5.1.	O
SQuAD	B-DatasetName
The	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(SQuAD)	I-DatasetName
provides	O
a	O
paragraph	O
of	O
context	O
and	O
a	O
question.	O
The	O
task	O
is	O
to	O
answer	O
the	O
question	O
by	O
extracting	O
the	O
relevant	O
span	O
from	O
the	O
context.	O
We	O
evaluate	O
on	O
two	O
versions	O
of	O
SQuAD:	B-DatasetName
V1.1	O
and	O
V2.0	O
(Rajpurkar	O
et	O
al.,	O
2016(Rajpurkar	O
et	O
al.,	O
,	O
2018.	O
In	O
V1.1	O
the	O
context	O
always	O
contains	O
an	O
answer,	O
whereas	O
in	O
V2.0	O
some	O
questions	O
are	O
not	O
answered	O
in	O
the	O
provided	O
context,	O
making	O
the	O
task	O
more	O
challenging.	O
For	O
SQuAD	B-DatasetName
V1.1	I-DatasetName
we	O
adopt	O
the	O
same	O
span	O
prediction	O
method	O
as	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
For	O
SQuAD	B-DatasetName
V2.0,	I-DatasetName
we	O
add	O
an	O
additional	O
binary	O
classifier	O
to	O
predict	O
whether	O
the	O
question	O
is	O
answerable,	O
which	O
we	O
train	O
jointly	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O
During	O
evaluation,	O
we	O
only	O
predict	O
span	O
indices	O
on	O
pairs	O
that	O
are	O
classified	O
as	O
answerable.	O
RACE	B-DatasetName
The	B-DatasetName
ReAding	I-DatasetName
Comprehension	I-DatasetName
from	I-DatasetName
Examinations	I-DatasetName
(RACE)	I-DatasetName
(Lai	O
et	O
al.,	O
2017)	O
task	O
is	O
a	O
large-scale	O
reading	O
comprehension	O
dataset	O
with	O
more	O
than	O
28,000	O
passages	O
and	O
nearly	O
100,000	O
questions.	O
The	O
dataset	O
is	O
collected	O
from	O
English	O
examinations	O
in	O
China,	O
which	O
are	O
designed	O
for	O
middle	O
and	O
high	O
school	O
students.	O
In	O
RACE,	B-DatasetName
each	O
passage	O
is	O
associated	O
with	O
multiple	O
questions.	O
For	O
every	O
question,	O
the	O
task	O
is	O
to	O
select	O
one	O
correct	O
answer	O
from	O
four	O
options.	O
RACE	B-DatasetName
has	O
significantly	O
longer	O
context	O
than	O
other	O
popular	O
reading	O
comprehension	O
datasets	O
and	O
the	O
proportion	O
of	O
questions	O
that	O
requires	O
reasoning	O
is	O
very	O
large.	O

BERT-style	B-MethodName
pretraining	O
crucially	O
relies	O
on	O
large	O
quantities	O
of	O
text.	O
demonstrate	O
that	O
increasing	O
data	O
size	O
can	O
result	O
in	O
improved	O
end-task	O
performance.	O
Several	O
efforts	O
have	O
trained	O
on	O
datasets	O
larger	O
and	O
more	O
diverse	O
than	O
the	O
original	O
BERT	B-MethodName
(Radford	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Zellers	O
et	O
al.,	O
2019).	O
Unfortunately,	O
not	O
all	O
of	O
the	O
additional	O
datasets	O
can	O
be	O
publicly	O
released.	O
For	O
our	O
study,	O
we	O
focus	O
on	O
gathering	O
as	O
much	O
data	O
as	O
possible	O
for	O
experimentation,	O
allowing	O
us	O
to	O
match	O
the	O
overall	O
quality	O
and	O
quantity	O
of	O
data	O
as	O
appropriate	O
for	O
each	O
comparison.	O
We	O
consider	O
five	O
English-language	O
corpora	O
of	O
varying	O
sizes	O
and	O
domains,	O
totaling	O
over	O
160GB	O
of	O
uncompressed	O
text.	O
We	O
use	O
the	O
following	O
text	O
corpora:	O
•	O
BOOKCORPUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA.	I-DatasetName
This	O
is	O
the	O
original	O
data	O
used	O
to	O
train	O
BERT.	B-MethodName
(16GB).	O
•	O
CC-NEWS,	B-DatasetName
which	O
we	O
collected	O
from	O
the	O
English	O
portion	O
of	O
the	O
CommonCrawl	B-DatasetName
News	I-DatasetName
dataset	I-DatasetName
(Nagel,	O
2016).	O
The	O
data	O
contains	O
63	B-HyperparameterName
million	I-HyperparameterName
English	I-HyperparameterName
news	I-HyperparameterName
articles	I-HyperparameterName
crawled	O
between	O
September	O
2016	O
and	O
February	O
2019.	O
(76GB	O
after	O
filtering).	O
4	O
•	O
OPENWEBTEXT	B-DatasetName
(Gokaslan	O
and	O
Cohen,	O
2019),	O
an	O
open-source	O
recreation	O
of	O
the	O
WebText	O
cor-pus	O
described	O
in	O
Radford	O
et	O
al.	O
(2019).	O
The	O
text	O
is	O
web	O
content	O
extracted	O
from	O
URLs	O
shared	O
on	O
Reddit	O
with	O
at	O
least	O
three	O
upvotes.	O
(38GB).	O
5	O
•	O
STORIES,	B-DatasetName
a	O
dataset	O
introduced	O
in	O
Trinh	O
and	O
Le	O
(2018)	O
containing	O
a	O
subset	O
of	O
CommonCrawl	B-DatasetName
data	O
filtered	O
to	O
match	O
the	O
story-like	O
style	O
of	O
Winograd	O
schemas.	O
(31GB).	O

We	O
reimplement	O
BERT	B-MethodName
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
We	O
primarily	O
follow	O
the	O
original	O
BERT	B-MethodName
optimization	O
hyperparameters,	O
given	O
in	O
Section	O
2,	O
except	O
for	O
the	O
peak	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
number	B-HyperparameterName
of	I-HyperparameterName
warmup	I-HyperparameterName
steps,	I-HyperparameterName
which	O
are	O
tuned	O
separately	O
for	O
each	O
setting.	O
We	O
additionally	O
found	O
training	O
to	O
be	O
very	O
sensitive	O
to	O
the	O
Adam	B-HyperparameterValue
epsilon	O
term,	O
and	O
in	O
some	O
cases	O
we	O
obtained	O
better	O
performance	O
or	O
improved	O
stability	O
after	O
tuning	O
it.	O
Similarly,	O
we	O
found	O
setting	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.98	B-HyperparameterValue
to	O
improve	O
stability	O
when	O
training	O
with	O
large	O
batch	O
sizes.	O
We	O
pretrain	O
with	O
sequences	O
of	O
at	O
most	O
T	O
=	O
512	O
tokens.	O
Unlike	O
Devlin	O
et	O
al.	O
(2019),	O
we	O
do	O
not	O
randomly	O
inject	O
short	O
sequences,	O
and	O
we	O
do	O
not	O
train	O
with	O
a	O
reduced	O
sequence	O
length	O
for	O
the	O
first	O
90%	B-MetricValue
of	O
updates.	O
We	O
train	O
only	O
with	O
full-length	O
sequences.	O
We	O
train	O
with	O
mixed	O
precision	O
floating	O
point	O
arithmetic	O
on	O
DGX-1	O
machines,	O
each	O
with	O
8	O
×	O
32GB	O
Nvidia	O
V100	O
GPUs	O
interconnected	O
by	O
Infiniband	O
(Micikevicius	O
et	O
al.,	O
2018).	O

In	O
this	O
section,	O
we	O
describe	O
the	O
experimental	O
setup	O
for	O
our	O
replication	O
study	O
of	O
BERT.	B-MethodName

BERT	B-MethodName
is	O
trained	O
on	O
a	O
combination	O
of	O
BOOKCOR-PUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA,	I-DatasetName
which	O
totals	O
16GB	O
of	O
uncompressed	O
text.	O
3	O

BERT	B-MethodName
is	O
optimized	O
with	O
Adam	B-HyperparameterValue
(Kingma	O
and	O
Ba,	O
2015)	O
using	O
the	O
following	O
parameters:	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9,	B-HyperparameterValue
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999,	B-HyperparameterValue
ǫ	B-HyperparameterName
=	O
1e-6	B-HyperparameterValue
and	O
L	B-HyperparameterName
2	I-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01.	B-HyperparameterValue
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
warmed	O
up	O
over	O
the	O
first	O
10,000	O
steps	O
to	O
a	O
peak	O
value	O
of	O
1e-4,	B-HyperparameterValue
and	O
then	O
linearly	O
decayed.	O
BERT	B-MethodName
trains	O
with	O
a	O
dropout	B-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers	O
and	O
attention	O
weights,	O
and	O
a	O
GELU	B-HyperparameterValue
activation	B-HyperparameterName
function	I-HyperparameterName
(Hendrycks	O
and	O
Gimpel,	O
2016).	O
Models	O
are	O
pretrained	O
for	O
S	B-HyperparameterName
=	O
1,000,000	B-HyperparameterValue
updates,	O
with	O
minibatches	B-HyperparameterName
containing	O
B	O
=	O
256	B-HyperparameterValue
sequences	O
of	O
maximum	O
length	O
T	O
=	O
512	O
tokens.	O

During	O
pretraining,	O
BERT	B-MethodName
uses	O
two	O
objectives:	O
masked	O
language	O
modeling	O
and	O
next	B-TaskName
sentence	I-TaskName
prediction.	I-TaskName
Masked	O
Language	O
Model	O
(MLM)	O
A	O
random	O
sample	O
of	O
the	O
tokens	O
in	O
the	O
input	O
sequence	O
is	O
selected	O
and	O
replaced	O
with	O
the	O
special	O
token	O
[MASK	O
].	O
The	O
MLM	O
objective	O
is	O
a	O
cross-entropy	B-MetricName
loss	O
on	O
predicting	O
the	O
masked	O
tokens.	O
BERT	B-MethodName
uniformly	O
selects	O
15%	B-MetricValue
of	O
the	O
input	O
tokens	O
for	O
possible	O
replacement.	O
Of	O
the	O
selected	O
tokens,	O
80%	B-MetricValue
are	O
replaced	O
with	O
[MASK	O
],	O
10%	B-MetricValue
are	O
left	O
unchanged,	O
and	O
10%	B-MetricValue
are	O
replaced	O
by	O
a	O
randomly	O
selected	O
vocabulary	O
token.	O
In	O
the	O
original	O
implementation,	O
random	O
masking	O
and	O
replacement	O
is	O
performed	O
once	O
in	O
the	O
beginning	O
and	O
saved	O
for	O
the	O
duration	O
of	O
training,	O
although	O
in	O
practice,	O
data	O
is	O
duplicated	O
so	O
the	O
mask	O
is	O
not	O
always	O
the	O
same	O
for	O
every	O
training	O
sentence	O
(see	O
Section	O
4.1).	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
(NSP)	O
NSP	B-TaskName
is	O
a	O
binary	O
classification	O
loss	O
for	O
predicting	O
whether	O
two	O
segments	O
follow	O
each	O
other	O
in	O
the	O
original	O
text.	O
Positive	O
examples	O
are	O
created	O
by	O
taking	O
consecutive	O
sentences	O
from	O
the	O
text	O
corpus.	O
Negative	O
examples	O
are	O
created	O
by	O
pairing	O
segments	O
from	O
different	O
documents.	O
Positive	O
and	O
negative	O
examples	O
are	O
sampled	O
with	O
equal	O
probability.	O
The	O
NSP	B-TaskName
objective	O
was	O
designed	O
to	O
improve	O
performance	O
on	O
downstream	O
tasks,	O
such	O
as	O
Natural	O
Language	O
Inference	O
(Bowman	O
et	O
al.,	O
2015),	O
which	O
require	O
reasoning	O
about	O
the	O
relationships	O
between	O
pairs	O
of	O
sentences.	O

BERT	B-MethodName
uses	O
the	O
now	O
ubiquitous	O
transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017),	O
which	O
we	O
will	O
not	O
review	O
in	O
detail.	O
We	O
use	O
a	O
transformer	O
architecture	O
with	O
L	O
layers.	O
Each	O
block	O
uses	O
A	O
self-attention	O
heads	O
and	O
hidden	O
dimension	O
H.	O

BERT	B-MethodName
takes	O
as	O
input	O
a	O
concatenation	O
of	O
two	O
segments	O
(sequences	O
of	O
tokens),	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
and	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
.	O
Segments	O
usually	O
consist	O
of	O
more	O
than	O
one	O
natural	O
sentence.	O
The	O
two	O
segments	O
are	O
presented	O
as	O
a	O
single	O
input	O
sequence	O
to	O
BERT	B-MethodName
with	O
special	O
tokens	O
delimiting	O
them:	O
[CLS	O
],	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
,	O
[SEP	O
],	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
,	O
[EOS	O
].	O
M	O
and	O
N	O
are	O
constrained	O
such	O
that	O
M	O
+	O
N	O
<	O
T	O
,	O
where	O
T	O
is	O
a	O
parameter	O
that	O
controls	O
the	O
maximum	O
sequence	O
length	O
during	O
training.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
a	O
large	O
unlabeled	O
text	O
corpus	O
and	O
subsequently	O
finetuned	O
using	O
end-task	O
labeled	O
data.	O

In	O
this	O
section,	O
we	O
give	O
a	O
brief	O
overview	O
of	O
the	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
pretraining	O
approach	O
and	O
some	O
of	O
the	O
training	O
choices	O
that	O
we	O
will	O
examine	O
experimentally	O
in	O
the	O
following	O
section.	O

Self-training	O
methods	O
such	O
as	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018),	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
XLM	B-MethodName
(Lample	O
and	O
Conneau,	O
2019),	O
and	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
have	O
brought	O
significant	O
performance	O
gains,	O
but	O
it	O
can	O
be	O
challenging	O
to	O
determine	O
which	O
aspects	O
of	O
the	O
methods	O
contribute	O
the	O
most.	O
Training	O
is	O
computationally	O
expensive,	O
limiting	O
the	O
amount	O
of	O
tuning	O
that	O
can	O
be	O
done,	O
and	O
is	O
often	O
done	O
with	O
private	O
training	O
data	O
of	O
varying	O
sizes,	O
limiting	O
our	O
ability	O
to	O
measure	O
the	O
effects	O
of	O
the	O
modeling	O
advances.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019),	O
which	O
includes	O
a	O
careful	O
evaluation	O
of	O
the	O
effects	O
of	O
hyperparmeter	O
tuning	O
and	O
training	O
set	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained	O
and	O
propose	O
an	O
improved	O
recipe	O
for	O
training	O
BERT	B-MethodName
models,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
that	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
all	O
of	O
the	O
post-BERT	O
methods.	O
Our	O
modifications	O
are	O
simple,	O
they	O
include:	O
(1)	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches,	O
over	O
more	O
data;	O
(2)	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
(3)	O
training	O
on	O
longer	O
sequences;	O
and	O
(4)	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
We	O
also	O
collect	O
a	O
large	O
new	O
dataset	O
(CC-NEWS)	B-DatasetName
of	O
comparable	O
size	O
to	O
other	O
privately	O
used	O
datasets,	O
to	O
better	O
control	O
for	O
training	O
set	O
size	O
effects.	O
When	O
controlling	O
for	O
training	O
data,	O
our	O
improved	O
training	O
procedure	O
improves	O
upon	O
the	O
published	O
BERT	B-MethodName
results	O
on	O
both	O
GLUE	B-DatasetName
and	O
SQuAD.	B-DatasetName
When	O
trained	O
for	O
longer	O
over	O
additional	O
data,	O
our	O
model	O
achieves	O
a	O
score	O
of	O
88.5	B-MetricValue
on	O
the	O
public	O
GLUE	B-DatasetName
leaderboard,	O
matching	O
the	O
88.4	B-MetricValue
reported	O
by	O
Yang	O
et	O
al.	O
(2019).	O
Our	O
model	O
establishes	O
a	O
new	O
state-of-the-art	O
on	O
4/9	O
of	O
the	O
GLUE	B-DatasetName
tasks:	O
MNLI,	B-DatasetName
QNLI,	B-DatasetName
RTE	B-DatasetName
and	O
STS-B.	B-DatasetName
We	O
also	O
match	O
state-of-the-art	O
results	O
on	O
SQuAD	B-DatasetName
and	O
RACE.	B-DatasetName
Overall,	O
we	O
re-establish	O
that	O
BERT's	B-MethodName
masked	O
language	O
model	O
training	O
objective	O
is	O
competitive	O
with	O
other	O
recently	O
proposed	O
training	O
objectives	O
such	O
as	O
perturbed	O
autoregressive	O
language	O
modeling	O
(Yang	O
et	O
al.,	O
2019).	O
2	O
In	O
summary,	O
the	O
contributions	O
of	O
this	O
paper	O
are:	O
(1)	O
We	O
present	O
a	O
set	O
of	O
important	O
BERT	B-MethodName
design	O
choices	O
and	O
training	O
strategies	O
and	O
introduce	O
alternatives	O
that	O
lead	O
to	O
better	O
downstream	O
task	O
performance;	O
(2)	O
We	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
confirm	O
that	O
using	O
more	O
data	O
for	O
pretraining	O
further	O
improves	O
performance	O
on	O
downstream	O
tasks;	O
(3)	O
Our	O
training	O
improvements	O
show	O
that	O
masked	O
language	O
model	O
pretraining,	O
under	O
the	O
right	O
design	O
choices,	O
is	O
competitive	O
with	O
all	O
other	O
recently	O
published	O
methods.	O
We	O
release	O
our	O
model,	O
pretraining	O
and	O
fine-tuning	O
code	O
implemented	O
in	O
PyTorch	O
(Paszke	O
et	O
al.,	O
2017).	O

Language	O
model	O
pretraining	O
has	O
led	O
to	O
significant	O
performance	O
gains	O
but	O
careful	O
comparison	O
between	O
different	O
approaches	O
is	O
challenging.	O
Training	O
is	O
computationally	O
expensive,	O
often	O
done	O
on	O
private	O
datasets	O
of	O
different	O
sizes,	O
and,	O
as	O
we	O
will	O
show,	O
hyperparameter	O
choices	O
have	O
significant	O
impact	O
on	O
the	O
final	O
results.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019)	O
that	O
carefully	O
measures	O
the	O
impact	O
of	O
many	O
key	O
hyperparameters	O
and	O
training	O
data	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained,	O
and	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
every	O
model	O
published	O
after	O
it.	O
Our	O
best	O
model	O
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD.	B-DatasetName
These	O
results	O
highlight	O
the	O
importance	O
of	O
previously	O
overlooked	O
design	O
choices,	O
and	O
raise	O
questions	O
about	O
the	O
source	O
of	O
recently	O
reported	O
improvements.	O
We	O
release	O
our	O
models	O
and	O
code.	O
1	O

In	O
Table	O
8	O
we	O
present	O
the	O
full	O
set	O
of	O
development	O
set	O
results	O
for	O
RoBERTa.	B-MethodName
We	O
present	O
results	O
for	O
a	O
LARGE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
as	O
well	O
as	O
a	O
BASE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
BASE	I-MethodName
.	O

Appendix	O
for	O
"RoBERTa:	B-MethodName
A	O
Robustly	O
Optimized	O
BERT	O
Pretraining	O
Approach"	O
Acknowledgements	O
We	O
are	O
grateful	O
to	O
Nal	O
Kalchbrenner	O
and	O
Stephan	O
Gouws	O
for	O
their	O
fruitful	O
comments,	O
corrections	O
and	O
inspiration.	O

In	O
this	O
work,	O
we	O
presented	O
the	O
Transformer,	B-MethodName
the	O
first	O
sequence	O
transduction	O
model	O
based	O
entirely	O
on	O
attention,	O
replacing	O
the	O
recurrent	O
layers	O
most	O
commonly	O
used	O
in	O
encoder-decoder	O
architectures	O
with	O
multi-headed	O
self-attention.	O
For	O
translation	O
tasks,	O
the	O
Transformer	B-MethodName
can	O
be	O
trained	O
significantly	O
faster	O
than	O
architectures	O
based	O
on	O
recurrent	O
or	O
convolutional	O
layers.	O
On	O
both	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
and	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
tasks,	O
we	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art.	O
In	O
the	O
former	O
task	O
our	O
best	O
model	O
outperforms	O
even	O
all	O
previously	O
reported	O
ensembles.	O
We	O
are	O
excited	O
about	O
the	O
future	O
of	O
attention-based	O
models	O
and	O
plan	O
to	O
apply	O
them	O
to	O
other	O
tasks.	O
We	O
plan	O
to	O
extend	O
the	O
Transformer	B-MethodName
to	O
problems	O
involving	O
input	O
and	O
output	O
modalities	O
other	O
than	O
text	O
and	O
to	O
investigate	O
local,	O
restricted	O
attention	O
mechanisms	O
to	O
efficiently	O
handle	O
large	O
inputs	O
and	O
outputs	O
such	O
as	O
images,	O
audio	O
and	O
video.	O
Making	O
generation	O
less	O
sequential	O
is	O
another	O
research	O
goals	O
of	O
ours.	O
The	O
code	O
we	O
used	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
is	O
available	O
at	O
tensorflow/tensor2tensor.	O

To	O
evaluate	O
if	O
the	O
Transformer	B-MethodName
can	O
generalize	O
to	O
other	O
tasks	O
we	O
performed	O
experiments	O
on	O
English	B-TaskName
constituency	I-TaskName
parsing.	I-TaskName
This	O
task	O
presents	O
specific	O
challenges:	O
the	O
output	O
is	O
subject	O
to	O
strong	O
structural	O
constraints	O
and	O
is	O
significantly	O
longer	O
than	O
the	O
input.	O
Furthermore,	O
RNN	O
sequence-to-sequence	O
models	O
have	O
not	O
been	O
able	O
to	O
attain	O
state-of-the-art	O
results	O
in	O
small-data	O
regimes	O
.	O
We	O
trained	O
a	O
4-layer	O
transformer	O
with	O
d	O
model	O
=	O
1024	O
on	O
the	O
Wall	B-DatasetName
Street	I-DatasetName
Journal	I-DatasetName
(WSJ)	I-DatasetName
portion	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
,	O
about	O
40K	O
training	O
sentences.	O
We	O
also	O
trained	O
it	O
in	O
a	O
semi-supervised	O
setting,	O
using	O
the	O
larger	O
high-confidence	O
and	O
BerkleyParser	B-DatasetName
corpora	I-DatasetName
from	O
with	O
approximately	O
17M	O
sentences	O
.	O
We	O
used	O
a	O
vocabulary	B-HyperparameterName
of	O
16K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
WSJ	O
only	O
setting	O
and	O
a	O
vocabulary	B-HyperparameterName
of	O
32K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
semi-supervised	O
setting.	O
We	O
performed	O
only	O
a	O
small	O
number	O
of	O
experiments	O
to	O
select	O
the	O
dropout,	B-HyperparameterName
both	O
attention	O
and	O
residual	O
(section	O
5.4),	O
learning	B-HyperparameterName
rates	I-HyperparameterName
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
on	O
the	O
Section	O
22	O
development	O
set,	O
all	O
other	O
parameters	O
remained	O
unchanged	O
from	O
the	O
English-to-German	O
base	O
translation	O
model.	O
During	O
inference,	O
we	O
increased	O
the	O
maximum	O
output	O
length	O
to	O
input	O
length	O
+	O
300.	O
We	O
used	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
21	B-HyperparameterValue
and	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
for	O
both	O
WSJ	O
only	O
and	O
the	O
semi-supervised	O
setting.	O
Our	O
results	O
in	O
Table	O
4	O
show	O
that	O
despite	O
the	O
lack	O
of	O
task-specific	O
tuning	O
our	O
model	O
performs	O
surprisingly	O
well,	O
yielding	O
better	O
results	O
than	O
all	O
previously	O
reported	O
models	O
with	O
the	O
exception	O
of	O
the	O
Recurrent	O
Neural	O
Network	O
Grammar	O
.	O
In	O
contrast	O
to	O
RNN	O
sequence-to-sequence	O
models	O
,	O
the	O
Transformer	B-MethodName
outperforms	O
the	O
Berkeley-Parser	B-MethodName
even	O
when	O
training	O
only	O
on	O
the	O
WSJ	B-DatasetName
training	O
set	O
of	O
40K	O
sentences.	O

To	O
evaluate	O
the	O
importance	O
of	O
different	O
components	O
of	O
the	O
Transformer,	B-MethodName
we	O
varied	O
our	O
base	O
model	O
in	O
different	O
ways,	O
measuring	O
the	O
change	O
in	O
performance	O
on	O
English-to-German	B-TaskName
translation	I-TaskName
on	O
the	O
development	O
set,	O
newstest2013.	B-DatasetName
We	O
used	O
beam	B-MethodName
search	I-MethodName
as	O
described	O
in	O
the	O
previous	O
section,	O
but	O
no	O
checkpoint	O
averaging.	O
We	O
present	O
these	O
results	O
in	O
Table	O
3.	O
In	O
Table	O
3	O
rows	O
(A),	O
we	O
vary	O
the	O
number	O
of	O
attention	O
heads	O
and	O
the	O
attention	O
key	O
and	O
value	O
dimensions,	O
keeping	O
the	O
amount	O
of	O
computation	O
constant,	O
as	O
described	O
in	O
Section	O
3.2.2.	O
While	O
single-head	O
attention	O
is	O
0.9	B-MetricValue
BLEU	B-MetricName
worse	O
than	O
the	O
best	O
setting,	O
quality	O
also	O
drops	O
off	O
with	O
too	O
many	O
heads.	O
In	O
Table	O
3	O
rows	O
(B),	O
we	O
observe	O
that	O
reducing	O
the	O
attention	O
key	O
size	O
d	O
k	O
hurts	O
model	O
quality.	O
This	O
suggests	O
that	O
determining	O
compatibility	O
is	O
not	O
easy	O
and	O
that	O
a	O
more	O
sophisticated	O
compatibility	O
function	O
than	O
dot	O
product	O
may	O
be	O
beneficial.	O
We	O
further	O
observe	O
in	O
rows	O
(C)	O
and	O
(D)	O
that,	O
as	O
expected,	O
bigger	O
models	O
are	O
better,	O
and	O
dropout	O
is	O
very	O
helpful	O
in	O
avoiding	O
over-fitting.	O
In	O
row	O
(E)	O
we	O
replace	O
our	O
sinusoidal	O
positional	O
encoding	O
with	O
learned	O
positional	O
embeddings	O
,	O
and	O
observe	O
nearly	O
identical	O
results	O
to	O
the	O
base	O
model.	O

On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
translation	O
task,	O
the	O
big	O
transformer	B-MethodName
model	O
(Transformer	B-MethodName
(big)	I-MethodName
in	O
Table	O
2)	O
outperforms	O
the	O
best	O
previously	O
reported	O
models	O
(including	O
ensembles)	O
by	O
more	O
than	O
2.0	B-MetricValue
BLEU,	B-MetricName
establishing	O
a	O
new	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
28.4.	B-MetricValue
The	O
configuration	O
of	O
this	O
model	O
is	O
listed	O
in	O
the	O
bottom	O
line	O
of	O
Table	O
3.	O
Training	O
took	O
3.5	O
days	O
on	O
8	O
P100	O
GPUs.	O
Even	O
our	O
base	O
model	O
surpasses	O
all	O
previously	O
published	O
models	O
and	O
ensembles,	O
at	O
a	O
fraction	O
of	O
the	O
training	O
cost	O
of	O
any	O
of	O
the	O
competitive	O
models.	O
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
big	O
model	O
achieves	O
a	O
BLEU	B-MetricName
score	O
of	O
41.0,	B-MetricValue
outperforming	O
all	O
of	O
the	O
previously	O
published	O
single	O
models,	O
at	O
less	O
than	O
1/4	O
the	O
training	O
cost	O
of	O
the	O
previous	O
state-of-the-art	O
model.	O
The	O
Transformer	O
(big)	O
model	O
trained	O
for	O
English-to-French	O
used	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
P	O
drop	B-HyperparameterName
=	O
0.1,	B-HyperparameterValue
instead	O
of	O
0.3.	B-HyperparameterValue
For	O
the	O
base	O
models,	O
we	O
used	O
a	O
single	O
model	O
obtained	O
by	O
averaging	O
the	O
last	O
5	O
checkpoints,	O
which	O
were	O
written	O
at	O
10-minute	O
intervals.	O
For	O
the	O
big	O
models,	O
we	O
averaged	O
the	O
last	O
20	O
checkpoints.	O
We	O
used	O
beam	B-MethodName
search	I-MethodName
with	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
4	B-HyperparameterValue
and	O
length	B-HyperparameterName
penalty	I-HyperparameterName
α	B-HyperparameterValue
=	I-HyperparameterValue
0.6	I-HyperparameterValue
.	O
These	O
hyperparameters	O
were	O
chosen	O
after	O
experimentation	O
on	O
the	O
development	O
set.	O
We	O
set	O
the	O
maximum	O
output	O
length	O
during	O
inference	O
to	O
input	O
length	O
+	O
50,	O
but	O
terminate	O
early	O
when	O
possible	O
.	O

We	O
employ	O
three	O
types	O
of	O
regularization	O
during	O
training:	O
Residual	O
Dropout	B-HyperparameterName
We	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
output	O
of	O
each	O
sub-layer,	O
before	O
it	O
is	O
added	O
to	O
the	O
sub-layer	O
input	O
and	O
normalized.	O
In	O
addition,	O
we	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
sums	O
of	O
the	O
embeddings	O
and	O
the	O
positional	O
encodings	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
For	O
the	O
base	O
model,	O
we	O
use	O
a	O
rate	O
of	O
P	O
drop	B-HyperparameterName
=	O
0.1.	B-HyperparameterValue
Label	B-HyperparameterName
Smoothing	I-HyperparameterName
During	O
training,	O
we	O
employed	O
label	O
smoothing	O
of	O
value	O
ls	B-HyperparameterName
=	O
0.1	B-HyperparameterValue
.	O
This	O
hurts	O
perplexity,	B-MetricName
as	O
the	O
model	O
learns	O
to	O
be	O
more	O
unsure,	O
but	O
improves	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
score.	O
6	B-MetricValue
Results	O

We	O
used	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
β	O
1	O
=	O
0.9,	O
β	O
2	O
=	O
0.98	O
and	O
=	O
10	O
−9	O
.	O
We	O
varied	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
over	O
the	O
course	O
of	O
training,	O
according	O
to	O
the	O
formula:	O
lrate	O
=	O
d	O
−0.5	O
model	O
•	O
min(step_num	O
−0.5	O
,	O
step_num	O
•	O
warmup_steps	O
−1.5	O
)(3)	O
This	O
corresponds	O
to	O
increasing	O
the	O
learning	O
rate	O
linearly	O
for	O
the	O
first	O
warmup_steps	O
training	O
steps,	O
and	O
decreasing	O
it	O
thereafter	O
proportionally	O
to	O
the	O
inverse	O
square	O
root	O
of	O
the	O
step	O
number.	O
We	O
used	O
warmup_steps	B-HyperparameterName
=	O
4000.	B-HyperparameterValue

We	O
trained	O
our	O
models	O
on	O
one	O
machine	O
with	O
8	O
NVIDIA	O
P100	O
GPUs.	O
For	O
our	O
base	O
models	O
using	O
the	O
hyperparameters	O
described	O
throughout	O
the	O
paper,	O
each	O
training	O
step	O
took	O
about	O
0.4	O
seconds.	O
We	O
trained	O
the	O
base	O
models	O
for	O
a	O
total	O
of	O
100,000	B-HyperparameterValue
steps	B-HyperparameterName
or	O
12	O
hours.	O
For	O
our	O
big	O
models,(described	O
on	O
the	O
bottom	O
line	O
of	O
table	O
3),	O
step	O
time	O
was	O
1.0	O
seconds.	O
The	O
big	O
models	O
were	O
trained	O
for	O
300,000	B-HyperparameterValue
steps	B-HyperparameterName
(3.5	O
days).	O

We	O
trained	O
on	O
the	O
standard	O
WMT	B-DatasetName
2014	I-DatasetName
English-German	I-DatasetName
dataset	O
consisting	O
of	O
about	O
4.5	O
million	O
sentence	O
pairs.	O
Sentences	O
were	O
encoded	O
using	O
byte-pair	O
encoding	O
,	O
which	O
has	O
a	O
shared	O
sourcetarget	O
vocabulary	O
of	O
about	O
37000	O
tokens.	O
For	O
English-French,	O
we	O
used	O
the	O
significantly	O
larger	O
WMT	B-DatasetName
2014	I-DatasetName
English-French	I-DatasetName
dataset	O
consisting	O
of	O
36M	O
sentences	O
and	O
split	O
tokens	O
into	O
a	O
32000	O
word-piece	O
vocabulary	O
.	O
Sentence	O
pairs	O
were	O
batched	O
together	O
by	O
approximate	O
sequence	O
length.	O
Each	O
training	O
batch	O
contained	O
a	O
set	O
of	O
sentence	O
pairs	O
containing	O
approximately	O
25000	O
source	O
tokens	O
and	O
25000	O
target	O
tokens.	O

This	O
section	O
describes	O
the	O
training	O
regime	O
for	O
our	O
models.	O

In	O
this	O
section	O
we	O
compare	O
various	O
aspects	O
of	O
self-attention	O
layers	O
to	O
the	O
recurrent	O
and	O
convolutional	O
layers	O
commonly	O
used	O
for	O
mapping	O
one	O
variable-length	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
another	O
sequence	O
of	O
equal	O
length	O
(z	O
1	O
,	O
...,	O
z	O
n	O
),	O
with	O
x	O
i	O
,	O
z	O
i	O
∈	O
R	O
d	O
,	O
such	O
as	O
a	O
hidden	O
layer	O
in	O
a	O
typical	O
sequence	O
transduction	O
encoder	O
or	O
decoder.	O
Motivating	O
our	O
use	O
of	O
self-attention	O
we	O
consider	O
three	O
desiderata.	O
One	O
is	O
the	O
total	O
computational	O
complexity	O
per	O
layer.	O
Another	O
is	O
the	O
amount	O
of	O
computation	O
that	O
can	O
be	O
parallelized,	O
as	O
measured	O
by	O
the	O
minimum	O
number	O
of	O
sequential	O
operations	O
required.	O
The	O
third	O
is	O
the	O
path	O
length	O
between	O
long-range	O
dependencies	O
in	O
the	O
network.	O
Learning	O
long-range	O
dependencies	O
is	O
a	O
key	O
challenge	O
in	O
many	O
sequence	O
transduction	O
tasks.	O
One	O
key	O
factor	O
affecting	O
the	O
ability	O
to	O
learn	O
such	O
dependencies	O
is	O
the	O
length	O
of	O
the	O
paths	O
forward	O
and	O
backward	O
signals	O
have	O
to	O
traverse	O
in	O
the	O
network.	O
The	O
shorter	O
these	O
paths	O
between	O
any	O
combination	O
of	O
positions	O
in	O
the	O
input	O
and	O
output	O
sequences,	O
the	O
easier	O
it	O
is	O
to	O
learn	O
long-range	O
dependencies	O
.	O
Hence	O
we	O
also	O
compare	O
the	O
maximum	O
path	O
length	O
between	O
any	O
two	O
input	O
and	O
output	O
positions	O
in	O
networks	O
composed	O
of	O
the	O
different	O
layer	O
types.	O
As	O
noted	O
in	O
Table	O
1,	O
a	O
self-attention	O
layer	O
connects	O
all	O
positions	O
with	O
a	O
constant	O
number	O
of	O
sequentially	O
executed	O
operations,	O
whereas	O
a	O
recurrent	O
layer	O
requires	O
O(n)	O
sequential	O
operations.	O
In	O
terms	O
of	O
computational	O
complexity,	O
self-attention	O
layers	O
are	O
faster	O
than	O
recurrent	O
layers	O
when	O
the	O
sequence	O
length	O
n	O
is	O
smaller	O
than	O
the	O
representation	O
dimensionality	O
d,	O
which	O
is	O
most	O
often	O
the	O
case	O
with	O
sentence	O
representations	O
used	O
by	O
state-of-the-art	O
models	O
in	O
machine	O
translations,	O
such	O
as	O
word-piece	O
and	O
byte-pair	O
representations.	O
To	O
improve	O
computational	O
performance	O
for	O
tasks	O
involving	O
very	O
long	O
sequences,	O
self-attention	O
could	O
be	O
restricted	O
to	O
considering	O
only	O
a	O
neighborhood	O
of	O
size	O
r	O
in	O
the	O
input	O
sequence	O
centered	O
around	O
the	O
respective	O
output	O
position.	O
This	O
would	O
increase	O
the	O
maximum	O
path	O
length	O
to	O
O(n/r).	O
We	O
plan	O
to	O
investigate	O
this	O
approach	O
further	O
in	O
future	O
work.	O
A	O
single	O
convolutional	O
layer	O
with	O
kernel	O
width	O
k	O
<	O
n	O
does	O
not	O
connect	O
all	O
pairs	O
of	O
input	O
and	O
output	O
positions.	O
Doing	O
so	O
requires	O
a	O
stack	O
of	O
O(n/k)	O
convolutional	O
layers	O
in	O
the	O
case	O
of	O
contiguous	O
kernels,	O
or	O
O(log	O
k	O
(n))	O
in	O
the	O
case	O
of	O
dilated	O
convolutions	O
,	O
increasing	O
the	O
length	O
of	O
the	O
longest	O
paths	O
between	O
any	O
two	O
positions	O
in	O
the	O
network.	O
Convolutional	O
layers	O
are	O
generally	O
more	O
expensive	O
than	O
recurrent	O
layers,	O
by	O
a	O
factor	O
of	O
k.	O
Separable	O
convolutions	O
,	O
however,	O
decrease	O
the	O
complexity	O
considerably,	O
to	O
O(k	O
•	O
n	O
•	O
d	O
+	O
n	O
•	O
d	O
2	O
)	O
.	O
Even	O
with	O
k	O
=	O
n,	O
however,	O
the	O
complexity	O
of	O
a	O
separable	O
convolution	O
is	O
equal	O
to	O
the	O
combination	O
of	O
a	O
self-attention	O
layer	O
and	O
a	O
point-wise	O
feed-forward	O
layer,	O
the	O
approach	O
we	O
take	O
in	O
our	O
model.	O
As	O
side	O
benefit,	O
self-attention	O
could	O
yield	O
more	O
interpretable	O
models.	O
We	O
inspect	O
attention	O
distributions	O
from	O
our	O
models	O
and	O
present	O
and	O
discuss	O
examples	O
in	O
the	O
appendix.	O
Not	O
only	O
do	O
individual	O
attention	O
heads	O
clearly	O
learn	O
to	O
perform	O
different	O
tasks,	O
many	O
appear	O
to	O
exhibit	O
behavior	O
related	O
to	O
the	O
syntactic	O
and	O
semantic	O
structure	O
of	O
the	O
sentences.	O

Since	O
our	O
model	O
contains	O
no	O
recurrence	O
and	O
no	O
convolution,	O
in	O
order	O
for	O
the	O
model	O
to	O
make	O
use	O
of	O
the	O
order	O
of	O
the	O
sequence,	O
we	O
must	O
inject	O
some	O
information	O
about	O
the	O
relative	O
or	O
absolute	O
position	O
of	O
the	O
O(n	O
2	O
•	O
d)	O
O(1)	O
O(1)	O
Recurrent	O
O(n	O
•	O
d	O
2	O
)	O
O(n)	O
O(n)	O
Convolutional	O
O(k	O
•	O
n	O
•	O
d	O
2	O
)	O
O(1)	O
O(log	O
k	O
(n))	O
Self-Attention	O
(restricted)	O
O(r	O
•	O
n	O
•	O
d)	O
O(1)	O
O(n/r)	O
tokens	O
in	O
the	O
sequence.	O
To	O
this	O
end,	O
we	O
add	O
"positional	O
encodings"	O
to	O
the	O
input	O
embeddings	O
at	O
the	O
bottoms	O
of	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
The	O
positional	O
encodings	O
have	O
the	O
same	O
dimension	O
d	O
model	O
as	O
the	O
embeddings,	O
so	O
that	O
the	O
two	O
can	O
be	O
summed.	O
There	O
are	O
many	O
choices	O
of	O
positional	O
encodings,	O
learned	O
and	O
fixed	O
.	O
In	O
this	O
work,	O
we	O
use	O
sine	O
and	O
cosine	O
functions	O
of	O
different	O
frequencies:	O
P	O
E	O
(pos,2i)	O
=	O
sin(pos/10000	O
2i/dmodel	O
)	O
P	O
E	O
(pos,2i+1)	O
=	O
cos(pos/10000	O
2i/dmodel	O
)	O
where	O
pos	O
is	O
the	O
position	O
and	O
i	O
is	O
the	O
dimension.	O
That	O
is,	O
each	O
dimension	O
of	O
the	O
positional	O
encoding	O
corresponds	O
to	O
a	O
sinusoid.	O
The	O
wavelengths	O
form	O
a	O
geometric	O
progression	O
from	O
2π	O
to	O
10000	O
•	O
2π.	O
We	O
chose	O
this	O
function	O
because	O
we	O
hypothesized	O
it	O
would	O
allow	O
the	O
model	O
to	O
easily	O
learn	O
to	O
attend	O
by	O
relative	O
positions,	O
since	O
for	O
any	O
fixed	O
offset	O
k,	O
P	O
E	O
pos+k	O
can	O
be	O
represented	O
as	O
a	O
linear	O
function	O
of	O
P	O
E	O
pos	O
.	O
We	O
also	O
experimented	O
with	O
using	O
learned	O
positional	O
embeddings	O
instead,	O
and	O
found	O
that	O
the	O
two	O
versions	O
produced	O
nearly	O
identical	O
results	O
(see	O
Table	O
3	O
row	O
(E)).	O
We	O
chose	O
the	O
sinusoidal	O
version	O
because	O
it	O
may	O
allow	O
the	O
model	O
to	O
extrapolate	O
to	O
sequence	O
lengths	O
longer	O
than	O
the	O
ones	O
encountered	O
during	O
training.	O

Similarly	O
to	O
other	O
sequence	O
transduction	O
models,	O
we	O
use	O
learned	O
embeddings	O
to	O
convert	O
the	O
input	O
tokens	O
and	O
output	O
tokens	O
to	O
vectors	O
of	O
dimension	O
d	O
model	O
.	O
We	O
also	O
use	O
the	O
usual	O
learned	O
linear	O
transformation	O
and	O
softmax	O
function	O
to	O
convert	O
the	O
decoder	O
output	O
to	O
predicted	O
next-token	O
probabilities.	O
In	O
our	O
model,	O
we	O
share	O
the	O
same	O
weight	O
matrix	O
between	O
the	O
two	O
embedding	O
layers	O
and	O
the	O
pre-softmax	O
linear	O
transformation,	O
similar	O
to	O
.	O
In	O
the	O
embedding	O
layers,	O
we	O
multiply	O
those	O
weights	O
by	O
√	O
d	O
model	O
.	O

In	O
addition	O
to	O
attention	O
sub-layers,	O
each	O
of	O
the	O
layers	O
in	O
our	O
encoder	O
and	O
decoder	O
contains	O
a	O
fully	O
connected	O
feed-forward	O
network,	O
which	O
is	O
applied	O
to	O
each	O
position	O
separately	O
and	O
identically.	O
This	O
consists	O
of	O
two	O
linear	O
transformations	O
with	O
a	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
in	O
between.	O
FFN(x)	O
=	O
max(0,	O
xW	O
1	O
+	O
b	O
1	O
)W	O
2	O
+	O
b	O
2	O
(2)	O
While	O
the	O
linear	O
transformations	O
are	O
the	O
same	O
across	O
different	O
positions,	O
they	O
use	O
different	O
parameters	O
from	O
layer	O
to	O
layer.	O
Another	O
way	O
of	O
describing	O
this	O
is	O
as	O
two	O
convolutions	O
with	O
kernel	B-HyperparameterName
size	I-HyperparameterName
1.	B-HyperparameterValue
The	O
dimensionality	O
of	O
input	O
and	O
output	O
is	O
d	O
model	O
=	O
512,	O
and	O
the	O
inner-layer	O
has	O
dimensionality	O
d	O
f	O
f	O
=	O
2048.	O

The	O
Transformer	B-MethodName
uses	O
multi-head	O
attention	O
in	O
three	O
different	O
ways:	O
•	O
In	O
"encoder-decoder	O
attention"	O
layers,	O
the	O
queries	O
come	O
from	O
the	O
previous	O
decoder	O
layer,	O
and	O
the	O
memory	O
keys	O
and	O
values	O
come	O
from	O
the	O
output	O
of	O
the	O
encoder.	O
This	O
allows	O
every	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
over	O
all	O
positions	O
in	O
the	O
input	O
sequence.	O
This	O
mimics	O
the	O
typical	O
encoder-decoder	O
attention	O
mechanisms	O
in	O
sequence-to-sequence	O
models	O
such	O
as	O
.	O
•	O
The	O
encoder	O
contains	O
self-attention	O
layers.	O
In	O
a	O
self-attention	O
layer	O
all	O
of	O
the	O
keys,	O
values	O
and	O
queries	O
come	O
from	O
the	O
same	O
place,	O
in	O
this	O
case,	O
the	O
output	O
of	O
the	O
previous	O
layer	O
in	O
the	O
encoder.	O
Each	O
position	O
in	O
the	O
encoder	O
can	O
attend	O
to	O
all	O
positions	O
in	O
the	O
previous	O
layer	O
of	O
the	O
encoder.	O
•	O
Similarly,	O
self-attention	O
layers	O
in	O
the	O
decoder	O
allow	O
each	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
to	O
all	O
positions	O
in	O
the	O
decoder	O
up	O
to	O
and	O
including	O
that	O
position.	O
We	O
need	O
to	O
prevent	O
leftward	O
information	O
flow	O
in	O
the	O
decoder	O
to	O
preserve	O
the	O
auto-regressive	O
property.	O
We	O
implement	O
this	O
inside	O
of	O
scaled	O
dot-product	O
attention	O
by	O
masking	O
out	O
(setting	O
to	O
−∞)	O
all	O
values	O
in	O
the	O
input	O
of	O
the	O
softmax	O
which	O
correspond	O
to	O
illegal	O
connections.	O
See	O
Figure	O
2.	O

Instead	O
of	O
performing	O
a	O
single	O
attention	O
function	O
with	O
d	O
model	O
-dimensional	O
keys,	O
values	O
and	O
queries,	O
we	O
found	O
it	O
beneficial	O
to	O
linearly	O
project	O
the	O
queries,	O
keys	O
and	O
values	O
h	O
times	O
with	O
different,	O
learned	O
linear	O
projections	O
to	O
d	O
k	O
,	O
d	O
k	O
and	O
d	O
v	O
dimensions,	O
respectively.	O
On	O
each	O
of	O
these	O
projected	O
versions	O
of	O
queries,	O
keys	O
and	O
values	O
we	O
then	O
perform	O
the	O
attention	O
function	O
in	O
parallel,	O
yielding	O
d	O
v	O
-dimensional	O
output	O
values.	O
These	O
are	O
concatenated	O
and	O
once	O
again	O
projected,	O
resulting	O
in	O
the	O
final	O
values,	O
as	O
depicted	O
in	O
Figure	O
2.	O
Multi-head	O
attention	O
allows	O
the	O
model	O
to	O
jointly	O
attend	O
to	O
information	O
from	O
different	O
representation	O
subspaces	O
at	O
different	O
positions.	O
With	O
a	O
single	O
attention	O
head,	O
averaging	O
inhibits	O
this.	O
MultiHead(Q,	O
K,	O
V	O
)	O
=	O
Concat(head	O
1	O
,	O
...,	O
head	O
h	O
)W	O
O	O
where	O
head	O
i	O
=	O
Attention(QW	O
Q	O
i	O
,	O
KW	O
K	O
i	O
,	O
V	O
W	O
V	O
i	O
)	O
Where	O
the	O
projections	O
are	O
parameter	O
matrices	O
W	O
Q	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
K	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
V	O
i	O
∈	O
R	O
dmodel×dv	O
and	O
W	O
O	O
∈	O
R	O
hdv×dmodel	O
.	O
In	O
this	O
work	O
we	O
employ	O
h	O
=	O
8	O
parallel	O
attention	O
layers,	O
or	O
heads.	O
For	O
each	O
of	O
these	O
we	O
use	O
d	O
k	O
=	O
d	O
v	O
=	O
d	O
model	O
/h	O
=	O
64.	O
Due	O
to	O
the	O
reduced	O
dimension	O
of	O
each	O
head,	O
the	O
total	O
computational	O
cost	O
is	O
similar	O
to	O
that	O
of	O
single-head	O
attention	O
with	O
full	O
dimensionality.	O

We	O
call	O
our	O
particular	O
attention	O
"Scaled	O
Dot-Product	O
Attention"	O
(Figure	O
2).	O
The	O
input	O
consists	O
of	O
queries	O
and	O
keys	O
of	O
dimension	O
d	O
k	O
,	O
and	O
values	O
of	O
dimension	O
d	O
v	O
.	O
We	O
compute	O
the	O
dot	O
products	O
of	O
the	O
query	O
with	O
all	O
keys,	O
divide	O
each	O
by	O
√	O
d	O
k	O
,	O
and	O
apply	O
a	O
softmax	O
function	O
to	O
obtain	O
the	O
weights	O
on	O
the	O
values.	O
In	O
practice,	O
we	O
compute	O
the	O
attention	O
function	O
on	O
a	O
set	O
of	O
queries	O
simultaneously,	O
packed	O
together	O
into	O
a	O
matrix	O
Q.	O
The	O
keys	O
and	O
values	O
are	O
also	O
packed	O
together	O
into	O
matrices	O
K	O
and	O
V	O
.	O
We	O
compute	O
the	O
matrix	O
of	O
outputs	O
as:	O
Attention(Q,	O
K,	O
V	O
)	O
=	O
softmax(	O
QK	O
T	O
√	O
d	O
k	O
)V	O
(1)	O
The	O
two	O
most	O
commonly	O
used	O
attention	O
functions	O
are	O
additive	O
attention	O
,	O
and	O
dot-product	O
(multiplicative)	O
attention.	O
Dot-product	O
attention	O
is	O
identical	O
to	O
our	O
algorithm,	O
except	O
for	O
the	O
scaling	O
factor	O
of	O
1	O
√	O
d	O
k	O
.	O
Additive	O
attention	O
computes	O
the	O
compatibility	O
function	O
using	O
a	O
feed-forward	O
network	O
with	O
a	O
single	O
hidden	O
layer.	O
While	O
the	O
two	O
are	O
similar	O
in	O
theoretical	O
complexity,	O
dot-product	O
attention	O
is	O
much	O
faster	O
and	O
more	O
space-efficient	O
in	O
practice,	O
since	O
it	O
can	O
be	O
implemented	O
using	O
highly	O
optimized	O
matrix	O
multiplication	O
code.	O
While	O
for	O
small	O
values	O
of	O
d	O
k	O
the	O
two	O
mechanisms	O
perform	O
similarly,	O
additive	O
attention	O
outperforms	O
dot	O
product	O
attention	O
without	O
scaling	O
for	O
larger	O
values	O
of	O
d	O
k	O
.	O
We	O
suspect	O
that	O
for	O
large	O
values	O
of	O
d	O
k	O
,	O
the	O
dot	O
products	O
grow	O
large	O
in	O
magnitude,	O
pushing	O
the	O
softmax	O
function	O
into	O
regions	O
where	O
it	O
has	O
extremely	O
small	O
gradients	O
4	O
.	O
To	O
counteract	O
this	O
effect,	O
we	O
scale	O
the	O
dot	O
products	O
by	O
1	O
√	O
d	O
k	O
.	O

An	O
attention	O
function	O
can	O
be	O
described	O
as	O
mapping	O
a	O
query	O
and	O
a	O
set	O
of	O
key-value	O
pairs	O
to	O
an	O
output,	O
where	O
the	O
query,	O
keys,	O
values,	O
and	O
output	O
are	O
all	O
vectors.	O
The	O
output	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
values,	O
where	O
the	O
weight	O
assigned	O
to	O
each	O
value	O
is	O
computed	O
by	O
a	O
compatibility	O
function	O
of	O
the	O
query	O
with	O
the	O
corresponding	O
key.	O

Encoder:	O
The	O
encoder	O
is	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
Each	O
layer	O
has	O
two	O
sub-layers.	O
The	O
first	O
is	O
a	O
multi-head	O
self-attention	O
mechanism,	O
and	O
the	O
second	O
is	O
a	O
simple,	O
positionwise	O
fully	O
connected	O
feed-forward	O
network.	O
We	O
employ	O
a	O
residual	O
connection	O
around	O
each	O
of	O
the	O
two	O
sub-layers,	O
followed	O
by	O
layer	O
normalization	O
.	O
That	O
is,	O
the	O
output	O
of	O
each	O
sub-layer	O
is	O
LayerNorm(x	O
+	O
Sublayer(x)),	O
where	O
Sublayer(x)	O
is	O
the	O
function	O
implemented	O
by	O
the	O
sub-layer	O
itself.	O
To	O
facilitate	O
these	O
residual	O
connections,	O
all	O
sub-layers	O
in	O
the	O
model,	O
as	O
well	O
as	O
the	O
embedding	O
layers,	O
produce	O
outputs	O
of	O
dimension	O
d	O
model	O
=	O
512.	O
Decoder:	O
The	O
decoder	O
is	O
also	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
In	O
addition	O
to	O
the	O
two	O
sub-layers	O
in	O
each	O
encoder	O
layer,	O
the	O
decoder	O
inserts	O
a	O
third	O
sub-layer,	O
which	O
performs	O
multi-head	O
attention	O
over	O
the	O
output	O
of	O
the	O
encoder	O
stack.	O
Similar	O
to	O
the	O
encoder,	O
we	O
employ	O
residual	O
connections	O
around	O
each	O
of	O
the	O
sub-layers,	O
followed	O
by	O
layer	O
normalization.	O
We	O
also	O
modify	O
the	O
self-attention	O
sub-layer	O
in	O
the	O
decoder	O
stack	O
to	O
prevent	O
positions	O
from	O
attending	O
to	O
subsequent	O
positions.	O
This	O
masking,	O
combined	O
with	O
fact	O
that	O
the	O
output	O
embeddings	O
are	O
offset	O
by	O
one	O
position,	O
ensures	O
that	O
the	O
predictions	O
for	O
position	O
i	O
can	O
depend	O
only	O
on	O
the	O
known	O
outputs	O
at	O
positions	O
less	O
than	O
i.	O

Most	O
competitive	O
neural	O
sequence	O
transduction	O
models	O
have	O
an	O
encoder-decoder	O
structure	O
.	O
Here,	O
the	O
encoder	O
maps	O
an	O
input	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
a	O
sequence	O
of	O
continuous	O
representations	O
z	O
=	O
(z	O
1	O
,	O
...,	O
z	O
n	O
).	O
Given	O
z,	O
the	O
decoder	O
then	O
generates	O
an	O
output	O
sequence	O
(y	O
1	O
,	O
...,	O
y	O
m	O
)	O
of	O
symbols	O
one	O
element	O
at	O
a	O
time.	O
At	O
each	O
step	O
the	O
model	O
is	O
auto-regressive	O
,	O
consuming	O
the	O
previously	O
generated	O
symbols	O
as	O
additional	O
input	O
when	O
generating	O
the	O
next.	O
The	O
Transformer	B-MethodName
follows	O
this	O
overall	O
architecture	O
using	O
stacked	O
self-attention	O
and	O
point-wise,	O
fully	O
connected	O
layers	O
for	O
both	O
the	O
encoder	O
and	O
decoder,	O
shown	O
in	O
the	O
left	O
and	O
right	O
halves	O
of	O
Figure	O
1,	O
respectively.	O
Figure	O
1:	O
The	O
Transformer	B-MethodName
-model	O
architecture.	O

The	O
goal	O
of	O
reducing	O
sequential	O
computation	O
also	O
forms	O
the	O
foundation	O
of	O
the	O
Extended	O
Neural	O
GPU	O
,	O
ByteNet	B-MethodName
and	O
ConvS2S	B-MethodName
,	O
all	O
of	O
which	O
use	O
convolutional	O
neural	O
networks	O
as	O
basic	O
building	O
block,	O
computing	O
hidden	O
representations	O
in	O
parallel	O
for	O
all	O
input	O
and	O
output	O
positions.	O
In	O
these	O
models,	O
the	O
number	O
of	O
operations	O
required	O
to	O
relate	O
signals	O
from	O
two	O
arbitrary	O
input	O
or	O
output	O
positions	O
grows	O
in	O
the	O
distance	O
between	O
positions,	O
linearly	O
for	O
ConvS2S	B-MethodName
and	O
logarithmically	O
for	O
ByteNet.	B-MethodName
This	O
makes	O
it	O
more	O
difficult	O
to	O
learn	O
dependencies	O
between	O
distant	O
positions	O
.	O
In	O
the	O
Transformer	B-MethodName
this	O
is	O
reduced	O
to	O
a	O
constant	O
number	O
of	O
operations,	O
albeit	O
at	O
the	O
cost	O
of	O
reduced	O
effective	O
resolution	O
due	O
to	O
averaging	O
attention-weighted	O
positions,	O
an	O
effect	O
we	O
counteract	O
with	O
Multi-Head	O
Attention	O
as	O
described	O
in	O
section	O
3.2.	O
Self-attention,	O
sometimes	O
called	O
intra-attention	O
is	O
an	O
attention	O
mechanism	O
relating	O
different	O
positions	O
of	O
a	O
single	O
sequence	O
in	O
order	O
to	O
compute	O
a	O
representation	O
of	O
the	O
sequence.	O
Self-attention	O
has	O
been	O
used	O
successfully	O
in	O
a	O
variety	O
of	O
tasks	O
including	O
reading	O
comprehension,	O
abstractive	O
summarization,	O
textual	O
entailment	O
and	O
learning	O
task-independent	O
sentence	O
representations	O
.	O
End-to-end	O
memory	O
networks	O
are	O
based	O
on	O
a	O
recurrent	O
attention	O
mechanism	O
instead	O
of	O
sequencealigned	O
recurrence	O
and	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
simple-language	B-TaskName
question	I-TaskName
answering	I-TaskName
and	O
language	B-TaskName
modeling	I-TaskName
tasks	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge,	O
however,	O
the	O
Transformer	B-MethodName
is	O
the	O
first	O
transduction	O
model	O
relying	O
entirely	O
on	O
self-attention	O
to	O
compute	O
representations	O
of	O
its	O
input	O
and	O
output	O
without	O
using	O
sequencealigned	O
RNNs	O
or	O
convolution.	O
In	O
the	O
following	O
sections,	O
we	O
will	O
describe	O
the	O
Transformer,	B-MethodName
motivate	O
self-attention	O
and	O
discuss	O
its	O
advantages	O
over	O
models	O
such	O
as	O
and	O
.	O

Recurrent	O
neural	O
networks,	O
long	O
short-term	O
memory	O
and	O
gated	O
recurrent	O
neural	O
networks	O
in	O
particular,	O
have	O
been	O
firmly	O
established	O
as	O
state	O
of	O
the	O
art	O
approaches	O
in	O
sequence	O
modeling	O
and	O
transduction	O
problems	O
such	O
as	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
Numerous	O
efforts	O
have	O
since	O
continued	O
to	O
push	O
the	O
boundaries	O
of	O
recurrent	O
language	O
models	O
and	O
encoder-decoder	O
architectures	O
.	O
Recurrent	O
models	O
typically	O
factor	O
computation	O
along	O
the	O
symbol	O
positions	O
of	O
the	O
input	O
and	O
output	O
sequences.	O
Aligning	O
the	O
positions	O
to	O
steps	O
in	O
computation	O
time,	O
they	O
generate	O
a	O
sequence	O
of	O
hidden	O
states	O
h	O
t	O
,	O
as	O
a	O
function	O
of	O
the	O
previous	O
hidden	O
state	O
h	O
t−1	O
and	O
the	O
input	O
for	O
position	O
t.	O
This	O
inherently	O
sequential	O
nature	O
precludes	O
parallelization	O
within	O
training	O
examples,	O
which	O
becomes	O
critical	O
at	O
longer	O
sequence	O
lengths,	O
as	O
memory	O
constraints	O
limit	O
batching	O
across	O
examples.	O
Recent	O
work	O
has	O
achieved	O
significant	O
improvements	O
in	O
computational	O
efficiency	O
through	O
factorization	O
tricks	O
and	O
conditional	O
computation	O
,	O
while	O
also	O
improving	O
model	O
performance	O
in	O
case	O
of	O
the	O
latter.	O
The	O
fundamental	O
constraint	O
of	O
sequential	O
computation,	O
however,	O
remains.	O
Attention	O
mechanisms	O
have	O
become	O
an	O
integral	O
part	O
of	O
compelling	O
sequence	O
modeling	O
and	O
transduction	O
models	O
in	O
various	O
tasks,	O
allowing	O
modeling	O
of	O
dependencies	O
without	O
regard	O
to	O
their	O
distance	O
in	O
the	O
input	O
or	O
output	O
sequences	O
.	O
In	O
all	O
but	O
a	O
few	O
cases	O
,	O
however,	O
such	O
attention	O
mechanisms	O
are	O
used	O
in	O
conjunction	O
with	O
a	O
recurrent	O
network.	O
In	O
this	O
work	O
we	O
propose	O
the	O
Transformer,	B-MethodName
a	O
model	O
architecture	O
eschewing	O
recurrence	O
and	O
instead	O
relying	O
entirely	O
on	O
an	O
attention	O
mechanism	O
to	O
draw	O
global	O
dependencies	O
between	O
input	O
and	O
output.	O
The	O
Transformer	B-MethodName
allows	O
for	O
significantly	O
more	O
parallelization	O
and	O
can	O
reach	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
translation	O
quality	O
after	O
being	O
trained	O
for	O
as	O
little	O
as	O
twelve	O
hours	O
on	O
eight	O
P100	O
GPUs.	O

The	O
dominant	O
sequence	O
transduction	O
models	O
are	O
based	O
on	O
complex	O
recurrent	O
or	O
convolutional	O
neural	O
networks	O
that	O
include	O
an	O
encoder	O
and	O
a	O
decoder.	O
The	O
best	O
performing	O
models	O
also	O
connect	O
the	O
encoder	O
and	O
decoder	O
through	O
an	O
attention	O
mechanism.	O
We	O
propose	O
a	O
new	O
simple	O
network	O
architecture,	O
the	O
Transformer,	B-MethodName
based	O
solely	O
on	O
attention	O
mechanisms,	O
dispensing	O
with	O
recurrence	O
and	O
convolutions	O
entirely.	O
Experiments	O
on	O
two	O
machine	B-TaskName
translation	I-TaskName
tasks	O
show	O
these	O
models	O
to	O
be	O
superior	O
in	O
quality	O
while	O
being	O
more	O
parallelizable	O
and	O
requiring	O
significantly	O
less	O
time	O
to	O
train.	O
Our	O
model	O
achieves	O
28.4	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
Englishto-German	I-DatasetName
translation	O
task,	O
improving	O
over	O
the	O
existing	O
best	O
results,	O
including	O
ensembles,	O
by	O
over	O
2	B-MetricValue
BLEU.	B-MetricName
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
model	O
establishes	O
a	O
new	O
single-model	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
41.8	B-MetricValue
after	O
training	O
for	O
3.5	O
days	O
on	O
eight	O
GPUs,	O
a	O
small	O
fraction	O
of	O
the	O
training	O
costs	O
of	O
the	O
best	O
models	O
from	O
the	O
literature.	O
We	O
show	O
that	O
the	O
Transformer	B-MethodName
generalizes	O
well	O
to	O
other	O
tasks	O
by	O
applying	O
it	O
successfully	O
to	O
English	B-TaskName
constituency	I-TaskName
parsing	I-TaskName
both	O
with	O
large	O
and	O
limited	O
training	O
data.	O
*	O
Equal	O
contribution.	O
Listing	O
order	O
is	O
random.	O
Jakob	O
proposed	O
replacing	O
RNNs	O
with	O
self-attention	O
and	O
started	O
the	O
effort	O
to	O
evaluate	O
this	O
idea.	O
Ashish,	O
with	O
Illia,	O
designed	O
and	O
implemented	O
the	O
first	O
Transformer	B-MethodName
models	O
and	O
has	O
been	O
crucially	O
involved	O
in	O
every	O
aspect	O
of	O
this	O
work.	O
Noam	O
proposed	O
scaled	O
dot-product	O
attention,	O
multi-head	O
attention	O
and	O
the	O
parameter-free	O
position	O
representation	O
and	O
became	O
the	O
other	O
person	O
involved	O
in	O
nearly	O
every	O
detail.	O
Niki	O
designed,	O
implemented,	O
tuned	O
and	O
evaluated	O
countless	O
model	O
variants	O
in	O
our	O
original	O
codebase	O
and	O
tensor2tensor.	O
Llion	O
also	O
experimented	O
with	O
novel	O
model	O
variants,	O
was	O
responsible	O
for	O
our	O
initial	O
codebase,	O
and	O
efficient	O
inference	O
and	O
visualizations.	O
Lukasz	O
and	O
Aidan	O
spent	O
countless	O
long	O
days	O
designing	O
various	O
parts	O
of	O
and	O
implementing	O
tensor2tensor,	O
replacing	O
our	O
earlier	O
codebase,	O
greatly	O
improving	O
results	O
and	O
massively	O
accelerating	O
our	O
research.	O
†	O
Work	O
performed	O
while	O
at	O
Google	O
Brain.	O
‡	O
Work	O
performed	O
while	O
at	O
Google	O
Research.	O

We	O
also	O
evaluated	O
performance	O
on	O
WMT16	B-DatasetName
Romanian-English,	I-DatasetName
augmented	O
with	O
back-translation	O
data	O
from	O
Sennrich	O
et	O
al.	O
(2016).	O
We	O
use	O
a	O
6-layer	O
transformer	O
source	O
encoder	O
to	O
map	O
Romanian	O
into	O
a	O
representation	O
that	O
BART	B-MethodName
is	O
able	O
to	O
de-noise	O
into	O
English,	O
following	O
the	O
approach	O
introduced	O
in	O
§3.4.	O
Experiment	O
results	O
are	O
presented	O
in	O
Table	O
6.	O
We	O
compare	O
our	O
results	O
against	O
a	O
baseline	O
Transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017)	O
with	O
Transformerlarge	O
settings	O
(the	O
baseline	O
row).	O
We	O
show	O
the	O
performance	O
of	O
both	O
steps	O
of	O
our	O
model	O
in	O
the	O
fixed	O
BART	B-MethodName
and	O
tuned	O
BART	B-MethodName
rows.	O
For	O
each	O
row	O
we	O
experiment	O
on	O
the	O
original	O
WMT16	B-DatasetName
Romanian-English	I-DatasetName
augmented	O
with	O
back-translation	O
data.	O
We	O
use	O
a	O
beam	B-HyperparameterName
width	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
α	B-HyperparameterValue
=	I-HyperparameterValue
1.	I-HyperparameterValue
Preliminary	O
results	O
suggested	O
that	O
our	O
approach	O
was	O
less	O
effective	O
without	O
back-translation	O
data,	O
and	O
prone	O
to	O
overfitting-future	O
work	O
should	O
explore	O
additional	O
regularization	O
techniques.	O

We	O
also	O
experiment	O
with	O
several	O
text	O
generation	O
tasks.	O
BART	B-MethodName
is	O
fine-tuned	O
as	O
a	O
standard	O
sequence-to-sequence	O
model	O
from	O
the	O
input	O
to	O
the	O
output	O
text.	O
During	O
finetuning	O
we	O
use	O
a	O
label	O
smoothed	O
cross	O
entropy	O
loss	O
(Pereyra	O
et	O
al.,	O
2017),	O
with	O
the	O
smoothing	B-HyperparameterName
parameter	I-HyperparameterName
set	O
to	O
0.1.	B-HyperparameterValue
During	O
generation,	O
we	O
set	O
beam	B-HyperparameterName
size	I-HyperparameterName
as	O
5,	B-HyperparameterValue
remove	O
duplicated	O
trigrams	O
in	O
beam	O
search,	O
and	O
tuned	O
the	O
model	O
with	O
min-len,	O
max-len,	O
length	O
penalty	O
on	O
the	O
validation	O
set	O
(Fan	O
et	O
al.,	O
2017	O
Summarization	O
To	O
provide	O
a	O
comparison	O
with	O
the	O
state-of-the-art	O
in	O
summarization,	O
we	O
present	O
results	O
on	O
two	O
summarization	O
datasets,	O
CNN/DailyMail	B-DatasetName
and	O
XSum,	B-DatasetName
which	O
have	O
distinct	O
properties.	O
Summaries	O
in	O
the	O
CNN/DailyMail	B-DatasetName
tend	O
to	O
resemble	O
source	O
sentences.	O
Extractive	O
models	O
do	O
well	O
here,	O
and	O
even	O
the	O
baseline	O
of	O
the	O
first-three	O
source	O
sentences	O
is	O
highly	O
competitive.	O
Nevertheless,	O
BART	O
outperforms	O
all	O
existing	O
work.	O
In	O
contrast,	O
XSum	B-MethodName
is	O
highly	O
abstractive,	O
and	O
extractive	O
models	O
perform	O
poorly.	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work,	O
which	O
leverages	O
BERT,	B-MethodName
by	O
roughly	O
6.0	B-MetricValue
points	I-MetricValue
on	O
all	O
ROUGE	B-MetricName
metrics-representing	O
a	O
significant	O
advance	O
in	O
performance	O
on	O
this	O
problem.	O
Qualitatively,	O
sample	O
quality	O
is	O
high	O
(see	O
§6).	O
Dialogue	O
We	O
evaluate	O
dialogue	O
response	O
generation	O
on	O
CONVAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
in	O
which	O
agents	O
must	O
generate	O
responses	O
conditioned	O
on	O
both	O
the	O
previous	O
context	O
and	O
a	O
textually-specified	O
persona.	O
BART	B-MethodName
outperforms	O
previous	O
work	O
on	O
two	O
automated	O
metrics.	O
Abstractive	B-TaskName
QA	I-TaskName
We	O
use	O
the	O
recently	O
proposed	O
ELI5	B-DatasetName
dataset	O
to	O
test	O
the	O
model's	O
ability	O
to	O
generate	O
long	O
freeform	O
answers.	O
We	O
find	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work	O
by	O
1.2	B-MetricValue
ROUGE-L,	B-MetricName
but	O
the	O
dataset	O
remains	O
a	O
challenging,	O
because	O
answers	O
are	O
only	O
weakly	O
specified	O
by	O
the	O
question.	O

Table	O
2	O
compares	O
the	O
performance	O
of	O
BART	B-MethodName
with	O
several	O
recent	O
approaches	O
on	O
the	O
well-studied	O
SQuAD	B-TaskName
and	O
GLUE	B-TaskName
tasks	O
(Warstadt	O
et	O
al.,	O
2018;Socher	O
et	O
al.,	O
2013;Dolan	O
&	O
Brockett,	O
2005;Agirre	O
et	O
al.,	O
2007;Williams	O
et	O
al.,	O
2018;Dagan	O
et	O
al.,	O
2006;Levesque	O
et	O
al.,	O
2011).	O
The	O
most	O
directly	O
comparable	O
baseline	O
is	O
RoBERTa,	B-MethodName
which	O
was	O
pre-trained	O
with	O
the	O
same	O
resources,	O
but	O
a	O
different	O
objective.	O
Overall,	O
BART	B-MethodName
performs	O
similarly,	O
with	O
only	O
small	O
differences	O
between	O
the	O
models	O
on	O
most	O
tasks.	O
suggesting	O
that	O
BART's	B-MethodName
improvements	O
on	O
generation	O
tasks	O
do	O
not	O
come	O
at	O
the	O
expense	O
of	O
classification	O
performance.	O

We	O
pre-train	O
a	O
large	O
model	O
with	O
12	O
layers	O
in	O
each	O
of	O
the	O
encoder	O
and	O
decoder,	O
and	O
a	O
hidden	O
size	O
of	O
1024.	O
Following	O
RoBERTa	B-MethodName
,	O
we	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8000,	B-HyperparameterValue
and	O
train	O
the	O
model	O
for	O
500000	B-HyperparameterValue
steps.	B-HyperparameterName
Documents	O
are	O
tokenized	O
with	O
the	O
same	O
byte-pair	O
encoding	O
as	O
GPT-2	B-MethodName
(Radford	O
et	O
al.,	O
2019).	O
Based	O
on	O
the	O
results	O
in	O
Section	O
§4,	O
we	O
use	O
a	O
combination	O
of	O
text	O
infilling	O
and	O
sentence	O
permutation.	O
We	O
mask	O
30%	B-MetricValue
of	O
tokens	O
in	O
each	O
document,	O
and	O
permute	O
all	O
sentences.	O
Although	O
sentence	O
permutation	O
only	O
shows	O
significant	O
additive	O
gains	O
on	O
the	O
CNN/DM	B-DatasetName
summarization	I-DatasetName
dataset,	O
we	O
hypothesised	O
that	O
larger	O
pre-trained	O
models	O
may	O
be	O
better	O
able	O
to	O
learn	O
from	O
this	O
task.	O
To	O
help	O
the	O
model	O
better	O
fit	O
the	O
data,	O
we	O
disabled	O
dropout	B-HyperparameterName
for	O
the	O
final	O
10%	B-MetricValue
of	O
training	O
steps.	O
We	O
use	O
the	O
same	O
pre-training	O
data	O
as	O
,	O
consisting	O
of	O
160Gb	O
of	O
news,	O
books,	O
stories,	O
and	O
web	O
text.	O

Recent	O
work	O
has	O
shown	O
that	O
downstream	O
performance	O
can	O
dramatically	O
improve	O
when	O
pre-training	O
is	O
scaled	O
to	O
large	O
batch	O
sizes	O
(Yang	O
et	O
al.,	O
2019;	O
and	O
corpora.	O
To	O
test	O
how	O
well	O
BART	B-MethodName
performs	O
in	O
this	O
regime,	O
and	O
to	O
create	O
a	O
useful	O
model	O
for	O
downstream	O
tasks,	O
we	O
trained	O
BART	B-MethodName
using	O
the	O
same	O
scale	O
as	O
the	O
RoBERTa	B-MethodName
model.	O

BART	B-MethodName
shows	O
large	O
improvements	O
on	O
summarization	O
metrics,	O
of	O
up	O
to	O
6	O
points	O
over	O
the	O
prior	O
state-of-the-art.	O
To	O
understand	O
BART's	B-MethodName
performance	O
beyond	O
automated	O
metrics,	O
we	O
analyse	O
its	O
generations	O
qualitatively.	O
Table	O
7	O
shows	O
example	O
summaries	O
generated	O
by	O
BART.	B-MethodName
Examples	O
are	O
taken	O
from	O
WikiNews	B-DatasetName
articles	I-DatasetName
published	O
after	O
the	O
creation	O
of	O
the	O
pre-training	O
corpus,	O
to	O
eliminate	O
the	O
possibility	O
of	O
the	O
events	O
described	O
being	O
present	O
in	O
the	O
model's	O
training	O
data.	O
Following	O
Narayan	O
et	O
al.	O
(2018),	O
we	O
remove	O
the	O
first	O
sentence	O
of	O
the	O
article	O
prior	O
to	O
summarizing	O
it,	O
so	O
there	O
is	O
no	O
easy	O
extractive	O
summary	O
of	O
the	O
document.	O
Unsurprisingly,	O
model	O
output	O
is	O
fluent	O
and	O
grammatical	O
English.	O
However,	O
model	O
output	O
is	O
also	O
highly	O
abstractive,	O
with	O
few	O
phrases	O
copied	O
from	O
the	O
input.	O
The	O
output	O
is	O
also	O
generally	O
factually	O
accurate,	O
and	O
integrates	O
supporting	O
evidence	O
from	O
across	O
the	O
input	O
document	O
with	O
background	O
knowledge	O
(for	O
example,	O
correctly	O
completing	O
names,	O
or	O
inferring	O
that	O
PG&E	O
operates	O
in	O
California).	O
In	O
the	O
first	O
example,	O
inferring	O
that	O
fish	O
are	O
protecting	O
reefs	O
from	O
global	O
warming	O
requires	O
non-trivial	O
inference	O
from	O
the	O
text.	O
However,	O
the	O
claim	O
that	O
the	O
work	O
was	O
published	O
in	O
Science	O
is	O
not	O
supported	O
by	O
the	O
source.	O
These	O
samples	O
demonstrate	O
that	O
the	O
BART	B-MethodName
pretraining	O
has	O
learned	O
a	O
strong	O
combination	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
and	I-TaskName
generation.	I-TaskName

Early	O
methods	O
for	O
pretraining	O
were	O
based	O
on	O
language	O
models.	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018)	O
only	O
models	O
leftward	O
context,	O
which	O
is	O
problematic	O
for	O
some	O
tasks.	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018)	O
concatenates	O
left-only	O
and	O
right-only	O
representations,	O
but	O
does	O
not	O
pre-train	O
interactions	O
between	O
these	O
features.	O
Radford	O
et	O
al.	O
(2019)	O
demonstrated	O
that	O
very	O
large	O
language	O
models	O
can	O
act	O
as	O
unsupervised	O
multitask	O
models.	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
introduced	O
masked	O
language	O
modelling,	O
which	O
allows	O
pre-training	O
to	O
learn	O
interactions	O
between	O
left	O
and	O
right	O
context	O
words.	O
Recent	O
work	O
has	O
shown	O
that	O
very	O
strong	O
performance	O
can	O
be	O
achieved	O
by	O
training	O
for	O
longer	O
,	O
by	O
tying	O
parameters	O
across	O
layers	O
(Lan	O
et	O
al.,	O
2019),	O
and	O
by	O
masking	O
spans	O
instead	O
of	O
words	O
.	O
Predictions	O
are	O
not	O
made	O
auto-regressively,	O
reducing	O
the	O
effectiveness	O
of	O
BERT	B-MethodName
for	O
generation	O
tasks.	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019)	O
fine-tunes	O
BERT	B-MethodName
with	O
an	O
ensemble	O
of	O
masks,	O
some	O
of	O
which	O
allow	O
only	O
leftward	O
context.	O
Like	O
BART,	B-MethodName
this	O
allows	O
UniLM	B-MethodName
to	O
be	O
used	O
for	O
both	O
generative	O
and	O
discriminative	O
tasks.	O
A	O
difference	O
is	O
that	O
UniLM	B-MethodName
predictions	O
are	O
conditionally	O
independent,	O
whereas	O
BART's	B-MethodName
are	O
autoregressive.	O
BART	B-MethodName
reduces	O
the	O
mismatch	O
between	O
pre-training	O
and	O
generation	O
tasks,	O
because	O
the	O
decoder	O
is	O
always	O
trained	O
on	O
uncorrupted	O
context.	O
MASS	B-MethodName
(Song	O
et	O
al.,	O
2019)	O
is	O
perhaps	O
the	O
most	O
similar	O
model	O
to	O
BART.	B-MethodName
An	O
input	O
sequence	O
where	O
a	O
contiguous	O
span	O
of	O
tokens	O
is	O
masked	O
is	O
mapped	O
to	O
a	O
sequence	O
consisting	O
of	O
the	O
missing	O
tokens.	O
MASS	B-MethodName
is	O
less	O
effective	O
for	O
discriminative	O
tasks,	O
because	O
disjoint	O
sets	O
of	O
tokens	O
are	O
fed	O
into	O
the	O
encoder	O
and	O
decoder.	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
extends	O
BERT	B-MethodName
by	O
pre-Source	O
Document	O
(abbreviated)	O

The	O
researchers	O
examined	O
three	O
types	O
of	O
coral	O
in	O
reefs	O
off	O
the	O
coast	O
of	O
Fiji	O
...	O
The	O
researchers	O
found	O
when	O
fish	O
were	O
plentiful,	O
they	O
would	O
eat	O
algae	O
and	O
seaweed	O
off	O
the	O
corals,	O
which	O
appeared	O
to	O
leave	O
them	O
more	O
resistant	O
to	O
the	O
bacterium	O
Vibrio	O
coralliilyticus,	O
a	O
bacterium	O
associated	O
with	O
bleaching.	O
The	O
researchers	O
suggested	O
the	O
algae,	O
like	O
warming	O
temperatures,	O
might	O
render	O
the	O
corals'	O
chemical	O
defenses	O
less	O
effective,	O
and	O
the	O
fish	O
were	O
protecting	O
the	O
coral	O
by	O
removing	O
the	O
algae.	O
Fisheries	O
off	O
the	O
coast	O
of	O
Fiji	O
are	O
protecting	O
coral	O
reefs	O
from	O
the	O
effects	O
of	O
global	O
warming,	O
according	O
to	O
a	O
study	O
in	O
the	O
journal	O
Science.	O
Sacoolas,	O
who	O
has	O
immunity	O
as	O
a	O
diplomat's	O
wife,	O
was	O
involved	O
in	O
a	O
traffic	O
collision	O
...	O
Prime	O
Minister	O
Johnson	O
was	O
questioned	O
about	O
the	O
case	O
while	O
speaking	O
to	O
the	O
press	O
at	O
a	O
hospital	O
in	O
Watford.	O
He	O
said,	O
"I	O
hope	O
that	O
Anne	O
Sacoolas	O
will	O
come	O
back	O
...	O
if	O
we	O
can't	O
resolve	O
it	O
then	O
of	O
course	O
I	O
will	O
be	O
raising	O
it	O
myself	O
personally	O
with	O
the	O
White	O
House."	O
Boris	O
Johnson	O
has	O
said	O
he	O
will	O
raise	O
the	O
issue	O
of	O
US	O
diplomat	O
Anne	O
Sacoolas'	O
diplomatic	O
immunity	O
with	O
the	O
White	O
House.	O
PG&E	O
stated	O
it	O
scheduled	O
the	O
blackouts	O
in	O
response	O
to	O
forecasts	O
for	O
high	O
winds	O
amid	O
dry	O
conditions.	O
The	O
aim	O
is	O
to	O
reduce	O
the	O
risk	O
of	O
wildfires.	O
Nearly	O
800	O
thousand	O
customers	O
were	O
scheduled	O
to	O
be	O
affected	O
by	O
the	O
shutoffs	O
which	O
were	O
expected	O
to	O
last	O
through	O
at	O
least	O
midday	O
tomorrow.	O
Power	O
has	O
been	O
turned	O
off	O
to	O
millions	O
of	O
customers	O
in	O
California	O
as	O
part	O
of	O
a	O
power	O
shutoff	O
plan.	O
dicting	O
masked	O
tokens	O
auto-regressively	O
in	O
a	O
permuted	O
order.	O
This	O
objective	O
allows	O
predictions	O
to	O
condition	O
on	O
both	O
left	O
and	O
right	O
context.	O
In	O
contrast,	O
the	O
BART	B-MethodName
decoder	O
works	O
left-to-right	O
during	O
pre-training,	O
matching	O
the	O
setting	O
during	O
generation.	O
Several	O
papers	O
have	O
explored	O
using	O
pre-trained	O
representations	O
to	O
improve	O
machine	B-TaskName
translation.	I-TaskName
The	O
largest	O
improvements	O
have	O
come	O
from	O
pre-training	O
on	O
both	O
source	O
and	O
target	O
languages	O
(Song	O
et	O
al.,	O
2019;Lample	O
&	O
Conneau,	O
2019),	O
but	O
this	O
requires	O
pretraining	O
on	O
all	O
languages	O
of	O
interest.	O
Other	O
work	O
has	O
shown	O
that	O
encoders	O
can	O
be	O
improved	O
using	O
pre-trained	O
representations	O
(Edunov	O
et	O
al.,	O
2019),	O
but	O
gains	O
in	O
decoders	O
are	O
more	O
limited.	O
We	O
show	O
how	O
BART	B-MethodName
can	O
be	O
used	O
to	O
improve	O
machine	O
translation	O
decoders.	O

We	O
introduced	O
BART,	B-MethodName
a	O
pre-training	O
approach	O
that	O
learns	O
to	O
map	O
corrupted	O
documents	O
to	O
the	O
original.	O
BART	B-MethodName
achieves	O
similar	O
performance	O
to	O
RoBERTa	B-MethodName
on	O
discriminative	O
tasks,	O
while	O
achieving	O
new	O
state-of-theart	O
results	O
on	O
a	O
number	O
of	O
text	B-TaskName
generation	I-TaskName
tasks.	O
Future	O
work	O
should	O
explore	O
new	O
methods	O
for	O
corrupting	O
documents	O
for	O
pre-training,	O
perhaps	O
tailoring	O
them	O
to	O
specific	O
end	O
tasks.	O

The	O
Masked	O
Language	O
Model	O
and	O
the	O
Permuted	O
Language	O
Model	O
perform	O
less	O
well	O
than	O
others	O
on	O
generation,	O
and	O
are	O
the	O
only	O
models	O
we	O
consider	O
that	O
do	O
not	O
include	O
left-to-right	O
auto-regressive	O
language	O
modelling	O
during	O
pre-training.	O
Bidirectional	O
encoders	O
are	O
crucial	O
for	O
SQuAD	B-DatasetName
As	O
noted	O
in	O
previous	O
work	O
(Devlin	O
et	O
al.,	O
2019),	O
just	O
left-to-right	O
decoder	O
performs	O
poorly	O
on	O
SQuAD,	B-DatasetName
because	O
future	O
context	O
is	O
crucial	O
in	O
classification	O
decisions.	O
However,	O
BART	B-MethodName
achieves	O
similar	O
performance	O
with	O
only	O
half	O
the	O
number	O
of	O
bidirectional	O
layers.	O
The	O
pre-training	O
objective	O
is	O
not	O
the	O
only	O
important	O
factor	O
Our	O
Permuted	O
Language	O
Model	O
performs	O
less	O
well	O
than	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019).	O
Some	O
of	O
this	O
difference	O
is	O
likely	O
due	O
to	O
not	O
including	O
other	O
architectural	O
improvements,	O
such	O
as	O
relative-position	O
embeddings	O
or	O
segment-level	O
recurrence.	O
Pure	O
language	O
models	O
perform	O
best	O
on	O
ELI5	B-DatasetName
The	O
ELI5	B-DatasetName
dataset	O
is	O
an	O
outlier,	O
with	O
much	O
higher	O
perplexities	B-MetricName
than	O
other	O
tasks,	O
and	O
is	O
the	O
only	O
generation	O
task	O
where	O
other	O
models	O
outperform	O
BART.	B-MethodName
A	O
pure	O
language	O
model	O
performs	O
best,	O
suggesting	O
that	O
BART	B-MethodName
is	O
less	O
effective	O
when	O
the	O
output	O
is	O
only	O
loosely	O
constrained	O
by	O
the	O
input.	O
BART	B-MethodName
achieves	O
the	O
most	O
consistently	O
strong	O
performance.	O
With	O
the	O
exception	O
of	O
ELI5,	B-DatasetName
BART	B-MethodName
models	O
using	O
text-infilling	O
perform	O
well	O
on	O
all	O
tasks.	O

SQuAD	B-MethodName
(Rajpurkar	O
et	O
al.,	O
2016)a	O
an	O
extractive	O
question	O
answering	O
task	O
on	O
Wikipedia	O
paragraphs.	O
Answers	O
are	O
text	O
spans	O
extracted	O
from	O
a	O
given	O
document	O
context.	O
Similar	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
use	O
concatenated	O
question	O
and	O
context	O
as	O
input	O
to	O
the	O
encoder	O
of	O
BART,	B-MethodName
and	O
additionally	O
pass	O
them	O
to	O
the	O
decoder.	O
The	O
model	O
includes	O
classifiers	O
to	O
predict	O
the	O
start	O
and	O
end	O
indices	O
of	O
each	O
token.	O
MNLI	B-DatasetName
(Williams	O
et	O
al.,	O
2017),	O
a	O
bitext	B-TaskName
classification	I-TaskName
task	I-TaskName
to	O
predict	O
whether	O
one	O
sentence	O
entails	O
another.	O
The	O
fine-tuned	O
model	O
concatenates	O
the	O
two	O
sentences	O
with	O
appended	O
an	O
EOS	O
token,	O
and	O
passes	O
them	O
to	O
both	O
the	O
BART	B-MethodName
encoder	O
and	O
decoder.	O
In	O
contrast	O
to	O
BERT,	B-MethodName
the	O
representation	O
of	O
the	O
EOS	O
token	O
is	O
used	O
to	O
classify	O
the	O
sentences	O
relations.	O
ELI5	B-DatasetName
(Fan	O
et	O
al.,	O
2019),	O
a	O
long-form	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
dataset.	O
Models	O
generate	O
answers	O
conditioned	O
on	O
the	O
concatenation	O
of	O
a	O
question	O
and	O
supporting	O
documents.	O
XSum	B-DatasetName
(Narayan	O
et	O
al.,	O
2018),	O
a	O
news	O
summarization	O
dataset	O
with	O
highly	O
abstractive	O
summaries.	O
ConvAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
a	O
dialogue	O
response	O
generation	O
task,	O
conditioned	O
on	O
context	O
and	O
a	O
persona.	O
(Hermann	O
et	O
al.,	O
2015),	O
a	O
news	O
summarization	O
dataset.	O
Summaries	O
here	O
are	O
typically	O
closely	O
related	O
to	O
source	O
sentences.	O

While	O
many	O
pre-training	O
objectives	O
have	O
been	O
proposed,	O
fair	O
comparisons	O
between	O
these	O
have	O
been	O
difficult	O
to	O
perform,	O
at	O
least	O
in	O
part	O
due	O
to	O
differences	O
in	O
training	O
data,	O
training	O
resources,	O
architectural	O
differences	O
between	O
models,	O
and	O
fine-tuning	O
procedures.	O
We	O
re-implement	O
strong	O
pre-training	O
approaches	O
recently	O
proposed	O
for	O
discriminative	B-TaskName
and	I-TaskName
generation	I-TaskName
tasks.	I-TaskName
We	O
aim,	O
as	O
much	O
as	O
possible,	O
to	O
control	O
for	O
differences	O
unrelated	O
to	O
the	O
pre-training	O
objective.	O
However,	O
we	O
do	O
make	O
minor	O
changes	O
to	O
the	O
learning	O
rate	O
and	O
usage	O
of	O
layer	O
normalisation	O
in	O
order	O
to	O
improve	O
performance	O
(tuning	O
these	O
separately	O
for	O
each	O
objective).	O
For	O
reference,	O
we	O
compare	O
our	O
implementations	O
with	O
published	O
numbers	O
from	O
BERT,	B-MethodName
which	O
was	O
also	O
trained	O
for	O
1M	B-HyperparameterValue
steps	B-HyperparameterName
on	O
a	O
combination	O
of	O
books	O
and	O
Wikipedia	O
data.	O
We	O
compare	O
the	O
following	O
approaches:	O
Language	O
Model	O
Similarly	O
to	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
we	O
train	O
a	O
left-to-right	O
Transformer	O
language	O
model.	O
This	O
model	O
is	O
equivalent	O
to	O
the	O
BART	B-MethodName
decoder,	O
without	O
cross-attention.	O
Permuted	O
Language	O
Model	O
Based	O
on	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
we	O
sample	O
1/6	O
of	O
the	O
tokens,	O
and	O
generate	O
them	O
in	O
a	O
random	O
order	O
autoregressively.	O
For	O
consistency	O
with	O
other	O
models,	O
we	O
do	O
not	O
implement	O
the	O
relative	O
positional	O
embeddings	O
or	O
attention	O
across	O
segments	O
from	O
XLNet.	B-MethodName
Masked	O
Language	O
Model	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
replace	O
15%	B-MetricValue
of	O
tokens	O
with	O
[MASK]	O
symbols,	O
and	O
train	O
the	O
model	O
to	O
independently	O
predict	O
the	O
original	O
tokens.	O
Multitask	O
Masked	O
Language	O
Model	O
As	O
in	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019),	O
we	O
train	O
a	O
Masked	O
Language	O
Model	O
with	O
additional	O
self-attention	O
masks.	O
Self	O
attention	O
masks	O
are	O
chosen	O
randomly	O
in	O
with	O
the	O
follow	O
proportions:	O
1/6	O
left-to-right,	O
1/6	O
right-to-left,	O
1/3	O
unmasked,	O
and	O
1/3	O
with	O
the	O
first	O
50%	B-MetricValue
of	O
tokens	O
unmasked	O
and	O
a	O
left-to-right	O
mask	O
for	O
the	O
remainder.	O
Masked	B-MethodName
Seq-to-Seq	I-MethodName
Inspired	I-MethodName
by	I-MethodName
MASS	I-MethodName
(Song	O
et	O
al.,	O
2019),	O
we	O
mask	O
a	O
span	O
containing	O
50%	B-MetricValue
of	O
tokens,	O
and	O
train	O
a	O
sequence	O
to	O
sequence	O
model	O
to	O
predict	O
the	O
masked	O
tokens.	O
For	O
the	O
Permuted	O
LM,	O
Masked	B-MethodName
LM	I-MethodName
and	O
Multitask	B-MethodName
Masked	I-MethodName
LM,	I-MethodName
we	O
use	O
two-stream	O
attention	O
(Yang	O
et	O
al.,	O
2019)	O
to	O
efficiently	O
compute	O
likelihoods	O
of	O
the	O
output	O
part	O
of	O
the	O
sequence	O
(using	O
a	O
diagonal	O
self-attention	O
mask	O
on	O
the	O
output	O
to	O
predict	O
words	O
left-to-right).	O
We	O
experiment	O
with	O
(1)	O
treating	O
the	O
task	O
as	O
a	O
standard	O
sequence-to-sequence	O
problem,	O
where	O
the	O
source	O
input	O
to	O
the	O
encoder	O
and	O
the	O
target	O
is	O
the	O
decoder	O
output,	O
or	O
(2)	O
adding	O
the	O
source	O
as	O
prefix	O
to	O
the	O
target	O
in	O
the	O
decoder,	O
with	O
a	O
loss	O
only	O
on	O
the	O
target	O
part	O
of	O
the	O
sequence.	O
We	O
find	O
the	O
former	O
works	O
better	O
for	O
BART	B-MethodName
models,	O
and	O
the	O
latter	O
for	O
other	O
models.	O
To	O
most	O
directly	O
compare	O
our	O
models	O
on	O
their	O
ability	O
to	O
model	O
their	O
fine-tuning	O
objective	O
(the	O
log	O
likelihood	O
of	O
the	O
human	O
text),	O
we	O
report	O
perplexity	B-MetricName
in	O
Table	O
1.	O

BART	B-MethodName
supports	O
a	O
much	O
wider	O
range	O
of	O
noising	O
schemes	O
during	O
pre-training	O
than	O
previous	O
work.	O
We	O
compare	O
a	O
range	O
of	O
options	O
using	O
base-size	O
models	O
(6	O
encoder	O
and	O
6	B-HyperparameterValue
decoder	B-HyperparameterName
layers,	I-HyperparameterName
with	O
a	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	O
768),	B-HyperparameterValue
evaluated	O
on	O
a	O
representative	O
subset	O
of	O
the	O
tasks	O
we	O
will	O
consider	O
for	O
the	O
full	O
large	O
scale	O
experiments	O
in	O
§5.	O

We	O
also	O
explore	O
using	O
BART	B-MethodName
to	O
improve	O
machine	O
translation	O
decoders	O
for	O
translating	O
into	O
English.	O
Previous	O
work	O
Edunov	O
et	O
al.	O
(2019)	O
has	O
shown	O
that	O
models	O
can	O
be	O
improved	O
by	O
incorporating	O
pre-trained	O
encoders,	O
but	O
gains	O
from	O
using	O
pre-trained	O
language	O
models	O
in	O
decoders	O
have	O
been	O
limited.	O
We	O
show	O
that	O
it	O
is	O
possible	O
to	O
use	O
the	O
entire	O
BART	B-MethodName
model	O
(both	O
encoder	O
and	O
decoder)	O
as	O
a	O
single	O
pretrained	O
decoder	O
for	O
machine	B-TaskName
translation,	I-TaskName
by	O
adding	O
a	O
new	O
set	O
of	O
encoder	O
parameters	O
that	O
are	O
learned	O
from	O
bitext	O
(see	O
Figure	O
3b).	O
More	O
precisely,	O
we	O
replace	O
BART's	B-MethodName
encoder	O
embedding	O
layer	O
with	O
a	O
new	O
randomly	O
initialized	O
encoder.	O
The	O
model	O
is	O
trained	O
end-to-end,	O
which	O
trains	O
the	O
new	O
encoder	O
to	O
map	O
foreign	O
words	O
into	O
an	O
input	O
that	O
BART	B-MethodName
can	O
de-noise	O
to	O
English.	O
The	O
new	O
encoder	O
can	O
use	O
a	O
separate	O
vocabulary	O
from	O
the	O
original	O
BART	B-MethodName
model.	O
We	O
train	O
the	O
source	O
encoder	O
in	O
two	O
steps,	O
in	O
both	O
cases	O
backpropagating	O
the	O
cross-entropy	B-MetricName
loss	O
from	O
the	O
output	O
of	O
the	O
BART	B-MethodName
model.	O
In	O
the	O
first	O
step,	O
we	O
freeze	O
most	O
of	O
BART	B-MethodName
parameters	O
and	O
only	O
update	O
the	O
randomly	O
initialized	O
source	O
encoder,	O
the	O
BART	B-MethodName
positional	O
embeddings,	O
and	O
the	O
self-attention	O
input	O
projection	O
matrix	O
of	O
BART's	B-MethodName
encoder	O
first	O
layer.	O
In	O
the	O
second	O
step,	O
we	O
train	O
all	O
model	O
parameters	O
for	O
a	O
small	O
number	O
of	O
iterations.	O

Because	O
BART	B-MethodName
has	O
an	O
autoregressive	O
decoder,	O
it	O
can	O
be	O
directly	O
fine	O
tuned	O
for	O
sequence	O
generation	O
tasks	O
such	O
as	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
and	I-TaskName
summarization.	I-TaskName
In	O
both	O
of	O
these	O
tasks,	O
information	O
is	O
copied	O
from	O
the	O
input	O
but	O
manipulated,	O
which	O
is	O
closely	O
related	O
to	O
the	O
denoising	O
pre-training	O
objective.	O
Here,	O
the	O
encoder	O
input	O
is	O
the	O
input	O
sequence,	O
and	O
the	O
decoder	O
generates	O
outputs	O
autoregressively.	O

For	O
token	B-TaskName
classification	I-TaskName
tasks,	I-TaskName
such	O
as	O
answer	O
endpoint	B-TaskName
classification	I-TaskName
for	O
SQuAD,	B-DatasetName
we	O
feed	O
the	O
complete	O
document	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
use	O
the	O
top	O
hidden	O
state	O
of	O
the	O
decoder	O
as	O
a	O
representation	O
for	O
each	O
word.	O
This	O
representation	O
is	O
used	O
to	O
classify	O
the	O
token.	O

For	O
sequence	B-TaskName
classification	I-TaskName
tasks,	O
the	O
same	O
input	O
is	O
fed	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
the	O
final	O
hidden	O
state	O
of	O
the	O
final	O
decoder	O
token	O
is	O
fed	O
into	O
new	O
multi-class	O
linear	O
classifier.	O
This	O
approach	O
is	O
related	O
to	O
the	O
CLS	O
token	O
in	O
BERT;	B-MethodName
however	O
we	O
add	O
the	O
additional	O
token	O
to	O
the	O
end	O
so	O
that	O
representation	O
for	O
the	O
token	O
in	O
the	O
decoder	O
can	O
attend	O
to	O
decoder	O
states	O
from	O
the	O
complete	O
input	O
(Figure	O
3a).	O

The	O
representations	O
produced	O
by	O
BART	B-MethodName
can	O
be	O
used	O
in	O
several	O
ways	O
for	O
downstream	O
applications.	O

BART	B-MethodName
is	O
trained	O
by	O
corrupting	O
documents	O
and	O
then	O
optimizing	O
a	O
reconstruction	B-MetricName
loss-the	I-MetricName
cross-entropy	B-MetricName
between	O
the	O
decoder's	O
output	O
and	O
the	O
original	O
document.	O
Unlike	O
existing	O
denoising	O
autoencoders,	O
which	O
are	O
tailored	O
to	O
specific	O
noising	O
schemes,	O
BART	B-MethodName
allows	O
us	O
to	O
apply	O
any	O
type	O
of	O
document	O
corruption.	O
In	O
the	O
extreme	O
case,	O
where	O
all	O
information	O
about	O
the	O
source	O
is	O
lost,	O
BART	B-MethodName
is	O
equivalent	O
to	O
a	O
language	O
model.	O
We	O
experiment	O
with	O
several	O
previously	O
proposed	O
and	O
novel	O
transformations,	O
but	O
we	O
believe	O
there	O
is	O
a	O
significant	O
potential	O
for	O
development	O
of	O
other	O
new	O
alternatives.	O
The	O
transformations	O
we	O
used	O
are	O
summarized	O
below,	O
and	O
examples	O
are	O
shown	O
in	O
Figure	O
2.	O
Token	O
Masking	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
random	O
tokens	O
are	O
sampled	O
and	O
replaced	O
with	O
[MASK]	O
elements.	O
Token	O
Deletion	O
Random	O
tokens	O
are	O
deleted	O
from	O
the	O
input.	O
In	O
contrast	O
to	O
token	O
masking,	O
the	O
model	O
must	O
decide	O
which	O
positions	O
are	O
missing	O
inputs.	O
Text	O
Infilling	O
A	O
number	O
of	O
text	O
spans	O
are	O
sampled,	O
with	O
span	O
lengths	O
drawn	O
from	O
a	O
Poisson	O
distribution	O
(λ	O
=	O
3).	O
Each	O
span	O
is	O
replaced	O
with	O
a	O
single	O
[MASK]	O
token.	O
0-length	O
spans	O
correspond	O
to	O
the	O
insertion	O
of	O
[MASK]	O
tokens.	O
Text	O
infilling	O
is	O
inspired	O
by	O
Span-BERT	B-MethodName
,	O
but	O
SpanBERT	B-MethodName
samples	O
span	O
lengths	O
from	O
a	O
different	O
(clamped	O
geometric)	O
distribution,	O
and	O
replaces	O
each	O
span	O
with	O
a	O
sequence	O
of	O
[MASK]	O
tokens	O
of	O
exactly	O
the	O
same	O
length.	O
Text	O
infilling	O
teaches	O
the	O
model	O
to	O
predict	O
how	O
many	O
tokens	O
are	O
missing	O
from	O
a	O
span.	O
Sentence	O
Permutation	O
A	O
document	O
is	O
divided	O
into	O
sentences	O
based	O
on	O
full	O
stops,	O
and	O
these	O
sentences	O
are	O
shuffled	O
in	O
a	O
random	O
order.	O
Document	O
Rotation	O
A	O
token	O
is	O
chosen	O
uniformly	O
at	O
random,	O
and	O
the	O
document	O
is	O
rotated	O
so	O
that	O
it	O
begins	O
with	O
that	O
token.	O
This	O
task	O
trains	O
the	O
model	O
to	O
identify	O
the	O
start	O
of	O
the	O
document.	O

BART	B-MethodName
uses	O
the	O
standard	O
sequence-to-sequence	O
Transformer	O
architecture	O
from	O
(Vaswani	O
et	O
al.,	O
2017),	O
except,	O
following	O
GPT,	B-MethodName
that	O
we	O
modify	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
functions	I-HyperparameterName
to	O
GeLUs	B-HyperparameterValue
(Hendrycks	O
&	O
Gimpel,	O
2016)	O
and	O
initialise	O
parameters	O
from	O
N	O
(0,	O
0.02).	O
For	O
our	O
base	O
model,	O
we	O
use	O
6	O
layers	O
in	O
the	O
encoder	O
and	O
de-coder,	O
and	O
for	O
our	O
large	O
model	O
we	O
use	O
12	O
layers	O
in	O
each.	O
The	O
architecture	O
is	O
closely	O
related	O
to	O
that	O
used	O
in	O
BERT,	B-MethodName
with	O
the	O
following	O
differences:	O
(1)	O
each	O
layer	O
of	O
the	O
decoder	O
additionally	O
performs	O
cross-attention	O
over	O
the	O
final	O
hidden	O
layer	O
of	O
the	O
encoder	O
(as	O
in	O
the	O
transformer	O
sequence-to-sequence	O
model);	O
and	O
(2)	O
BERT	B-MethodName
uses	O
an	O
additional	O
feed-forward	O
network	O
before	O
wordprediction,	O
which	O
BART	B-MethodName
does	O
not.	O
In	O
total,	O
BART	B-MethodName
contains	O
roughly	O
10%	B-MetricValue
more	O
parameters	O
than	O
the	O
equivalently	O
sized	O
BERT	B-MethodName
model.	O

BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
that	O
maps	O
a	O
corrupted	O
document	O
to	O
the	O
original	O
document	O
it	O
was	O
derived	O
from.	O
It	O
is	O
implemented	O
as	O
a	O
sequence-to-sequence	O
model	O
with	O
a	O
bidirectional	O
encoder	O
over	O
corrupted	O
text	O
and	O
a	O
left-to-right	O
autoregressive	O
decoder.	O
For	O
pre-training,	O
we	O
optimize	O
the	O
negative	B-MetricName
log	I-MetricName
likelihood	I-MetricName
of	O
the	O
original	O
document.	O

Bidirectional	O
Encoder	O
A	O
B	O
C	O
D	O
E	O
A	O
_	O
B	O
_	O
E	O
<s>	O
A	O
B	O
C	O
D(	O
c)	O
BART:	O
Inputs	O
to	O
the	O
encoder	O
need	O
not	O
be	O
aligned	O
with	O
decoder	O
outputs,	O
allowing	O
arbitary	O
noise	O
transformations.	O
Here,	O
a	O
document	O
has	O
been	O
corrupted	O
by	O
replacing	O
spans	O
of	O
text	O
with	O
mask	O
symbols.	O
The	O
corrupted	O
document	O
(left)	O
is	O
encoded	O
with	O
a	O
bidirectional	O
model,	O
and	O
then	O
the	O
likelihood	O
of	O
the	O
original	O
document	O
(right)	O
is	O
calculated	O
with	O
an	O
autoregressive	O
decoder.	O
For	O
fine-tuning,	O
an	O
uncorrupted	O
document	O
is	O
input	O
to	O
both	O
the	O
encoder	O
and	O
decoder,	O
and	O
we	O
use	O
representations	O
from	O
the	O
final	O
hidden	O
state	O
of	O
the	O
decoder.	O
Figure	O
1:	O
A	O
schematic	O
comparison	O
of	O
BART	B-MethodName
with	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018).	O
English,	O
by	O
propagation	O
through	O
BART,	B-MethodName
thereby	O
using	O
BART	B-MethodName
as	O
a	O
pre-trained	O
target-side	O
language	O
model.	O
This	O
approach	O
improves	O
performance	O
over	O
a	O
strong	O
back-translation	O
MT	O
baseline	O
by	O
1.1	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
Romanian-English	I-DatasetName
benchmark.	O
To	O
better	O
understand	O
these	O
effects,	O
we	O
also	O
report	O
an	O
ablation	O
analysis	O
that	O
replicates	O
other	O
recently	O
proposed	O
training	O
objectives.	O
This	O
study	O
allows	O
us	O
to	O
carefully	O
control	O
for	O
a	O
number	O
of	O
factors,	O
including	O
data	O
and	O
optimization	O
parameters,	O
which	O
have	O
been	O
shown	O
to	O
be	O
as	O
important	O
for	O
overall	O
performance	O
as	O
the	O
selection	O
of	O
training	O
objectives	O
.	O
We	O
find	O
that	O
BART	B-MethodName
exhibits	O
the	O
most	O
consistently	O
strong	O
performance	O
across	O
the	O
full	O
range	O
of	O
tasks	O
we	O
consider.	O

A	O
B	O
C	O
D	O
E	O
<s>	O
A	O
B	O
C	O
D	O
(b)	O
GPT:	B-MethodName
Tokens	O
are	O
predicted	O
auto-regressively,	O
meaning	O
GPT	B-MethodName
can	O
be	O
used	O
for	O
generation.	O
However	O
words	O
can	O
only	O
condition	O
on	O
leftward	O
context,	O
so	O
it	O
cannot	O
learn	O
bidirectional	O
interactions.	O

Self-supervised	O
methods	O
have	O
achieved	O
remarkable	O
success	O
in	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
(Mikolov	O
et	O
al.,	O
2013;Peters	O
et	O
al.,	O
2018;Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;.	O
The	O
most	O
successful	O
approaches	O
have	O
been	O
variants	O
of	O
masked	O
language	O
models,	O
which	O
are	O
denoising	O
autoencoders	O
that	O
are	O
trained	O
to	O
reconstruct	O
text	O
where	O
a	O
random	O
subset	O
of	O
the	O
words	O
has	O
been	O
masked	O
out.	O
Recent	O
work	O
has	O
shown	O
gains	O
by	O
improving	O
the	O
distribution	O
of	O
masked	O
tokens	O
,	O
the	O
order	O
in	O
which	O
masked	O
tokens	O
are	O
predicted	O
(Yang	O
et	O
al.,	O
2019),	O
and	O
the	O
available	O
context	O
for	O
replacing	O
masked	O
tokens	O
(Dong	O
et	O
al.,	O
2019).	O
However,	O
these	O
methods	O
typically	O
focus	O
on	O
particular	O
types	O
of	O
end	O
tasks	O
(e.g.	O
span	B-TaskName
prediction,	I-TaskName
generation,	B-TaskName
etc.),	O
limiting	O
their	O
applicability.	O
In	O
this	O
paper,	O
we	O
present	O
BART,	B-MethodName
which	O
pre-trains	O
a	O
model	O
combining	O
Bidirectional	O
and	O
Auto-Regressive	O
Transformers.	O
BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
built	O
with	O
a	O
sequence-to-sequence	O
model	O
that	O
is	O
applicable	O
to	O
a	O
very	O
wide	O
range	O
of	O
end	O
tasks.	O
Pretraining	O
has	O
two	O
stages	O
(1)	O
text	O
is	O
corrupted	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
a	O
sequence-to-sequence	O
model	O
is	O
learned	O
to	O
reconstruct	O
the	O
original	O
text.	O
BART	B-MethodName
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes	O
(see	O
Figure	O
1).	O
A	O
key	O
advantage	O
of	O
this	O
setup	O
is	O
the	O
noising	O
flexibility;	O
arbitrary	O
transformations	O
can	O
be	O
applied	O
to	O
the	O
original	O
text,	O
including	O
changing	O
its	O
length.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
arbitrary	O
length	O
spans	O
of	O
text	O
(including	O
zero	O
length)	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
This	O
approach	O
generalizes	O
the	O
original	O
word	O
masking	O
and	O
next	O
sentence	O
prediction	O
objectives	O
in	O
BERT	B-MethodName
by	O
forcing	O
the	O
model	O
to	O
reason	O
more	O
about	O
overall	O
sentence	O
length	O
and	O
make	O
longer	O
range	O
transformations	O
to	O
the	O
input.	O
BART	B-MethodName
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	O
generation	O
but	O
also	O
works	O
well	O
for	O
comprehension	O
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2018)	O
and	O
SQuAD	B-DatasetName
(Rajpurkar	O
et	O
al.,	O
2016),	O
and	O
achieves	O
new	O
state-of-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	O
dialogue,	O
question	O
answering,	O
and	O
summarization	O
tasks.	O
For	O
example,	O
it	O
improves	O
performance	O
by	O
6	B-MetricValue
ROUGE	B-MetricName
over	O
previous	O
work	O
on	O
XSum	O
(Narayan	O
et	O
al.,	O
2018).	O
BART	B-MethodName
also	O
opens	O
up	O
new	O
ways	O
of	O
thinking	O
about	O
fine	O
tuning.	O
We	O
present	O
a	O
new	O
scheme	O
for	O
machine	O
translation	O
where	O
a	O
BART	B-MethodName
model	O
is	O
stacked	O
above	O
a	O
few	O
additional	O
transformer	O
layers.	O
These	O
layers	O
are	O
trained	O
to	O
essentially	O
translate	O
the	O
foreign	O
language	O
to	O
noised	O
Bidirectional	O
Encoder	O
A	O
_	O
C	O
_	O
E	O
B	O
D	O
(a)	O
BERT:	B-MethodName
Random	O
tokens	O
are	O
replaced	O
with	O
masks,	O
and	O
the	O
document	O
is	O
encoded	O
bidirectionally.	O
Missing	O
tokens	O
are	O
predicted	O
independently,	O
so	O
BERT	B-MethodName
cannot	O
easily	O
be	O
used	O
for	O
generation.	O

We	O
present	O
BART,	B-MethodName
a	O
denoising	O
autoencoder	O
for	O
pretraining	O
sequence-to-sequence	O
models.	O
BART	B-MethodName
is	O
trained	O
by	O
(1)	O
corrupting	O
text	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
learning	O
a	O
model	O
to	O
reconstruct	O
the	O
original	O
text.	O
It	O
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
spans	O
of	O
text	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
BART	O
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	B-TaskName
generation	I-TaskName
but	O
also	O
works	O
well	O
for	O
comprehension	B-TaskName
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	O
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
and	O
SQuAD,	B-DatasetName
achieves	O
new	O
stateof-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	B-TaskName
dialogue,	I-TaskName
question	B-TaskName
answering,	I-TaskName
and	O
summarization	O
tasks,	O
with	O
gains	O
of	O
up	O
to	O
6	B-MetricValue
ROUGE.	B-MetricName
BART	B-MethodName
also	O
provides	O
a	O
1.1	B-MetricValue
BLEU	B-MetricName
increase	O
over	O
a	O
back-translation	O
system	O
for	O
machine	O
translation,	B-TaskName
with	O
only	O
target	O
language	O
pretraining.	O
We	O
also	O
report	O
ablation	O
experiments	O
that	O
replicate	O
other	O
pretraining	O
schemes	O
within	O
the	O
BART	B-MethodName
framework,	O
to	O
better	O
measure	O
which	O
factors	O
most	O
influence	O
end-task	O
performance.	O

Task-specific	O
distillation	O
Most	O
of	O
the	O
prior	O
works	O
focus	O
on	O
building	O
task-specific	O
distillation	O
setups.	O
Tang	O
et	O
al.	O
transfer	O
fine-tune	O
classification	O
model	O
BERT	B-MethodName
to	O
an	O
LSTM-based	B-MethodName
classifier.	O
Chatterjee	O
distill	B-MethodName
BERT	I-MethodName
model	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
in	O
a	O
smaller	O
Transformer	B-MethodName
model	O
previously	O
initialized	O
from	O
BERT.	B-MethodName
In	O
the	O
present	O
work,	O
we	O
found	O
it	O
beneficial	O
to	O
use	O
a	O
general-purpose	O
pre-training	O
distillation	O
rather	O
than	O
a	O
task-specific	O
distillation.	O
Turc	O
et	O
al.	O
use	O
the	O
original	O
pretraining	O
objective	O
to	O
train	O
smaller	O
student,	O
then	O
fine-tuned	O
via	O
distillation.	B-MethodName
As	O
shown	O
in	O
the	O
ablation	O
study,	O
we	O
found	O
it	O
beneficial	O
to	O
leverage	O
the	O
teacher's	O
knowledge	O
to	O
pre-train	O
with	O
additional	O
distillation	O
signal.	O
Multi-distillation	O
Yang	O
et	O
al.	O
combine	O
the	O
knowledge	O
of	O
an	O
ensemble	O
of	O
teachers	O
using	O
multi-task	O
learning	O
to	O
regularize	O
the	O
distillation.	O
The	O
authors	O
apply	O
Multi-Task	O
Knowledge	O
Distillation	O
to	O
learn	O
a	O
compact	O
question	O
answering	O
model	O
from	O
a	O
set	O
of	O
large	O
question	B-TaskName
answering	I-TaskName
models.	O
An	O
application	O
of	O
multi-distillation	O
is	O
multi-linguality:	B-TaskName
Tsai	O
et	O
al.	O
adopts	O
a	O
similar	O
approach	O
to	O
us	O
by	O
pre-training	O
a	O
multilingual	O
model	O
from	O
scratch	O
solely	O
through	O
distillation.	O
However,	O
as	O
shown	O
in	O
the	O
ablation	O
study,	O
leveraging	O
the	O
teacher's	O
knowledge	O
with	O
initialization	O
and	O
additional	O
losses	O
leads	O
to	O
substantial	O
gains.	O
Other	O
compression	O
techniques	O
have	O
been	O
studied	O
to	O
compress	O
large	O
models.	O
Recent	O
developments	O
in	O
weights	O
pruning	O
reveal	O
that	O
it	O
is	O
possible	O
to	O
remove	O
some	O
heads	O
in	O
the	O
self-attention	O
at	O
test	O
time	O
without	O
significantly	O
degrading	O
the	O
performance	O
Michel	O
et	O
al.	O
.	O
Some	O
layers	O
can	O
be	O
reduced	O
to	O
one	O
head.	O
A	O
separate	O
line	O
of	O
study	O
leverages	O
quantization	O
to	O
derive	O
smaller	O
models	O
(Gupta	O
et	O
al.	O
).	O
Pruning	O
and	O
quantization	O
are	O
orthogonal	O
to	O
the	O
present	O
work.	O

We	O
introduced	O
DistilBERT,	B-MethodName
a	O
general-purpose	O
pre-trained	O
version	O
of	O
BERT,	B-MethodName
40%	B-MetricValue
smaller,	O
60%	B-MetricValue
faster,	O
that	O
retains	O
97%	B-MetricValue
of	O
the	O
language	O
understanding	O
capabilities.	O
We	O
showed	O
that	O
a	O
general-purpose	O
language	O
model	O
can	O
be	O
successfully	O
trained	O
with	O
distillation	O
and	O
analyzed	O
the	O
various	O
components	O
with	O
an	O
ablation	O
study.	O
We	O
further	O
demonstrated	O
that	O
DistilBERT	B-MethodName
is	O
a	O
compelling	O
option	O
for	O
edge	O
applications.	O

In	O
this	O
section,	O
we	O
investigate	O
the	O
influence	O
of	O
various	O
components	O
of	O
the	O
triple	O
loss	O
and	O
the	O
student	O
initialization	O
on	O
the	O
performances	O
of	O
the	O
distilled	O
model.	O
We	O
report	O
the	O
macro-score	O
on	O
GLUE.	B-DatasetName
Table	O
4	O
presents	O
the	O
deltas	O
with	O
the	O
full	O
triple	O
loss:	O
removing	O
the	O
Masked	B-MetricName
Language	I-MetricName
Modeling	I-MetricName
loss	I-MetricName
has	O
little	O
impact	O
while	O
the	O
two	O
distillation	O
losses	O
account	O
for	O
a	O
large	O
portion	O
of	O
the	O
performance.	O

To	O
further	O
investigate	O
the	O
speed-up/size	O
trade-off	O
of	O
DistilBERT,	B-MethodName
we	O
compare	O
(in	O
Table	O
3)	O
the	O
number	O
of	O
parameters	O
of	O
each	O
model	O
along	O
with	O
the	O
inference	O
time	O
needed	O
to	O
do	O
a	O
full	O
pass	O
on	O
the	O
STS-B	B-DatasetName
development	O
set	O
on	O
CPU	O
(Intel	O
Xeon	O
E5-2690	O
v3	O
Haswell	O
@2.9GHz)	O
using	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1.	B-HyperparameterValue
DistilBERT	B-MethodName
has	O
40%	B-MetricValue
fewer	O
parameters	O
than	O
BERT	B-MethodName
and	O
is	O
60%	B-MetricValue
faster	O
than	O
BERT.	B-MethodName
On	O
device	O
computation	O
We	O
studied	O
whether	O
DistilBERT	B-MethodName
could	O
be	O
used	O
for	O
on-the-edge	O
applications	O
by	O
building	O
a	O
mobile	O
application	O
for	O
question	O
answering.	O
We	O
compare	O
the	O
average	O
inference	O
time	O
on	O
a	O
recent	O
smartphone	O
(iPhone	O
7	O
Plus)	O
against	O
our	O
previously	O
trained	O
question	O
answering	O
model	O
based	O
on	O
BERT-base.	B-MethodName
Excluding	O
the	O
tokenization	O
step,	O
DistilBERT	B-MethodName
is	O
71%	B-MetricValue
faster	O
than	O
BERT,	B-MethodName
and	O
the	O
whole	O
model	O
weighs	O
207	O
MB	O
(which	O
could	O
be	O
further	O
reduced	O
with	O
quantization).	O
Our	O
code	O
is	O
available	O
5	O
.	O

Downstream	O
tasks	O
We	O
further	O
study	O
the	O
performances	O
of	O
DistilBERT	B-MethodName
on	O
several	O
downstream	O
tasks	O
under	O
efficient	O
inference	O
constraints:	O
a	O
classification	O
task	O
(IMDb	O
sentiment	O
classification	O
-	O
Maas	O
et	O
al.	O
)	O
and	O
a	O
question	O
answering	O
task	O
(SQuAD	B-DatasetName
v1.1	O
-Rajpurkar	O
et	O
al.	O
).	O
As	O
shown	O
in	O
Table	O
2,	O
DistilBERT	B-MethodName
is	O
only	O
0.6%	B-MetricValue
point	O
behind	O
BERT	B-MethodName
in	O
test	O
accuracy	B-MetricName
on	O
the	O
IMDb	B-DatasetName
benchmark	O
while	O
being	O
40%	B-MetricValue
smaller.	O
On	O
SQuAD,	B-DatasetName
DistilBERT	B-MethodName
is	O
within	O
3.9	B-MetricValue
points	I-MetricValue
of	O
the	O
full	O
BERT.	B-MethodName
We	O
also	O
studied	O
whether	O
we	O
could	O
add	O
another	O
step	O
of	O
distillation	B-MethodName
during	O
the	O
adaptation	O
phase	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
on	O
SQuAD	B-DatasetName
using	O
a	O
BERT	B-MethodName
model	O
previously	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
as	O
a	O
∅	O
-L	O
cos	O
-L	O
mlm	O
-2.96	O
L	O
ce	O
-∅	O
-L	O
mlm	O
-1.46	O
L	O
ce	O
-L	O
cos	O
-∅	O
-0.	O
31	O
Triple	B-MetricName
loss	I-MetricName
+	O
random	O
weights	O
initialization	O
-3.69	O
teacher	O
for	O
an	O
additional	O
term	O
in	O
the	O
loss	O
(knowledge	O
distillation).	O
In	O
this	O
setting,	O
there	O
are	O
thus	O
two	O
successive	O
steps	O
of	O
distillation,	B-MethodName
one	O
during	O
the	O
pre-training	O
phase	O
and	O
one	O
during	O
the	O
adaptation	O
phase.	O
In	O
this	O
case,	O
we	O
were	O
able	O
to	O
reach	O
interesting	O
performances	O
given	O
the	O
size	O
of	O
the	O
model:	O
79.8	O
F1	O
and	O
70.4	O
EM,	O
i.e.	O
within	O
3	O
points	O
of	O
the	O
full	O
model.	O

General	O
Language	O
Understanding	O
We	O
assess	O
the	O
language	O
understanding	O
and	O
generalization	O
capabilities	O
of	B-MethodName
DistilBERT	I-MethodName
on	O
the	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	I-DatasetName
[Wang	O
et	O
al.,	O
2018],	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
We	O
report	O
scores	O
on	O
the	O
development	O
sets	O
for	O
each	O
task	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
without	O
the	O
use	O
of	O
ensembling	O
or	O
multi-tasking	O
scheme	O
for	O
fine-tuning	O
(which	O
are	O
mostly	O
orthogonal	O
to	O
the	O
present	O
work).	O
We	O
compare	O
the	O
results	O
to	O
the	O
baseline	O
provided	O
by	O
the	O
authors	O
of	O
GLUE:	B-DatasetName
an	O
ELMo	B-MethodName
(Peters	O
et	O
al.	O
)	O
encoder	O
followed	O
by	O
two	O
BiLSTMs.	B-MethodName
4	O
The	O
results	O
on	O
each	O
of	O
the	O
9	O
tasks	O
are	O
showed	O
on	O
Table	O
1	O
along	O
with	O
the	O
macro-score	O
(average	O
of	O
individual	O
scores).	O
Among	O
the	O
9	O
tasks,	O
DistilBERT	B-MethodName
is	O
always	O
on	O
par	O
or	O
improving	O
over	O
the	O
ELMo	B-MethodName
baseline	O
(up	O
to	O
19	B-MetricValue
points	I-MetricValue
of	O
accuracy	B-MetricName
on	O
STS-B).	B-DatasetName
DistilBERT	B-MethodName
also	O
compares	O
surprisingly	O
well	O
to	O
BERT,	B-MethodName
retaining	O
97%	B-MetricValue
of	O
the	O
performance	B-MetricName
with	O
40%	B-MetricValue
fewer	O
parameters.	O

In	O
addition	O
to	O
the	O
previously	O
described	O
optimization	O
and	O
architectural	O
choices,	O
an	O
important	O
element	O
in	O
our	O
training	O
procedure	O
is	O
to	O
find	O
the	O
right	O
initialization	O
for	O
the	O
sub-network	O
to	O
converge.	O
Taking	O
advantage	O
of	O
the	O
common	O
dimensionality	O
between	O
teacher	O
and	O
student	O
networks,	O
we	O
initialize	O
the	O
student	O
from	O
the	O
teacher	O
by	O
taking	O
one	O
layer	O
out	O
of	O
two.	O

Knowledge	B-MethodName
distillation	I-MethodName
[Bucila	O
et	O
al.,	O
2006,	O
Hinton	O
et	O
al.,	O
2015]	O
is	O
a	O
compression	O
technique	O
in	O
which	O
a	O
compact	O
model	O
-the	O
student	O
-is	O
trained	O
to	O
reproduce	O
the	O
behaviour	O
of	O
a	O
larger	O
model	O
-the	O
teacheror	O
an	O
ensemble	O
of	O
models.	O
In	O
supervised	O
learning,	O
a	O
classification	O
model	O
is	O
generally	O
trained	O
to	O
predict	O
an	O
instance	O
class	O
by	O
maximizing	O
the	O
estimated	O
probability	O
of	O
gold	O
labels.	O
A	O
standard	O
training	O
objective	O
thus	O
involves	O
minimizing	O
the	O
cross-entropy	O
between	O
the	O
model's	O
predicted	O
distribution	O
and	O
the	O
one-hot	O
empirical	O
distribution	O
of	O
training	O
labels.	O
A	O
model	O
performing	O
well	O
on	O
the	O
training	O
set	O
will	O
predict	O
an	O
output	O
distribution	O
with	O
high	O
probability	O
on	O
the	O
correct	O
class	O
and	O
with	O
near-zero	O
probabilities	O
on	O
other	O
classes.	O
But	O
some	O
of	O
these	O
"near-zero"	O
probabilities	O
are	O
larger	O
than	O
others	O
and	O
reflect,	O
in	O
part,	O
the	O
generalization	O
capabilities	O
of	O
the	O
model	O
and	O
how	O
well	O
it	O
will	O
perform	O
on	O
the	O
test	O
set	O
3	O
.	O
Training	O
loss	O
The	O
student	O
is	O
trained	O
with	O
a	O
distillation	B-MetricName
loss	I-MetricName
over	O
the	O
soft	O
target	O
probabilities	O
of	O
the	O
teacher:	O
L	O
ce	O
=	O
i	O
t	O
i	O
*	O
log(s	O
i	O
)	O
where	O
t	O
i	O
(resp.	O
s	O
i	O
)	O
is	O
a	O
probability	O
estimated	O
by	O
the	O
teacher	O
(resp.	O
the	O
student).	O
This	O
objective	O
results	O
in	O
a	O
rich	O
training	O
signal	O
by	O
leveraging	O
the	O
full	O
teacher	O
distribution.	O
Following	O
Hinton	O
et	O
al.	O
we	O
used	O
a	O
softmax-temperature:	B-HyperparameterName
p	O
i	O
=	O
exp(zi/T	O
)	O
j	O
exp(zj	O
/T	O
)	O
where	O
T	B-HyperparameterName
controls	O
the	O
smoothness	O
of	O
the	O
output	O
distribution	O
and	O
z	O
i	O
is	O
the	O
model	O
score	O
for	O
the	O
class	O
i.	O
The	O
same	O
temperature	O
T	O
is	O
applied	O
to	O
the	O
student	O
and	O
the	O
teacher	O
at	O
training	O
time,	O
while	O
at	O
inference,	O
T	B-HyperparameterName
is	O
set	O
to	O
1	B-HyperparameterValue
to	O
recover	O
a	O
standard	O
softmax.	O
The	O
final	O
training	O
objective	O
is	O
a	O
linear	O
combination	O
of	O
the	O
distillation	B-MetricName
loss	I-MetricName
L	I-MetricName
ce	O
with	O
the	O
supervised	O
training	O
loss,	O
in	O
our	O
case	O
the	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
loss	I-MetricName
L	O
mlm	O
[Devlin	O
et	O
al.,	O
2018].	O
We	O
found	O
it	O
beneficial	O
to	O
add	O
a	O
cosine	B-MetricName
embedding	I-MetricName
loss	I-MetricName
(L	I-MetricName
cos	I-MetricName
)	I-MetricName
which	O
will	O
tend	O
to	O
align	O
the	O
directions	O
of	O
the	O
student	O
and	O
teacher	O
hidden	O
states	O
vectors.	O
3	O
DistilBERT:	B-MethodName
a	O
distilled	O
version	O
of	O
BERT	B-MethodName
Student	O
architecture	O
In	O
the	O
present	O
work,	O
the	O
student	O
-DistilBERT	B-MethodName
-has	O
the	O
same	O
general	O
architecture	O
as	O
BERT.	B-MethodName
The	O
token-type	O
embeddings	O
and	O
the	O
pooler	O
are	O
removed	O
while	O
the	O
number	O
of	O
layers	O
is	O
reduced	O
by	O
a	O
factor	O
of	O
2.	O
Most	O
of	O
the	O
operations	O
used	O
in	O
the	O
Transformer	B-MethodName
architecture	O
(linear	O
layer	O
and	O
layer	O
normalisation)	O
are	O
highly	O
optimized	O
in	O
modern	O
linear	O
algebra	O
frameworks	O
and	O
our	O
investigations	O
showed	O
that	O
variations	O
on	O
the	O
last	O
dimension	O
of	O
the	O
tensor	O
(hidden	O
size	O
dimension)	O
have	O
a	O
smaller	O
impact	O
on	O
computation	O
efficiency	O
(for	O
a	O
fixed	O
parameters	O
budget)	O
than	O
variations	O
on	O
other	O
factors	O
like	O
the	O
number	O
of	O
layers.	O
Thus	O
we	O
focus	O
on	O
reducing	O
the	O
number	O
of	O
layers.	O

The	O
last	O
two	O
years	O
have	O
seen	O
the	O
rise	O
of	O
Transfer	B-MethodName
Learning	I-MethodName
approaches	O
in	O
Natural	O
Language	O
Processing	O
(NLP)	O
with	O
large-scale	O
pre-trained	O
language	O
models	O
becoming	O
a	O
basic	O
tool	O
in	O
many	O
NLP	O
tasks	O
[Devlin	O
et	O
al.,	O
2018,	O
Radford	O
et	O
al.,	O
2019.	O
While	O
these	O
models	O
lead	O
to	O
significant	O
improvement,	O
they	O
often	O
have	O
several	O
hundred	O
million	O
parameters	O
and	O
current	O
research	O
1	O
on	O
pre-trained	O
models	O
indicates	O
that	O
training	O
even	O
larger	O
models	O
still	O
leads	O
to	O
better	O
performances	O
on	O
downstream	O
tasks.	O
The	O
trend	O
toward	O
bigger	O
models	O
raises	O
several	O
concerns.	O
First	O
is	O
the	O
environmental	O
cost	O
of	O
exponentially	O
scaling	O
these	O
models'	O
computational	O
requirements	O
as	O
mentioned	O
in	O
Schwartz	O
et	O
al.	O
,	O
Strubell	O
et	O
al.	O
.	O
Second,	O
while	O
operating	O
these	O
models	O
on-device	O
in	O
real-time	O
has	O
the	O
potential	O
to	O
enable	O
novel	O
and	O
interesting	O
language	O
processing	O
applications,	O
the	O
growing	O
computational	O
and	O
memory	O
requirements	O
of	O
these	O
models	O
may	O
hamper	O
wide	O
adoption.	O
In	O
this	O
paper,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
reach	O
similar	O
performances	O
on	O
many	O
downstream-tasks	O
using	O
much	O
smaller	O
language	O
models	O
pre-trained	O
with	O
knowledge	O
distillation,	B-MethodName
resulting	O
in	O
models	O
that	O
are	O
lighter	O
and	O
faster	O
at	O
inference	O
time,	O
while	O
also	O
requiring	O
a	O
smaller	O
computational	O
training	O
budget.	O
Our	O
general-purpose	O
pre-trained	O
models	O
can	O
be	O
fine-tuned	O
with	O
good	O
performances	O
on	O
several	O
downstream	O
tasks,	O
keeping	O
the	O
flexibility	O
of	O
larger	O
models.	O
We	O
also	O
show	O
that	O
our	O
compressed	O
models	O
are	O
small	O
enough	O
to	O
run	O
on	O
the	O
edge,	O
e.g.	O
on	O
mobile	O
devices.	O
Using	O
a	O
triple	O
loss,	O
we	O
show	O
that	O
a	O
40%	B-MetricValue
smaller	O
Transformer	O
(Vaswani	O
et	O
al.	O
)	O
pre-trained	O
through	O
distillation	B-MethodName
via	O
the	O
supervision	O
of	O
a	O
bigger	O
Transformer	B-MethodName
language	O
model	O
can	O
achieve	O
similar	O
performance	O
on	O
a	O
variety	O
of	O
downstream	O
tasks,	O
while	O
being	O
60%	O
faster	O
at	O
inference	O
time.	O
Further	O
ablation	O
studies	O
indicate	O
that	O
all	O
the	O
components	O
of	O
the	O
triple	O
loss	O
are	O
important	O
for	O
best	O
performances.	O
We	O
have	O
made	O
the	O
trained	O
weights	O
available	O
along	O
with	O
the	O
training	O
code	O
in	O
the	O
Transformers	O
2	O
library	O
from	O
HuggingFace	O
[Wolf	O
et	O
al.,	O
2019].	O

As	O
Transfer	B-MethodName
Learning	I-MethodName
from	O
large-scale	O
pre-trained	O
models	O
becomes	O
more	O
prevalent	O
in	O
Natural	O
Language	O
Processing	O
(NLP),	O
operating	O
these	O
large	O
models	O
in	O
on-theedge	O
and/or	O
under	O
constrained	O
computational	O
training	O
or	O
inference	O
budgets	O
remains	O
challenging.	O
In	O
this	O
work,	O
we	O
propose	O
a	O
method	O
to	O
pre-train	O
a	O
smaller	O
generalpurpose	O
language	O
representation	O
model,	O
called	O
DistilBERT,	B-MethodName
which	O
can	O
then	O
be	O
finetuned	O
with	O
good	O
performances	O
on	O
a	O
wide	O
range	O
of	O
tasks	O
like	O
its	O
larger	O
counterparts.	O
While	O
most	O
prior	O
work	O
investigated	O
the	O
use	O
of	O
distillation	B-MethodName
for	O
building	O
task-specific	O
models,	O
we	O
leverage	O
knowledge	O
distillation	B-MethodName
during	O
the	O
pre-training	O
phase	O
and	O
show	O
that	O
it	O
is	O
possible	O
to	O
reduce	O
the	O
size	O
of	O
a	O
BERT	B-MethodName
model	O
by	O
40%,	B-MetricValue
while	O
retaining	O
97%	B-MetricValue
of	O
its	O
language	O
understanding	O
capabilities	O
and	O
being	O
60%	B-MetricValue
faster.	O
To	O
leverage	O
the	O
inductive	O
biases	O
learned	O
by	O
larger	O
models	O
during	O
pre-training,	O
we	O
introduce	O
a	O
triple	O
loss	O
combining	O
language	O
modeling,	O
distillation	O
and	O
cosine-distance	O
losses.	O
Our	O
smaller,	O
faster	O
and	O
lighter	O
model	O
is	O
cheaper	O
to	O
pre-train	O
and	O
we	O
demonstrate	O
its	O
capabilities	O
for	O
on-device	O
computations	O
in	O
a	O
proof-of-concept	O
experiment	O
and	O
a	O
comparative	O
on-device	O
study.	O

Byte-Pair	B-MethodName
Encoding	I-MethodName
(BPE)	I-MethodName
(Sennrich	O
et	O
al.,	O
2016)	O
is	O
a	O
hybrid	O
between	O
character-and	O
word-level	O
representations	O
that	O
allows	O
handling	O
the	O
large	O
vocabularies	O
common	O
in	O
natural	O
language	O
corpora.	O
Instead	O
of	O
full	O
words,	O
BPE	O
relies	O
on	O
subwords	O
units,	O
which	O
are	O
extracted	O
by	O
performing	O
statistical	O
analysis	O
of	O
the	O
training	O
corpus.	O
BPE	O
vocabulary	O
sizes	O
typically	O
range	O
from	O
10K-100K	O
subword	O
units.	O
However,	O
unicode	O
characters	O
can	O
account	O
for	O
a	O
sizeable	O
portion	O
of	O
this	O
vocabulary	O
when	O
modeling	O
large	O
and	O
diverse	O
corpora,	O
such	O
as	O
the	O
ones	O
considered	O
in	O
this	O
work.	O
Radford	O
et	O
al.	O
(2019)	O
introduce	O
a	O
clever	O
implementation	O
of	O
BPE	O
that	O
uses	O
bytes	O
instead	O
of	O
unicode	O
characters	O
as	O
the	O
base	O
subword	O
units.	O
Using	O
bytes	O
makes	O
it	O
possible	O
to	O
learn	O
a	O
subword	O
vocabulary	O
of	O
a	O
modest	O
size	O
(50K	O
units)	O
that	O
can	O
still	O
encode	O
any	O
input	O
text	O
without	O
introducing	O
any	O
"unknown"	O
tokens.	O
8	O
Large	O
batch	O
training	O
can	O
improve	O
training	O
efficiency	O
even	O
without	O
large	O
scale	O
parallel	O
hardware	O
through	O
gradient	O
accumulation,	O
whereby	O
gradients	O
from	O
multiple	O
mini-batches	O
are	O
accumulated	O
locally	O
before	O
each	O
optimization	O
step.	O
This	O
functionality	O
is	O
supported	O
natively	O
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
The	O
original	O
BERT	B-MethodName
implementation	O
(Devlin	O
et	O
al.,	O
2019)	O
uses	O
a	O
character-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
of	O
size	O
30K,	B-HyperparameterValue
which	O
is	O
learned	O
after	O
preprocessing	O
the	O
input	O
with	O
heuristic	O
tokenization	O
rules.	O
Following	O
Radford	O
et	O
al.	O
(2019),	O
we	O
instead	O
consider	O
training	O
BERT	B-MethodName
with	O
a	O
larger	O
byte-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
containing	O
50K	B-HyperparameterValue
subword	I-HyperparameterValue
units,	I-HyperparameterValue
without	O
any	O
additional	O
preprocessing	O
or	O
tokenization	O
of	O
the	O
input.	O
This	O
adds	O
approximately	O
15M	O
and	O
20M	O
additional	O
parameters	O
for	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
respectively.	O
Early	O
experiments	O
revealed	O
only	O
slight	O
differences	O
between	O
these	O
encodings,	O
with	O
the	O
Radford	O
et	O
al.	O
(	O
2019)	O
BPE	B-MethodName
achieving	O
slightly	O
worse	O
end-task	O
performance	O
on	O
some	O
tasks.	O
Nevertheless,	O
we	O
believe	O
the	O
advantages	O
of	O
a	O
universal	O
encoding	O
scheme	O
outweighs	O
the	O
minor	O
degredation	O
in	O
performance	O
and	O
use	O
this	O
encoding	O
in	O
the	O
remainder	O
of	O
our	O
experiments.	O
A	O
more	O
detailed	O
comparison	O
of	O
these	O
encodings	O
is	O
left	O
to	O
future	O
work.	O

In	O
the	O
previous	O
section	O
we	O
propose	O
modifications	O
to	O
the	O
BERT	B-MethodName
pretraining	O
procedure	O
that	O
improve	O
end-task	O
performance.	O
We	O
now	O
aggregate	O
these	O
improvements	O
and	O
evaluate	O
their	O
combined	O
impact.	O
We	O
call	O
this	O
configuration	O
RoBERTa	B-MethodName
for	O
Robustly	B-MethodName
optimized	I-MethodName
BERT	I-MethodName
approach.	O
Specifically,	O
RoBERTa	B-MethodName
is	O
trained	O
with	O
dynamic	O
masking	O
(Section	O
4.1),	O
FULL-SENTENCES	O
without	O
NSP	O
loss	O
(Section	O
4.2),	O
large	O
mini-batches	O
(Section	O
4.3)	O
and	O
a	O
larger	O
byte-level	O
BPE	O
(Section	O
4.4).	O
Additionally,	O
we	O
investigate	O
two	O
other	O
important	O
factors	O
that	O
have	O
been	O
under-emphasized	O
in	O
previous	O
work:	O
(1)	O
the	O
data	O
used	O
for	O
pretraining,	O
and	O
(2)	O
the	O
number	O
of	O
training	O
passes	O
through	O
the	O
data.	O
For	O
example,	O
the	O
recently	O
proposed	O
XLNet	B-MethodName
architecture	O
(Yang	O
et	O
al.,	O
2019)	O
is	O
pretrained	O
using	O
nearly	O
10	O
times	O
more	O
data	O
than	O
the	O
original	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
It	O
is	O
also	O
trained	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
eight	O
times	O
larger	O
for	O
half	O
as	O
many	O
optimization	O
steps,	O
thus	O
seeing	O
four	O
times	O
as	O
many	O
sequences	O
in	O
pretraining	O
compared	O
to	O
BERT.	B-MethodName
To	O
help	O
disentangle	O
the	O
importance	O
of	O
these	O
factors	O
from	O
other	O
modeling	O
choices	O
(e.g.,	O
the	O
pretraining	O
objective),	O
we	O
begin	O
by	O
training	O
RoBERTa	B-MethodName
following	O
the	O
BERT	B-MethodName
LARGE	I-MethodName
architecture	O
(L	B-HyperparameterName
=	O
24,	B-HyperparameterValue
H	B-HyperparameterName
=	O
1024,	B-HyperparameterValue
A	B-HyperparameterName
=	O
16,	B-HyperparameterValue
355M	O
parameters).	O
We	O
pretrain	O
for	O
100K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
a	O
comparable	O
BOOK-CORPUS	B-DatasetName
plus	O
WIKIPEDIA	B-DatasetName
dataset	O
as	O
was	O
used	O
in	O
Yang	O
et	O
al.	O
(2019),	O
respectively.	O
Complete	O
results	O
on	O
all	O
GLUE	B-DatasetName
tasks	O
can	O
be	O
found	O
in	O
the	O
Appendix.	O
Devlin	O
et	O
al.	O
(2019).	O
We	O
pretrain	O
our	O
model	O
using	O
1024	O
V100	O
GPUs	O
for	O
approximately	O
one	O
day.	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
4.	O
When	O
controlling	O
for	O
training	O
data,	O
we	O
observe	O
that	O
RoBERTa	B-MethodName
provides	O
a	O
large	O
improvement	O
over	O
the	O
originally	O
reported	O
BERT	B-MethodName
LARGE	I-MethodName
results,	O
reaffirming	O
the	O
importance	O
of	O
the	O
design	O
choices	O
we	O
explored	O
in	O
Section	O
4.	O
Next,	O
we	O
combine	O
this	O
data	O
with	O
the	O
three	O
additional	O
datasets	O
described	O
in	O
Section	O
3.2.	O
We	O
train	O
RoBERTa	B-MethodName
over	O
the	O
combined	O
data	O
with	O
the	O
same	O
number	O
of	O
training	B-HyperparameterName
steps	I-HyperparameterName
as	O
before	O
(100K).	B-HyperparameterValue
In	O
total,	O
we	O
pretrain	O
over	O
160GB	O
of	O
text.	O
We	O
observe	O
further	O
improvements	O
in	O
performance	O
across	O
all	O
downstream	O
tasks,	O
validating	O
the	O
importance	O
of	O
data	O
size	O
and	O
diversity	O
in	O
pretraining.	O
9	O
Finally,	O
we	O
pretrain	O
RoBERTa	B-MethodName
for	O
significantly	O
longer,	O
increasing	O
the	O
number	O
of	O
pretraining	B-HyperparameterName
steps	I-HyperparameterName
from	O
100K	B-HyperparameterValue
to	I-HyperparameterValue
300K,	I-HyperparameterValue
and	O
then	O
further	O
to	O
500K.	B-HyperparameterValue
We	O
again	O
observe	O
significant	O
gains	O
in	O
downstream	O
task	O
performance,	O
and	O
the	O
300K	O
and	O
500K	O
step	O
models	O
outperform	O
XLNet	B-MethodName
LARGE	I-MethodName
across	O
most	O
tasks.	O
We	O
note	O
that	O
even	O
our	O
longest-trained	O
model	O
does	O
not	O
appear	O
to	O
overfit	O
our	O
data	O
and	O
would	O
likely	O
benefit	O
from	O
additional	O
training.	O
In	O
the	O
rest	O
of	O
the	O
paper,	O
we	O
evaluate	O
our	O
best	O
RoBERTa	B-MethodName
model	O
on	O
the	O
three	O
different	O
benchmarks:	O
GLUE,	B-DatasetName
SQuaD	B-DatasetName
and	O
RACE.	B-DatasetName
Specifically	O
we	O
consider	O
RoBERTa	B-MethodName
trained	O
for	O
500K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
all	O
five	O
of	O
the	O
datasets	O
introduced	O
in	O
Section	O
3.2.	O

For	O
GLUE	B-DatasetName
we	O
consider	O
two	O
finetuning	O
settings.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev)	O
we	O
finetune	O
RoBERTa	B-MethodName
separately	O
for	O
each	O
of	O
the	O
GLUE	B-DatasetName
tasks,	O
using	O
only	O
the	O
training	O
data	O
for	O
the	O
corresponding	O
task.	O
We	O
consider	O
a	O
limited	O
hyperparameter	O
sweep	O
for	O
each	O
task,	O
with	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
∈	O
{16,	B-HyperparameterValue
32}	I-HyperparameterValue
and	O
learning	B-HyperparameterName
rates	I-HyperparameterName
∈	O
{1e−5,	B-HyperparameterValue
2e−5,	I-HyperparameterValue
3e−5},	I-HyperparameterValue
with	O
a	O
linear	B-HyperparameterName
warmup	I-HyperparameterName
for	O
the	O
first	O
6%	B-HyperparameterValue
of	O
steps	O
followed	O
by	O
a	O
linear	B-HyperparameterName
decay	I-HyperparameterName
to	O
0.	O
We	O
finetune	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
perform	O
early	O
stopping	O
based	O
on	O
each	O
task's	O
evaluation	O
metric	O
on	O
the	O
dev	O
set.	O
The	O
rest	O
of	O
the	O
hyperparameters	O
remain	O
the	O
same	O
as	O
during	O
pretraining.	O
In	O
this	O
setting,	O
we	O
report	O
the	O
median	O
development	O
set	O
results	O
for	O
each	O
task	O
over	O
five	O
random	O
initializations,	O
without	O
model	O
ensembling.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
compare	O
RoBERTa	B-MethodName
to	O
other	O
approaches	O
on	O
the	O
test	O
set	O
via	O
the	O
GLUE	B-DatasetName
leaderboard.	O
While	O
many	O
submissions	O
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
depend	O
on	O
multitask	O
finetuning,	O
our	O
submission	O
depends	O
only	O
on	O
single-task	O
finetuning.	O
For	O
RTE,	B-DatasetName
STS	B-DatasetName
and	O
MRPC	B-DatasetName
we	O
found	O
it	O
helpful	O
to	O
finetune	O
starting	O
from	O
the	O
MNLI	O
single-task	O
model,	O
rather	O
than	O
the	O
baseline	O
pretrained	O
RoBERTa.	O
We	O
explore	O
a	O
slightly	O
wider	O
hyperparameter	O
space,	O
described	O
in	O
the	O
Appendix,	O
and	O
ensemble	O
between	O
5	O
and	O
7	O
models	O
per	O
task.	O
Task-specific	O
modifications	O
Two	O
of	O
the	O
GLUE	B-DatasetName
tasks	O
require	O
task-specific	O
finetuning	O
approaches	O
to	O
achieve	O
competitive	O
leaderboard	O
results.	O
QNLI:	B-DatasetName
Recent	O
submissions	O
on	O
the	O
GLUE	B-DatasetName
leaderboard	O
adopt	O
a	O
pairwise	O
ranking	O
formulation	O
for	O
the	O
QNLI	B-DatasetName
task,	O
in	O
which	O
candidate	O
answers	O
are	O
mined	O
from	O
the	O
training	O
set	O
and	O
compared	O
to	O
one	O
another,	O
and	O
a	O
single	O
(question,	O
candidate)	O
pair	O
is	O
classified	O
as	O
positive	O
(Liu	O
et	O
al.,	O
2019b,a;Yang	O
et	O
al.,	O
2019).	O
This	O
formulation	O
significantly	O
simplifies	O
the	O
task,	O
but	O
is	O
not	O
directly	O
comparable	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
Following	O
recent	O
work,	O
we	O
adopt	O
the	O
ranking	O
approach	O
for	O
our	O
test	O
submission,	O
but	O
for	O
direct	O
comparison	O
with	O
BERT	B-MethodName
we	O
report	O
development	O
set	O
results	O
based	O
on	O
a	O
pure	O
classification	O
approach.	O
WNLI:	B-DatasetName
We	O
found	O
the	O
provided	O
NLI-format	O
data	O
to	O
be	O
challenging	O
to	O
work	O
with.	O
Instead	O
we	O
use	O
the	O
reformatted	O
WNLI	B-DatasetName
data	O
from	O
Super-GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2019a),	O
which	O
indicates	O
the	O
span	O
of	O
the	O
query	O
pronoun	O
and	O
referent.	O
We	O
finetune	O
RoBERTa	B-MethodName
using	O
the	O
margin	O
ranking	O
loss	O
from	O
Kocijan	O
et	O
al.	O
(2019).	O
For	O
a	O
given	O
input	O
sentence,	O
we	O
use	O
spaCy	O
(Honnibal	O
and	O
Montani,	O
2017)	O
to	O
extract	O
additional	O
candidate	O
noun	O
phrases	O
from	O
the	O
sentence	O
and	O
finetune	O
our	O
model	O
so	O
that	O
it	O
assigns	O
higher	O
scores	O
to	O
positive	O
referent	O
phrases	O
than	O
for	O
any	O
of	O
the	O
generated	O
negative	O
candidate	O
phrases.	O
One	O
unfortunate	O
consequence	O
of	O
this	O
formulation	O
is	O
that	O
we	O
can	O
only	O
make	O
use	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
5.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev),	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
all	O
9	O
of	O
the	O
GLUE	O
task	O
development	O
sets.	O
Crucially,	O
RoBERTa	B-MethodName
uses	O
the	O
same	O
masked	O
language	O
modeling	O
pretraining	O
objective	O
and	O
architecture	O
as	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
yet	O
consistently	O
outperforms	O
both	O
BERT	B-MethodName
LARGE	I-MethodName
and	O
XLNet	B-MethodName
LARGE	I-MethodName
.	O
This	O
raises	O
questions	O
about	O
the	O
relative	O
importance	O
of	O
model	O
architecture	O
and	O
pretraining	O
objective,	O
compared	O
to	O
more	O
mundane	O
details	O
like	O
dataset	O
size	O
and	O
training	O
time	O
that	O
we	O
explore	O
in	O
this	O
work.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
and	O
achieve	O
state-of-the-art	O
results	O
on	O
4	O
out	O
of	O
9	O
tasks	O
and	O
the	O
highest	O
average	O
score	O
to	O
date.	O
This	O
is	O
especially	O
exciting	O
because	O
RoBERTa	B-MethodName
does	O
not	O
depend	O
on	O
multi-task	O
finetuning,	O
unlike	O
most	O
of	O
the	O
other	O
top	O
submissions.	O
We	O
expect	O
future	O
work	O
may	O
further	O
improve	O
these	O
results	O
by	O
incorporating	O
more	O
sophisticated	O
multi-task	O
finetuning	O
procedures.	O

We	O
adopt	O
a	O
much	O
simpler	O
approach	O
for	O
SQuAD	B-DatasetName
compared	O
to	O
past	O
work.	O
In	O
particular,	O
while	O
both	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
XLNet,	B-MethodName
while	O
we	O
use	O
the	O
same	O
learning	O
rate	O
for	O
all	O
layers.	O
For	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
we	O
follow	O
the	O
same	O
finetuning	O
procedure	O
as	O
Devlin	O
et	O
al.	O
(2019).	O
For	O
SQuAD	B-DatasetName
v2.0,	I-DatasetName
we	O
additionally	O
classify	O
whether	O
a	O
given	O
question	O
is	O
answerable;	O
we	O
train	O
this	O
classifier	O
jointly	O
with	O
the	O
span	O
predictor	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O

We	O
present	O
our	O
results	O
in	O
Table	O
6.	O
On	O
the	O
SQuAD	O
v1.1	O
development	O
set,	O
RoBERTa	B-MethodName
matches	O
the	O
state-of-the-art	O
set	O
by	O
XLNet.	B-MethodName
On	O
the	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
development	O
set,	O
RoBERTa	B-DatasetName
sets	O
a	O
new	O
state-of-the-art,	O
improving	O
over	O
XLNet	B-DatasetName
by	B-MetricValue
0.4	I-MetricValue
points	I-MetricValue
(EM)	B-MetricName
and	O
0.6	B-MetricValue
points	I-MetricValue
(F1).	B-MetricName
We	O
also	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
public	O
SQuAD	B-DatasetName
2.0	I-DatasetName
leaderboard	O
and	O
evaluate	O
its	O
performance	O
relative	O
to	O
other	O
systems.	O
Most	O
of	O
the	O
top	O
systems	O
build	O
upon	O
either	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
or	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
both	O
of	O
which	O
rely	O
on	O
additional	O
external	O
training	O
data.	O
In	O
contrast,	O
our	O
submission	O
does	O
not	O
use	O
any	O
additional	O
data.	O
Our	O
single	B-MethodName
RoBERTa	I-MethodName
model	O
outperforms	O
all	O
but	O
one	O
of	O
the	O
single	O
model	O
submissions,	O
and	O
is	O
the	O
top	O
scoring	O
system	O
among	O
those	O
that	O
do	O
not	O
rely	O
on	O
data	O
augmentation.	O

In	O
RACE,	B-DatasetName
systems	O
are	O
provided	O
with	O
a	O
passage	O
of	O
text,	O
an	O
associated	O
question,	O
and	O
four	O
candidate	O
answers.	O
Systems	O
are	O
required	O
to	O
classify	O
which	O
of	O
the	O
four	O
candidate	O
answers	O
is	O
correct.	O
We	O
modify	O
RoBERTa	B-MethodName
for	O
this	O
task	O
by	O
concate-	O
Yang	O
et	O
al.	O
(2019).	O
nating	O
each	O
candidate	O
answer	O
with	O
the	O
corresponding	O
question	O
and	O
passage.	O
We	O
then	O
encode	O
each	O
of	O
these	O
four	O
sequences	O
and	O
pass	O
the	O
resulting	O
[CLS]	O
representations	O
through	O
a	O
fully-connected	O
layer,	O
which	O
is	O
used	O
to	O
predict	O
the	O
correct	O
answer.	O
We	O
truncate	O
question-answer	O
pairs	O
that	O
are	O
longer	O
than	O
128	O
tokens	O
and,	O
if	O
needed,	O
the	O
passage	O
so	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Results	O
on	O
the	O
RACE	B-DatasetName
test	O
sets	O
are	O
presented	O
in	O
Table	O
7.	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
both	O
middle-school	O
and	O
high-school	O
settings.	O

Past	O
work	O
in	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
has	O
shown	O
that	O
training	O
with	O
very	O
large	O
mini-batches	B-HyperparameterName
can	O
both	O
improve	O
optimization	O
speed	O
and	O
end-task	O
performance	O
when	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
increased	O
appropriately	O
(Ott	O
et	O
al.,	O
2018).	O
Recent	O
work	O
has	O
shown	O
that	O
BERT	B-MethodName
is	O
also	O
amenable	O
to	O
large	O
batch	O
training	O
(You	O
et	O
al.,	O
2019	O

In	O
the	O
original	O
BERT	B-MethodName
pretraining	O
procedure,	O
the	O
model	O
observes	O
two	O
concatenated	O
document	O
segments,	O
which	O
are	O
either	O
sampled	O
contiguously	O
from	O
the	O
same	O
document	O
(with	O
p	B-MetricName
=	O
0.5)	B-MetricValue
or	O
from	O
distinct	O
documents.	O
In	O
addition	O
to	O
the	O
masked	O
language	O
modeling	O
objective,	O
the	O
model	O
is	O
trained	O
to	O
predict	O
whether	O
the	O
observed	O
document	O
segments	O
come	O
from	O
the	O
same	O
or	O
distinct	O
documents	O
via	O
an	O
auxiliary	O
Next	O
Sentence	O
Prediction	O
(NSP)	O
loss.	O
The	O
NSP	O
loss	O
was	O
hypothesized	O
to	O
be	O
an	O
important	O
factor	O
in	O
training	O
the	O
original	O
BERT	B-MethodName
model.	O
Devlin	O
et	O
al.	O
(2019)	O
observe	O
that	O
removing	O
NSP	O
hurts	O
performance,	O
with	O
significant	O
performance	O
degradation	O
on	O
QNLI,	B-DatasetName
MNLI,	B-DatasetName
and	O
SQuAD	B-DatasetName
1.1.	I-DatasetName
However,	O
some	O
recent	O
work	O
has	O
questioned	O
the	O
necessity	O
of	O
the	O
NSP	O
loss	O
(Lample	O
and	O
Conneau,	O
2019;Yang	O
et	O
al.,	O
2019;Joshi	O
et	O
al.,	O
2019).	O
To	O
better	O
understand	O
this	O
discrepancy,	O
we	O
compare	O
several	O
alternative	O
training	O
formats:	O
•	O
SEGMENT-PAIR+NSP:	O
This	O
follows	O
the	O
original	O
input	O
format	O
used	O
in	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
with	O
the	O
NSP	O
loss.	O
Each	O
input	O
has	O
a	O
pair	O
of	O
segments,	O
which	O
can	O
each	O
contain	O
multiple	O
natural	O
sentences,	O
but	O
the	O
total	O
combined	O
length	O
must	O
be	O
less	O
than	O
512	O
tokens.	O
•	O
SENTENCE-PAIR+NSP:	O
Each	O
input	O
contains	O
a	O
pair	O
of	O
natural	O
sentences,	O
either	O
sampled	O
from	O
a	O
contiguous	O
portion	O
of	O
one	O
document	O
or	O
from	O
separate	O
documents.	O
Since	O
these	O
inputs	O
are	O
significantly	O
shorter	O
than	O
512	O
tokens,	O
we	O
increase	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
so	O
that	O
the	O
total	O
number	O
of	O
tokens	O
remains	O
similar	O
to	O
SEGMENT-PAIR+NSP.	O
We	O
retain	O
the	O
NSP	O
loss.	O
•	O
FULL-SENTENCES:	O
Each	O
input	O
is	O
packed	O
with	O
full	O
sentences	O
sampled	O
contiguously	O
from	O
one	O
or	O
more	O
documents,	O
such	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Inputs	O
may	O
cross	O
document	O
boundaries.	O
When	O
we	O
reach	O
the	O
end	O
of	O
one	O
document,	O
we	O
begin	O
sampling	O
sentences	O
from	O
the	O
next	O
document	O
and	O
add	O
an	O
extra	O
separator	O
token	O
between	O
documents.	O
We	O
remove	O
the	O
NSP	O
loss.	O
•	O
DOC-SENTENCES:	O
Inputs	O
are	O
constructed	O
similarly	O
to	O
FULL-SENTENCES,	O
except	O
that	O
they	O
may	O
not	O
cross	O
document	O
boundaries.	O
Inputs	O
sampled	O
near	O
the	O
end	O
of	O
a	O
document	O
may	O
be	O
shorter	O
than	O
512	O
tokens,	O
so	O
we	O
dynamically	O
increase	O
the	O
batch	O
size	O
in	O
these	O
cases	O
to	O
achieve	O
a	O
similar	O
number	O
of	O
total	O
tokens	O
as	O
FULL-SENTENCES.	O
We	O
remove	O
the	O
NSP	O
loss.	O
Results	O
Table	O
2	O
shows	O
results	O
for	O
the	O
four	O
different	O
settings.	O
We	O
first	O
compare	O
the	O
original	O
SEGMENT-PAIR	O
input	O
format	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
the	O
SENTENCE-PAIR	O
format;	O
both	O
formats	O
retain	O
the	O
NSP	O
loss,	O
but	O
the	O
latter	O
uses	O
single	O
sentences.	O
We	O
find	O
that	O
using	O
individual	O
sentences	O
hurts	O
performance	O
on	O
downstream	O
tasks,	O
which	O
we	O
hypothesize	O
is	O
because	O
the	O
model	O
is	O
not	O
able	O
to	O
learn	O
long-range	O
dependencies.	O
We	O
next	O
compare	O
training	O
without	O
the	O
NSP	O
loss	O
and	O
training	O
with	O
blocks	O
of	O
text	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES).	O
We	O
find	O
that	O
this	O
setting	O
outperforms	O
the	O
originally	O
published	O
BERT	B-MethodName
BASE	I-MethodName
results	O
and	O
that	O
removing	O
the	O
NSP	O
loss	O
matches	O
or	O
slightly	O
improves	O
downstream	O
task	O
performance,	O
in	O
contrast	O
to	O
Devlin	O
et	O
al.	O
(2019).	O
It	O
is	O
possible	O
that	O
the	O
original	O
BERT	B-MethodName
implementation	O
may	O
only	O
have	O
removed	O
the	O
loss	O
term	O
while	O
still	O
retaining	O
the	O
SEGMENT-PAIR	O
input	O
format.	O
Finally	O
we	O
find	O
that	O
restricting	O
sequences	O
to	O
come	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES)	O
performs	O
slightly	O
better	O
than	O
packing	O
sequences	O
from	O
multiple	O
documents	O
(FULL-SENTENCES).	O
However,	O
because	O
the	O
DOC-SENTENCES	O
format	O
results	O
in	O
variable	O
batch	O
sizes,	O
we	O
use	O
FULL-SENTENCES	O
in	O
the	O
remainder	O
of	O
our	O
experiments	O
for	O
easier	O
comparison	O
with	O
related	O
work.	O

As	O
discussed	O
in	O
Section	O
2,	O
BERT	B-MethodName
relies	O
on	O
randomly	O
masking	O
and	O
predicting	O
tokens.	O
The	O
original	O
BERT	B-MethodName
implementation	O
performed	O
masking	O
once	O
during	O
data	O
preprocessing,	O
resulting	O
in	O
a	O
single	O
static	O
mask.	O
To	O
avoid	O
using	O
the	O
same	O
mask	O
for	O
each	O
training	O
instance	O
in	O
every	O
epoch,	O
training	O
data	O
was	O
duplicated	O
10	O
times	O
so	O
that	O
each	O
sequence	O
is	O
masked	O
in	O
10	O
different	O
ways	O
over	O
the	O
40	B-HyperparameterValue
epochs	B-HyperparameterName
of	O
training.	O
Thus,	O
each	O
training	O
sequence	O
was	O
seen	O
with	O
the	O
same	O
mask	O
four	O
times	O
during	O
training.	O
We	O
compare	O
this	O
strategy	O
with	O
dynamic	O
masking	O
where	O
we	O
generate	O
the	O
masking	O
pattern	O
every	O
time	O
we	O
feed	O
a	O
sequence	O
to	O
the	O
model.	O
This	O
becomes	O
crucial	O
when	O
pretraining	O
for	O
more	O
steps	O
or	O
with	O
larger	O
datasets.	O
Results	O
Table	O
1	O
compares	O
the	O
published	O
BERT	O
BASE	O
results	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
our	O
reimplementation	O
with	O
either	O
static	O
or	O
dynamic	O
masking.	O
We	O
find	O
that	O
our	O
reimplementation	O
with	O
static	O
masking	O
performs	O
similar	O
to	O
the	O
original	O
BERT	B-MethodName
model,	O
and	O
dynamic	O
masking	O
is	O
comparable	O
or	O
slightly	O
better	O
than	O
static	O
masking.	O
Given	O
these	O
results	O
and	O
the	O
additional	O
efficiency	O
benefits	O
of	O
dynamic	O
masking,	O
we	O
use	O
dynamic	O
masking	O
in	O
the	O
remainder	O
of	O
the	O
experiments.	O

Pretraining	O
methods	O
have	O
been	O
designed	O
with	O
different	O
training	O
objectives,	O
including	O
language	O
modeling	O
(Dai	O
and	O
Le,	O
2015;Peters	O
et	O
al.,	O
2018;Howard	O
and	O
Ruder,	O
2018),	O
machine	B-TaskName
translation	I-TaskName
(McCann	O
et	O
al.,	O
2017),	O
and	O
masked	O
language	O
modeling	O
(Devlin	O
et	O
al.,	O
2019;Lample	O
and	O
Conneau,	O
2019).	O
Many	O
recent	O
papers	O
have	O
used	O
a	O
basic	O
recipe	O
of	O
finetuning	O
models	O
for	O
each	O
end	O
task	O
(Howard	O
and	O
Ruder,	O
2018;Radford	O
et	O
al.,	O
2018),	O
and	O
pretraining	O
with	O
some	O
variant	O
of	O
a	O
masked	O
language	O
model	O
objective.	O
However,	O
newer	O
methods	O
have	O
improved	O
performance	O
by	O
multi-task	O
fine	O
tuning	O
(Dong	O
et	O
al.,	O
2019),	O
incorporating	O
entity	O
embeddings	O
(Sun	O
et	O
al.,	O
2019),	O
span	O
prediction	O
(Joshi	O
et	O
al.,	O
2019),	O
and	O
multiple	O
variants	O
of	O
autoregressive	O
pretraining	O
Chan	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019).	O
Performance	O
is	O
also	O
typically	O
improved	O
by	O
training	O
bigger	O
models	O
on	O
more	O
data	O
(Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Radford	O
et	O
al.,	O
2019).	O
Our	O
goal	O
was	O
to	O
replicate,	O
simplify,	O
and	O
better	O
tune	O
the	O
training	O
of	O
BERT,	B-MethodName
as	O
a	O
reference	O
point	O
for	O
better	O
understanding	O
the	O
relative	O
performance	O
of	O
all	O
of	O
these	O
methods.	O
We	O
carefully	O
evaluate	O
a	O
number	O
of	O
design	O
decisions	O
when	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
find	O
that	O
performance	O
can	O
be	O
substantially	O
improved	O
by	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches	O
over	O
more	O
data;	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
training	O
on	O
longer	O
sequences;	O
and	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
Our	O
improved	O
pretraining	O
procedure,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD,	B-DatasetName
without	O
multi-task	O
finetuning	O
for	O
GLUE	B-DatasetName
or	O
additional	O
data	O
for	O
SQuAD.	B-DatasetName
These	O
results	O
illustrate	O
the	O
importance	O
of	O
these	O
previously	O
overlooked	O
design	O
decisions	O
and	O
suggest	O
that	O
BERT's	B-MethodName
pretraining	O
objective	O
remains	O
competitive	O
with	O
recently	O
proposed	O
alternatives.	O
We	O
additionally	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
release	O
our	O
models	O
and	O
code	O
for	O
pretraining	O
and	O
finetuning	O
at:	O

This	O
section	O
explores	O
and	O
quantifies	O
which	O
choices	O
are	O
important	O
for	O
successfully	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
keep	O
the	O
model	O
architecture	O
fixed.	O
7	O
Specifically,	O
we	O
begin	O
by	O
training	O
BERT	B-MethodName
models	O
with	O
the	O
same	O
configuration	O
as	O
BERT	B-MethodName
BASE	I-MethodName
(L	B-HyperparameterName
=	O
12,	B-HyperparameterValue
H	B-HyperparameterName
=	O
768,	B-HyperparameterValue
A	B-HyperparameterName
=	O
12,	B-HyperparameterValue
110M	B-MetricValue
params).	I-MetricValue

Following	O
previous	O
work,	O
we	O
evaluate	O
our	O
pretrained	O
models	O
on	O
downstream	O
tasks	O
using	O
the	O
following	O
three	O
benchmarks.	O
GLUE	B-DatasetName
The	B-DatasetName
General	I-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	O
(Wang	O
et	O
al.,	O
2019b)	O
is	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
6	O
Tasks	O
are	O
framed	O
as	O
either	O
single-sentence	O
classification	O
or	O
sentence-pair	O
classification	O
tasks.	O
The	O
GLUE	B-DatasetName
organizers	O
provide	O
training	O
and	O
development	O
data	O
splits	O
as	O
well	O
as	O
a	O
submission	O
server	O
and	O
leaderboard	O
that	O
allows	O
participants	O
to	O
evaluate	O
and	O
compare	O
their	O
systems	O
on	O
private	O
held-out	O
test	O
data.	O
For	O
the	O
replication	O
study	O
in	O
Section	O
4,	O
we	O
report	O
results	O
on	O
the	O
development	O
sets	O
after	O
finetuning	O
the	O
pretrained	O
models	O
on	O
the	O
corresponding	O
singletask	O
training	O
data	O
(i.e.,	O
without	O
multi-task	O
training	O
or	O
ensembling).	O
Our	O
finetuning	O
procedure	O
follows	O
the	O
original	O
BERT	B-MethodName
paper	O
(Devlin	O
et	O
al.,	O
2019).	O
In	O
Section	O
5	O
we	O
additionally	O
report	O
test	O
set	O
results	O
obtained	O
from	O
the	O
public	O
leaderboard.	O
These	O
results	O
depend	O
on	O
a	O
several	O
task-specific	O
modifications,	O
which	O
we	O
describe	O
in	O
Section	O
5.1.	O
SQuAD	B-DatasetName
The	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(SQuAD)	I-DatasetName
provides	O
a	O
paragraph	O
of	O
context	O
and	O
a	O
question.	O
The	O
task	O
is	O
to	O
answer	O
the	O
question	O
by	O
extracting	O
the	O
relevant	O
span	O
from	O
the	O
context.	O
We	O
evaluate	O
on	O
two	O
versions	O
of	O
SQuAD:	B-DatasetName
V1.1	O
and	O
V2.0	O
(Rajpurkar	O
et	O
al.,	O
2016(Rajpurkar	O
et	O
al.,	O
,	O
2018.	O
In	O
V1.1	O
the	O
context	O
always	O
contains	O
an	O
answer,	O
whereas	O
in	O
V2.0	O
some	O
questions	O
are	O
not	O
answered	O
in	O
the	O
provided	O
context,	O
making	O
the	O
task	O
more	O
challenging.	O
For	O
SQuAD	B-DatasetName
V1.1	I-DatasetName
we	O
adopt	O
the	O
same	O
span	O
prediction	O
method	O
as	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
For	O
SQuAD	B-DatasetName
V2.0,	I-DatasetName
we	O
add	O
an	O
additional	O
binary	O
classifier	O
to	O
predict	O
whether	O
the	O
question	O
is	O
answerable,	O
which	O
we	O
train	O
jointly	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O
During	O
evaluation,	O
we	O
only	O
predict	O
span	O
indices	O
on	O
pairs	O
that	O
are	O
classified	O
as	O
answerable.	O
RACE	B-DatasetName
The	B-DatasetName
ReAding	I-DatasetName
Comprehension	I-DatasetName
from	I-DatasetName
Examinations	I-DatasetName
(RACE)	I-DatasetName
(Lai	O
et	O
al.,	O
2017)	O
task	O
is	O
a	O
large-scale	O
reading	O
comprehension	O
dataset	O
with	O
more	O
than	O
28,000	O
passages	O
and	O
nearly	O
100,000	O
questions.	O
The	O
dataset	O
is	O
collected	O
from	O
English	O
examinations	O
in	O
China,	O
which	O
are	O
designed	O
for	O
middle	O
and	O
high	O
school	O
students.	O
In	O
RACE,	B-DatasetName
each	O
passage	O
is	O
associated	O
with	O
multiple	O
questions.	O
For	O
every	O
question,	O
the	O
task	O
is	O
to	O
select	O
one	O
correct	O
answer	O
from	O
four	O
options.	O
RACE	B-DatasetName
has	O
significantly	O
longer	O
context	O
than	O
other	O
popular	O
reading	O
comprehension	O
datasets	O
and	O
the	O
proportion	O
of	O
questions	O
that	O
requires	O
reasoning	O
is	O
very	O
large.	O

BERT-style	B-MethodName
pretraining	O
crucially	O
relies	O
on	O
large	O
quantities	O
of	O
text.	O
demonstrate	O
that	O
increasing	O
data	O
size	O
can	O
result	O
in	O
improved	O
end-task	O
performance.	O
Several	O
efforts	O
have	O
trained	O
on	O
datasets	O
larger	O
and	O
more	O
diverse	O
than	O
the	O
original	O
BERT	B-MethodName
(Radford	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Zellers	O
et	O
al.,	O
2019).	O
Unfortunately,	O
not	O
all	O
of	O
the	O
additional	O
datasets	O
can	O
be	O
publicly	O
released.	O
For	O
our	O
study,	O
we	O
focus	O
on	O
gathering	O
as	O
much	O
data	O
as	O
possible	O
for	O
experimentation,	O
allowing	O
us	O
to	O
match	O
the	O
overall	O
quality	O
and	O
quantity	O
of	O
data	O
as	O
appropriate	O
for	O
each	O
comparison.	O
We	O
consider	O
five	O
English-language	O
corpora	O
of	O
varying	O
sizes	O
and	O
domains,	O
totaling	O
over	O
160GB	O
of	O
uncompressed	O
text.	O
We	O
use	O
the	O
following	O
text	O
corpora:	O
•	O
BOOKCORPUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA.	I-DatasetName
This	O
is	O
the	O
original	O
data	O
used	O
to	O
train	O
BERT.	B-MethodName
(16GB).	O
•	O
CC-NEWS,	B-DatasetName
which	O
we	O
collected	O
from	O
the	O
English	O
portion	O
of	O
the	O
CommonCrawl	B-DatasetName
News	I-DatasetName
dataset	I-DatasetName
(Nagel,	O
2016).	O
The	O
data	O
contains	O
63	B-HyperparameterName
million	I-HyperparameterName
English	I-HyperparameterName
news	I-HyperparameterName
articles	I-HyperparameterName
crawled	O
between	O
September	O
2016	O
and	O
February	O
2019.	O
(76GB	O
after	O
filtering).	O
4	O
•	O
OPENWEBTEXT	B-DatasetName
(Gokaslan	O
and	O
Cohen,	O
2019),	O
an	O
open-source	O
recreation	O
of	O
the	O
WebText	O
cor-pus	O
described	O
in	O
Radford	O
et	O
al.	O
(2019).	O
The	O
text	O
is	O
web	O
content	O
extracted	O
from	O
URLs	O
shared	O
on	O
Reddit	O
with	O
at	O
least	O
three	O
upvotes.	O
(38GB).	O
5	O
•	O
STORIES,	B-DatasetName
a	O
dataset	O
introduced	O
in	O
Trinh	O
and	O
Le	O
(2018)	O
containing	O
a	O
subset	O
of	O
CommonCrawl	B-DatasetName
data	O
filtered	O
to	O
match	O
the	O
story-like	O
style	O
of	O
Winograd	O
schemas.	O
(31GB).	O

We	O
reimplement	O
BERT	B-MethodName
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
We	O
primarily	O
follow	O
the	O
original	O
BERT	B-MethodName
optimization	O
hyperparameters,	O
given	O
in	O
Section	O
2,	O
except	O
for	O
the	O
peak	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
number	B-HyperparameterName
of	I-HyperparameterName
warmup	I-HyperparameterName
steps,	I-HyperparameterName
which	O
are	O
tuned	O
separately	O
for	O
each	O
setting.	O
We	O
additionally	O
found	O
training	O
to	O
be	O
very	O
sensitive	O
to	O
the	O
Adam	B-HyperparameterValue
epsilon	O
term,	O
and	O
in	O
some	O
cases	O
we	O
obtained	O
better	O
performance	O
or	O
improved	O
stability	O
after	O
tuning	O
it.	O
Similarly,	O
we	O
found	O
setting	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.98	B-HyperparameterValue
to	O
improve	O
stability	O
when	O
training	O
with	O
large	O
batch	O
sizes.	O
We	O
pretrain	O
with	O
sequences	O
of	O
at	O
most	O
T	O
=	O
512	O
tokens.	O
Unlike	O
Devlin	O
et	O
al.	O
(2019),	O
we	O
do	O
not	O
randomly	O
inject	O
short	O
sequences,	O
and	O
we	O
do	O
not	O
train	O
with	O
a	O
reduced	O
sequence	O
length	O
for	O
the	O
first	O
90%	B-MetricValue
of	O
updates.	O
We	O
train	O
only	O
with	O
full-length	O
sequences.	O
We	O
train	O
with	O
mixed	O
precision	O
floating	O
point	O
arithmetic	O
on	O
DGX-1	O
machines,	O
each	O
with	O
8	O
×	O
32GB	O
Nvidia	O
V100	O
GPUs	O
interconnected	O
by	O
Infiniband	O
(Micikevicius	O
et	O
al.,	O
2018).	O

In	O
this	O
section,	O
we	O
describe	O
the	O
experimental	O
setup	O
for	O
our	O
replication	O
study	O
of	O
BERT.	B-MethodName

BERT	B-MethodName
is	O
trained	O
on	O
a	O
combination	O
of	O
BOOKCOR-PUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA,	I-DatasetName
which	O
totals	O
16GB	O
of	O
uncompressed	O
text.	O
3	O

BERT	B-MethodName
is	O
optimized	O
with	O
Adam	B-HyperparameterValue
(Kingma	O
and	O
Ba,	O
2015)	O
using	O
the	O
following	O
parameters:	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9,	B-HyperparameterValue
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999,	B-HyperparameterValue
ǫ	B-HyperparameterName
=	O
1e-6	B-HyperparameterValue
and	O
L	B-HyperparameterName
2	I-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01.	B-HyperparameterValue
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
warmed	O
up	O
over	O
the	O
first	O
10,000	O
steps	O
to	O
a	O
peak	O
value	O
of	O
1e-4,	B-HyperparameterValue
and	O
then	O
linearly	O
decayed.	O
BERT	B-MethodName
trains	O
with	O
a	O
dropout	B-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers	O
and	O
attention	O
weights,	O
and	O
a	O
GELU	B-HyperparameterValue
activation	B-HyperparameterName
function	I-HyperparameterName
(Hendrycks	O
and	O
Gimpel,	O
2016).	O
Models	O
are	O
pretrained	O
for	O
S	B-HyperparameterName
=	O
1,000,000	B-HyperparameterValue
updates,	O
with	O
minibatches	B-HyperparameterName
containing	O
B	O
=	O
256	B-HyperparameterValue
sequences	O
of	O
maximum	O
length	O
T	O
=	O
512	O
tokens.	O

During	O
pretraining,	O
BERT	B-MethodName
uses	O
two	O
objectives:	O
masked	O
language	O
modeling	O
and	O
next	B-TaskName
sentence	I-TaskName
prediction.	I-TaskName
Masked	O
Language	O
Model	O
(MLM)	O
A	O
random	O
sample	O
of	O
the	O
tokens	O
in	O
the	O
input	O
sequence	O
is	O
selected	O
and	O
replaced	O
with	O
the	O
special	O
token	O
[MASK	O
].	O
The	O
MLM	O
objective	O
is	O
a	O
cross-entropy	B-MetricName
loss	O
on	O
predicting	O
the	O
masked	O
tokens.	O
BERT	B-MethodName
uniformly	O
selects	O
15%	B-MetricValue
of	O
the	O
input	O
tokens	O
for	O
possible	O
replacement.	O
Of	O
the	O
selected	O
tokens,	O
80%	B-MetricValue
are	O
replaced	O
with	O
[MASK	O
],	O
10%	B-MetricValue
are	O
left	O
unchanged,	O
and	O
10%	B-MetricValue
are	O
replaced	O
by	O
a	O
randomly	O
selected	O
vocabulary	O
token.	O
In	O
the	O
original	O
implementation,	O
random	O
masking	O
and	O
replacement	O
is	O
performed	O
once	O
in	O
the	O
beginning	O
and	O
saved	O
for	O
the	O
duration	O
of	O
training,	O
although	O
in	O
practice,	O
data	O
is	O
duplicated	O
so	O
the	O
mask	O
is	O
not	O
always	O
the	O
same	O
for	O
every	O
training	O
sentence	O
(see	O
Section	O
4.1).	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
(NSP)	O
NSP	B-TaskName
is	O
a	O
binary	O
classification	O
loss	O
for	O
predicting	O
whether	O
two	O
segments	O
follow	O
each	O
other	O
in	O
the	O
original	O
text.	O
Positive	O
examples	O
are	O
created	O
by	O
taking	O
consecutive	O
sentences	O
from	O
the	O
text	O
corpus.	O
Negative	O
examples	O
are	O
created	O
by	O
pairing	O
segments	O
from	O
different	O
documents.	O
Positive	O
and	O
negative	O
examples	O
are	O
sampled	O
with	O
equal	O
probability.	O
The	O
NSP	B-TaskName
objective	O
was	O
designed	O
to	O
improve	O
performance	O
on	O
downstream	O
tasks,	O
such	O
as	O
Natural	O
Language	O
Inference	O
(Bowman	O
et	O
al.,	O
2015),	O
which	O
require	O
reasoning	O
about	O
the	O
relationships	O
between	O
pairs	O
of	O
sentences.	O

BERT	B-MethodName
uses	O
the	O
now	O
ubiquitous	O
transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017),	O
which	O
we	O
will	O
not	O
review	O
in	O
detail.	O
We	O
use	O
a	O
transformer	O
architecture	O
with	O
L	O
layers.	O
Each	O
block	O
uses	O
A	O
self-attention	O
heads	O
and	O
hidden	O
dimension	O
H.	O

BERT	B-MethodName
takes	O
as	O
input	O
a	O
concatenation	O
of	O
two	O
segments	O
(sequences	O
of	O
tokens),	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
and	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
.	O
Segments	O
usually	O
consist	O
of	O
more	O
than	O
one	O
natural	O
sentence.	O
The	O
two	O
segments	O
are	O
presented	O
as	O
a	O
single	O
input	O
sequence	O
to	O
BERT	B-MethodName
with	O
special	O
tokens	O
delimiting	O
them:	O
[CLS	O
],	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
,	O
[SEP	O
],	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
,	O
[EOS	O
].	O
M	O
and	O
N	O
are	O
constrained	O
such	O
that	O
M	O
+	O
N	O
<	O
T	O
,	O
where	O
T	O
is	O
a	O
parameter	O
that	O
controls	O
the	O
maximum	O
sequence	O
length	O
during	O
training.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
a	O
large	O
unlabeled	O
text	O
corpus	O
and	O
subsequently	O
finetuned	O
using	O
end-task	O
labeled	O
data.	O

In	O
this	O
section,	O
we	O
give	O
a	O
brief	O
overview	O
of	O
the	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
pretraining	O
approach	O
and	O
some	O
of	O
the	O
training	O
choices	O
that	O
we	O
will	O
examine	O
experimentally	O
in	O
the	O
following	O
section.	O

Self-training	O
methods	O
such	O
as	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018),	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
XLM	B-MethodName
(Lample	O
and	O
Conneau,	O
2019),	O
and	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
have	O
brought	O
significant	O
performance	O
gains,	O
but	O
it	O
can	O
be	O
challenging	O
to	O
determine	O
which	O
aspects	O
of	O
the	O
methods	O
contribute	O
the	O
most.	O
Training	O
is	O
computationally	O
expensive,	O
limiting	O
the	O
amount	O
of	O
tuning	O
that	O
can	O
be	O
done,	O
and	O
is	O
often	O
done	O
with	O
private	O
training	O
data	O
of	O
varying	O
sizes,	O
limiting	O
our	O
ability	O
to	O
measure	O
the	O
effects	O
of	O
the	O
modeling	O
advances.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019),	O
which	O
includes	O
a	O
careful	O
evaluation	O
of	O
the	O
effects	O
of	O
hyperparmeter	O
tuning	O
and	O
training	O
set	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained	O
and	O
propose	O
an	O
improved	O
recipe	O
for	O
training	O
BERT	B-MethodName
models,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
that	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
all	O
of	O
the	O
post-BERT	O
methods.	O
Our	O
modifications	O
are	O
simple,	O
they	O
include:	O
(1)	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches,	O
over	O
more	O
data;	O
(2)	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
(3)	O
training	O
on	O
longer	O
sequences;	O
and	O
(4)	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
We	O
also	O
collect	O
a	O
large	O
new	O
dataset	O
(CC-NEWS)	B-DatasetName
of	O
comparable	O
size	O
to	O
other	O
privately	O
used	O
datasets,	O
to	O
better	O
control	O
for	O
training	O
set	O
size	O
effects.	O
When	O
controlling	O
for	O
training	O
data,	O
our	O
improved	O
training	O
procedure	O
improves	O
upon	O
the	O
published	O
BERT	B-MethodName
results	O
on	O
both	O
GLUE	B-DatasetName
and	O
SQuAD.	B-DatasetName
When	O
trained	O
for	O
longer	O
over	O
additional	O
data,	O
our	O
model	O
achieves	O
a	O
score	O
of	O
88.5	B-MetricValue
on	O
the	O
public	O
GLUE	B-DatasetName
leaderboard,	O
matching	O
the	O
88.4	B-MetricValue
reported	O
by	O
Yang	O
et	O
al.	O
(2019).	O
Our	O
model	O
establishes	O
a	O
new	O
state-of-the-art	O
on	O
4/9	O
of	O
the	O
GLUE	B-DatasetName
tasks:	O
MNLI,	B-DatasetName
QNLI,	B-DatasetName
RTE	B-DatasetName
and	O
STS-B.	B-DatasetName
We	O
also	O
match	O
state-of-the-art	O
results	O
on	O
SQuAD	B-DatasetName
and	O
RACE.	B-DatasetName
Overall,	O
we	O
re-establish	O
that	O
BERT's	B-MethodName
masked	O
language	O
model	O
training	O
objective	O
is	O
competitive	O
with	O
other	O
recently	O
proposed	O
training	O
objectives	O
such	O
as	O
perturbed	O
autoregressive	O
language	O
modeling	O
(Yang	O
et	O
al.,	O
2019).	O
2	O
In	O
summary,	O
the	O
contributions	O
of	O
this	O
paper	O
are:	O
(1)	O
We	O
present	O
a	O
set	O
of	O
important	O
BERT	B-MethodName
design	O
choices	O
and	O
training	O
strategies	O
and	O
introduce	O
alternatives	O
that	O
lead	O
to	O
better	O
downstream	O
task	O
performance;	O
(2)	O
We	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
confirm	O
that	O
using	O
more	O
data	O
for	O
pretraining	O
further	O
improves	O
performance	O
on	O
downstream	O
tasks;	O
(3)	O
Our	O
training	O
improvements	O
show	O
that	O
masked	O
language	O
model	O
pretraining,	O
under	O
the	O
right	O
design	O
choices,	O
is	O
competitive	O
with	O
all	O
other	O
recently	O
published	O
methods.	O
We	O
release	O
our	O
model,	O
pretraining	O
and	O
fine-tuning	O
code	O
implemented	O
in	O
PyTorch	O
(Paszke	O
et	O
al.,	O
2017).	O

Language	O
model	O
pretraining	O
has	O
led	O
to	O
significant	O
performance	O
gains	O
but	O
careful	O
comparison	O
between	O
different	O
approaches	O
is	O
challenging.	O
Training	O
is	O
computationally	O
expensive,	O
often	O
done	O
on	O
private	O
datasets	O
of	O
different	O
sizes,	O
and,	O
as	O
we	O
will	O
show,	O
hyperparameter	O
choices	O
have	O
significant	O
impact	O
on	O
the	O
final	O
results.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019)	O
that	O
carefully	O
measures	O
the	O
impact	O
of	O
many	O
key	O
hyperparameters	O
and	O
training	O
data	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained,	O
and	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
every	O
model	O
published	O
after	O
it.	O
Our	O
best	O
model	O
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD.	B-DatasetName
These	O
results	O
highlight	O
the	O
importance	O
of	O
previously	O
overlooked	O
design	O
choices,	O
and	O
raise	O
questions	O
about	O
the	O
source	O
of	O
recently	O
reported	O
improvements.	O
We	O
release	O
our	O
models	O
and	O
code.	O
1	O

In	O
Table	O
8	O
we	O
present	O
the	O
full	O
set	O
of	O
development	O
set	O
results	O
for	O
RoBERTa.	B-MethodName
We	O
present	O
results	O
for	O
a	O
LARGE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
as	O
well	O
as	O
a	O
BASE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
BASE	I-MethodName
.	O

Appendix	O
for	O
"RoBERTa:	B-MethodName
A	O
Robustly	O
Optimized	O
BERT	O
Pretraining	O
Approach"	O
Acknowledgements	O
We	O
are	O
grateful	O
to	O
Nal	O
Kalchbrenner	O
and	O
Stephan	O
Gouws	O
for	O
their	O
fruitful	O
comments,	O
corrections	O
and	O
inspiration.	O

In	O
this	O
work,	O
we	O
presented	O
the	O
Transformer,	B-MethodName
the	O
first	O
sequence	O
transduction	O
model	O
based	O
entirely	O
on	O
attention,	O
replacing	O
the	O
recurrent	O
layers	O
most	O
commonly	O
used	O
in	O
encoder-decoder	O
architectures	O
with	O
multi-headed	O
self-attention.	O
For	O
translation	O
tasks,	O
the	O
Transformer	B-MethodName
can	O
be	O
trained	O
significantly	O
faster	O
than	O
architectures	O
based	O
on	O
recurrent	O
or	O
convolutional	O
layers.	O
On	O
both	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
and	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
tasks,	O
we	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art.	O
In	O
the	O
former	O
task	O
our	O
best	O
model	O
outperforms	O
even	O
all	O
previously	O
reported	O
ensembles.	O
We	O
are	O
excited	O
about	O
the	O
future	O
of	O
attention-based	O
models	O
and	O
plan	O
to	O
apply	O
them	O
to	O
other	O
tasks.	O
We	O
plan	O
to	O
extend	O
the	O
Transformer	B-MethodName
to	O
problems	O
involving	O
input	O
and	O
output	O
modalities	O
other	O
than	O
text	O
and	O
to	O
investigate	O
local,	O
restricted	O
attention	O
mechanisms	O
to	O
efficiently	O
handle	O
large	O
inputs	O
and	O
outputs	O
such	O
as	O
images,	O
audio	O
and	O
video.	O
Making	O
generation	O
less	O
sequential	O
is	O
another	O
research	O
goals	O
of	O
ours.	O
The	O
code	O
we	O
used	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
is	O
available	O
at	O
tensorflow/tensor2tensor.	O

To	O
evaluate	O
if	O
the	O
Transformer	B-MethodName
can	O
generalize	O
to	O
other	O
tasks	O
we	O
performed	O
experiments	O
on	O
English	B-TaskName
constituency	I-TaskName
parsing.	I-TaskName
This	O
task	O
presents	O
specific	O
challenges:	O
the	O
output	O
is	O
subject	O
to	O
strong	O
structural	O
constraints	O
and	O
is	O
significantly	O
longer	O
than	O
the	O
input.	O
Furthermore,	O
RNN	O
sequence-to-sequence	O
models	O
have	O
not	O
been	O
able	O
to	O
attain	O
state-of-the-art	O
results	O
in	O
small-data	O
regimes	O
.	O
We	O
trained	O
a	O
4-layer	O
transformer	O
with	O
d	O
model	O
=	O
1024	O
on	O
the	O
Wall	B-DatasetName
Street	I-DatasetName
Journal	I-DatasetName
(WSJ)	I-DatasetName
portion	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
,	O
about	O
40K	O
training	O
sentences.	O
We	O
also	O
trained	O
it	O
in	O
a	O
semi-supervised	O
setting,	O
using	O
the	O
larger	O
high-confidence	O
and	O
BerkleyParser	B-DatasetName
corpora	I-DatasetName
from	O
with	O
approximately	O
17M	O
sentences	O
.	O
We	O
used	O
a	O
vocabulary	B-HyperparameterName
of	O
16K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
WSJ	O
only	O
setting	O
and	O
a	O
vocabulary	B-HyperparameterName
of	O
32K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
semi-supervised	O
setting.	O
We	O
performed	O
only	O
a	O
small	O
number	O
of	O
experiments	O
to	O
select	O
the	O
dropout,	B-HyperparameterName
both	O
attention	O
and	O
residual	O
(section	O
5.4),	O
learning	B-HyperparameterName
rates	I-HyperparameterName
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
on	O
the	O
Section	O
22	O
development	O
set,	O
all	O
other	O
parameters	O
remained	O
unchanged	O
from	O
the	O
English-to-German	O
base	O
translation	O
model.	O
During	O
inference,	O
we	O
increased	O
the	O
maximum	O
output	O
length	O
to	O
input	O
length	O
+	O
300.	O
We	O
used	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
21	B-HyperparameterValue
and	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
for	O
both	O
WSJ	O
only	O
and	O
the	O
semi-supervised	O
setting.	O
Our	O
results	O
in	O
Table	O
4	O
show	O
that	O
despite	O
the	O
lack	O
of	O
task-specific	O
tuning	O
our	O
model	O
performs	O
surprisingly	O
well,	O
yielding	O
better	O
results	O
than	O
all	O
previously	O
reported	O
models	O
with	O
the	O
exception	O
of	O
the	O
Recurrent	O
Neural	O
Network	O
Grammar	O
.	O
In	O
contrast	O
to	O
RNN	O
sequence-to-sequence	O
models	O
,	O
the	O
Transformer	B-MethodName
outperforms	O
the	O
Berkeley-Parser	B-MethodName
even	O
when	O
training	O
only	O
on	O
the	O
WSJ	B-DatasetName
training	O
set	O
of	O
40K	O
sentences.	O

To	O
evaluate	O
the	O
importance	O
of	O
different	O
components	O
of	O
the	O
Transformer,	B-MethodName
we	O
varied	O
our	O
base	O
model	O
in	O
different	O
ways,	O
measuring	O
the	O
change	O
in	O
performance	O
on	O
English-to-German	B-TaskName
translation	I-TaskName
on	O
the	O
development	O
set,	O
newstest2013.	B-DatasetName
We	O
used	O
beam	B-MethodName
search	I-MethodName
as	O
described	O
in	O
the	O
previous	O
section,	O
but	O
no	O
checkpoint	O
averaging.	O
We	O
present	O
these	O
results	O
in	O
Table	O
3.	O
In	O
Table	O
3	O
rows	O
(A),	O
we	O
vary	O
the	O
number	O
of	O
attention	O
heads	O
and	O
the	O
attention	O
key	O
and	O
value	O
dimensions,	O
keeping	O
the	O
amount	O
of	O
computation	O
constant,	O
as	O
described	O
in	O
Section	O
3.2.2.	O
While	O
single-head	O
attention	O
is	O
0.9	B-MetricValue
BLEU	B-MetricName
worse	O
than	O
the	O
best	O
setting,	O
quality	O
also	O
drops	O
off	O
with	O
too	O
many	O
heads.	O
In	O
Table	O
3	O
rows	O
(B),	O
we	O
observe	O
that	O
reducing	O
the	O
attention	O
key	O
size	O
d	O
k	O
hurts	O
model	O
quality.	O
This	O
suggests	O
that	O
determining	O
compatibility	O
is	O
not	O
easy	O
and	O
that	O
a	O
more	O
sophisticated	O
compatibility	O
function	O
than	O
dot	O
product	O
may	O
be	O
beneficial.	O
We	O
further	O
observe	O
in	O
rows	O
(C)	O
and	O
(D)	O
that,	O
as	O
expected,	O
bigger	O
models	O
are	O
better,	O
and	O
dropout	O
is	O
very	O
helpful	O
in	O
avoiding	O
over-fitting.	O
In	O
row	O
(E)	O
we	O
replace	O
our	O
sinusoidal	O
positional	O
encoding	O
with	O
learned	O
positional	O
embeddings	O
,	O
and	O
observe	O
nearly	O
identical	O
results	O
to	O
the	O
base	O
model.	O

On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
translation	O
task,	O
the	O
big	O
transformer	B-MethodName
model	O
(Transformer	B-MethodName
(big)	I-MethodName
in	O
Table	O
2)	O
outperforms	O
the	O
best	O
previously	O
reported	O
models	O
(including	O
ensembles)	O
by	O
more	O
than	O
2.0	B-MetricValue
BLEU,	B-MetricName
establishing	O
a	O
new	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
28.4.	B-MetricValue
The	O
configuration	O
of	O
this	O
model	O
is	O
listed	O
in	O
the	O
bottom	O
line	O
of	O
Table	O
3.	O
Training	O
took	O
3.5	O
days	O
on	O
8	O
P100	O
GPUs.	O
Even	O
our	O
base	O
model	O
surpasses	O
all	O
previously	O
published	O
models	O
and	O
ensembles,	O
at	O
a	O
fraction	O
of	O
the	O
training	O
cost	O
of	O
any	O
of	O
the	O
competitive	O
models.	O
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
big	O
model	O
achieves	O
a	O
BLEU	B-MetricName
score	O
of	O
41.0,	B-MetricValue
outperforming	O
all	O
of	O
the	O
previously	O
published	O
single	O
models,	O
at	O
less	O
than	O
1/4	O
the	O
training	O
cost	O
of	O
the	O
previous	O
state-of-the-art	O
model.	O
The	O
Transformer	O
(big)	O
model	O
trained	O
for	O
English-to-French	O
used	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
P	O
drop	B-HyperparameterName
=	O
0.1,	B-HyperparameterValue
instead	O
of	O
0.3.	B-HyperparameterValue
For	O
the	O
base	O
models,	O
we	O
used	O
a	O
single	O
model	O
obtained	O
by	O
averaging	O
the	O
last	O
5	O
checkpoints,	O
which	O
were	O
written	O
at	O
10-minute	O
intervals.	O
For	O
the	O
big	O
models,	O
we	O
averaged	O
the	O
last	O
20	O
checkpoints.	O
We	O
used	O
beam	B-MethodName
search	I-MethodName
with	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
4	B-HyperparameterValue
and	O
length	B-HyperparameterName
penalty	I-HyperparameterName
α	B-HyperparameterValue
=	I-HyperparameterValue
0.6	I-HyperparameterValue
.	O
These	O
hyperparameters	O
were	O
chosen	O
after	O
experimentation	O
on	O
the	O
development	O
set.	O
We	O
set	O
the	O
maximum	O
output	O
length	O
during	O
inference	O
to	O
input	O
length	O
+	O
50,	O
but	O
terminate	O
early	O
when	O
possible	O
.	O

We	O
employ	O
three	O
types	O
of	O
regularization	O
during	O
training:	O
Residual	O
Dropout	B-HyperparameterName
We	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
output	O
of	O
each	O
sub-layer,	O
before	O
it	O
is	O
added	O
to	O
the	O
sub-layer	O
input	O
and	O
normalized.	O
In	O
addition,	O
we	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
sums	O
of	O
the	O
embeddings	O
and	O
the	O
positional	O
encodings	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
For	O
the	O
base	O
model,	O
we	O
use	O
a	O
rate	O
of	O
P	O
drop	B-HyperparameterName
=	O
0.1.	B-HyperparameterValue
Label	B-HyperparameterName
Smoothing	I-HyperparameterName
During	O
training,	O
we	O
employed	O
label	O
smoothing	O
of	O
value	O
ls	B-HyperparameterName
=	O
0.1	B-HyperparameterValue
.	O
This	O
hurts	O
perplexity,	B-MetricName
as	O
the	O
model	O
learns	O
to	O
be	O
more	O
unsure,	O
but	O
improves	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
score.	O
6	B-MetricValue
Results	O

We	O
used	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
β	O
1	O
=	O
0.9,	O
β	O
2	O
=	O
0.98	O
and	O
=	O
10	O
−9	O
.	O
We	O
varied	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
over	O
the	O
course	O
of	O
training,	O
according	O
to	O
the	O
formula:	O
lrate	O
=	O
d	O
−0.5	O
model	O
•	O
min(step_num	O
−0.5	O
,	O
step_num	O
•	O
warmup_steps	O
−1.5	O
)(3)	O
This	O
corresponds	O
to	O
increasing	O
the	O
learning	O
rate	O
linearly	O
for	O
the	O
first	O
warmup_steps	O
training	O
steps,	O
and	O
decreasing	O
it	O
thereafter	O
proportionally	O
to	O
the	O
inverse	O
square	O
root	O
of	O
the	O
step	O
number.	O
We	O
used	O
warmup_steps	B-HyperparameterName
=	O
4000.	B-HyperparameterValue

We	O
trained	O
our	O
models	O
on	O
one	O
machine	O
with	O
8	O
NVIDIA	O
P100	O
GPUs.	O
For	O
our	O
base	O
models	O
using	O
the	O
hyperparameters	O
described	O
throughout	O
the	O
paper,	O
each	O
training	O
step	O
took	O
about	O
0.4	O
seconds.	O
We	O
trained	O
the	O
base	O
models	O
for	O
a	O
total	O
of	O
100,000	B-HyperparameterValue
steps	B-HyperparameterName
or	O
12	O
hours.	O
For	O
our	O
big	O
models,(described	O
on	O
the	O
bottom	O
line	O
of	O
table	O
3),	O
step	O
time	O
was	O
1.0	O
seconds.	O
The	O
big	O
models	O
were	O
trained	O
for	O
300,000	B-HyperparameterValue
steps	B-HyperparameterName
(3.5	O
days).	O

We	O
trained	O
on	O
the	O
standard	O
WMT	B-DatasetName
2014	I-DatasetName
English-German	I-DatasetName
dataset	O
consisting	O
of	O
about	O
4.5	O
million	O
sentence	O
pairs.	O
Sentences	O
were	O
encoded	O
using	O
byte-pair	O
encoding	O
,	O
which	O
has	O
a	O
shared	O
sourcetarget	O
vocabulary	O
of	O
about	O
37000	O
tokens.	O
For	O
English-French,	O
we	O
used	O
the	O
significantly	O
larger	O
WMT	B-DatasetName
2014	I-DatasetName
English-French	I-DatasetName
dataset	O
consisting	O
of	O
36M	O
sentences	O
and	O
split	O
tokens	O
into	O
a	O
32000	O
word-piece	O
vocabulary	O
.	O
Sentence	O
pairs	O
were	O
batched	O
together	O
by	O
approximate	O
sequence	O
length.	O
Each	O
training	O
batch	O
contained	O
a	O
set	O
of	O
sentence	O
pairs	O
containing	O
approximately	O
25000	O
source	O
tokens	O
and	O
25000	O
target	O
tokens.	O

This	O
section	O
describes	O
the	O
training	O
regime	O
for	O
our	O
models.	O

In	O
this	O
section	O
we	O
compare	O
various	O
aspects	O
of	O
self-attention	O
layers	O
to	O
the	O
recurrent	O
and	O
convolutional	O
layers	O
commonly	O
used	O
for	O
mapping	O
one	O
variable-length	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
another	O
sequence	O
of	O
equal	O
length	O
(z	O
1	O
,	O
...,	O
z	O
n	O
),	O
with	O
x	O
i	O
,	O
z	O
i	O
∈	O
R	O
d	O
,	O
such	O
as	O
a	O
hidden	O
layer	O
in	O
a	O
typical	O
sequence	O
transduction	O
encoder	O
or	O
decoder.	O
Motivating	O
our	O
use	O
of	O
self-attention	O
we	O
consider	O
three	O
desiderata.	O
One	O
is	O
the	O
total	O
computational	O
complexity	O
per	O
layer.	O
Another	O
is	O
the	O
amount	O
of	O
computation	O
that	O
can	O
be	O
parallelized,	O
as	O
measured	O
by	O
the	O
minimum	O
number	O
of	O
sequential	O
operations	O
required.	O
The	O
third	O
is	O
the	O
path	O
length	O
between	O
long-range	O
dependencies	O
in	O
the	O
network.	O
Learning	O
long-range	O
dependencies	O
is	O
a	O
key	O
challenge	O
in	O
many	O
sequence	O
transduction	O
tasks.	O
One	O
key	O
factor	O
affecting	O
the	O
ability	O
to	O
learn	O
such	O
dependencies	O
is	O
the	O
length	O
of	O
the	O
paths	O
forward	O
and	O
backward	O
signals	O
have	O
to	O
traverse	O
in	O
the	O
network.	O
The	O
shorter	O
these	O
paths	O
between	O
any	O
combination	O
of	O
positions	O
in	O
the	O
input	O
and	O
output	O
sequences,	O
the	O
easier	O
it	O
is	O
to	O
learn	O
long-range	O
dependencies	O
.	O
Hence	O
we	O
also	O
compare	O
the	O
maximum	O
path	O
length	O
between	O
any	O
two	O
input	O
and	O
output	O
positions	O
in	O
networks	O
composed	O
of	O
the	O
different	O
layer	O
types.	O
As	O
noted	O
in	O
Table	O
1,	O
a	O
self-attention	O
layer	O
connects	O
all	O
positions	O
with	O
a	O
constant	O
number	O
of	O
sequentially	O
executed	O
operations,	O
whereas	O
a	O
recurrent	O
layer	O
requires	O
O(n)	O
sequential	O
operations.	O
In	O
terms	O
of	O
computational	O
complexity,	O
self-attention	O
layers	O
are	O
faster	O
than	O
recurrent	O
layers	O
when	O
the	O
sequence	O
length	O
n	O
is	O
smaller	O
than	O
the	O
representation	O
dimensionality	O
d,	O
which	O
is	O
most	O
often	O
the	O
case	O
with	O
sentence	O
representations	O
used	O
by	O
state-of-the-art	O
models	O
in	O
machine	O
translations,	O
such	O
as	O
word-piece	O
and	O
byte-pair	O
representations.	O
To	O
improve	O
computational	O
performance	O
for	O
tasks	O
involving	O
very	O
long	O
sequences,	O
self-attention	O
could	O
be	O
restricted	O
to	O
considering	O
only	O
a	O
neighborhood	O
of	O
size	O
r	O
in	O
the	O
input	O
sequence	O
centered	O
around	O
the	O
respective	O
output	O
position.	O
This	O
would	O
increase	O
the	O
maximum	O
path	O
length	O
to	O
O(n/r).	O
We	O
plan	O
to	O
investigate	O
this	O
approach	O
further	O
in	O
future	O
work.	O
A	O
single	O
convolutional	O
layer	O
with	O
kernel	O
width	O
k	O
<	O
n	O
does	O
not	O
connect	O
all	O
pairs	O
of	O
input	O
and	O
output	O
positions.	O
Doing	O
so	O
requires	O
a	O
stack	O
of	O
O(n/k)	O
convolutional	O
layers	O
in	O
the	O
case	O
of	O
contiguous	O
kernels,	O
or	O
O(log	O
k	O
(n))	O
in	O
the	O
case	O
of	O
dilated	O
convolutions	O
,	O
increasing	O
the	O
length	O
of	O
the	O
longest	O
paths	O
between	O
any	O
two	O
positions	O
in	O
the	O
network.	O
Convolutional	O
layers	O
are	O
generally	O
more	O
expensive	O
than	O
recurrent	O
layers,	O
by	O
a	O
factor	O
of	O
k.	O
Separable	O
convolutions	O
,	O
however,	O
decrease	O
the	O
complexity	O
considerably,	O
to	O
O(k	O
•	O
n	O
•	O
d	O
+	O
n	O
•	O
d	O
2	O
)	O
.	O
Even	O
with	O
k	O
=	O
n,	O
however,	O
the	O
complexity	O
of	O
a	O
separable	O
convolution	O
is	O
equal	O
to	O
the	O
combination	O
of	O
a	O
self-attention	O
layer	O
and	O
a	O
point-wise	O
feed-forward	O
layer,	O
the	O
approach	O
we	O
take	O
in	O
our	O
model.	O
As	O
side	O
benefit,	O
self-attention	O
could	O
yield	O
more	O
interpretable	O
models.	O
We	O
inspect	O
attention	O
distributions	O
from	O
our	O
models	O
and	O
present	O
and	O
discuss	O
examples	O
in	O
the	O
appendix.	O
Not	O
only	O
do	O
individual	O
attention	O
heads	O
clearly	O
learn	O
to	O
perform	O
different	O
tasks,	O
many	O
appear	O
to	O
exhibit	O
behavior	O
related	O
to	O
the	O
syntactic	O
and	O
semantic	O
structure	O
of	O
the	O
sentences.	O

Since	O
our	O
model	O
contains	O
no	O
recurrence	O
and	O
no	O
convolution,	O
in	O
order	O
for	O
the	O
model	O
to	O
make	O
use	O
of	O
the	O
order	O
of	O
the	O
sequence,	O
we	O
must	O
inject	O
some	O
information	O
about	O
the	O
relative	O
or	O
absolute	O
position	O
of	O
the	O
O(n	O
2	O
•	O
d)	O
O(1)	O
O(1)	O
Recurrent	O
O(n	O
•	O
d	O
2	O
)	O
O(n)	O
O(n)	O
Convolutional	O
O(k	O
•	O
n	O
•	O
d	O
2	O
)	O
O(1)	O
O(log	O
k	O
(n))	O
Self-Attention	O
(restricted)	O
O(r	O
•	O
n	O
•	O
d)	O
O(1)	O
O(n/r)	O
tokens	O
in	O
the	O
sequence.	O
To	O
this	O
end,	O
we	O
add	O
"positional	O
encodings"	O
to	O
the	O
input	O
embeddings	O
at	O
the	O
bottoms	O
of	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
The	O
positional	O
encodings	O
have	O
the	O
same	O
dimension	O
d	O
model	O
as	O
the	O
embeddings,	O
so	O
that	O
the	O
two	O
can	O
be	O
summed.	O
There	O
are	O
many	O
choices	O
of	O
positional	O
encodings,	O
learned	O
and	O
fixed	O
.	O
In	O
this	O
work,	O
we	O
use	O
sine	O
and	O
cosine	O
functions	O
of	O
different	O
frequencies:	O
P	O
E	O
(pos,2i)	O
=	O
sin(pos/10000	O
2i/dmodel	O
)	O
P	O
E	O
(pos,2i+1)	O
=	O
cos(pos/10000	O
2i/dmodel	O
)	O
where	O
pos	O
is	O
the	O
position	O
and	O
i	O
is	O
the	O
dimension.	O
That	O
is,	O
each	O
dimension	O
of	O
the	O
positional	O
encoding	O
corresponds	O
to	O
a	O
sinusoid.	O
The	O
wavelengths	O
form	O
a	O
geometric	O
progression	O
from	O
2π	O
to	O
10000	O
•	O
2π.	O
We	O
chose	O
this	O
function	O
because	O
we	O
hypothesized	O
it	O
would	O
allow	O
the	O
model	O
to	O
easily	O
learn	O
to	O
attend	O
by	O
relative	O
positions,	O
since	O
for	O
any	O
fixed	O
offset	O
k,	O
P	O
E	O
pos+k	O
can	O
be	O
represented	O
as	O
a	O
linear	O
function	O
of	O
P	O
E	O
pos	O
.	O
We	O
also	O
experimented	O
with	O
using	O
learned	O
positional	O
embeddings	O
instead,	O
and	O
found	O
that	O
the	O
two	O
versions	O
produced	O
nearly	O
identical	O
results	O
(see	O
Table	O
3	O
row	O
(E)).	O
We	O
chose	O
the	O
sinusoidal	O
version	O
because	O
it	O
may	O
allow	O
the	O
model	O
to	O
extrapolate	O
to	O
sequence	O
lengths	O
longer	O
than	O
the	O
ones	O
encountered	O
during	O
training.	O

Similarly	O
to	O
other	O
sequence	O
transduction	O
models,	O
we	O
use	O
learned	O
embeddings	O
to	O
convert	O
the	O
input	O
tokens	O
and	O
output	O
tokens	O
to	O
vectors	O
of	O
dimension	O
d	O
model	O
.	O
We	O
also	O
use	O
the	O
usual	O
learned	O
linear	O
transformation	O
and	O
softmax	O
function	O
to	O
convert	O
the	O
decoder	O
output	O
to	O
predicted	O
next-token	O
probabilities.	O
In	O
our	O
model,	O
we	O
share	O
the	O
same	O
weight	O
matrix	O
between	O
the	O
two	O
embedding	O
layers	O
and	O
the	O
pre-softmax	O
linear	O
transformation,	O
similar	O
to	O
.	O
In	O
the	O
embedding	O
layers,	O
we	O
multiply	O
those	O
weights	O
by	O
√	O
d	O
model	O
.	O

In	O
addition	O
to	O
attention	O
sub-layers,	O
each	O
of	O
the	O
layers	O
in	O
our	O
encoder	O
and	O
decoder	O
contains	O
a	O
fully	O
connected	O
feed-forward	O
network,	O
which	O
is	O
applied	O
to	O
each	O
position	O
separately	O
and	O
identically.	O
This	O
consists	O
of	O
two	O
linear	O
transformations	O
with	O
a	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
in	O
between.	O
FFN(x)	O
=	O
max(0,	O
xW	O
1	O
+	O
b	O
1	O
)W	O
2	O
+	O
b	O
2	O
(2)	O
While	O
the	O
linear	O
transformations	O
are	O
the	O
same	O
across	O
different	O
positions,	O
they	O
use	O
different	O
parameters	O
from	O
layer	O
to	O
layer.	O
Another	O
way	O
of	O
describing	O
this	O
is	O
as	O
two	O
convolutions	O
with	O
kernel	B-HyperparameterName
size	I-HyperparameterName
1.	B-HyperparameterValue
The	O
dimensionality	O
of	O
input	O
and	O
output	O
is	O
d	O
model	O
=	O
512,	O
and	O
the	O
inner-layer	O
has	O
dimensionality	O
d	O
f	O
f	O
=	O
2048.	O

The	O
Transformer	B-MethodName
uses	O
multi-head	O
attention	O
in	O
three	O
different	O
ways:	O
•	O
In	O
"encoder-decoder	O
attention"	O
layers,	O
the	O
queries	O
come	O
from	O
the	O
previous	O
decoder	O
layer,	O
and	O
the	O
memory	O
keys	O
and	O
values	O
come	O
from	O
the	O
output	O
of	O
the	O
encoder.	O
This	O
allows	O
every	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
over	O
all	O
positions	O
in	O
the	O
input	O
sequence.	O
This	O
mimics	O
the	O
typical	O
encoder-decoder	O
attention	O
mechanisms	O
in	O
sequence-to-sequence	O
models	O
such	O
as	O
.	O
•	O
The	O
encoder	O
contains	O
self-attention	O
layers.	O
In	O
a	O
self-attention	O
layer	O
all	O
of	O
the	O
keys,	O
values	O
and	O
queries	O
come	O
from	O
the	O
same	O
place,	O
in	O
this	O
case,	O
the	O
output	O
of	O
the	O
previous	O
layer	O
in	O
the	O
encoder.	O
Each	O
position	O
in	O
the	O
encoder	O
can	O
attend	O
to	O
all	O
positions	O
in	O
the	O
previous	O
layer	O
of	O
the	O
encoder.	O
•	O
Similarly,	O
self-attention	O
layers	O
in	O
the	O
decoder	O
allow	O
each	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
to	O
all	O
positions	O
in	O
the	O
decoder	O
up	O
to	O
and	O
including	O
that	O
position.	O
We	O
need	O
to	O
prevent	O
leftward	O
information	O
flow	O
in	O
the	O
decoder	O
to	O
preserve	O
the	O
auto-regressive	O
property.	O
We	O
implement	O
this	O
inside	O
of	O
scaled	O
dot-product	O
attention	O
by	O
masking	O
out	O
(setting	O
to	O
−∞)	O
all	O
values	O
in	O
the	O
input	O
of	O
the	O
softmax	O
which	O
correspond	O
to	O
illegal	O
connections.	O
See	O
Figure	O
2.	O

Instead	O
of	O
performing	O
a	O
single	O
attention	O
function	O
with	O
d	O
model	O
-dimensional	O
keys,	O
values	O
and	O
queries,	O
we	O
found	O
it	O
beneficial	O
to	O
linearly	O
project	O
the	O
queries,	O
keys	O
and	O
values	O
h	O
times	O
with	O
different,	O
learned	O
linear	O
projections	O
to	O
d	O
k	O
,	O
d	O
k	O
and	O
d	O
v	O
dimensions,	O
respectively.	O
On	O
each	O
of	O
these	O
projected	O
versions	O
of	O
queries,	O
keys	O
and	O
values	O
we	O
then	O
perform	O
the	O
attention	O
function	O
in	O
parallel,	O
yielding	O
d	O
v	O
-dimensional	O
output	O
values.	O
These	O
are	O
concatenated	O
and	O
once	O
again	O
projected,	O
resulting	O
in	O
the	O
final	O
values,	O
as	O
depicted	O
in	O
Figure	O
2.	O
Multi-head	O
attention	O
allows	O
the	O
model	O
to	O
jointly	O
attend	O
to	O
information	O
from	O
different	O
representation	O
subspaces	O
at	O
different	O
positions.	O
With	O
a	O
single	O
attention	O
head,	O
averaging	O
inhibits	O
this.	O
MultiHead(Q,	O
K,	O
V	O
)	O
=	O
Concat(head	O
1	O
,	O
...,	O
head	O
h	O
)W	O
O	O
where	O
head	O
i	O
=	O
Attention(QW	O
Q	O
i	O
,	O
KW	O
K	O
i	O
,	O
V	O
W	O
V	O
i	O
)	O
Where	O
the	O
projections	O
are	O
parameter	O
matrices	O
W	O
Q	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
K	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
V	O
i	O
∈	O
R	O
dmodel×dv	O
and	O
W	O
O	O
∈	O
R	O
hdv×dmodel	O
.	O
In	O
this	O
work	O
we	O
employ	O
h	O
=	O
8	O
parallel	O
attention	O
layers,	O
or	O
heads.	O
For	O
each	O
of	O
these	O
we	O
use	O
d	O
k	O
=	O
d	O
v	O
=	O
d	O
model	O
/h	O
=	O
64.	O
Due	O
to	O
the	O
reduced	O
dimension	O
of	O
each	O
head,	O
the	O
total	O
computational	O
cost	O
is	O
similar	O
to	O
that	O
of	O
single-head	O
attention	O
with	O
full	O
dimensionality.	O

We	O
call	O
our	O
particular	O
attention	O
"Scaled	O
Dot-Product	O
Attention"	O
(Figure	O
2).	O
The	O
input	O
consists	O
of	O
queries	O
and	O
keys	O
of	O
dimension	O
d	O
k	O
,	O
and	O
values	O
of	O
dimension	O
d	O
v	O
.	O
We	O
compute	O
the	O
dot	O
products	O
of	O
the	O
query	O
with	O
all	O
keys,	O
divide	O
each	O
by	O
√	O
d	O
k	O
,	O
and	O
apply	O
a	O
softmax	O
function	O
to	O
obtain	O
the	O
weights	O
on	O
the	O
values.	O
In	O
practice,	O
we	O
compute	O
the	O
attention	O
function	O
on	O
a	O
set	O
of	O
queries	O
simultaneously,	O
packed	O
together	O
into	O
a	O
matrix	O
Q.	O
The	O
keys	O
and	O
values	O
are	O
also	O
packed	O
together	O
into	O
matrices	O
K	O
and	O
V	O
.	O
We	O
compute	O
the	O
matrix	O
of	O
outputs	O
as:	O
Attention(Q,	O
K,	O
V	O
)	O
=	O
softmax(	O
QK	O
T	O
√	O
d	O
k	O
)V	O
(1)	O
The	O
two	O
most	O
commonly	O
used	O
attention	O
functions	O
are	O
additive	O
attention	O
,	O
and	O
dot-product	O
(multiplicative)	O
attention.	O
Dot-product	O
attention	O
is	O
identical	O
to	O
our	O
algorithm,	O
except	O
for	O
the	O
scaling	O
factor	O
of	O
1	O
√	O
d	O
k	O
.	O
Additive	O
attention	O
computes	O
the	O
compatibility	O
function	O
using	O
a	O
feed-forward	O
network	O
with	O
a	O
single	O
hidden	O
layer.	O
While	O
the	O
two	O
are	O
similar	O
in	O
theoretical	O
complexity,	O
dot-product	O
attention	O
is	O
much	O
faster	O
and	O
more	O
space-efficient	O
in	O
practice,	O
since	O
it	O
can	O
be	O
implemented	O
using	O
highly	O
optimized	O
matrix	O
multiplication	O
code.	O
While	O
for	O
small	O
values	O
of	O
d	O
k	O
the	O
two	O
mechanisms	O
perform	O
similarly,	O
additive	O
attention	O
outperforms	O
dot	O
product	O
attention	O
without	O
scaling	O
for	O
larger	O
values	O
of	O
d	O
k	O
.	O
We	O
suspect	O
that	O
for	O
large	O
values	O
of	O
d	O
k	O
,	O
the	O
dot	O
products	O
grow	O
large	O
in	O
magnitude,	O
pushing	O
the	O
softmax	O
function	O
into	O
regions	O
where	O
it	O
has	O
extremely	O
small	O
gradients	O
4	O
.	O
To	O
counteract	O
this	O
effect,	O
we	O
scale	O
the	O
dot	O
products	O
by	O
1	O
√	O
d	O
k	O
.	O

An	O
attention	O
function	O
can	O
be	O
described	O
as	O
mapping	O
a	O
query	O
and	O
a	O
set	O
of	O
key-value	O
pairs	O
to	O
an	O
output,	O
where	O
the	O
query,	O
keys,	O
values,	O
and	O
output	O
are	O
all	O
vectors.	O
The	O
output	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
values,	O
where	O
the	O
weight	O
assigned	O
to	O
each	O
value	O
is	O
computed	O
by	O
a	O
compatibility	O
function	O
of	O
the	O
query	O
with	O
the	O
corresponding	O
key.	O

Encoder:	O
The	O
encoder	O
is	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
Each	O
layer	O
has	O
two	O
sub-layers.	O
The	O
first	O
is	O
a	O
multi-head	O
self-attention	O
mechanism,	O
and	O
the	O
second	O
is	O
a	O
simple,	O
positionwise	O
fully	O
connected	O
feed-forward	O
network.	O
We	O
employ	O
a	O
residual	O
connection	O
around	O
each	O
of	O
the	O
two	O
sub-layers,	O
followed	O
by	O
layer	O
normalization	O
.	O
That	O
is,	O
the	O
output	O
of	O
each	O
sub-layer	O
is	O
LayerNorm(x	O
+	O
Sublayer(x)),	O
where	O
Sublayer(x)	O
is	O
the	O
function	O
implemented	O
by	O
the	O
sub-layer	O
itself.	O
To	O
facilitate	O
these	O
residual	O
connections,	O
all	O
sub-layers	O
in	O
the	O
model,	O
as	O
well	O
as	O
the	O
embedding	O
layers,	O
produce	O
outputs	O
of	O
dimension	O
d	O
model	O
=	O
512.	O
Decoder:	O
The	O
decoder	O
is	O
also	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
In	O
addition	O
to	O
the	O
two	O
sub-layers	O
in	O
each	O
encoder	O
layer,	O
the	O
decoder	O
inserts	O
a	O
third	O
sub-layer,	O
which	O
performs	O
multi-head	O
attention	O
over	O
the	O
output	O
of	O
the	O
encoder	O
stack.	O
Similar	O
to	O
the	O
encoder,	O
we	O
employ	O
residual	O
connections	O
around	O
each	O
of	O
the	O
sub-layers,	O
followed	O
by	O
layer	O
normalization.	O
We	O
also	O
modify	O
the	O
self-attention	O
sub-layer	O
in	O
the	O
decoder	O
stack	O
to	O
prevent	O
positions	O
from	O
attending	O
to	O
subsequent	O
positions.	O
This	O
masking,	O
combined	O
with	O
fact	O
that	O
the	O
output	O
embeddings	O
are	O
offset	O
by	O
one	O
position,	O
ensures	O
that	O
the	O
predictions	O
for	O
position	O
i	O
can	O
depend	O
only	O
on	O
the	O
known	O
outputs	O
at	O
positions	O
less	O
than	O
i.	O

Most	O
competitive	O
neural	O
sequence	O
transduction	O
models	O
have	O
an	O
encoder-decoder	O
structure	O
.	O
Here,	O
the	O
encoder	O
maps	O
an	O
input	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
a	O
sequence	O
of	O
continuous	O
representations	O
z	O
=	O
(z	O
1	O
,	O
...,	O
z	O
n	O
).	O
Given	O
z,	O
the	O
decoder	O
then	O
generates	O
an	O
output	O
sequence	O
(y	O
1	O
,	O
...,	O
y	O
m	O
)	O
of	O
symbols	O
one	O
element	O
at	O
a	O
time.	O
At	O
each	O
step	O
the	O
model	O
is	O
auto-regressive	O
,	O
consuming	O
the	O
previously	O
generated	O
symbols	O
as	O
additional	O
input	O
when	O
generating	O
the	O
next.	O
The	O
Transformer	B-MethodName
follows	O
this	O
overall	O
architecture	O
using	O
stacked	O
self-attention	O
and	O
point-wise,	O
fully	O
connected	O
layers	O
for	O
both	O
the	O
encoder	O
and	O
decoder,	O
shown	O
in	O
the	O
left	O
and	O
right	O
halves	O
of	O
Figure	O
1,	O
respectively.	O
Figure	O
1:	O
The	O
Transformer	B-MethodName
-model	O
architecture.	O

The	O
goal	O
of	O
reducing	O
sequential	O
computation	O
also	O
forms	O
the	O
foundation	O
of	O
the	O
Extended	O
Neural	O
GPU	O
,	O
ByteNet	B-MethodName
and	O
ConvS2S	B-MethodName
,	O
all	O
of	O
which	O
use	O
convolutional	O
neural	O
networks	O
as	O
basic	O
building	O
block,	O
computing	O
hidden	O
representations	O
in	O
parallel	O
for	O
all	O
input	O
and	O
output	O
positions.	O
In	O
these	O
models,	O
the	O
number	O
of	O
operations	O
required	O
to	O
relate	O
signals	O
from	O
two	O
arbitrary	O
input	O
or	O
output	O
positions	O
grows	O
in	O
the	O
distance	O
between	O
positions,	O
linearly	O
for	O
ConvS2S	B-MethodName
and	O
logarithmically	O
for	O
ByteNet.	B-MethodName
This	O
makes	O
it	O
more	O
difficult	O
to	O
learn	O
dependencies	O
between	O
distant	O
positions	O
.	O
In	O
the	O
Transformer	B-MethodName
this	O
is	O
reduced	O
to	O
a	O
constant	O
number	O
of	O
operations,	O
albeit	O
at	O
the	O
cost	O
of	O
reduced	O
effective	O
resolution	O
due	O
to	O
averaging	O
attention-weighted	O
positions,	O
an	O
effect	O
we	O
counteract	O
with	O
Multi-Head	O
Attention	O
as	O
described	O
in	O
section	O
3.2.	O
Self-attention,	O
sometimes	O
called	O
intra-attention	O
is	O
an	O
attention	O
mechanism	O
relating	O
different	O
positions	O
of	O
a	O
single	O
sequence	O
in	O
order	O
to	O
compute	O
a	O
representation	O
of	O
the	O
sequence.	O
Self-attention	O
has	O
been	O
used	O
successfully	O
in	O
a	O
variety	O
of	O
tasks	O
including	O
reading	O
comprehension,	O
abstractive	O
summarization,	O
textual	O
entailment	O
and	O
learning	O
task-independent	O
sentence	O
representations	O
.	O
End-to-end	O
memory	O
networks	O
are	O
based	O
on	O
a	O
recurrent	O
attention	O
mechanism	O
instead	O
of	O
sequencealigned	O
recurrence	O
and	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
simple-language	B-TaskName
question	I-TaskName
answering	I-TaskName
and	O
language	B-TaskName
modeling	I-TaskName
tasks	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge,	O
however,	O
the	O
Transformer	B-MethodName
is	O
the	O
first	O
transduction	O
model	O
relying	O
entirely	O
on	O
self-attention	O
to	O
compute	O
representations	O
of	O
its	O
input	O
and	O
output	O
without	O
using	O
sequencealigned	O
RNNs	O
or	O
convolution.	O
In	O
the	O
following	O
sections,	O
we	O
will	O
describe	O
the	O
Transformer,	B-MethodName
motivate	O
self-attention	O
and	O
discuss	O
its	O
advantages	O
over	O
models	O
such	O
as	O
and	O
.	O

Recurrent	O
neural	O
networks,	O
long	O
short-term	O
memory	O
and	O
gated	O
recurrent	O
neural	O
networks	O
in	O
particular,	O
have	O
been	O
firmly	O
established	O
as	O
state	O
of	O
the	O
art	O
approaches	O
in	O
sequence	O
modeling	O
and	O
transduction	O
problems	O
such	O
as	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
Numerous	O
efforts	O
have	O
since	O
continued	O
to	O
push	O
the	O
boundaries	O
of	O
recurrent	O
language	O
models	O
and	O
encoder-decoder	O
architectures	O
.	O
Recurrent	O
models	O
typically	O
factor	O
computation	O
along	O
the	O
symbol	O
positions	O
of	O
the	O
input	O
and	O
output	O
sequences.	O
Aligning	O
the	O
positions	O
to	O
steps	O
in	O
computation	O
time,	O
they	O
generate	O
a	O
sequence	O
of	O
hidden	O
states	O
h	O
t	O
,	O
as	O
a	O
function	O
of	O
the	O
previous	O
hidden	O
state	O
h	O
t−1	O
and	O
the	O
input	O
for	O
position	O
t.	O
This	O
inherently	O
sequential	O
nature	O
precludes	O
parallelization	O
within	O
training	O
examples,	O
which	O
becomes	O
critical	O
at	O
longer	O
sequence	O
lengths,	O
as	O
memory	O
constraints	O
limit	O
batching	O
across	O
examples.	O
Recent	O
work	O
has	O
achieved	O
significant	O
improvements	O
in	O
computational	O
efficiency	O
through	O
factorization	O
tricks	O
and	O
conditional	O
computation	O
,	O
while	O
also	O
improving	O
model	O
performance	O
in	O
case	O
of	O
the	O
latter.	O
The	O
fundamental	O
constraint	O
of	O
sequential	O
computation,	O
however,	O
remains.	O
Attention	O
mechanisms	O
have	O
become	O
an	O
integral	O
part	O
of	O
compelling	O
sequence	O
modeling	O
and	O
transduction	O
models	O
in	O
various	O
tasks,	O
allowing	O
modeling	O
of	O
dependencies	O
without	O
regard	O
to	O
their	O
distance	O
in	O
the	O
input	O
or	O
output	O
sequences	O
.	O
In	O
all	O
but	O
a	O
few	O
cases	O
,	O
however,	O
such	O
attention	O
mechanisms	O
are	O
used	O
in	O
conjunction	O
with	O
a	O
recurrent	O
network.	O
In	O
this	O
work	O
we	O
propose	O
the	O
Transformer,	B-MethodName
a	O
model	O
architecture	O
eschewing	O
recurrence	O
and	O
instead	O
relying	O
entirely	O
on	O
an	O
attention	O
mechanism	O
to	O
draw	O
global	O
dependencies	O
between	O
input	O
and	O
output.	O
The	O
Transformer	B-MethodName
allows	O
for	O
significantly	O
more	O
parallelization	O
and	O
can	O
reach	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
translation	O
quality	O
after	O
being	O
trained	O
for	O
as	O
little	O
as	O
twelve	O
hours	O
on	O
eight	O
P100	O
GPUs.	O

The	O
dominant	O
sequence	O
transduction	O
models	O
are	O
based	O
on	O
complex	O
recurrent	O
or	O
convolutional	O
neural	O
networks	O
that	O
include	O
an	O
encoder	O
and	O
a	O
decoder.	O
The	O
best	O
performing	O
models	O
also	O
connect	O
the	O
encoder	O
and	O
decoder	O
through	O
an	O
attention	O
mechanism.	O
We	O
propose	O
a	O
new	O
simple	O
network	O
architecture,	O
the	O
Transformer,	B-MethodName
based	O
solely	O
on	O
attention	O
mechanisms,	O
dispensing	O
with	O
recurrence	O
and	O
convolutions	O
entirely.	O
Experiments	O
on	O
two	O
machine	B-TaskName
translation	I-TaskName
tasks	O
show	O
these	O
models	O
to	O
be	O
superior	O
in	O
quality	O
while	O
being	O
more	O
parallelizable	O
and	O
requiring	O
significantly	O
less	O
time	O
to	O
train.	O
Our	O
model	O
achieves	O
28.4	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
Englishto-German	I-DatasetName
translation	O
task,	O
improving	O
over	O
the	O
existing	O
best	O
results,	O
including	O
ensembles,	O
by	O
over	O
2	B-MetricValue
BLEU.	B-MetricName
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
model	O
establishes	O
a	O
new	O
single-model	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
41.8	B-MetricValue
after	O
training	O
for	O
3.5	O
days	O
on	O
eight	O
GPUs,	O
a	O
small	O
fraction	O
of	O
the	O
training	O
costs	O
of	O
the	O
best	O
models	O
from	O
the	O
literature.	O
We	O
show	O
that	O
the	O
Transformer	B-MethodName
generalizes	O
well	O
to	O
other	O
tasks	O
by	O
applying	O
it	O
successfully	O
to	O
English	B-TaskName
constituency	I-TaskName
parsing	I-TaskName
both	O
with	O
large	O
and	O
limited	O
training	O
data.	O
*	O
Equal	O
contribution.	O
Listing	O
order	O
is	O
random.	O
Jakob	O
proposed	O
replacing	O
RNNs	O
with	O
self-attention	O
and	O
started	O
the	O
effort	O
to	O
evaluate	O
this	O
idea.	O
Ashish,	O
with	O
Illia,	O
designed	O
and	O
implemented	O
the	O
first	O
Transformer	B-MethodName
models	O
and	O
has	O
been	O
crucially	O
involved	O
in	O
every	O
aspect	O
of	O
this	O
work.	O
Noam	O
proposed	O
scaled	O
dot-product	O
attention,	O
multi-head	O
attention	O
and	O
the	O
parameter-free	O
position	O
representation	O
and	O
became	O
the	O
other	O
person	O
involved	O
in	O
nearly	O
every	O
detail.	O
Niki	O
designed,	O
implemented,	O
tuned	O
and	O
evaluated	O
countless	O
model	O
variants	O
in	O
our	O
original	O
codebase	O
and	O
tensor2tensor.	O
Llion	O
also	O
experimented	O
with	O
novel	O
model	O
variants,	O
was	O
responsible	O
for	O
our	O
initial	O
codebase,	O
and	O
efficient	O
inference	O
and	O
visualizations.	O
Lukasz	O
and	O
Aidan	O
spent	O
countless	O
long	O
days	O
designing	O
various	O
parts	O
of	O
and	O
implementing	O
tensor2tensor,	O
replacing	O
our	O
earlier	O
codebase,	O
greatly	O
improving	O
results	O
and	O
massively	O
accelerating	O
our	O
research.	O
†	O
Work	O
performed	O
while	O
at	O
Google	O
Brain.	O
‡	O
Work	O
performed	O
while	O
at	O
Google	O
Research.	O

We	O
also	O
evaluated	O
performance	O
on	O
WMT16	B-DatasetName
Romanian-English,	I-DatasetName
augmented	O
with	O
back-translation	O
data	O
from	O
Sennrich	O
et	O
al.	O
(2016).	O
We	O
use	O
a	O
6-layer	O
transformer	O
source	O
encoder	O
to	O
map	O
Romanian	O
into	O
a	O
representation	O
that	O
BART	B-MethodName
is	O
able	O
to	O
de-noise	O
into	O
English,	O
following	O
the	O
approach	O
introduced	O
in	O
§3.4.	O
Experiment	O
results	O
are	O
presented	O
in	O
Table	O
6.	O
We	O
compare	O
our	O
results	O
against	O
a	O
baseline	O
Transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017)	O
with	O
Transformerlarge	O
settings	O
(the	O
baseline	O
row).	O
We	O
show	O
the	O
performance	O
of	O
both	O
steps	O
of	O
our	O
model	O
in	O
the	O
fixed	O
BART	B-MethodName
and	O
tuned	O
BART	B-MethodName
rows.	O
For	O
each	O
row	O
we	O
experiment	O
on	O
the	O
original	O
WMT16	B-DatasetName
Romanian-English	I-DatasetName
augmented	O
with	O
back-translation	O
data.	O
We	O
use	O
a	O
beam	B-HyperparameterName
width	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
α	B-HyperparameterValue
=	I-HyperparameterValue
1.	I-HyperparameterValue
Preliminary	O
results	O
suggested	O
that	O
our	O
approach	O
was	O
less	O
effective	O
without	O
back-translation	O
data,	O
and	O
prone	O
to	O
overfitting-future	O
work	O
should	O
explore	O
additional	O
regularization	O
techniques.	O

We	O
also	O
experiment	O
with	O
several	O
text	O
generation	O
tasks.	O
BART	B-MethodName
is	O
fine-tuned	O
as	O
a	O
standard	O
sequence-to-sequence	O
model	O
from	O
the	O
input	O
to	O
the	O
output	O
text.	O
During	O
finetuning	O
we	O
use	O
a	O
label	O
smoothed	O
cross	O
entropy	O
loss	O
(Pereyra	O
et	O
al.,	O
2017),	O
with	O
the	O
smoothing	B-HyperparameterName
parameter	I-HyperparameterName
set	O
to	O
0.1.	B-HyperparameterValue
During	O
generation,	O
we	O
set	O
beam	B-HyperparameterName
size	I-HyperparameterName
as	O
5,	B-HyperparameterValue
remove	O
duplicated	O
trigrams	O
in	O
beam	O
search,	O
and	O
tuned	O
the	O
model	O
with	O
min-len,	O
max-len,	O
length	O
penalty	O
on	O
the	O
validation	O
set	O
(Fan	O
et	O
al.,	O
2017	O
Summarization	O
To	O
provide	O
a	O
comparison	O
with	O
the	O
state-of-the-art	O
in	O
summarization,	O
we	O
present	O
results	O
on	O
two	O
summarization	O
datasets,	O
CNN/DailyMail	B-DatasetName
and	O
XSum,	B-DatasetName
which	O
have	O
distinct	O
properties.	O
Summaries	O
in	O
the	O
CNN/DailyMail	B-DatasetName
tend	O
to	O
resemble	O
source	O
sentences.	O
Extractive	O
models	O
do	O
well	O
here,	O
and	O
even	O
the	O
baseline	O
of	O
the	O
first-three	O
source	O
sentences	O
is	O
highly	O
competitive.	O
Nevertheless,	O
BART	O
outperforms	O
all	O
existing	O
work.	O
In	O
contrast,	O
XSum	B-MethodName
is	O
highly	O
abstractive,	O
and	O
extractive	O
models	O
perform	O
poorly.	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work,	O
which	O
leverages	O
BERT,	B-MethodName
by	O
roughly	O
6.0	B-MetricValue
points	I-MetricValue
on	O
all	O
ROUGE	B-MetricName
metrics-representing	O
a	O
significant	O
advance	O
in	O
performance	O
on	O
this	O
problem.	O
Qualitatively,	O
sample	O
quality	O
is	O
high	O
(see	O
§6).	O
Dialogue	O
We	O
evaluate	O
dialogue	O
response	O
generation	O
on	O
CONVAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
in	O
which	O
agents	O
must	O
generate	O
responses	O
conditioned	O
on	O
both	O
the	O
previous	O
context	O
and	O
a	O
textually-specified	O
persona.	O
BART	B-MethodName
outperforms	O
previous	O
work	O
on	O
two	O
automated	O
metrics.	O
Abstractive	B-TaskName
QA	I-TaskName
We	O
use	O
the	O
recently	O
proposed	O
ELI5	B-DatasetName
dataset	O
to	O
test	O
the	O
model's	O
ability	O
to	O
generate	O
long	O
freeform	O
answers.	O
We	O
find	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work	O
by	O
1.2	B-MetricValue
ROUGE-L,	B-MetricName
but	O
the	O
dataset	O
remains	O
a	O
challenging,	O
because	O
answers	O
are	O
only	O
weakly	O
specified	O
by	O
the	O
question.	O

Table	O
2	O
compares	O
the	O
performance	O
of	O
BART	B-MethodName
with	O
several	O
recent	O
approaches	O
on	O
the	O
well-studied	O
SQuAD	B-TaskName
and	O
GLUE	B-TaskName
tasks	O
(Warstadt	O
et	O
al.,	O
2018;Socher	O
et	O
al.,	O
2013;Dolan	O
&	O
Brockett,	O
2005;Agirre	O
et	O
al.,	O
2007;Williams	O
et	O
al.,	O
2018;Dagan	O
et	O
al.,	O
2006;Levesque	O
et	O
al.,	O
2011).	O
The	O
most	O
directly	O
comparable	O
baseline	O
is	O
RoBERTa,	B-MethodName
which	O
was	O
pre-trained	O
with	O
the	O
same	O
resources,	O
but	O
a	O
different	O
objective.	O
Overall,	O
BART	B-MethodName
performs	O
similarly,	O
with	O
only	O
small	O
differences	O
between	O
the	O
models	O
on	O
most	O
tasks.	O
suggesting	O
that	O
BART's	B-MethodName
improvements	O
on	O
generation	O
tasks	O
do	O
not	O
come	O
at	O
the	O
expense	O
of	O
classification	O
performance.	O

We	O
pre-train	O
a	O
large	O
model	O
with	O
12	O
layers	O
in	O
each	O
of	O
the	O
encoder	O
and	O
decoder,	O
and	O
a	O
hidden	O
size	O
of	O
1024.	O
Following	O
RoBERTa	B-MethodName
,	O
we	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8000,	B-HyperparameterValue
and	O
train	O
the	O
model	O
for	O
500000	B-HyperparameterValue
steps.	B-HyperparameterName
Documents	O
are	O
tokenized	O
with	O
the	O
same	O
byte-pair	O
encoding	O
as	O
GPT-2	B-MethodName
(Radford	O
et	O
al.,	O
2019).	O
Based	O
on	O
the	O
results	O
in	O
Section	O
§4,	O
we	O
use	O
a	O
combination	O
of	O
text	O
infilling	O
and	O
sentence	O
permutation.	O
We	O
mask	O
30%	B-MetricValue
of	O
tokens	O
in	O
each	O
document,	O
and	O
permute	O
all	O
sentences.	O
Although	O
sentence	O
permutation	O
only	O
shows	O
significant	O
additive	O
gains	O
on	O
the	O
CNN/DM	B-DatasetName
summarization	I-DatasetName
dataset,	O
we	O
hypothesised	O
that	O
larger	O
pre-trained	O
models	O
may	O
be	O
better	O
able	O
to	O
learn	O
from	O
this	O
task.	O
To	O
help	O
the	O
model	O
better	O
fit	O
the	O
data,	O
we	O
disabled	O
dropout	B-HyperparameterName
for	O
the	O
final	O
10%	B-MetricValue
of	O
training	O
steps.	O
We	O
use	O
the	O
same	O
pre-training	O
data	O
as	O
,	O
consisting	O
of	O
160Gb	O
of	O
news,	O
books,	O
stories,	O
and	O
web	O
text.	O

Recent	O
work	O
has	O
shown	O
that	O
downstream	O
performance	O
can	O
dramatically	O
improve	O
when	O
pre-training	O
is	O
scaled	O
to	O
large	O
batch	O
sizes	O
(Yang	O
et	O
al.,	O
2019;	O
and	O
corpora.	O
To	O
test	O
how	O
well	O
BART	B-MethodName
performs	O
in	O
this	O
regime,	O
and	O
to	O
create	O
a	O
useful	O
model	O
for	O
downstream	O
tasks,	O
we	O
trained	O
BART	B-MethodName
using	O
the	O
same	O
scale	O
as	O
the	O
RoBERTa	B-MethodName
model.	O

BART	B-MethodName
shows	O
large	O
improvements	O
on	O
summarization	O
metrics,	O
of	O
up	O
to	O
6	O
points	O
over	O
the	O
prior	O
state-of-the-art.	O
To	O
understand	O
BART's	B-MethodName
performance	O
beyond	O
automated	O
metrics,	O
we	O
analyse	O
its	O
generations	O
qualitatively.	O
Table	O
7	O
shows	O
example	O
summaries	O
generated	O
by	O
BART.	B-MethodName
Examples	O
are	O
taken	O
from	O
WikiNews	B-DatasetName
articles	I-DatasetName
published	O
after	O
the	O
creation	O
of	O
the	O
pre-training	O
corpus,	O
to	O
eliminate	O
the	O
possibility	O
of	O
the	O
events	O
described	O
being	O
present	O
in	O
the	O
model's	O
training	O
data.	O
Following	O
Narayan	O
et	O
al.	O
(2018),	O
we	O
remove	O
the	O
first	O
sentence	O
of	O
the	O
article	O
prior	O
to	O
summarizing	O
it,	O
so	O
there	O
is	O
no	O
easy	O
extractive	O
summary	O
of	O
the	O
document.	O
Unsurprisingly,	O
model	O
output	O
is	O
fluent	O
and	O
grammatical	O
English.	O
However,	O
model	O
output	O
is	O
also	O
highly	O
abstractive,	O
with	O
few	O
phrases	O
copied	O
from	O
the	O
input.	O
The	O
output	O
is	O
also	O
generally	O
factually	O
accurate,	O
and	O
integrates	O
supporting	O
evidence	O
from	O
across	O
the	O
input	O
document	O
with	O
background	O
knowledge	O
(for	O
example,	O
correctly	O
completing	O
names,	O
or	O
inferring	O
that	O
PG&E	O
operates	O
in	O
California).	O
In	O
the	O
first	O
example,	O
inferring	O
that	O
fish	O
are	O
protecting	O
reefs	O
from	O
global	O
warming	O
requires	O
non-trivial	O
inference	O
from	O
the	O
text.	O
However,	O
the	O
claim	O
that	O
the	O
work	O
was	O
published	O
in	O
Science	O
is	O
not	O
supported	O
by	O
the	O
source.	O
These	O
samples	O
demonstrate	O
that	O
the	O
BART	B-MethodName
pretraining	O
has	O
learned	O
a	O
strong	O
combination	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
and	I-TaskName
generation.	I-TaskName

Early	O
methods	O
for	O
pretraining	O
were	O
based	O
on	O
language	O
models.	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018)	O
only	O
models	O
leftward	O
context,	O
which	O
is	O
problematic	O
for	O
some	O
tasks.	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018)	O
concatenates	O
left-only	O
and	O
right-only	O
representations,	O
but	O
does	O
not	O
pre-train	O
interactions	O
between	O
these	O
features.	O
Radford	O
et	O
al.	O
(2019)	O
demonstrated	O
that	O
very	O
large	O
language	O
models	O
can	O
act	O
as	O
unsupervised	O
multitask	O
models.	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
introduced	O
masked	O
language	O
modelling,	O
which	O
allows	O
pre-training	O
to	O
learn	O
interactions	O
between	O
left	O
and	O
right	O
context	O
words.	O
Recent	O
work	O
has	O
shown	O
that	O
very	O
strong	O
performance	O
can	O
be	O
achieved	O
by	O
training	O
for	O
longer	O
,	O
by	O
tying	O
parameters	O
across	O
layers	O
(Lan	O
et	O
al.,	O
2019),	O
and	O
by	O
masking	O
spans	O
instead	O
of	O
words	O
.	O
Predictions	O
are	O
not	O
made	O
auto-regressively,	O
reducing	O
the	O
effectiveness	O
of	O
BERT	B-MethodName
for	O
generation	O
tasks.	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019)	O
fine-tunes	O
BERT	B-MethodName
with	O
an	O
ensemble	O
of	O
masks,	O
some	O
of	O
which	O
allow	O
only	O
leftward	O
context.	O
Like	O
BART,	B-MethodName
this	O
allows	O
UniLM	B-MethodName
to	O
be	O
used	O
for	O
both	O
generative	O
and	O
discriminative	O
tasks.	O
A	O
difference	O
is	O
that	O
UniLM	B-MethodName
predictions	O
are	O
conditionally	O
independent,	O
whereas	O
BART's	B-MethodName
are	O
autoregressive.	O
BART	B-MethodName
reduces	O
the	O
mismatch	O
between	O
pre-training	O
and	O
generation	O
tasks,	O
because	O
the	O
decoder	O
is	O
always	O
trained	O
on	O
uncorrupted	O
context.	O
MASS	B-MethodName
(Song	O
et	O
al.,	O
2019)	O
is	O
perhaps	O
the	O
most	O
similar	O
model	O
to	O
BART.	B-MethodName
An	O
input	O
sequence	O
where	O
a	O
contiguous	O
span	O
of	O
tokens	O
is	O
masked	O
is	O
mapped	O
to	O
a	O
sequence	O
consisting	O
of	O
the	O
missing	O
tokens.	O
MASS	B-MethodName
is	O
less	O
effective	O
for	O
discriminative	O
tasks,	O
because	O
disjoint	O
sets	O
of	O
tokens	O
are	O
fed	O
into	O
the	O
encoder	O
and	O
decoder.	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
extends	O
BERT	B-MethodName
by	O
pre-Source	O
Document	O
(abbreviated)	O

The	O
researchers	O
examined	O
three	O
types	O
of	O
coral	O
in	O
reefs	O
off	O
the	O
coast	O
of	O
Fiji	O
...	O
The	O
researchers	O
found	O
when	O
fish	O
were	O
plentiful,	O
they	O
would	O
eat	O
algae	O
and	O
seaweed	O
off	O
the	O
corals,	O
which	O
appeared	O
to	O
leave	O
them	O
more	O
resistant	O
to	O
the	O
bacterium	O
Vibrio	O
coralliilyticus,	O
a	O
bacterium	O
associated	O
with	O
bleaching.	O
The	O
researchers	O
suggested	O
the	O
algae,	O
like	O
warming	O
temperatures,	O
might	O
render	O
the	O
corals'	O
chemical	O
defenses	O
less	O
effective,	O
and	O
the	O
fish	O
were	O
protecting	O
the	O
coral	O
by	O
removing	O
the	O
algae.	O
Fisheries	O
off	O
the	O
coast	O
of	O
Fiji	O
are	O
protecting	O
coral	O
reefs	O
from	O
the	O
effects	O
of	O
global	O
warming,	O
according	O
to	O
a	O
study	O
in	O
the	O
journal	O
Science.	O
Sacoolas,	O
who	O
has	O
immunity	O
as	O
a	O
diplomat's	O
wife,	O
was	O
involved	O
in	O
a	O
traffic	O
collision	O
...	O
Prime	O
Minister	O
Johnson	O
was	O
questioned	O
about	O
the	O
case	O
while	O
speaking	O
to	O
the	O
press	O
at	O
a	O
hospital	O
in	O
Watford.	O
He	O
said,	O
"I	O
hope	O
that	O
Anne	O
Sacoolas	O
will	O
come	O
back	O
...	O
if	O
we	O
can't	O
resolve	O
it	O
then	O
of	O
course	O
I	O
will	O
be	O
raising	O
it	O
myself	O
personally	O
with	O
the	O
White	O
House."	O
Boris	O
Johnson	O
has	O
said	O
he	O
will	O
raise	O
the	O
issue	O
of	O
US	O
diplomat	O
Anne	O
Sacoolas'	O
diplomatic	O
immunity	O
with	O
the	O
White	O
House.	O
PG&E	O
stated	O
it	O
scheduled	O
the	O
blackouts	O
in	O
response	O
to	O
forecasts	O
for	O
high	O
winds	O
amid	O
dry	O
conditions.	O
The	O
aim	O
is	O
to	O
reduce	O
the	O
risk	O
of	O
wildfires.	O
Nearly	O
800	O
thousand	O
customers	O
were	O
scheduled	O
to	O
be	O
affected	O
by	O
the	O
shutoffs	O
which	O
were	O
expected	O
to	O
last	O
through	O
at	O
least	O
midday	O
tomorrow.	O
Power	O
has	O
been	O
turned	O
off	O
to	O
millions	O
of	O
customers	O
in	O
California	O
as	O
part	O
of	O
a	O
power	O
shutoff	O
plan.	O
dicting	O
masked	O
tokens	O
auto-regressively	O
in	O
a	O
permuted	O
order.	O
This	O
objective	O
allows	O
predictions	O
to	O
condition	O
on	O
both	O
left	O
and	O
right	O
context.	O
In	O
contrast,	O
the	O
BART	B-MethodName
decoder	O
works	O
left-to-right	O
during	O
pre-training,	O
matching	O
the	O
setting	O
during	O
generation.	O
Several	O
papers	O
have	O
explored	O
using	O
pre-trained	O
representations	O
to	O
improve	O
machine	B-TaskName
translation.	I-TaskName
The	O
largest	O
improvements	O
have	O
come	O
from	O
pre-training	O
on	O
both	O
source	O
and	O
target	O
languages	O
(Song	O
et	O
al.,	O
2019;Lample	O
&	O
Conneau,	O
2019),	O
but	O
this	O
requires	O
pretraining	O
on	O
all	O
languages	O
of	O
interest.	O
Other	O
work	O
has	O
shown	O
that	O
encoders	O
can	O
be	O
improved	O
using	O
pre-trained	O
representations	O
(Edunov	O
et	O
al.,	O
2019),	O
but	O
gains	O
in	O
decoders	O
are	O
more	O
limited.	O
We	O
show	O
how	O
BART	B-MethodName
can	O
be	O
used	O
to	O
improve	O
machine	O
translation	O
decoders.	O

We	O
introduced	O
BART,	B-MethodName
a	O
pre-training	O
approach	O
that	O
learns	O
to	O
map	O
corrupted	O
documents	O
to	O
the	O
original.	O
BART	B-MethodName
achieves	O
similar	O
performance	O
to	O
RoBERTa	B-MethodName
on	O
discriminative	O
tasks,	O
while	O
achieving	O
new	O
state-of-theart	O
results	O
on	O
a	O
number	O
of	O
text	B-TaskName
generation	I-TaskName
tasks.	O
Future	O
work	O
should	O
explore	O
new	O
methods	O
for	O
corrupting	O
documents	O
for	O
pre-training,	O
perhaps	O
tailoring	O
them	O
to	O
specific	O
end	O
tasks.	O

The	O
Masked	O
Language	O
Model	O
and	O
the	O
Permuted	O
Language	O
Model	O
perform	O
less	O
well	O
than	O
others	O
on	O
generation,	O
and	O
are	O
the	O
only	O
models	O
we	O
consider	O
that	O
do	O
not	O
include	O
left-to-right	O
auto-regressive	O
language	O
modelling	O
during	O
pre-training.	O
Bidirectional	O
encoders	O
are	O
crucial	O
for	O
SQuAD	B-DatasetName
As	O
noted	O
in	O
previous	O
work	O
(Devlin	O
et	O
al.,	O
2019),	O
just	O
left-to-right	O
decoder	O
performs	O
poorly	O
on	O
SQuAD,	B-DatasetName
because	O
future	O
context	O
is	O
crucial	O
in	O
classification	O
decisions.	O
However,	O
BART	B-MethodName
achieves	O
similar	O
performance	O
with	O
only	O
half	O
the	O
number	O
of	O
bidirectional	O
layers.	O
The	O
pre-training	O
objective	O
is	O
not	O
the	O
only	O
important	O
factor	O
Our	O
Permuted	O
Language	O
Model	O
performs	O
less	O
well	O
than	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019).	O
Some	O
of	O
this	O
difference	O
is	O
likely	O
due	O
to	O
not	O
including	O
other	O
architectural	O
improvements,	O
such	O
as	O
relative-position	O
embeddings	O
or	O
segment-level	O
recurrence.	O
Pure	O
language	O
models	O
perform	O
best	O
on	O
ELI5	B-DatasetName
The	O
ELI5	B-DatasetName
dataset	O
is	O
an	O
outlier,	O
with	O
much	O
higher	O
perplexities	B-MetricName
than	O
other	O
tasks,	O
and	O
is	O
the	O
only	O
generation	O
task	O
where	O
other	O
models	O
outperform	O
BART.	B-MethodName
A	O
pure	O
language	O
model	O
performs	O
best,	O
suggesting	O
that	O
BART	B-MethodName
is	O
less	O
effective	O
when	O
the	O
output	O
is	O
only	O
loosely	O
constrained	O
by	O
the	O
input.	O
BART	B-MethodName
achieves	O
the	O
most	O
consistently	O
strong	O
performance.	O
With	O
the	O
exception	O
of	O
ELI5,	B-DatasetName
BART	B-MethodName
models	O
using	O
text-infilling	O
perform	O
well	O
on	O
all	O
tasks.	O

SQuAD	B-MethodName
(Rajpurkar	O
et	O
al.,	O
2016)a	O
an	O
extractive	O
question	O
answering	O
task	O
on	O
Wikipedia	O
paragraphs.	O
Answers	O
are	O
text	O
spans	O
extracted	O
from	O
a	O
given	O
document	O
context.	O
Similar	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
use	O
concatenated	O
question	O
and	O
context	O
as	O
input	O
to	O
the	O
encoder	O
of	O
BART,	B-MethodName
and	O
additionally	O
pass	O
them	O
to	O
the	O
decoder.	O
The	O
model	O
includes	O
classifiers	O
to	O
predict	O
the	O
start	O
and	O
end	O
indices	O
of	O
each	O
token.	O
MNLI	B-DatasetName
(Williams	O
et	O
al.,	O
2017),	O
a	O
bitext	B-TaskName
classification	I-TaskName
task	I-TaskName
to	O
predict	O
whether	O
one	O
sentence	O
entails	O
another.	O
The	O
fine-tuned	O
model	O
concatenates	O
the	O
two	O
sentences	O
with	O
appended	O
an	O
EOS	O
token,	O
and	O
passes	O
them	O
to	O
both	O
the	O
BART	B-MethodName
encoder	O
and	O
decoder.	O
In	O
contrast	O
to	O
BERT,	B-MethodName
the	O
representation	O
of	O
the	O
EOS	O
token	O
is	O
used	O
to	O
classify	O
the	O
sentences	O
relations.	O
ELI5	B-DatasetName
(Fan	O
et	O
al.,	O
2019),	O
a	O
long-form	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
dataset.	O
Models	O
generate	O
answers	O
conditioned	O
on	O
the	O
concatenation	O
of	O
a	O
question	O
and	O
supporting	O
documents.	O
XSum	B-DatasetName
(Narayan	O
et	O
al.,	O
2018),	O
a	O
news	O
summarization	O
dataset	O
with	O
highly	O
abstractive	O
summaries.	O
ConvAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
a	O
dialogue	O
response	O
generation	O
task,	O
conditioned	O
on	O
context	O
and	O
a	O
persona.	O
(Hermann	O
et	O
al.,	O
2015),	O
a	O
news	O
summarization	O
dataset.	O
Summaries	O
here	O
are	O
typically	O
closely	O
related	O
to	O
source	O
sentences.	O

While	O
many	O
pre-training	O
objectives	O
have	O
been	O
proposed,	O
fair	O
comparisons	O
between	O
these	O
have	O
been	O
difficult	O
to	O
perform,	O
at	O
least	O
in	O
part	O
due	O
to	O
differences	O
in	O
training	O
data,	O
training	O
resources,	O
architectural	O
differences	O
between	O
models,	O
and	O
fine-tuning	O
procedures.	O
We	O
re-implement	O
strong	O
pre-training	O
approaches	O
recently	O
proposed	O
for	O
discriminative	B-TaskName
and	I-TaskName
generation	I-TaskName
tasks.	I-TaskName
We	O
aim,	O
as	O
much	O
as	O
possible,	O
to	O
control	O
for	O
differences	O
unrelated	O
to	O
the	O
pre-training	O
objective.	O
However,	O
we	O
do	O
make	O
minor	O
changes	O
to	O
the	O
learning	O
rate	O
and	O
usage	O
of	O
layer	O
normalisation	O
in	O
order	O
to	O
improve	O
performance	O
(tuning	O
these	O
separately	O
for	O
each	O
objective).	O
For	O
reference,	O
we	O
compare	O
our	O
implementations	O
with	O
published	O
numbers	O
from	O
BERT,	B-MethodName
which	O
was	O
also	O
trained	O
for	O
1M	B-HyperparameterValue
steps	B-HyperparameterName
on	O
a	O
combination	O
of	O
books	O
and	O
Wikipedia	O
data.	O
We	O
compare	O
the	O
following	O
approaches:	O
Language	O
Model	O
Similarly	O
to	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
we	O
train	O
a	O
left-to-right	O
Transformer	O
language	O
model.	O
This	O
model	O
is	O
equivalent	O
to	O
the	O
BART	B-MethodName
decoder,	O
without	O
cross-attention.	O
Permuted	O
Language	O
Model	O
Based	O
on	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
we	O
sample	O
1/6	O
of	O
the	O
tokens,	O
and	O
generate	O
them	O
in	O
a	O
random	O
order	O
autoregressively.	O
For	O
consistency	O
with	O
other	O
models,	O
we	O
do	O
not	O
implement	O
the	O
relative	O
positional	O
embeddings	O
or	O
attention	O
across	O
segments	O
from	O
XLNet.	B-MethodName
Masked	O
Language	O
Model	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
replace	O
15%	B-MetricValue
of	O
tokens	O
with	O
[MASK]	O
symbols,	O
and	O
train	O
the	O
model	O
to	O
independently	O
predict	O
the	O
original	O
tokens.	O
Multitask	O
Masked	O
Language	O
Model	O
As	O
in	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019),	O
we	O
train	O
a	O
Masked	O
Language	O
Model	O
with	O
additional	O
self-attention	O
masks.	O
Self	O
attention	O
masks	O
are	O
chosen	O
randomly	O
in	O
with	O
the	O
follow	O
proportions:	O
1/6	O
left-to-right,	O
1/6	O
right-to-left,	O
1/3	O
unmasked,	O
and	O
1/3	O
with	O
the	O
first	O
50%	B-MetricValue
of	O
tokens	O
unmasked	O
and	O
a	O
left-to-right	O
mask	O
for	O
the	O
remainder.	O
Masked	B-MethodName
Seq-to-Seq	I-MethodName
Inspired	I-MethodName
by	I-MethodName
MASS	I-MethodName
(Song	O
et	O
al.,	O
2019),	O
we	O
mask	O
a	O
span	O
containing	O
50%	B-MetricValue
of	O
tokens,	O
and	O
train	O
a	O
sequence	O
to	O
sequence	O
model	O
to	O
predict	O
the	O
masked	O
tokens.	O
For	O
the	O
Permuted	O
LM,	O
Masked	B-MethodName
LM	I-MethodName
and	O
Multitask	B-MethodName
Masked	I-MethodName
LM,	I-MethodName
we	O
use	O
two-stream	O
attention	O
(Yang	O
et	O
al.,	O
2019)	O
to	O
efficiently	O
compute	O
likelihoods	O
of	O
the	O
output	O
part	O
of	O
the	O
sequence	O
(using	O
a	O
diagonal	O
self-attention	O
mask	O
on	O
the	O
output	O
to	O
predict	O
words	O
left-to-right).	O
We	O
experiment	O
with	O
(1)	O
treating	O
the	O
task	O
as	O
a	O
standard	O
sequence-to-sequence	O
problem,	O
where	O
the	O
source	O
input	O
to	O
the	O
encoder	O
and	O
the	O
target	O
is	O
the	O
decoder	O
output,	O
or	O
(2)	O
adding	O
the	O
source	O
as	O
prefix	O
to	O
the	O
target	O
in	O
the	O
decoder,	O
with	O
a	O
loss	O
only	O
on	O
the	O
target	O
part	O
of	O
the	O
sequence.	O
We	O
find	O
the	O
former	O
works	O
better	O
for	O
BART	B-MethodName
models,	O
and	O
the	O
latter	O
for	O
other	O
models.	O
To	O
most	O
directly	O
compare	O
our	O
models	O
on	O
their	O
ability	O
to	O
model	O
their	O
fine-tuning	O
objective	O
(the	O
log	O
likelihood	O
of	O
the	O
human	O
text),	O
we	O
report	O
perplexity	B-MetricName
in	O
Table	O
1.	O

BART	B-MethodName
supports	O
a	O
much	O
wider	O
range	O
of	O
noising	O
schemes	O
during	O
pre-training	O
than	O
previous	O
work.	O
We	O
compare	O
a	O
range	O
of	O
options	O
using	O
base-size	O
models	O
(6	O
encoder	O
and	O
6	B-HyperparameterValue
decoder	B-HyperparameterName
layers,	I-HyperparameterName
with	O
a	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	O
768),	B-HyperparameterValue
evaluated	O
on	O
a	O
representative	O
subset	O
of	O
the	O
tasks	O
we	O
will	O
consider	O
for	O
the	O
full	O
large	O
scale	O
experiments	O
in	O
§5.	O

We	O
also	O
explore	O
using	O
BART	B-MethodName
to	O
improve	O
machine	O
translation	O
decoders	O
for	O
translating	O
into	O
English.	O
Previous	O
work	O
Edunov	O
et	O
al.	O
(2019)	O
has	O
shown	O
that	O
models	O
can	O
be	O
improved	O
by	O
incorporating	O
pre-trained	O
encoders,	O
but	O
gains	O
from	O
using	O
pre-trained	O
language	O
models	O
in	O
decoders	O
have	O
been	O
limited.	O
We	O
show	O
that	O
it	O
is	O
possible	O
to	O
use	O
the	O
entire	O
BART	B-MethodName
model	O
(both	O
encoder	O
and	O
decoder)	O
as	O
a	O
single	O
pretrained	O
decoder	O
for	O
machine	B-TaskName
translation,	I-TaskName
by	O
adding	O
a	O
new	O
set	O
of	O
encoder	O
parameters	O
that	O
are	O
learned	O
from	O
bitext	O
(see	O
Figure	O
3b).	O
More	O
precisely,	O
we	O
replace	O
BART's	B-MethodName
encoder	O
embedding	O
layer	O
with	O
a	O
new	O
randomly	O
initialized	O
encoder.	O
The	O
model	O
is	O
trained	O
end-to-end,	O
which	O
trains	O
the	O
new	O
encoder	O
to	O
map	O
foreign	O
words	O
into	O
an	O
input	O
that	O
BART	B-MethodName
can	O
de-noise	O
to	O
English.	O
The	O
new	O
encoder	O
can	O
use	O
a	O
separate	O
vocabulary	O
from	O
the	O
original	O
BART	B-MethodName
model.	O
We	O
train	O
the	O
source	O
encoder	O
in	O
two	O
steps,	O
in	O
both	O
cases	O
backpropagating	O
the	O
cross-entropy	B-MetricName
loss	O
from	O
the	O
output	O
of	O
the	O
BART	B-MethodName
model.	O
In	O
the	O
first	O
step,	O
we	O
freeze	O
most	O
of	O
BART	B-MethodName
parameters	O
and	O
only	O
update	O
the	O
randomly	O
initialized	O
source	O
encoder,	O
the	O
BART	B-MethodName
positional	O
embeddings,	O
and	O
the	O
self-attention	O
input	O
projection	O
matrix	O
of	O
BART's	B-MethodName
encoder	O
first	O
layer.	O
In	O
the	O
second	O
step,	O
we	O
train	O
all	O
model	O
parameters	O
for	O
a	O
small	O
number	O
of	O
iterations.	O

Because	O
BART	B-MethodName
has	O
an	O
autoregressive	O
decoder,	O
it	O
can	O
be	O
directly	O
fine	O
tuned	O
for	O
sequence	O
generation	O
tasks	O
such	O
as	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
and	I-TaskName
summarization.	I-TaskName
In	O
both	O
of	O
these	O
tasks,	O
information	O
is	O
copied	O
from	O
the	O
input	O
but	O
manipulated,	O
which	O
is	O
closely	O
related	O
to	O
the	O
denoising	O
pre-training	O
objective.	O
Here,	O
the	O
encoder	O
input	O
is	O
the	O
input	O
sequence,	O
and	O
the	O
decoder	O
generates	O
outputs	O
autoregressively.	O

For	O
token	B-TaskName
classification	I-TaskName
tasks,	I-TaskName
such	O
as	O
answer	O
endpoint	B-TaskName
classification	I-TaskName
for	O
SQuAD,	B-DatasetName
we	O
feed	O
the	O
complete	O
document	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
use	O
the	O
top	O
hidden	O
state	O
of	O
the	O
decoder	O
as	O
a	O
representation	O
for	O
each	O
word.	O
This	O
representation	O
is	O
used	O
to	O
classify	O
the	O
token.	O

For	O
sequence	B-TaskName
classification	I-TaskName
tasks,	O
the	O
same	O
input	O
is	O
fed	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
the	O
final	O
hidden	O
state	O
of	O
the	O
final	O
decoder	O
token	O
is	O
fed	O
into	O
new	O
multi-class	O
linear	O
classifier.	O
This	O
approach	O
is	O
related	O
to	O
the	O
CLS	O
token	O
in	O
BERT;	B-MethodName
however	O
we	O
add	O
the	O
additional	O
token	O
to	O
the	O
end	O
so	O
that	O
representation	O
for	O
the	O
token	O
in	O
the	O
decoder	O
can	O
attend	O
to	O
decoder	O
states	O
from	O
the	O
complete	O
input	O
(Figure	O
3a).	O

The	O
representations	O
produced	O
by	O
BART	B-MethodName
can	O
be	O
used	O
in	O
several	O
ways	O
for	O
downstream	O
applications.	O

BART	B-MethodName
is	O
trained	O
by	O
corrupting	O
documents	O
and	O
then	O
optimizing	O
a	O
reconstruction	B-MetricName
loss-the	I-MetricName
cross-entropy	B-MetricName
between	O
the	O
decoder's	O
output	O
and	O
the	O
original	O
document.	O
Unlike	O
existing	O
denoising	O
autoencoders,	O
which	O
are	O
tailored	O
to	O
specific	O
noising	O
schemes,	O
BART	B-MethodName
allows	O
us	O
to	O
apply	O
any	O
type	O
of	O
document	O
corruption.	O
In	O
the	O
extreme	O
case,	O
where	O
all	O
information	O
about	O
the	O
source	O
is	O
lost,	O
BART	B-MethodName
is	O
equivalent	O
to	O
a	O
language	O
model.	O
We	O
experiment	O
with	O
several	O
previously	O
proposed	O
and	O
novel	O
transformations,	O
but	O
we	O
believe	O
there	O
is	O
a	O
significant	O
potential	O
for	O
development	O
of	O
other	O
new	O
alternatives.	O
The	O
transformations	O
we	O
used	O
are	O
summarized	O
below,	O
and	O
examples	O
are	O
shown	O
in	O
Figure	O
2.	O
Token	O
Masking	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
random	O
tokens	O
are	O
sampled	O
and	O
replaced	O
with	O
[MASK]	O
elements.	O
Token	O
Deletion	O
Random	O
tokens	O
are	O
deleted	O
from	O
the	O
input.	O
In	O
contrast	O
to	O
token	O
masking,	O
the	O
model	O
must	O
decide	O
which	O
positions	O
are	O
missing	O
inputs.	O
Text	O
Infilling	O
A	O
number	O
of	O
text	O
spans	O
are	O
sampled,	O
with	O
span	O
lengths	O
drawn	O
from	O
a	O
Poisson	O
distribution	O
(λ	O
=	O
3).	O
Each	O
span	O
is	O
replaced	O
with	O
a	O
single	O
[MASK]	O
token.	O
0-length	O
spans	O
correspond	O
to	O
the	O
insertion	O
of	O
[MASK]	O
tokens.	O
Text	O
infilling	O
is	O
inspired	O
by	O
Span-BERT	B-MethodName
,	O
but	O
SpanBERT	B-MethodName
samples	O
span	O
lengths	O
from	O
a	O
different	O
(clamped	O
geometric)	O
distribution,	O
and	O
replaces	O
each	O
span	O
with	O
a	O
sequence	O
of	O
[MASK]	O
tokens	O
of	O
exactly	O
the	O
same	O
length.	O
Text	O
infilling	O
teaches	O
the	O
model	O
to	O
predict	O
how	O
many	O
tokens	O
are	O
missing	O
from	O
a	O
span.	O
Sentence	O
Permutation	O
A	O
document	O
is	O
divided	O
into	O
sentences	O
based	O
on	O
full	O
stops,	O
and	O
these	O
sentences	O
are	O
shuffled	O
in	O
a	O
random	O
order.	O
Document	O
Rotation	O
A	O
token	O
is	O
chosen	O
uniformly	O
at	O
random,	O
and	O
the	O
document	O
is	O
rotated	O
so	O
that	O
it	O
begins	O
with	O
that	O
token.	O
This	O
task	O
trains	O
the	O
model	O
to	O
identify	O
the	O
start	O
of	O
the	O
document.	O

BART	B-MethodName
uses	O
the	O
standard	O
sequence-to-sequence	O
Transformer	O
architecture	O
from	O
(Vaswani	O
et	O
al.,	O
2017),	O
except,	O
following	O
GPT,	B-MethodName
that	O
we	O
modify	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
functions	I-HyperparameterName
to	O
GeLUs	B-HyperparameterValue
(Hendrycks	O
&	O
Gimpel,	O
2016)	O
and	O
initialise	O
parameters	O
from	O
N	O
(0,	O
0.02).	O
For	O
our	O
base	O
model,	O
we	O
use	O
6	O
layers	O
in	O
the	O
encoder	O
and	O
de-coder,	O
and	O
for	O
our	O
large	O
model	O
we	O
use	O
12	O
layers	O
in	O
each.	O
The	O
architecture	O
is	O
closely	O
related	O
to	O
that	O
used	O
in	O
BERT,	B-MethodName
with	O
the	O
following	O
differences:	O
(1)	O
each	O
layer	O
of	O
the	O
decoder	O
additionally	O
performs	O
cross-attention	O
over	O
the	O
final	O
hidden	O
layer	O
of	O
the	O
encoder	O
(as	O
in	O
the	O
transformer	O
sequence-to-sequence	O
model);	O
and	O
(2)	O
BERT	B-MethodName
uses	O
an	O
additional	O
feed-forward	O
network	O
before	O
wordprediction,	O
which	O
BART	B-MethodName
does	O
not.	O
In	O
total,	O
BART	B-MethodName
contains	O
roughly	O
10%	B-MetricValue
more	O
parameters	O
than	O
the	O
equivalently	O
sized	O
BERT	B-MethodName
model.	O

BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
that	O
maps	O
a	O
corrupted	O
document	O
to	O
the	O
original	O
document	O
it	O
was	O
derived	O
from.	O
It	O
is	O
implemented	O
as	O
a	O
sequence-to-sequence	O
model	O
with	O
a	O
bidirectional	O
encoder	O
over	O
corrupted	O
text	O
and	O
a	O
left-to-right	O
autoregressive	O
decoder.	O
For	O
pre-training,	O
we	O
optimize	O
the	O
negative	B-MetricName
log	I-MetricName
likelihood	I-MetricName
of	O
the	O
original	O
document.	O

Bidirectional	O
Encoder	O
A	O
B	O
C	O
D	O
E	O
A	O
_	O
B	O
_	O
E	O
<s>	O
A	O
B	O
C	O
D(	O
c)	O
BART:	O
Inputs	O
to	O
the	O
encoder	O
need	O
not	O
be	O
aligned	O
with	O
decoder	O
outputs,	O
allowing	O
arbitary	O
noise	O
transformations.	O
Here,	O
a	O
document	O
has	O
been	O
corrupted	O
by	O
replacing	O
spans	O
of	O
text	O
with	O
mask	O
symbols.	O
The	O
corrupted	O
document	O
(left)	O
is	O
encoded	O
with	O
a	O
bidirectional	O
model,	O
and	O
then	O
the	O
likelihood	O
of	O
the	O
original	O
document	O
(right)	O
is	O
calculated	O
with	O
an	O
autoregressive	O
decoder.	O
For	O
fine-tuning,	O
an	O
uncorrupted	O
document	O
is	O
input	O
to	O
both	O
the	O
encoder	O
and	O
decoder,	O
and	O
we	O
use	O
representations	O
from	O
the	O
final	O
hidden	O
state	O
of	O
the	O
decoder.	O
Figure	O
1:	O
A	O
schematic	O
comparison	O
of	O
BART	B-MethodName
with	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018).	O
English,	O
by	O
propagation	O
through	O
BART,	B-MethodName
thereby	O
using	O
BART	B-MethodName
as	O
a	O
pre-trained	O
target-side	O
language	O
model.	O
This	O
approach	O
improves	O
performance	O
over	O
a	O
strong	O
back-translation	O
MT	O
baseline	O
by	O
1.1	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
Romanian-English	I-DatasetName
benchmark.	O
To	O
better	O
understand	O
these	O
effects,	O
we	O
also	O
report	O
an	O
ablation	O
analysis	O
that	O
replicates	O
other	O
recently	O
proposed	O
training	O
objectives.	O
This	O
study	O
allows	O
us	O
to	O
carefully	O
control	O
for	O
a	O
number	O
of	O
factors,	O
including	O
data	O
and	O
optimization	O
parameters,	O
which	O
have	O
been	O
shown	O
to	O
be	O
as	O
important	O
for	O
overall	O
performance	O
as	O
the	O
selection	O
of	O
training	O
objectives	O
.	O
We	O
find	O
that	O
BART	B-MethodName
exhibits	O
the	O
most	O
consistently	O
strong	O
performance	O
across	O
the	O
full	O
range	O
of	O
tasks	O
we	O
consider.	O

A	O
B	O
C	O
D	O
E	O
<s>	O
A	O
B	O
C	O
D	O
(b)	O
GPT:	B-MethodName
Tokens	O
are	O
predicted	O
auto-regressively,	O
meaning	O
GPT	B-MethodName
can	O
be	O
used	O
for	O
generation.	O
However	O
words	O
can	O
only	O
condition	O
on	O
leftward	O
context,	O
so	O
it	O
cannot	O
learn	O
bidirectional	O
interactions.	O

Self-supervised	O
methods	O
have	O
achieved	O
remarkable	O
success	O
in	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
(Mikolov	O
et	O
al.,	O
2013;Peters	O
et	O
al.,	O
2018;Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;.	O
The	O
most	O
successful	O
approaches	O
have	O
been	O
variants	O
of	O
masked	O
language	O
models,	O
which	O
are	O
denoising	O
autoencoders	O
that	O
are	O
trained	O
to	O
reconstruct	O
text	O
where	O
a	O
random	O
subset	O
of	O
the	O
words	O
has	O
been	O
masked	O
out.	O
Recent	O
work	O
has	O
shown	O
gains	O
by	O
improving	O
the	O
distribution	O
of	O
masked	O
tokens	O
,	O
the	O
order	O
in	O
which	O
masked	O
tokens	O
are	O
predicted	O
(Yang	O
et	O
al.,	O
2019),	O
and	O
the	O
available	O
context	O
for	O
replacing	O
masked	O
tokens	O
(Dong	O
et	O
al.,	O
2019).	O
However,	O
these	O
methods	O
typically	O
focus	O
on	O
particular	O
types	O
of	O
end	O
tasks	O
(e.g.	O
span	B-TaskName
prediction,	I-TaskName
generation,	B-TaskName
etc.),	O
limiting	O
their	O
applicability.	O
In	O
this	O
paper,	O
we	O
present	O
BART,	B-MethodName
which	O
pre-trains	O
a	O
model	O
combining	O
Bidirectional	O
and	O
Auto-Regressive	O
Transformers.	O
BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
built	O
with	O
a	O
sequence-to-sequence	O
model	O
that	O
is	O
applicable	O
to	O
a	O
very	O
wide	O
range	O
of	O
end	O
tasks.	O
Pretraining	O
has	O
two	O
stages	O
(1)	O
text	O
is	O
corrupted	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
a	O
sequence-to-sequence	O
model	O
is	O
learned	O
to	O
reconstruct	O
the	O
original	O
text.	O
BART	B-MethodName
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes	O
(see	O
Figure	O
1).	O
A	O
key	O
advantage	O
of	O
this	O
setup	O
is	O
the	O
noising	O
flexibility;	O
arbitrary	O
transformations	O
can	O
be	O
applied	O
to	O
the	O
original	O
text,	O
including	O
changing	O
its	O
length.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
arbitrary	O
length	O
spans	O
of	O
text	O
(including	O
zero	O
length)	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
This	O
approach	O
generalizes	O
the	O
original	O
word	O
masking	O
and	O
next	O
sentence	O
prediction	O
objectives	O
in	O
BERT	B-MethodName
by	O
forcing	O
the	O
model	O
to	O
reason	O
more	O
about	O
overall	O
sentence	O
length	O
and	O
make	O
longer	O
range	O
transformations	O
to	O
the	O
input.	O
BART	B-MethodName
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	O
generation	O
but	O
also	O
works	O
well	O
for	O
comprehension	O
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2018)	O
and	O
SQuAD	B-DatasetName
(Rajpurkar	O
et	O
al.,	O
2016),	O
and	O
achieves	O
new	O
state-of-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	O
dialogue,	O
question	O
answering,	O
and	O
summarization	O
tasks.	O
For	O
example,	O
it	O
improves	O
performance	O
by	O
6	B-MetricValue
ROUGE	B-MetricName
over	O
previous	O
work	O
on	O
XSum	O
(Narayan	O
et	O
al.,	O
2018).	O
BART	B-MethodName
also	O
opens	O
up	O
new	O
ways	O
of	O
thinking	O
about	O
fine	O
tuning.	O
We	O
present	O
a	O
new	O
scheme	O
for	O
machine	O
translation	O
where	O
a	O
BART	B-MethodName
model	O
is	O
stacked	O
above	O
a	O
few	O
additional	O
transformer	O
layers.	O
These	O
layers	O
are	O
trained	O
to	O
essentially	O
translate	O
the	O
foreign	O
language	O
to	O
noised	O
Bidirectional	O
Encoder	O
A	O
_	O
C	O
_	O
E	O
B	O
D	O
(a)	O
BERT:	B-MethodName
Random	O
tokens	O
are	O
replaced	O
with	O
masks,	O
and	O
the	O
document	O
is	O
encoded	O
bidirectionally.	O
Missing	O
tokens	O
are	O
predicted	O
independently,	O
so	O
BERT	B-MethodName
cannot	O
easily	O
be	O
used	O
for	O
generation.	O

We	O
present	O
BART,	B-MethodName
a	O
denoising	O
autoencoder	O
for	O
pretraining	O
sequence-to-sequence	O
models.	O
BART	B-MethodName
is	O
trained	O
by	O
(1)	O
corrupting	O
text	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
learning	O
a	O
model	O
to	O
reconstruct	O
the	O
original	O
text.	O
It	O
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
spans	O
of	O
text	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
BART	O
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	B-TaskName
generation	I-TaskName
but	O
also	O
works	O
well	O
for	O
comprehension	B-TaskName
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	O
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
and	O
SQuAD,	B-DatasetName
achieves	O
new	O
stateof-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	B-TaskName
dialogue,	I-TaskName
question	B-TaskName
answering,	I-TaskName
and	O
summarization	O
tasks,	O
with	O
gains	O
of	O
up	O
to	O
6	B-MetricValue
ROUGE.	B-MetricName
BART	B-MethodName
also	O
provides	O
a	O
1.1	B-MetricValue
BLEU	B-MetricName
increase	O
over	O
a	O
back-translation	O
system	O
for	O
machine	O
translation,	B-TaskName
with	O
only	O
target	O
language	O
pretraining.	O
We	O
also	O
report	O
ablation	O
experiments	O
that	O
replicate	O
other	O
pretraining	O
schemes	O
within	O
the	O
BART	B-MethodName
framework,	O
to	O
better	O
measure	O
which	O
factors	O
most	O
influence	O
end-task	O
performance.	O

Task-specific	O
distillation	O
Most	O
of	O
the	O
prior	O
works	O
focus	O
on	O
building	O
task-specific	O
distillation	O
setups.	O
Tang	O
et	O
al.	O
transfer	O
fine-tune	O
classification	O
model	O
BERT	B-MethodName
to	O
an	O
LSTM-based	B-MethodName
classifier.	O
Chatterjee	O
distill	B-MethodName
BERT	I-MethodName
model	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
in	O
a	O
smaller	O
Transformer	B-MethodName
model	O
previously	O
initialized	O
from	O
BERT.	B-MethodName
In	O
the	O
present	O
work,	O
we	O
found	O
it	O
beneficial	O
to	O
use	O
a	O
general-purpose	O
pre-training	O
distillation	O
rather	O
than	O
a	O
task-specific	O
distillation.	O
Turc	O
et	O
al.	O
use	O
the	O
original	O
pretraining	O
objective	O
to	O
train	O
smaller	O
student,	O
then	O
fine-tuned	O
via	O
distillation.	B-MethodName
As	O
shown	O
in	O
the	O
ablation	O
study,	O
we	O
found	O
it	O
beneficial	O
to	O
leverage	O
the	O
teacher's	O
knowledge	O
to	O
pre-train	O
with	O
additional	O
distillation	O
signal.	O
Multi-distillation	O
Yang	O
et	O
al.	O
combine	O
the	O
knowledge	O
of	O
an	O
ensemble	O
of	O
teachers	O
using	O
multi-task	O
learning	O
to	O
regularize	O
the	O
distillation.	O
The	O
authors	O
apply	O
Multi-Task	O
Knowledge	O
Distillation	O
to	O
learn	O
a	O
compact	O
question	O
answering	O
model	O
from	O
a	O
set	O
of	O
large	O
question	B-TaskName
answering	I-TaskName
models.	O
An	O
application	O
of	O
multi-distillation	O
is	O
multi-linguality:	B-TaskName
Tsai	O
et	O
al.	O
adopts	O
a	O
similar	O
approach	O
to	O
us	O
by	O
pre-training	O
a	O
multilingual	O
model	O
from	O
scratch	O
solely	O
through	O
distillation.	O
However,	O
as	O
shown	O
in	O
the	O
ablation	O
study,	O
leveraging	O
the	O
teacher's	O
knowledge	O
with	O
initialization	O
and	O
additional	O
losses	O
leads	O
to	O
substantial	O
gains.	O
Other	O
compression	O
techniques	O
have	O
been	O
studied	O
to	O
compress	O
large	O
models.	O
Recent	O
developments	O
in	O
weights	O
pruning	O
reveal	O
that	O
it	O
is	O
possible	O
to	O
remove	O
some	O
heads	O
in	O
the	O
self-attention	O
at	O
test	O
time	O
without	O
significantly	O
degrading	O
the	O
performance	O
Michel	O
et	O
al.	O
.	O
Some	O
layers	O
can	O
be	O
reduced	O
to	O
one	O
head.	O
A	O
separate	O
line	O
of	O
study	O
leverages	O
quantization	O
to	O
derive	O
smaller	O
models	O
(Gupta	O
et	O
al.	O
).	O
Pruning	O
and	O
quantization	O
are	O
orthogonal	O
to	O
the	O
present	O
work.	O

We	O
introduced	O
DistilBERT,	B-MethodName
a	O
general-purpose	O
pre-trained	O
version	O
of	O
BERT,	B-MethodName
40%	B-MetricValue
smaller,	O
60%	B-MetricValue
faster,	O
that	O
retains	O
97%	B-MetricValue
of	O
the	O
language	O
understanding	O
capabilities.	O
We	O
showed	O
that	O
a	O
general-purpose	O
language	O
model	O
can	O
be	O
successfully	O
trained	O
with	O
distillation	O
and	O
analyzed	O
the	O
various	O
components	O
with	O
an	O
ablation	O
study.	O
We	O
further	O
demonstrated	O
that	O
DistilBERT	B-MethodName
is	O
a	O
compelling	O
option	O
for	O
edge	O
applications.	O

In	O
this	O
section,	O
we	O
investigate	O
the	O
influence	O
of	O
various	O
components	O
of	O
the	O
triple	O
loss	O
and	O
the	O
student	O
initialization	O
on	O
the	O
performances	O
of	O
the	O
distilled	O
model.	O
We	O
report	O
the	O
macro-score	O
on	O
GLUE.	B-DatasetName
Table	O
4	O
presents	O
the	O
deltas	O
with	O
the	O
full	O
triple	O
loss:	O
removing	O
the	O
Masked	B-MetricName
Language	I-MetricName
Modeling	I-MetricName
loss	I-MetricName
has	O
little	O
impact	O
while	O
the	O
two	O
distillation	O
losses	O
account	O
for	O
a	O
large	O
portion	O
of	O
the	O
performance.	O

To	O
further	O
investigate	O
the	O
speed-up/size	O
trade-off	O
of	O
DistilBERT,	B-MethodName
we	O
compare	O
(in	O
Table	O
3)	O
the	O
number	O
of	O
parameters	O
of	O
each	O
model	O
along	O
with	O
the	O
inference	O
time	O
needed	O
to	O
do	O
a	O
full	O
pass	O
on	O
the	O
STS-B	B-DatasetName
development	O
set	O
on	O
CPU	O
(Intel	O
Xeon	O
E5-2690	O
v3	O
Haswell	O
@2.9GHz)	O
using	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1.	B-HyperparameterValue
DistilBERT	B-MethodName
has	O
40%	B-MetricValue
fewer	O
parameters	O
than	O
BERT	B-MethodName
and	O
is	O
60%	B-MetricValue
faster	O
than	O
BERT.	B-MethodName
On	O
device	O
computation	O
We	O
studied	O
whether	O
DistilBERT	B-MethodName
could	O
be	O
used	O
for	O
on-the-edge	O
applications	O
by	O
building	O
a	O
mobile	O
application	O
for	O
question	O
answering.	O
We	O
compare	O
the	O
average	O
inference	O
time	O
on	O
a	O
recent	O
smartphone	O
(iPhone	O
7	O
Plus)	O
against	O
our	O
previously	O
trained	O
question	O
answering	O
model	O
based	O
on	O
BERT-base.	B-MethodName
Excluding	O
the	O
tokenization	O
step,	O
DistilBERT	B-MethodName
is	O
71%	B-MetricValue
faster	O
than	O
BERT,	B-MethodName
and	O
the	O
whole	O
model	O
weighs	O
207	O
MB	O
(which	O
could	O
be	O
further	O
reduced	O
with	O
quantization).	O
Our	O
code	O
is	O
available	O
5	O
.	O

Downstream	O
tasks	O
We	O
further	O
study	O
the	O
performances	O
of	O
DistilBERT	B-MethodName
on	O
several	O
downstream	O
tasks	O
under	O
efficient	O
inference	O
constraints:	O
a	O
classification	O
task	O
(IMDb	O
sentiment	O
classification	O
-	O
Maas	O
et	O
al.	O
)	O
and	O
a	O
question	O
answering	O
task	O
(SQuAD	B-DatasetName
v1.1	O
-Rajpurkar	O
et	O
al.	O
).	O
As	O
shown	O
in	O
Table	O
2,	O
DistilBERT	B-MethodName
is	O
only	O
0.6%	B-MetricValue
point	O
behind	O
BERT	B-MethodName
in	O
test	O
accuracy	B-MetricName
on	O
the	O
IMDb	B-DatasetName
benchmark	O
while	O
being	O
40%	B-MetricValue
smaller.	O
On	O
SQuAD,	B-DatasetName
DistilBERT	B-MethodName
is	O
within	O
3.9	B-MetricValue
points	I-MetricValue
of	O
the	O
full	O
BERT.	B-MethodName
We	O
also	O
studied	O
whether	O
we	O
could	O
add	O
another	O
step	O
of	O
distillation	B-MethodName
during	O
the	O
adaptation	O
phase	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
on	O
SQuAD	B-DatasetName
using	O
a	O
BERT	B-MethodName
model	O
previously	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
as	O
a	O
∅	O
-L	O
cos	O
-L	O
mlm	O
-2.96	O
L	O
ce	O
-∅	O
-L	O
mlm	O
-1.46	O
L	O
ce	O
-L	O
cos	O
-∅	O
-0.	O
31	O
Triple	B-MetricName
loss	I-MetricName
+	O
random	O
weights	O
initialization	O
-3.69	O
teacher	O
for	O
an	O
additional	O
term	O
in	O
the	O
loss	O
(knowledge	O
distillation).	O
In	O
this	O
setting,	O
there	O
are	O
thus	O
two	O
successive	O
steps	O
of	O
distillation,	B-MethodName
one	O
during	O
the	O
pre-training	O
phase	O
and	O
one	O
during	O
the	O
adaptation	O
phase.	O
In	O
this	O
case,	O
we	O
were	O
able	O
to	O
reach	O
interesting	O
performances	O
given	O
the	O
size	O
of	O
the	O
model:	O
79.8	O
F1	O
and	O
70.4	O
EM,	O
i.e.	O
within	O
3	O
points	O
of	O
the	O
full	O
model.	O

General	O
Language	O
Understanding	O
We	O
assess	O
the	O
language	O
understanding	O
and	O
generalization	O
capabilities	O
of	B-MethodName
DistilBERT	I-MethodName
on	O
the	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	I-DatasetName
[Wang	O
et	O
al.,	O
2018],	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
We	O
report	O
scores	O
on	O
the	O
development	O
sets	O
for	O
each	O
task	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
without	O
the	O
use	O
of	O
ensembling	O
or	O
multi-tasking	O
scheme	O
for	O
fine-tuning	O
(which	O
are	O
mostly	O
orthogonal	O
to	O
the	O
present	O
work).	O
We	O
compare	O
the	O
results	O
to	O
the	O
baseline	O
provided	O
by	O
the	O
authors	O
of	O
GLUE:	B-DatasetName
an	O
ELMo	B-MethodName
(Peters	O
et	O
al.	O
)	O
encoder	O
followed	O
by	O
two	O
BiLSTMs.	B-MethodName
4	O
The	O
results	O
on	O
each	O
of	O
the	O
9	O
tasks	O
are	O
showed	O
on	O
Table	O
1	O
along	O
with	O
the	O
macro-score	O
(average	O
of	O
individual	O
scores).	O
Among	O
the	O
9	O
tasks,	O
DistilBERT	B-MethodName
is	O
always	O
on	O
par	O
or	O
improving	O
over	O
the	O
ELMo	B-MethodName
baseline	O
(up	O
to	O
19	B-MetricValue
points	I-MetricValue
of	O
accuracy	B-MetricName
on	O
STS-B).	B-DatasetName
DistilBERT	B-MethodName
also	O
compares	O
surprisingly	O
well	O
to	O
BERT,	B-MethodName
retaining	O
97%	B-MetricValue
of	O
the	O
performance	B-MetricName
with	O
40%	B-MetricValue
fewer	O
parameters.	O

In	O
addition	O
to	O
the	O
previously	O
described	O
optimization	O
and	O
architectural	O
choices,	O
an	O
important	O
element	O
in	O
our	O
training	O
procedure	O
is	O
to	O
find	O
the	O
right	O
initialization	O
for	O
the	O
sub-network	O
to	O
converge.	O
Taking	O
advantage	O
of	O
the	O
common	O
dimensionality	O
between	O
teacher	O
and	O
student	O
networks,	O
we	O
initialize	O
the	O
student	O
from	O
the	O
teacher	O
by	O
taking	O
one	O
layer	O
out	O
of	O
two.	O

Knowledge	B-MethodName
distillation	I-MethodName
[Bucila	O
et	O
al.,	O
2006,	O
Hinton	O
et	O
al.,	O
2015]	O
is	O
a	O
compression	O
technique	O
in	O
which	O
a	O
compact	O
model	O
-the	O
student	O
-is	O
trained	O
to	O
reproduce	O
the	O
behaviour	O
of	O
a	O
larger	O
model	O
-the	O
teacheror	O
an	O
ensemble	O
of	O
models.	O
In	O
supervised	O
learning,	O
a	O
classification	O
model	O
is	O
generally	O
trained	O
to	O
predict	O
an	O
instance	O
class	O
by	O
maximizing	O
the	O
estimated	O
probability	O
of	O
gold	O
labels.	O
A	O
standard	O
training	O
objective	O
thus	O
involves	O
minimizing	O
the	O
cross-entropy	O
between	O
the	O
model's	O
predicted	O
distribution	O
and	O
the	O
one-hot	O
empirical	O
distribution	O
of	O
training	O
labels.	O
A	O
model	O
performing	O
well	O
on	O
the	O
training	O
set	O
will	O
predict	O
an	O
output	O
distribution	O
with	O
high	O
probability	O
on	O
the	O
correct	O
class	O
and	O
with	O
near-zero	O
probabilities	O
on	O
other	O
classes.	O
But	O
some	O
of	O
these	O
"near-zero"	O
probabilities	O
are	O
larger	O
than	O
others	O
and	O
reflect,	O
in	O
part,	O
the	O
generalization	O
capabilities	O
of	O
the	O
model	O
and	O
how	O
well	O
it	O
will	O
perform	O
on	O
the	O
test	O
set	O
3	O
.	O
Training	O
loss	O
The	O
student	O
is	O
trained	O
with	O
a	O
distillation	B-MetricName
loss	I-MetricName
over	O
the	O
soft	O
target	O
probabilities	O
of	O
the	O
teacher:	O
L	O
ce	O
=	O
i	O
t	O
i	O
*	O
log(s	O
i	O
)	O
where	O
t	O
i	O
(resp.	O
s	O
i	O
)	O
is	O
a	O
probability	O
estimated	O
by	O
the	O
teacher	O
(resp.	O
the	O
student).	O
This	O
objective	O
results	O
in	O
a	O
rich	O
training	O
signal	O
by	O
leveraging	O
the	O
full	O
teacher	O
distribution.	O
Following	O
Hinton	O
et	O
al.	O
we	O
used	O
a	O
softmax-temperature:	B-HyperparameterName
p	O
i	O
=	O
exp(zi/T	O
)	O
j	O
exp(zj	O
/T	O
)	O
where	O
T	B-HyperparameterName
controls	O
the	O
smoothness	O
of	O
the	O
output	O
distribution	O
and	O
z	O
i	O
is	O
the	O
model	O
score	O
for	O
the	O
class	O
i.	O
The	O
same	O
temperature	O
T	O
is	O
applied	O
to	O
the	O
student	O
and	O
the	O
teacher	O
at	O
training	O
time,	O
while	O
at	O
inference,	O
T	B-HyperparameterName
is	O
set	O
to	O
1	B-HyperparameterValue
to	O
recover	O
a	O
standard	O
softmax.	O
The	O
final	O
training	O
objective	O
is	O
a	O
linear	O
combination	O
of	O
the	O
distillation	B-MetricName
loss	I-MetricName
L	I-MetricName
ce	O
with	O
the	O
supervised	O
training	O
loss,	O
in	O
our	O
case	O
the	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
loss	I-MetricName
L	O
mlm	O
[Devlin	O
et	O
al.,	O
2018].	O
We	O
found	O
it	O
beneficial	O
to	O
add	O
a	O
cosine	B-MetricName
embedding	I-MetricName
loss	I-MetricName
(L	I-MetricName
cos	I-MetricName
)	I-MetricName
which	O
will	O
tend	O
to	O
align	O
the	O
directions	O
of	O
the	O
student	O
and	O
teacher	O
hidden	O
states	O
vectors.	O
3	O
DistilBERT:	B-MethodName
a	O
distilled	O
version	O
of	O
BERT	B-MethodName
Student	O
architecture	O
In	O
the	O
present	O
work,	O
the	O
student	O
-DistilBERT	B-MethodName
-has	O
the	O
same	O
general	O
architecture	O
as	O
BERT.	B-MethodName
The	O
token-type	O
embeddings	O
and	O
the	O
pooler	O
are	O
removed	O
while	O
the	O
number	O
of	O
layers	O
is	O
reduced	O
by	O
a	O
factor	O
of	O
2.	O
Most	O
of	O
the	O
operations	O
used	O
in	O
the	O
Transformer	B-MethodName
architecture	O
(linear	O
layer	O
and	O
layer	O
normalisation)	O
are	O
highly	O
optimized	O
in	O
modern	O
linear	O
algebra	O
frameworks	O
and	O
our	O
investigations	O
showed	O
that	O
variations	O
on	O
the	O
last	O
dimension	O
of	O
the	O
tensor	O
(hidden	O
size	O
dimension)	O
have	O
a	O
smaller	O
impact	O
on	O
computation	O
efficiency	O
(for	O
a	O
fixed	O
parameters	O
budget)	O
than	O
variations	O
on	O
other	O
factors	O
like	O
the	O
number	O
of	O
layers.	O
Thus	O
we	O
focus	O
on	O
reducing	O
the	O
number	O
of	O
layers.	O

The	O
last	O
two	O
years	O
have	O
seen	O
the	O
rise	O
of	O
Transfer	B-MethodName
Learning	I-MethodName
approaches	O
in	O
Natural	O
Language	O
Processing	O
(NLP)	O
with	O
large-scale	O
pre-trained	O
language	O
models	O
becoming	O
a	O
basic	O
tool	O
in	O
many	O
NLP	O
tasks	O
[Devlin	O
et	O
al.,	O
2018,	O
Radford	O
et	O
al.,	O
2019.	O
While	O
these	O
models	O
lead	O
to	O
significant	O
improvement,	O
they	O
often	O
have	O
several	O
hundred	O
million	O
parameters	O
and	O
current	O
research	O
1	O
on	O
pre-trained	O
models	O
indicates	O
that	O
training	O
even	O
larger	O
models	O
still	O
leads	O
to	O
better	O
performances	O
on	O
downstream	O
tasks.	O
The	O
trend	O
toward	O
bigger	O
models	O
raises	O
several	O
concerns.	O
First	O
is	O
the	O
environmental	O
cost	O
of	O
exponentially	O
scaling	O
these	O
models'	O
computational	O
requirements	O
as	O
mentioned	O
in	O
Schwartz	O
et	O
al.	O
,	O
Strubell	O
et	O
al.	O
.	O
Second,	O
while	O
operating	O
these	O
models	O
on-device	O
in	O
real-time	O
has	O
the	O
potential	O
to	O
enable	O
novel	O
and	O
interesting	O
language	O
processing	O
applications,	O
the	O
growing	O
computational	O
and	O
memory	O
requirements	O
of	O
these	O
models	O
may	O
hamper	O
wide	O
adoption.	O
In	O
this	O
paper,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
reach	O
similar	O
performances	O
on	O
many	O
downstream-tasks	O
using	O
much	O
smaller	O
language	O
models	O
pre-trained	O
with	O
knowledge	O
distillation,	B-MethodName
resulting	O
in	O
models	O
that	O
are	O
lighter	O
and	O
faster	O
at	O
inference	O
time,	O
while	O
also	O
requiring	O
a	O
smaller	O
computational	O
training	O
budget.	O
Our	O
general-purpose	O
pre-trained	O
models	O
can	O
be	O
fine-tuned	O
with	O
good	O
performances	O
on	O
several	O
downstream	O
tasks,	O
keeping	O
the	O
flexibility	O
of	O
larger	O
models.	O
We	O
also	O
show	O
that	O
our	O
compressed	O
models	O
are	O
small	O
enough	O
to	O
run	O
on	O
the	O
edge,	O
e.g.	O
on	O
mobile	O
devices.	O
Using	O
a	O
triple	O
loss,	O
we	O
show	O
that	O
a	O
40%	B-MetricValue
smaller	O
Transformer	O
(Vaswani	O
et	O
al.	O
)	O
pre-trained	O
through	O
distillation	B-MethodName
via	O
the	O
supervision	O
of	O
a	O
bigger	O
Transformer	B-MethodName
language	O
model	O
can	O
achieve	O
similar	O
performance	O
on	O
a	O
variety	O
of	O
downstream	O
tasks,	O
while	O
being	O
60%	O
faster	O
at	O
inference	O
time.	O
Further	O
ablation	O
studies	O
indicate	O
that	O
all	O
the	O
components	O
of	O
the	O
triple	O
loss	O
are	O
important	O
for	O
best	O
performances.	O
We	O
have	O
made	O
the	O
trained	O
weights	O
available	O
along	O
with	O
the	O
training	O
code	O
in	O
the	O
Transformers	O
2	O
library	O
from	O
HuggingFace	O
[Wolf	O
et	O
al.,	O
2019].	O

As	O
Transfer	B-MethodName
Learning	I-MethodName
from	O
large-scale	O
pre-trained	O
models	O
becomes	O
more	O
prevalent	O
in	O
Natural	O
Language	O
Processing	O
(NLP),	O
operating	O
these	O
large	O
models	O
in	O
on-theedge	O
and/or	O
under	O
constrained	O
computational	O
training	O
or	O
inference	O
budgets	O
remains	O
challenging.	O
In	O
this	O
work,	O
we	O
propose	O
a	O
method	O
to	O
pre-train	O
a	O
smaller	O
generalpurpose	O
language	O
representation	O
model,	O
called	O
DistilBERT,	B-MethodName
which	O
can	O
then	O
be	O
finetuned	O
with	O
good	O
performances	O
on	O
a	O
wide	O
range	O
of	O
tasks	O
like	O
its	O
larger	O
counterparts.	O
While	O
most	O
prior	O
work	O
investigated	O
the	O
use	O
of	O
distillation	B-MethodName
for	O
building	O
task-specific	O
models,	O
we	O
leverage	O
knowledge	O
distillation	B-MethodName
during	O
the	O
pre-training	O
phase	O
and	O
show	O
that	O
it	O
is	O
possible	O
to	O
reduce	O
the	O
size	O
of	O
a	O
BERT	B-MethodName
model	O
by	O
40%,	B-MetricValue
while	O
retaining	O
97%	B-MetricValue
of	O
its	O
language	O
understanding	O
capabilities	O
and	O
being	O
60%	B-MetricValue
faster.	O
To	O
leverage	O
the	O
inductive	O
biases	O
learned	O
by	O
larger	O
models	O
during	O
pre-training,	O
we	O
introduce	O
a	O
triple	O
loss	O
combining	O
language	O
modeling,	O
distillation	O
and	O
cosine-distance	O
losses.	O
Our	O
smaller,	O
faster	O
and	O
lighter	O
model	O
is	O
cheaper	O
to	O
pre-train	O
and	O
we	O
demonstrate	O
its	O
capabilities	O
for	O
on-device	O
computations	O
in	O
a	O
proof-of-concept	O
experiment	O
and	O
a	O
comparative	O
on-device	O
study.	O

Byte-Pair	B-MethodName
Encoding	I-MethodName
(BPE)	I-MethodName
(Sennrich	O
et	O
al.,	O
2016)	O
is	O
a	O
hybrid	O
between	O
character-and	O
word-level	O
representations	O
that	O
allows	O
handling	O
the	O
large	O
vocabularies	O
common	O
in	O
natural	O
language	O
corpora.	O
Instead	O
of	O
full	O
words,	O
BPE	O
relies	O
on	O
subwords	O
units,	O
which	O
are	O
extracted	O
by	O
performing	O
statistical	O
analysis	O
of	O
the	O
training	O
corpus.	O
BPE	O
vocabulary	O
sizes	O
typically	O
range	O
from	O
10K-100K	O
subword	O
units.	O
However,	O
unicode	O
characters	O
can	O
account	O
for	O
a	O
sizeable	O
portion	O
of	O
this	O
vocabulary	O
when	O
modeling	O
large	O
and	O
diverse	O
corpora,	O
such	O
as	O
the	O
ones	O
considered	O
in	O
this	O
work.	O
Radford	O
et	O
al.	O
(2019)	O
introduce	O
a	O
clever	O
implementation	O
of	O
BPE	O
that	O
uses	O
bytes	O
instead	O
of	O
unicode	O
characters	O
as	O
the	O
base	O
subword	O
units.	O
Using	O
bytes	O
makes	O
it	O
possible	O
to	O
learn	O
a	O
subword	O
vocabulary	O
of	O
a	O
modest	O
size	O
(50K	O
units)	O
that	O
can	O
still	O
encode	O
any	O
input	O
text	O
without	O
introducing	O
any	O
"unknown"	O
tokens.	O
8	O
Large	O
batch	O
training	O
can	O
improve	O
training	O
efficiency	O
even	O
without	O
large	O
scale	O
parallel	O
hardware	O
through	O
gradient	O
accumulation,	O
whereby	O
gradients	O
from	O
multiple	O
mini-batches	O
are	O
accumulated	O
locally	O
before	O
each	O
optimization	O
step.	O
This	O
functionality	O
is	O
supported	O
natively	O
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
The	O
original	O
BERT	B-MethodName
implementation	O
(Devlin	O
et	O
al.,	O
2019)	O
uses	O
a	O
character-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
of	O
size	O
30K,	B-HyperparameterValue
which	O
is	O
learned	O
after	O
preprocessing	O
the	O
input	O
with	O
heuristic	O
tokenization	O
rules.	O
Following	O
Radford	O
et	O
al.	O
(2019),	O
we	O
instead	O
consider	O
training	O
BERT	B-MethodName
with	O
a	O
larger	O
byte-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
containing	O
50K	B-HyperparameterValue
subword	I-HyperparameterValue
units,	I-HyperparameterValue
without	O
any	O
additional	O
preprocessing	O
or	O
tokenization	O
of	O
the	O
input.	O
This	O
adds	O
approximately	O
15M	O
and	O
20M	O
additional	O
parameters	O
for	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
respectively.	O
Early	O
experiments	O
revealed	O
only	O
slight	O
differences	O
between	O
these	O
encodings,	O
with	O
the	O
Radford	O
et	O
al.	O
(	O
2019)	O
BPE	B-MethodName
achieving	O
slightly	O
worse	O
end-task	O
performance	O
on	O
some	O
tasks.	O
Nevertheless,	O
we	O
believe	O
the	O
advantages	O
of	O
a	O
universal	O
encoding	O
scheme	O
outweighs	O
the	O
minor	O
degredation	O
in	O
performance	O
and	O
use	O
this	O
encoding	O
in	O
the	O
remainder	O
of	O
our	O
experiments.	O
A	O
more	O
detailed	O
comparison	O
of	O
these	O
encodings	O
is	O
left	O
to	O
future	O
work.	O

In	O
the	O
previous	O
section	O
we	O
propose	O
modifications	O
to	O
the	O
BERT	B-MethodName
pretraining	O
procedure	O
that	O
improve	O
end-task	O
performance.	O
We	O
now	O
aggregate	O
these	O
improvements	O
and	O
evaluate	O
their	O
combined	O
impact.	O
We	O
call	O
this	O
configuration	O
RoBERTa	B-MethodName
for	O
Robustly	B-MethodName
optimized	I-MethodName
BERT	I-MethodName
approach.	O
Specifically,	O
RoBERTa	B-MethodName
is	O
trained	O
with	O
dynamic	O
masking	O
(Section	O
4.1),	O
FULL-SENTENCES	O
without	O
NSP	O
loss	O
(Section	O
4.2),	O
large	O
mini-batches	O
(Section	O
4.3)	O
and	O
a	O
larger	O
byte-level	O
BPE	O
(Section	O
4.4).	O
Additionally,	O
we	O
investigate	O
two	O
other	O
important	O
factors	O
that	O
have	O
been	O
under-emphasized	O
in	O
previous	O
work:	O
(1)	O
the	O
data	O
used	O
for	O
pretraining,	O
and	O
(2)	O
the	O
number	O
of	O
training	O
passes	O
through	O
the	O
data.	O
For	O
example,	O
the	O
recently	O
proposed	O
XLNet	B-MethodName
architecture	O
(Yang	O
et	O
al.,	O
2019)	O
is	O
pretrained	O
using	O
nearly	O
10	O
times	O
more	O
data	O
than	O
the	O
original	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
It	O
is	O
also	O
trained	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
eight	O
times	O
larger	O
for	O
half	O
as	O
many	O
optimization	O
steps,	O
thus	O
seeing	O
four	O
times	O
as	O
many	O
sequences	O
in	O
pretraining	O
compared	O
to	O
BERT.	B-MethodName
To	O
help	O
disentangle	O
the	O
importance	O
of	O
these	O
factors	O
from	O
other	O
modeling	O
choices	O
(e.g.,	O
the	O
pretraining	O
objective),	O
we	O
begin	O
by	O
training	O
RoBERTa	B-MethodName
following	O
the	O
BERT	B-MethodName
LARGE	I-MethodName
architecture	O
(L	B-HyperparameterName
=	O
24,	B-HyperparameterValue
H	B-HyperparameterName
=	O
1024,	B-HyperparameterValue
A	B-HyperparameterName
=	O
16,	B-HyperparameterValue
355M	O
parameters).	O
We	O
pretrain	O
for	O
100K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
a	O
comparable	O
BOOK-CORPUS	B-DatasetName
plus	O
WIKIPEDIA	B-DatasetName
dataset	O
as	O
was	O
used	O
in	O
Yang	O
et	O
al.	O
(2019),	O
respectively.	O
Complete	O
results	O
on	O
all	O
GLUE	B-DatasetName
tasks	O
can	O
be	O
found	O
in	O
the	O
Appendix.	O
Devlin	O
et	O
al.	O
(2019).	O
We	O
pretrain	O
our	O
model	O
using	O
1024	O
V100	O
GPUs	O
for	O
approximately	O
one	O
day.	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
4.	O
When	O
controlling	O
for	O
training	O
data,	O
we	O
observe	O
that	O
RoBERTa	B-MethodName
provides	O
a	O
large	O
improvement	O
over	O
the	O
originally	O
reported	O
BERT	B-MethodName
LARGE	I-MethodName
results,	O
reaffirming	O
the	O
importance	O
of	O
the	O
design	O
choices	O
we	O
explored	O
in	O
Section	O
4.	O
Next,	O
we	O
combine	O
this	O
data	O
with	O
the	O
three	O
additional	O
datasets	O
described	O
in	O
Section	O
3.2.	O
We	O
train	O
RoBERTa	B-MethodName
over	O
the	O
combined	O
data	O
with	O
the	O
same	O
number	O
of	O
training	B-HyperparameterName
steps	I-HyperparameterName
as	O
before	O
(100K).	B-HyperparameterValue
In	O
total,	O
we	O
pretrain	O
over	O
160GB	O
of	O
text.	O
We	O
observe	O
further	O
improvements	O
in	O
performance	O
across	O
all	O
downstream	O
tasks,	O
validating	O
the	O
importance	O
of	O
data	O
size	O
and	O
diversity	O
in	O
pretraining.	O
9	O
Finally,	O
we	O
pretrain	O
RoBERTa	B-MethodName
for	O
significantly	O
longer,	O
increasing	O
the	O
number	O
of	O
pretraining	B-HyperparameterName
steps	I-HyperparameterName
from	O
100K	B-HyperparameterValue
to	I-HyperparameterValue
300K,	I-HyperparameterValue
and	O
then	O
further	O
to	O
500K.	B-HyperparameterValue
We	O
again	O
observe	O
significant	O
gains	O
in	O
downstream	O
task	O
performance,	O
and	O
the	O
300K	O
and	O
500K	O
step	O
models	O
outperform	O
XLNet	B-MethodName
LARGE	I-MethodName
across	O
most	O
tasks.	O
We	O
note	O
that	O
even	O
our	O
longest-trained	O
model	O
does	O
not	O
appear	O
to	O
overfit	O
our	O
data	O
and	O
would	O
likely	O
benefit	O
from	O
additional	O
training.	O
In	O
the	O
rest	O
of	O
the	O
paper,	O
we	O
evaluate	O
our	O
best	O
RoBERTa	B-MethodName
model	O
on	O
the	O
three	O
different	O
benchmarks:	O
GLUE,	B-DatasetName
SQuaD	B-DatasetName
and	O
RACE.	B-DatasetName
Specifically	O
we	O
consider	O
RoBERTa	B-MethodName
trained	O
for	O
500K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
all	O
five	O
of	O
the	O
datasets	O
introduced	O
in	O
Section	O
3.2.	O

For	O
GLUE	B-DatasetName
we	O
consider	O
two	O
finetuning	O
settings.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev)	O
we	O
finetune	O
RoBERTa	B-MethodName
separately	O
for	O
each	O
of	O
the	O
GLUE	B-DatasetName
tasks,	O
using	O
only	O
the	O
training	O
data	O
for	O
the	O
corresponding	O
task.	O
We	O
consider	O
a	O
limited	O
hyperparameter	O
sweep	O
for	O
each	O
task,	O
with	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
∈	O
{16,	B-HyperparameterValue
32}	I-HyperparameterValue
and	O
learning	B-HyperparameterName
rates	I-HyperparameterName
∈	O
{1e−5,	B-HyperparameterValue
2e−5,	I-HyperparameterValue
3e−5},	I-HyperparameterValue
with	O
a	O
linear	B-HyperparameterName
warmup	I-HyperparameterName
for	O
the	O
first	O
6%	B-HyperparameterValue
of	O
steps	O
followed	O
by	O
a	O
linear	B-HyperparameterName
decay	I-HyperparameterName
to	O
0.	O
We	O
finetune	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
perform	O
early	O
stopping	O
based	O
on	O
each	O
task's	O
evaluation	O
metric	O
on	O
the	O
dev	O
set.	O
The	O
rest	O
of	O
the	O
hyperparameters	O
remain	O
the	O
same	O
as	O
during	O
pretraining.	O
In	O
this	O
setting,	O
we	O
report	O
the	O
median	O
development	O
set	O
results	O
for	O
each	O
task	O
over	O
five	O
random	O
initializations,	O
without	O
model	O
ensembling.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
compare	O
RoBERTa	B-MethodName
to	O
other	O
approaches	O
on	O
the	O
test	O
set	O
via	O
the	O
GLUE	B-DatasetName
leaderboard.	O
While	O
many	O
submissions	O
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
depend	O
on	O
multitask	O
finetuning,	O
our	O
submission	O
depends	O
only	O
on	O
single-task	O
finetuning.	O
For	O
RTE,	B-DatasetName
STS	B-DatasetName
and	O
MRPC	B-DatasetName
we	O
found	O
it	O
helpful	O
to	O
finetune	O
starting	O
from	O
the	O
MNLI	O
single-task	O
model,	O
rather	O
than	O
the	O
baseline	O
pretrained	O
RoBERTa.	O
We	O
explore	O
a	O
slightly	O
wider	O
hyperparameter	O
space,	O
described	O
in	O
the	O
Appendix,	O
and	O
ensemble	O
between	O
5	O
and	O
7	O
models	O
per	O
task.	O
Task-specific	O
modifications	O
Two	O
of	O
the	O
GLUE	B-DatasetName
tasks	O
require	O
task-specific	O
finetuning	O
approaches	O
to	O
achieve	O
competitive	O
leaderboard	O
results.	O
QNLI:	B-DatasetName
Recent	O
submissions	O
on	O
the	O
GLUE	B-DatasetName
leaderboard	O
adopt	O
a	O
pairwise	O
ranking	O
formulation	O
for	O
the	O
QNLI	B-DatasetName
task,	O
in	O
which	O
candidate	O
answers	O
are	O
mined	O
from	O
the	O
training	O
set	O
and	O
compared	O
to	O
one	O
another,	O
and	O
a	O
single	O
(question,	O
candidate)	O
pair	O
is	O
classified	O
as	O
positive	O
(Liu	O
et	O
al.,	O
2019b,a;Yang	O
et	O
al.,	O
2019).	O
This	O
formulation	O
significantly	O
simplifies	O
the	O
task,	O
but	O
is	O
not	O
directly	O
comparable	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
Following	O
recent	O
work,	O
we	O
adopt	O
the	O
ranking	O
approach	O
for	O
our	O
test	O
submission,	O
but	O
for	O
direct	O
comparison	O
with	O
BERT	B-MethodName
we	O
report	O
development	O
set	O
results	O
based	O
on	O
a	O
pure	O
classification	O
approach.	O
WNLI:	B-DatasetName
We	O
found	O
the	O
provided	O
NLI-format	O
data	O
to	O
be	O
challenging	O
to	O
work	O
with.	O
Instead	O
we	O
use	O
the	O
reformatted	O
WNLI	B-DatasetName
data	O
from	O
Super-GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2019a),	O
which	O
indicates	O
the	O
span	O
of	O
the	O
query	O
pronoun	O
and	O
referent.	O
We	O
finetune	O
RoBERTa	B-MethodName
using	O
the	O
margin	O
ranking	O
loss	O
from	O
Kocijan	O
et	O
al.	O
(2019).	O
For	O
a	O
given	O
input	O
sentence,	O
we	O
use	O
spaCy	O
(Honnibal	O
and	O
Montani,	O
2017)	O
to	O
extract	O
additional	O
candidate	O
noun	O
phrases	O
from	O
the	O
sentence	O
and	O
finetune	O
our	O
model	O
so	O
that	O
it	O
assigns	O
higher	O
scores	O
to	O
positive	O
referent	O
phrases	O
than	O
for	O
any	O
of	O
the	O
generated	O
negative	O
candidate	O
phrases.	O
One	O
unfortunate	O
consequence	O
of	O
this	O
formulation	O
is	O
that	O
we	O
can	O
only	O
make	O
use	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
5.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev),	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
all	O
9	O
of	O
the	O
GLUE	O
task	O
development	O
sets.	O
Crucially,	O
RoBERTa	B-MethodName
uses	O
the	O
same	O
masked	O
language	O
modeling	O
pretraining	O
objective	O
and	O
architecture	O
as	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
yet	O
consistently	O
outperforms	O
both	O
BERT	B-MethodName
LARGE	I-MethodName
and	O
XLNet	B-MethodName
LARGE	I-MethodName
.	O
This	O
raises	O
questions	O
about	O
the	O
relative	O
importance	O
of	O
model	O
architecture	O
and	O
pretraining	O
objective,	O
compared	O
to	O
more	O
mundane	O
details	O
like	O
dataset	O
size	O
and	O
training	O
time	O
that	O
we	O
explore	O
in	O
this	O
work.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
and	O
achieve	O
state-of-the-art	O
results	O
on	O
4	O
out	O
of	O
9	O
tasks	O
and	O
the	O
highest	O
average	O
score	O
to	O
date.	O
This	O
is	O
especially	O
exciting	O
because	O
RoBERTa	B-MethodName
does	O
not	O
depend	O
on	O
multi-task	O
finetuning,	O
unlike	O
most	O
of	O
the	O
other	O
top	O
submissions.	O
We	O
expect	O
future	O
work	O
may	O
further	O
improve	O
these	O
results	O
by	O
incorporating	O
more	O
sophisticated	O
multi-task	O
finetuning	O
procedures.	O

We	O
adopt	O
a	O
much	O
simpler	O
approach	O
for	O
SQuAD	B-DatasetName
compared	O
to	O
past	O
work.	O
In	O
particular,	O
while	O
both	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
XLNet,	B-MethodName
while	O
we	O
use	O
the	O
same	O
learning	O
rate	O
for	O
all	O
layers.	O
For	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
we	O
follow	O
the	O
same	O
finetuning	O
procedure	O
as	O
Devlin	O
et	O
al.	O
(2019).	O
For	O
SQuAD	B-DatasetName
v2.0,	I-DatasetName
we	O
additionally	O
classify	O
whether	O
a	O
given	O
question	O
is	O
answerable;	O
we	O
train	O
this	O
classifier	O
jointly	O
with	O
the	O
span	O
predictor	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O

We	O
present	O
our	O
results	O
in	O
Table	O
6.	O
On	O
the	O
SQuAD	O
v1.1	O
development	O
set,	O
RoBERTa	B-MethodName
matches	O
the	O
state-of-the-art	O
set	O
by	O
XLNet.	B-MethodName
On	O
the	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
development	O
set,	O
RoBERTa	B-DatasetName
sets	O
a	O
new	O
state-of-the-art,	O
improving	O
over	O
XLNet	B-DatasetName
by	B-MetricValue
0.4	I-MetricValue
points	I-MetricValue
(EM)	B-MetricName
and	O
0.6	B-MetricValue
points	I-MetricValue
(F1).	B-MetricName
We	O
also	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
public	O
SQuAD	B-DatasetName
2.0	I-DatasetName
leaderboard	O
and	O
evaluate	O
its	O
performance	O
relative	O
to	O
other	O
systems.	O
Most	O
of	O
the	O
top	O
systems	O
build	O
upon	O
either	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
or	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
both	O
of	O
which	O
rely	O
on	O
additional	O
external	O
training	O
data.	O
In	O
contrast,	O
our	O
submission	O
does	O
not	O
use	O
any	O
additional	O
data.	O
Our	O
single	B-MethodName
RoBERTa	I-MethodName
model	O
outperforms	O
all	O
but	O
one	O
of	O
the	O
single	O
model	O
submissions,	O
and	O
is	O
the	O
top	O
scoring	O
system	O
among	O
those	O
that	O
do	O
not	O
rely	O
on	O
data	O
augmentation.	O

In	O
RACE,	B-DatasetName
systems	O
are	O
provided	O
with	O
a	O
passage	O
of	O
text,	O
an	O
associated	O
question,	O
and	O
four	O
candidate	O
answers.	O
Systems	O
are	O
required	O
to	O
classify	O
which	O
of	O
the	O
four	O
candidate	O
answers	O
is	O
correct.	O
We	O
modify	O
RoBERTa	B-MethodName
for	O
this	O
task	O
by	O
concate-	O
Yang	O
et	O
al.	O
(2019).	O
nating	O
each	O
candidate	O
answer	O
with	O
the	O
corresponding	O
question	O
and	O
passage.	O
We	O
then	O
encode	O
each	O
of	O
these	O
four	O
sequences	O
and	O
pass	O
the	O
resulting	O
[CLS]	O
representations	O
through	O
a	O
fully-connected	O
layer,	O
which	O
is	O
used	O
to	O
predict	O
the	O
correct	O
answer.	O
We	O
truncate	O
question-answer	O
pairs	O
that	O
are	O
longer	O
than	O
128	O
tokens	O
and,	O
if	O
needed,	O
the	O
passage	O
so	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Results	O
on	O
the	O
RACE	B-DatasetName
test	O
sets	O
are	O
presented	O
in	O
Table	O
7.	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
both	O
middle-school	O
and	O
high-school	O
settings.	O

Past	O
work	O
in	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
has	O
shown	O
that	O
training	O
with	O
very	O
large	O
mini-batches	B-HyperparameterName
can	O
both	O
improve	O
optimization	O
speed	O
and	O
end-task	O
performance	O
when	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
increased	O
appropriately	O
(Ott	O
et	O
al.,	O
2018).	O
Recent	O
work	O
has	O
shown	O
that	O
BERT	B-MethodName
is	O
also	O
amenable	O
to	O
large	O
batch	O
training	O
(You	O
et	O
al.,	O
2019	O

In	O
the	O
original	O
BERT	B-MethodName
pretraining	O
procedure,	O
the	O
model	O
observes	O
two	O
concatenated	O
document	O
segments,	O
which	O
are	O
either	O
sampled	O
contiguously	O
from	O
the	O
same	O
document	O
(with	O
p	B-MetricName
=	O
0.5)	B-MetricValue
or	O
from	O
distinct	O
documents.	O
In	O
addition	O
to	O
the	O
masked	O
language	O
modeling	O
objective,	O
the	O
model	O
is	O
trained	O
to	O
predict	O
whether	O
the	O
observed	O
document	O
segments	O
come	O
from	O
the	O
same	O
or	O
distinct	O
documents	O
via	O
an	O
auxiliary	O
Next	O
Sentence	O
Prediction	O
(NSP)	O
loss.	O
The	O
NSP	O
loss	O
was	O
hypothesized	O
to	O
be	O
an	O
important	O
factor	O
in	O
training	O
the	O
original	O
BERT	B-MethodName
model.	O
Devlin	O
et	O
al.	O
(2019)	O
observe	O
that	O
removing	O
NSP	O
hurts	O
performance,	O
with	O
significant	O
performance	O
degradation	O
on	O
QNLI,	B-DatasetName
MNLI,	B-DatasetName
and	O
SQuAD	B-DatasetName
1.1.	I-DatasetName
However,	O
some	O
recent	O
work	O
has	O
questioned	O
the	O
necessity	O
of	O
the	O
NSP	O
loss	O
(Lample	O
and	O
Conneau,	O
2019;Yang	O
et	O
al.,	O
2019;Joshi	O
et	O
al.,	O
2019).	O
To	O
better	O
understand	O
this	O
discrepancy,	O
we	O
compare	O
several	O
alternative	O
training	O
formats:	O
•	O
SEGMENT-PAIR+NSP:	O
This	O
follows	O
the	O
original	O
input	O
format	O
used	O
in	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
with	O
the	O
NSP	O
loss.	O
Each	O
input	O
has	O
a	O
pair	O
of	O
segments,	O
which	O
can	O
each	O
contain	O
multiple	O
natural	O
sentences,	O
but	O
the	O
total	O
combined	O
length	O
must	O
be	O
less	O
than	O
512	O
tokens.	O
•	O
SENTENCE-PAIR+NSP:	O
Each	O
input	O
contains	O
a	O
pair	O
of	O
natural	O
sentences,	O
either	O
sampled	O
from	O
a	O
contiguous	O
portion	O
of	O
one	O
document	O
or	O
from	O
separate	O
documents.	O
Since	O
these	O
inputs	O
are	O
significantly	O
shorter	O
than	O
512	O
tokens,	O
we	O
increase	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
so	O
that	O
the	O
total	O
number	O
of	O
tokens	O
remains	O
similar	O
to	O
SEGMENT-PAIR+NSP.	O
We	O
retain	O
the	O
NSP	O
loss.	O
•	O
FULL-SENTENCES:	O
Each	O
input	O
is	O
packed	O
with	O
full	O
sentences	O
sampled	O
contiguously	O
from	O
one	O
or	O
more	O
documents,	O
such	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Inputs	O
may	O
cross	O
document	O
boundaries.	O
When	O
we	O
reach	O
the	O
end	O
of	O
one	O
document,	O
we	O
begin	O
sampling	O
sentences	O
from	O
the	O
next	O
document	O
and	O
add	O
an	O
extra	O
separator	O
token	O
between	O
documents.	O
We	O
remove	O
the	O
NSP	O
loss.	O
•	O
DOC-SENTENCES:	O
Inputs	O
are	O
constructed	O
similarly	O
to	O
FULL-SENTENCES,	O
except	O
that	O
they	O
may	O
not	O
cross	O
document	O
boundaries.	O
Inputs	O
sampled	O
near	O
the	O
end	O
of	O
a	O
document	O
may	O
be	O
shorter	O
than	O
512	O
tokens,	O
so	O
we	O
dynamically	O
increase	O
the	O
batch	O
size	O
in	O
these	O
cases	O
to	O
achieve	O
a	O
similar	O
number	O
of	O
total	O
tokens	O
as	O
FULL-SENTENCES.	O
We	O
remove	O
the	O
NSP	O
loss.	O
Results	O
Table	O
2	O
shows	O
results	O
for	O
the	O
four	O
different	O
settings.	O
We	O
first	O
compare	O
the	O
original	O
SEGMENT-PAIR	O
input	O
format	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
the	O
SENTENCE-PAIR	O
format;	O
both	O
formats	O
retain	O
the	O
NSP	O
loss,	O
but	O
the	O
latter	O
uses	O
single	O
sentences.	O
We	O
find	O
that	O
using	O
individual	O
sentences	O
hurts	O
performance	O
on	O
downstream	O
tasks,	O
which	O
we	O
hypothesize	O
is	O
because	O
the	O
model	O
is	O
not	O
able	O
to	O
learn	O
long-range	O
dependencies.	O
We	O
next	O
compare	O
training	O
without	O
the	O
NSP	O
loss	O
and	O
training	O
with	O
blocks	O
of	O
text	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES).	O
We	O
find	O
that	O
this	O
setting	O
outperforms	O
the	O
originally	O
published	O
BERT	B-MethodName
BASE	I-MethodName
results	O
and	O
that	O
removing	O
the	O
NSP	O
loss	O
matches	O
or	O
slightly	O
improves	O
downstream	O
task	O
performance,	O
in	O
contrast	O
to	O
Devlin	O
et	O
al.	O
(2019).	O
It	O
is	O
possible	O
that	O
the	O
original	O
BERT	B-MethodName
implementation	O
may	O
only	O
have	O
removed	O
the	O
loss	O
term	O
while	O
still	O
retaining	O
the	O
SEGMENT-PAIR	O
input	O
format.	O
Finally	O
we	O
find	O
that	O
restricting	O
sequences	O
to	O
come	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES)	O
performs	O
slightly	O
better	O
than	O
packing	O
sequences	O
from	O
multiple	O
documents	O
(FULL-SENTENCES).	O
However,	O
because	O
the	O
DOC-SENTENCES	O
format	O
results	O
in	O
variable	O
batch	O
sizes,	O
we	O
use	O
FULL-SENTENCES	O
in	O
the	O
remainder	O
of	O
our	O
experiments	O
for	O
easier	O
comparison	O
with	O
related	O
work.	O

As	O
discussed	O
in	O
Section	O
2,	O
BERT	B-MethodName
relies	O
on	O
randomly	O
masking	O
and	O
predicting	O
tokens.	O
The	O
original	O
BERT	B-MethodName
implementation	O
performed	O
masking	O
once	O
during	O
data	O
preprocessing,	O
resulting	O
in	O
a	O
single	O
static	O
mask.	O
To	O
avoid	O
using	O
the	O
same	O
mask	O
for	O
each	O
training	O
instance	O
in	O
every	O
epoch,	O
training	O
data	O
was	O
duplicated	O
10	O
times	O
so	O
that	O
each	O
sequence	O
is	O
masked	O
in	O
10	O
different	O
ways	O
over	O
the	O
40	B-HyperparameterValue
epochs	B-HyperparameterName
of	O
training.	O
Thus,	O
each	O
training	O
sequence	O
was	O
seen	O
with	O
the	O
same	O
mask	O
four	O
times	O
during	O
training.	O
We	O
compare	O
this	O
strategy	O
with	O
dynamic	O
masking	O
where	O
we	O
generate	O
the	O
masking	O
pattern	O
every	O
time	O
we	O
feed	O
a	O
sequence	O
to	O
the	O
model.	O
This	O
becomes	O
crucial	O
when	O
pretraining	O
for	O
more	O
steps	O
or	O
with	O
larger	O
datasets.	O
Results	O
Table	O
1	O
compares	O
the	O
published	O
BERT	O
BASE	O
results	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
our	O
reimplementation	O
with	O
either	O
static	O
or	O
dynamic	O
masking.	O
We	O
find	O
that	O
our	O
reimplementation	O
with	O
static	O
masking	O
performs	O
similar	O
to	O
the	O
original	O
BERT	B-MethodName
model,	O
and	O
dynamic	O
masking	O
is	O
comparable	O
or	O
slightly	O
better	O
than	O
static	O
masking.	O
Given	O
these	O
results	O
and	O
the	O
additional	O
efficiency	O
benefits	O
of	O
dynamic	O
masking,	O
we	O
use	O
dynamic	O
masking	O
in	O
the	O
remainder	O
of	O
the	O
experiments.	O

Pretraining	O
methods	O
have	O
been	O
designed	O
with	O
different	O
training	O
objectives,	O
including	O
language	O
modeling	O
(Dai	O
and	O
Le,	O
2015;Peters	O
et	O
al.,	O
2018;Howard	O
and	O
Ruder,	O
2018),	O
machine	B-TaskName
translation	I-TaskName
(McCann	O
et	O
al.,	O
2017),	O
and	O
masked	O
language	O
modeling	O
(Devlin	O
et	O
al.,	O
2019;Lample	O
and	O
Conneau,	O
2019).	O
Many	O
recent	O
papers	O
have	O
used	O
a	O
basic	O
recipe	O
of	O
finetuning	O
models	O
for	O
each	O
end	O
task	O
(Howard	O
and	O
Ruder,	O
2018;Radford	O
et	O
al.,	O
2018),	O
and	O
pretraining	O
with	O
some	O
variant	O
of	O
a	O
masked	O
language	O
model	O
objective.	O
However,	O
newer	O
methods	O
have	O
improved	O
performance	O
by	O
multi-task	O
fine	O
tuning	O
(Dong	O
et	O
al.,	O
2019),	O
incorporating	O
entity	O
embeddings	O
(Sun	O
et	O
al.,	O
2019),	O
span	O
prediction	O
(Joshi	O
et	O
al.,	O
2019),	O
and	O
multiple	O
variants	O
of	O
autoregressive	O
pretraining	O
Chan	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019).	O
Performance	O
is	O
also	O
typically	O
improved	O
by	O
training	O
bigger	O
models	O
on	O
more	O
data	O
(Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Radford	O
et	O
al.,	O
2019).	O
Our	O
goal	O
was	O
to	O
replicate,	O
simplify,	O
and	O
better	O
tune	O
the	O
training	O
of	O
BERT,	B-MethodName
as	O
a	O
reference	O
point	O
for	O
better	O
understanding	O
the	O
relative	O
performance	O
of	O
all	O
of	O
these	O
methods.	O
We	O
carefully	O
evaluate	O
a	O
number	O
of	O
design	O
decisions	O
when	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
find	O
that	O
performance	O
can	O
be	O
substantially	O
improved	O
by	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches	O
over	O
more	O
data;	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
training	O
on	O
longer	O
sequences;	O
and	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
Our	O
improved	O
pretraining	O
procedure,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD,	B-DatasetName
without	O
multi-task	O
finetuning	O
for	O
GLUE	B-DatasetName
or	O
additional	O
data	O
for	O
SQuAD.	B-DatasetName
These	O
results	O
illustrate	O
the	O
importance	O
of	O
these	O
previously	O
overlooked	O
design	O
decisions	O
and	O
suggest	O
that	O
BERT's	B-MethodName
pretraining	O
objective	O
remains	O
competitive	O
with	O
recently	O
proposed	O
alternatives.	O
We	O
additionally	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
release	O
our	O
models	O
and	O
code	O
for	O
pretraining	O
and	O
finetuning	O
at:	O

This	O
section	O
explores	O
and	O
quantifies	O
which	O
choices	O
are	O
important	O
for	O
successfully	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
keep	O
the	O
model	O
architecture	O
fixed.	O
7	O
Specifically,	O
we	O
begin	O
by	O
training	O
BERT	B-MethodName
models	O
with	O
the	O
same	O
configuration	O
as	O
BERT	B-MethodName
BASE	I-MethodName
(L	B-HyperparameterName
=	O
12,	B-HyperparameterValue
H	B-HyperparameterName
=	O
768,	B-HyperparameterValue
A	B-HyperparameterName
=	O
12,	B-HyperparameterValue
110M	B-MetricValue
params).	I-MetricValue

Following	O
previous	O
work,	O
we	O
evaluate	O
our	O
pretrained	O
models	O
on	O
downstream	O
tasks	O
using	O
the	O
following	O
three	O
benchmarks.	O
GLUE	B-DatasetName
The	B-DatasetName
General	I-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	O
(Wang	O
et	O
al.,	O
2019b)	O
is	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
6	O
Tasks	O
are	O
framed	O
as	O
either	O
single-sentence	O
classification	O
or	O
sentence-pair	O
classification	O
tasks.	O
The	O
GLUE	B-DatasetName
organizers	O
provide	O
training	O
and	O
development	O
data	O
splits	O
as	O
well	O
as	O
a	O
submission	O
server	O
and	O
leaderboard	O
that	O
allows	O
participants	O
to	O
evaluate	O
and	O
compare	O
their	O
systems	O
on	O
private	O
held-out	O
test	O
data.	O
For	O
the	O
replication	O
study	O
in	O
Section	O
4,	O
we	O
report	O
results	O
on	O
the	O
development	O
sets	O
after	O
finetuning	O
the	O
pretrained	O
models	O
on	O
the	O
corresponding	O
singletask	O
training	O
data	O
(i.e.,	O
without	O
multi-task	O
training	O
or	O
ensembling).	O
Our	O
finetuning	O
procedure	O
follows	O
the	O
original	O
BERT	B-MethodName
paper	O
(Devlin	O
et	O
al.,	O
2019).	O
In	O
Section	O
5	O
we	O
additionally	O
report	O
test	O
set	O
results	O
obtained	O
from	O
the	O
public	O
leaderboard.	O
These	O
results	O
depend	O
on	O
a	O
several	O
task-specific	O
modifications,	O
which	O
we	O
describe	O
in	O
Section	O
5.1.	O
SQuAD	B-DatasetName
The	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(SQuAD)	I-DatasetName
provides	O
a	O
paragraph	O
of	O
context	O
and	O
a	O
question.	O
The	O
task	O
is	O
to	O
answer	O
the	O
question	O
by	O
extracting	O
the	O
relevant	O
span	O
from	O
the	O
context.	O
We	O
evaluate	O
on	O
two	O
versions	O
of	O
SQuAD:	B-DatasetName
V1.1	O
and	O
V2.0	O
(Rajpurkar	O
et	O
al.,	O
2016(Rajpurkar	O
et	O
al.,	O
,	O
2018.	O
In	O
V1.1	O
the	O
context	O
always	O
contains	O
an	O
answer,	O
whereas	O
in	O
V2.0	O
some	O
questions	O
are	O
not	O
answered	O
in	O
the	O
provided	O
context,	O
making	O
the	O
task	O
more	O
challenging.	O
For	O
SQuAD	B-DatasetName
V1.1	I-DatasetName
we	O
adopt	O
the	O
same	O
span	O
prediction	O
method	O
as	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
For	O
SQuAD	B-DatasetName
V2.0,	I-DatasetName
we	O
add	O
an	O
additional	O
binary	O
classifier	O
to	O
predict	O
whether	O
the	O
question	O
is	O
answerable,	O
which	O
we	O
train	O
jointly	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O
During	O
evaluation,	O
we	O
only	O
predict	O
span	O
indices	O
on	O
pairs	O
that	O
are	O
classified	O
as	O
answerable.	O
RACE	B-DatasetName
The	B-DatasetName
ReAding	I-DatasetName
Comprehension	I-DatasetName
from	I-DatasetName
Examinations	I-DatasetName
(RACE)	I-DatasetName
(Lai	O
et	O
al.,	O
2017)	O
task	O
is	O
a	O
large-scale	O
reading	O
comprehension	O
dataset	O
with	O
more	O
than	O
28,000	O
passages	O
and	O
nearly	O
100,000	O
questions.	O
The	O
dataset	O
is	O
collected	O
from	O
English	O
examinations	O
in	O
China,	O
which	O
are	O
designed	O
for	O
middle	O
and	O
high	O
school	O
students.	O
In	O
RACE,	B-DatasetName
each	O
passage	O
is	O
associated	O
with	O
multiple	O
questions.	O
For	O
every	O
question,	O
the	O
task	O
is	O
to	O
select	O
one	O
correct	O
answer	O
from	O
four	O
options.	O
RACE	B-DatasetName
has	O
significantly	O
longer	O
context	O
than	O
other	O
popular	O
reading	O
comprehension	O
datasets	O
and	O
the	O
proportion	O
of	O
questions	O
that	O
requires	O
reasoning	O
is	O
very	O
large.	O

BERT-style	B-MethodName
pretraining	O
crucially	O
relies	O
on	O
large	O
quantities	O
of	O
text.	O
demonstrate	O
that	O
increasing	O
data	O
size	O
can	O
result	O
in	O
improved	O
end-task	O
performance.	O
Several	O
efforts	O
have	O
trained	O
on	O
datasets	O
larger	O
and	O
more	O
diverse	O
than	O
the	O
original	O
BERT	B-MethodName
(Radford	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Zellers	O
et	O
al.,	O
2019).	O
Unfortunately,	O
not	O
all	O
of	O
the	O
additional	O
datasets	O
can	O
be	O
publicly	O
released.	O
For	O
our	O
study,	O
we	O
focus	O
on	O
gathering	O
as	O
much	O
data	O
as	O
possible	O
for	O
experimentation,	O
allowing	O
us	O
to	O
match	O
the	O
overall	O
quality	O
and	O
quantity	O
of	O
data	O
as	O
appropriate	O
for	O
each	O
comparison.	O
We	O
consider	O
five	O
English-language	O
corpora	O
of	O
varying	O
sizes	O
and	O
domains,	O
totaling	O
over	O
160GB	O
of	O
uncompressed	O
text.	O
We	O
use	O
the	O
following	O
text	O
corpora:	O
•	O
BOOKCORPUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA.	I-DatasetName
This	O
is	O
the	O
original	O
data	O
used	O
to	O
train	O
BERT.	B-MethodName
(16GB).	O
•	O
CC-NEWS,	B-DatasetName
which	O
we	O
collected	O
from	O
the	O
English	O
portion	O
of	O
the	O
CommonCrawl	B-DatasetName
News	I-DatasetName
dataset	I-DatasetName
(Nagel,	O
2016).	O
The	O
data	O
contains	O
63	B-HyperparameterName
million	I-HyperparameterName
English	I-HyperparameterName
news	I-HyperparameterName
articles	I-HyperparameterName
crawled	O
between	O
September	O
2016	O
and	O
February	O
2019.	O
(76GB	O
after	O
filtering).	O
4	O
•	O
OPENWEBTEXT	B-DatasetName
(Gokaslan	O
and	O
Cohen,	O
2019),	O
an	O
open-source	O
recreation	O
of	O
the	O
WebText	O
cor-pus	O
described	O
in	O
Radford	O
et	O
al.	O
(2019).	O
The	O
text	O
is	O
web	O
content	O
extracted	O
from	O
URLs	O
shared	O
on	O
Reddit	O
with	O
at	O
least	O
three	O
upvotes.	O
(38GB).	O
5	O
•	O
STORIES,	B-DatasetName
a	O
dataset	O
introduced	O
in	O
Trinh	O
and	O
Le	O
(2018)	O
containing	O
a	O
subset	O
of	O
CommonCrawl	B-DatasetName
data	O
filtered	O
to	O
match	O
the	O
story-like	O
style	O
of	O
Winograd	O
schemas.	O
(31GB).	O

We	O
reimplement	O
BERT	B-MethodName
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
We	O
primarily	O
follow	O
the	O
original	O
BERT	B-MethodName
optimization	O
hyperparameters,	O
given	O
in	O
Section	O
2,	O
except	O
for	O
the	O
peak	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
number	B-HyperparameterName
of	I-HyperparameterName
warmup	I-HyperparameterName
steps,	I-HyperparameterName
which	O
are	O
tuned	O
separately	O
for	O
each	O
setting.	O
We	O
additionally	O
found	O
training	O
to	O
be	O
very	O
sensitive	O
to	O
the	O
Adam	B-HyperparameterValue
epsilon	O
term,	O
and	O
in	O
some	O
cases	O
we	O
obtained	O
better	O
performance	O
or	O
improved	O
stability	O
after	O
tuning	O
it.	O
Similarly,	O
we	O
found	O
setting	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.98	B-HyperparameterValue
to	O
improve	O
stability	O
when	O
training	O
with	O
large	O
batch	O
sizes.	O
We	O
pretrain	O
with	O
sequences	O
of	O
at	O
most	O
T	O
=	O
512	O
tokens.	O
Unlike	O
Devlin	O
et	O
al.	O
(2019),	O
we	O
do	O
not	O
randomly	O
inject	O
short	O
sequences,	O
and	O
we	O
do	O
not	O
train	O
with	O
a	O
reduced	O
sequence	O
length	O
for	O
the	O
first	O
90%	B-MetricValue
of	O
updates.	O
We	O
train	O
only	O
with	O
full-length	O
sequences.	O
We	O
train	O
with	O
mixed	O
precision	O
floating	O
point	O
arithmetic	O
on	O
DGX-1	O
machines,	O
each	O
with	O
8	O
×	O
32GB	O
Nvidia	O
V100	O
GPUs	O
interconnected	O
by	O
Infiniband	O
(Micikevicius	O
et	O
al.,	O
2018).	O

In	O
this	O
section,	O
we	O
describe	O
the	O
experimental	O
setup	O
for	O
our	O
replication	O
study	O
of	O
BERT.	B-MethodName

BERT	B-MethodName
is	O
trained	O
on	O
a	O
combination	O
of	O
BOOKCOR-PUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA,	I-DatasetName
which	O
totals	O
16GB	O
of	O
uncompressed	O
text.	O
3	O

BERT	B-MethodName
is	O
optimized	O
with	O
Adam	B-HyperparameterValue
(Kingma	O
and	O
Ba,	O
2015)	O
using	O
the	O
following	O
parameters:	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9,	B-HyperparameterValue
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999,	B-HyperparameterValue
ǫ	B-HyperparameterName
=	O
1e-6	B-HyperparameterValue
and	O
L	B-HyperparameterName
2	I-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01.	B-HyperparameterValue
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
warmed	O
up	O
over	O
the	O
first	O
10,000	O
steps	O
to	O
a	O
peak	O
value	O
of	O
1e-4,	B-HyperparameterValue
and	O
then	O
linearly	O
decayed.	O
BERT	B-MethodName
trains	O
with	O
a	O
dropout	B-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers	O
and	O
attention	O
weights,	O
and	O
a	O
GELU	B-HyperparameterValue
activation	B-HyperparameterName
function	I-HyperparameterName
(Hendrycks	O
and	O
Gimpel,	O
2016).	O
Models	O
are	O
pretrained	O
for	O
S	B-HyperparameterName
=	O
1,000,000	B-HyperparameterValue
updates,	O
with	O
minibatches	B-HyperparameterName
containing	O
B	O
=	O
256	B-HyperparameterValue
sequences	O
of	O
maximum	O
length	O
T	O
=	O
512	O
tokens.	O

During	O
pretraining,	O
BERT	B-MethodName
uses	O
two	O
objectives:	O
masked	O
language	O
modeling	O
and	O
next	B-TaskName
sentence	I-TaskName
prediction.	I-TaskName
Masked	O
Language	O
Model	O
(MLM)	O
A	O
random	O
sample	O
of	O
the	O
tokens	O
in	O
the	O
input	O
sequence	O
is	O
selected	O
and	O
replaced	O
with	O
the	O
special	O
token	O
[MASK	O
].	O
The	O
MLM	O
objective	O
is	O
a	O
cross-entropy	B-MetricName
loss	O
on	O
predicting	O
the	O
masked	O
tokens.	O
BERT	B-MethodName
uniformly	O
selects	O
15%	B-MetricValue
of	O
the	O
input	O
tokens	O
for	O
possible	O
replacement.	O
Of	O
the	O
selected	O
tokens,	O
80%	B-MetricValue
are	O
replaced	O
with	O
[MASK	O
],	O
10%	B-MetricValue
are	O
left	O
unchanged,	O
and	O
10%	B-MetricValue
are	O
replaced	O
by	O
a	O
randomly	O
selected	O
vocabulary	O
token.	O
In	O
the	O
original	O
implementation,	O
random	O
masking	O
and	O
replacement	O
is	O
performed	O
once	O
in	O
the	O
beginning	O
and	O
saved	O
for	O
the	O
duration	O
of	O
training,	O
although	O
in	O
practice,	O
data	O
is	O
duplicated	O
so	O
the	O
mask	O
is	O
not	O
always	O
the	O
same	O
for	O
every	O
training	O
sentence	O
(see	O
Section	O
4.1).	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
(NSP)	O
NSP	B-TaskName
is	O
a	O
binary	O
classification	O
loss	O
for	O
predicting	O
whether	O
two	O
segments	O
follow	O
each	O
other	O
in	O
the	O
original	O
text.	O
Positive	O
examples	O
are	O
created	O
by	O
taking	O
consecutive	O
sentences	O
from	O
the	O
text	O
corpus.	O
Negative	O
examples	O
are	O
created	O
by	O
pairing	O
segments	O
from	O
different	O
documents.	O
Positive	O
and	O
negative	O
examples	O
are	O
sampled	O
with	O
equal	O
probability.	O
The	O
NSP	B-TaskName
objective	O
was	O
designed	O
to	O
improve	O
performance	O
on	O
downstream	O
tasks,	O
such	O
as	O
Natural	O
Language	O
Inference	O
(Bowman	O
et	O
al.,	O
2015),	O
which	O
require	O
reasoning	O
about	O
the	O
relationships	O
between	O
pairs	O
of	O
sentences.	O

BERT	B-MethodName
uses	O
the	O
now	O
ubiquitous	O
transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017),	O
which	O
we	O
will	O
not	O
review	O
in	O
detail.	O
We	O
use	O
a	O
transformer	O
architecture	O
with	O
L	O
layers.	O
Each	O
block	O
uses	O
A	O
self-attention	O
heads	O
and	O
hidden	O
dimension	O
H.	O

BERT	B-MethodName
takes	O
as	O
input	O
a	O
concatenation	O
of	O
two	O
segments	O
(sequences	O
of	O
tokens),	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
and	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
.	O
Segments	O
usually	O
consist	O
of	O
more	O
than	O
one	O
natural	O
sentence.	O
The	O
two	O
segments	O
are	O
presented	O
as	O
a	O
single	O
input	O
sequence	O
to	O
BERT	B-MethodName
with	O
special	O
tokens	O
delimiting	O
them:	O
[CLS	O
],	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
,	O
[SEP	O
],	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
,	O
[EOS	O
].	O
M	O
and	O
N	O
are	O
constrained	O
such	O
that	O
M	O
+	O
N	O
<	O
T	O
,	O
where	O
T	O
is	O
a	O
parameter	O
that	O
controls	O
the	O
maximum	O
sequence	O
length	O
during	O
training.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
a	O
large	O
unlabeled	O
text	O
corpus	O
and	O
subsequently	O
finetuned	O
using	O
end-task	O
labeled	O
data.	O

In	O
this	O
section,	O
we	O
give	O
a	O
brief	O
overview	O
of	O
the	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
pretraining	O
approach	O
and	O
some	O
of	O
the	O
training	O
choices	O
that	O
we	O
will	O
examine	O
experimentally	O
in	O
the	O
following	O
section.	O

Self-training	O
methods	O
such	O
as	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018),	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
XLM	B-MethodName
(Lample	O
and	O
Conneau,	O
2019),	O
and	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
have	O
brought	O
significant	O
performance	O
gains,	O
but	O
it	O
can	O
be	O
challenging	O
to	O
determine	O
which	O
aspects	O
of	O
the	O
methods	O
contribute	O
the	O
most.	O
Training	O
is	O
computationally	O
expensive,	O
limiting	O
the	O
amount	O
of	O
tuning	O
that	O
can	O
be	O
done,	O
and	O
is	O
often	O
done	O
with	O
private	O
training	O
data	O
of	O
varying	O
sizes,	O
limiting	O
our	O
ability	O
to	O
measure	O
the	O
effects	O
of	O
the	O
modeling	O
advances.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019),	O
which	O
includes	O
a	O
careful	O
evaluation	O
of	O
the	O
effects	O
of	O
hyperparmeter	O
tuning	O
and	O
training	O
set	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained	O
and	O
propose	O
an	O
improved	O
recipe	O
for	O
training	O
BERT	B-MethodName
models,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
that	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
all	O
of	O
the	O
post-BERT	O
methods.	O
Our	O
modifications	O
are	O
simple,	O
they	O
include:	O
(1)	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches,	O
over	O
more	O
data;	O
(2)	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
(3)	O
training	O
on	O
longer	O
sequences;	O
and	O
(4)	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
We	O
also	O
collect	O
a	O
large	O
new	O
dataset	O
(CC-NEWS)	B-DatasetName
of	O
comparable	O
size	O
to	O
other	O
privately	O
used	O
datasets,	O
to	O
better	O
control	O
for	O
training	O
set	O
size	O
effects.	O
When	O
controlling	O
for	O
training	O
data,	O
our	O
improved	O
training	O
procedure	O
improves	O
upon	O
the	O
published	O
BERT	B-MethodName
results	O
on	O
both	O
GLUE	B-DatasetName
and	O
SQuAD.	B-DatasetName
When	O
trained	O
for	O
longer	O
over	O
additional	O
data,	O
our	O
model	O
achieves	O
a	O
score	O
of	O
88.5	B-MetricValue
on	O
the	O
public	O
GLUE	B-DatasetName
leaderboard,	O
matching	O
the	O
88.4	B-MetricValue
reported	O
by	O
Yang	O
et	O
al.	O
(2019).	O
Our	O
model	O
establishes	O
a	O
new	O
state-of-the-art	O
on	O
4/9	O
of	O
the	O
GLUE	B-DatasetName
tasks:	O
MNLI,	B-DatasetName
QNLI,	B-DatasetName
RTE	B-DatasetName
and	O
STS-B.	B-DatasetName
We	O
also	O
match	O
state-of-the-art	O
results	O
on	O
SQuAD	B-DatasetName
and	O
RACE.	B-DatasetName
Overall,	O
we	O
re-establish	O
that	O
BERT's	B-MethodName
masked	O
language	O
model	O
training	O
objective	O
is	O
competitive	O
with	O
other	O
recently	O
proposed	O
training	O
objectives	O
such	O
as	O
perturbed	O
autoregressive	O
language	O
modeling	O
(Yang	O
et	O
al.,	O
2019).	O
2	O
In	O
summary,	O
the	O
contributions	O
of	O
this	O
paper	O
are:	O
(1)	O
We	O
present	O
a	O
set	O
of	O
important	O
BERT	B-MethodName
design	O
choices	O
and	O
training	O
strategies	O
and	O
introduce	O
alternatives	O
that	O
lead	O
to	O
better	O
downstream	O
task	O
performance;	O
(2)	O
We	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
confirm	O
that	O
using	O
more	O
data	O
for	O
pretraining	O
further	O
improves	O
performance	O
on	O
downstream	O
tasks;	O
(3)	O
Our	O
training	O
improvements	O
show	O
that	O
masked	O
language	O
model	O
pretraining,	O
under	O
the	O
right	O
design	O
choices,	O
is	O
competitive	O
with	O
all	O
other	O
recently	O
published	O
methods.	O
We	O
release	O
our	O
model,	O
pretraining	O
and	O
fine-tuning	O
code	O
implemented	O
in	O
PyTorch	O
(Paszke	O
et	O
al.,	O
2017).	O

Language	O
model	O
pretraining	O
has	O
led	O
to	O
significant	O
performance	O
gains	O
but	O
careful	O
comparison	O
between	O
different	O
approaches	O
is	O
challenging.	O
Training	O
is	O
computationally	O
expensive,	O
often	O
done	O
on	O
private	O
datasets	O
of	O
different	O
sizes,	O
and,	O
as	O
we	O
will	O
show,	O
hyperparameter	O
choices	O
have	O
significant	O
impact	O
on	O
the	O
final	O
results.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019)	O
that	O
carefully	O
measures	O
the	O
impact	O
of	O
many	O
key	O
hyperparameters	O
and	O
training	O
data	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained,	O
and	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
every	O
model	O
published	O
after	O
it.	O
Our	O
best	O
model	O
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD.	B-DatasetName
These	O
results	O
highlight	O
the	O
importance	O
of	O
previously	O
overlooked	O
design	O
choices,	O
and	O
raise	O
questions	O
about	O
the	O
source	O
of	O
recently	O
reported	O
improvements.	O
We	O
release	O
our	O
models	O
and	O
code.	O
1	O

In	O
Table	O
8	O
we	O
present	O
the	O
full	O
set	O
of	O
development	O
set	O
results	O
for	O
RoBERTa.	B-MethodName
We	O
present	O
results	O
for	O
a	O
LARGE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
as	O
well	O
as	O
a	O
BASE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
BASE	I-MethodName
.	O

Appendix	O
for	O
"RoBERTa:	B-MethodName
A	O
Robustly	O
Optimized	O
BERT	O
Pretraining	O
Approach"	O

Acknowledgements	O
We	O
are	O
grateful	O
to	O
Nal	O
Kalchbrenner	O
and	O
Stephan	O
Gouws	O
for	O
their	O
fruitful	O
comments,	O
corrections	O
and	O
inspiration.	O

In	O
this	O
work,	O
we	O
presented	O
the	O
Transformer,	B-MethodName
the	O
first	O
sequence	O
transduction	O
model	O
based	O
entirely	O
on	O
attention,	O
replacing	O
the	O
recurrent	O
layers	O
most	O
commonly	O
used	O
in	O
encoder-decoder	O
architectures	O
with	O
multi-headed	O
self-attention.	O
For	O
translation	O
tasks,	O
the	O
Transformer	B-MethodName
can	O
be	O
trained	O
significantly	O
faster	O
than	O
architectures	O
based	O
on	O
recurrent	O
or	O
convolutional	O
layers.	O
On	O
both	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
and	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
tasks,	O
we	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art.	O
In	O
the	O
former	O
task	O
our	O
best	O
model	O
outperforms	O
even	O
all	O
previously	O
reported	O
ensembles.	O
We	O
are	O
excited	O
about	O
the	O
future	O
of	O
attention-based	O
models	O
and	O
plan	O
to	O
apply	O
them	O
to	O
other	O
tasks.	O
We	O
plan	O
to	O
extend	O
the	O
Transformer	B-MethodName
to	O
problems	O
involving	O
input	O
and	O
output	O
modalities	O
other	O
than	O
text	O
and	O
to	O
investigate	O
local,	O
restricted	O
attention	O
mechanisms	O
to	O
efficiently	O
handle	O
large	O
inputs	O
and	O
outputs	O
such	O
as	O
images,	O
audio	O
and	O
video.	O
Making	O
generation	O
less	O
sequential	O
is	O
another	O
research	O
goals	O
of	O
ours.	O
The	O
code	O
we	O
used	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
is	O
available	O
at	O
tensorflow/tensor2tensor.	O

To	O
evaluate	O
if	O
the	O
Transformer	B-MethodName
can	O
generalize	O
to	O
other	O
tasks	O
we	O
performed	O
experiments	O
on	O
English	B-TaskName
constituency	I-TaskName
parsing.	I-TaskName
This	O
task	O
presents	O
specific	O
challenges:	O
the	O
output	O
is	O
subject	O
to	O
strong	O
structural	O
constraints	O
and	O
is	O
significantly	O
longer	O
than	O
the	O
input.	O
Furthermore,	O
RNN	O
sequence-to-sequence	O
models	O
have	O
not	O
been	O
able	O
to	O
attain	O
state-of-the-art	O
results	O
in	O
small-data	O
regimes	O
.	O
We	O
trained	O
a	O
4-layer	O
transformer	O
with	O
d	O
model	O
=	O
1024	O
on	O
the	O
Wall	B-DatasetName
Street	I-DatasetName
Journal	I-DatasetName
(WSJ)	I-DatasetName
portion	O
of	O
the	O
Penn	B-DatasetName
Treebank	I-DatasetName
,	O
about	O
40K	O
training	O
sentences.	O
We	O
also	O
trained	O
it	O
in	O
a	O
semi-supervised	O
setting,	O
using	O
the	O
larger	O
high-confidence	O
and	O
BerkleyParser	B-DatasetName
corpora	I-DatasetName
from	O
with	O
approximately	O
17M	O
sentences	O
.	O
We	O
used	O
a	O
vocabulary	B-HyperparameterName
of	O
16K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
WSJ	O
only	O
setting	O
and	O
a	O
vocabulary	B-HyperparameterName
of	O
32K	B-HyperparameterValue
tokens	I-HyperparameterValue
for	O
the	O
semi-supervised	O
setting.	O
We	O
performed	O
only	O
a	O
small	O
number	O
of	O
experiments	O
to	O
select	O
the	O
dropout,	B-HyperparameterName
both	O
attention	O
and	O
residual	O
(section	O
5.4),	O
learning	B-HyperparameterName
rates	I-HyperparameterName
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
on	O
the	O
Section	O
22	O
development	O
set,	O
all	O
other	O
parameters	O
remained	O
unchanged	O
from	O
the	O
English-to-German	O
base	O
translation	O
model.	O
During	O
inference,	O
we	O
increased	O
the	O
maximum	O
output	O
length	O
to	O
input	O
length	O
+	O
300.	O
We	O
used	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
21	B-HyperparameterValue
and	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
for	O
both	O
WSJ	O
only	O
and	O
the	O
semi-supervised	O
setting.	O
Our	O
results	O
in	O
Table	O
4	O
show	O
that	O
despite	O
the	O
lack	O
of	O
task-specific	O
tuning	O
our	O
model	O
performs	O
surprisingly	O
well,	O
yielding	O
better	O
results	O
than	O
all	O
previously	O
reported	O
models	O
with	O
the	O
exception	O
of	O
the	O
Recurrent	O
Neural	O
Network	O
Grammar	O
.	O
In	O
contrast	O
to	O
RNN	O
sequence-to-sequence	O
models	O
,	O
the	O
Transformer	B-MethodName
outperforms	O
the	O
Berkeley-Parser	B-MethodName
even	O
when	O
training	O
only	O
on	O
the	O
WSJ	B-DatasetName
training	O
set	O
of	O
40K	O
sentences.	O

To	O
evaluate	O
the	O
importance	O
of	O
different	O
components	O
of	O
the	O
Transformer,	B-MethodName
we	O
varied	O
our	O
base	O
model	O
in	O
different	O
ways,	O
measuring	O
the	O
change	O
in	O
performance	O
on	O
English-to-German	B-TaskName
translation	I-TaskName
on	O
the	O
development	O
set,	O
newstest2013.	B-DatasetName
We	O
used	O
beam	B-MethodName
search	I-MethodName
as	O
described	O
in	O
the	O
previous	O
section,	O
but	O
no	O
checkpoint	O
averaging.	O
We	O
present	O
these	O
results	O
in	O
Table	O
3.	O
In	O
Table	O
3	O
rows	O
(A),	O
we	O
vary	O
the	O
number	O
of	O
attention	O
heads	O
and	O
the	O
attention	O
key	O
and	O
value	O
dimensions,	O
keeping	O
the	O
amount	O
of	O
computation	O
constant,	O
as	O
described	O
in	O
Section	O
3.2.2.	O
While	O
single-head	O
attention	O
is	O
0.9	B-MetricValue
BLEU	B-MetricName
worse	O
than	O
the	O
best	O
setting,	O
quality	O
also	O
drops	O
off	O
with	O
too	O
many	O
heads.	O
In	O
Table	O
3	O
rows	O
(B),	O
we	O
observe	O
that	O
reducing	O
the	O
attention	O
key	O
size	O
d	O
k	O
hurts	O
model	O
quality.	O
This	O
suggests	O
that	O
determining	O
compatibility	O
is	O
not	O
easy	O
and	O
that	O
a	O
more	O
sophisticated	O
compatibility	O
function	O
than	O
dot	O
product	O
may	O
be	O
beneficial.	O
We	O
further	O
observe	O
in	O
rows	O
(C)	O
and	O
(D)	O
that,	O
as	O
expected,	O
bigger	O
models	O
are	O
better,	O
and	O
dropout	O
is	O
very	O
helpful	O
in	O
avoiding	O
over-fitting.	O
In	O
row	O
(E)	O
we	O
replace	O
our	O
sinusoidal	O
positional	O
encoding	O
with	O
learned	O
positional	O
embeddings	O
,	O
and	O
observe	O
nearly	O
identical	O
results	O
to	O
the	O
base	O
model.	O

On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-German	I-DatasetName
translation	O
task,	O
the	O
big	O
transformer	B-MethodName
model	O
(Transformer	B-MethodName
(big)	I-MethodName
in	O
Table	O
2)	O
outperforms	O
the	O
best	O
previously	O
reported	O
models	O
(including	O
ensembles)	O
by	O
more	O
than	O
2.0	B-MetricValue
BLEU,	B-MetricName
establishing	O
a	O
new	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
28.4.	B-MetricValue
The	O
configuration	O
of	O
this	O
model	O
is	O
listed	O
in	O
the	O
bottom	O
line	O
of	O
Table	O
3.	O
Training	O
took	O
3.5	O
days	O
on	O
8	O
P100	O
GPUs.	O
Even	O
our	O
base	O
model	O
surpasses	O
all	O
previously	O
published	O
models	O
and	O
ensembles,	O
at	O
a	O
fraction	O
of	O
the	O
training	O
cost	O
of	O
any	O
of	O
the	O
competitive	O
models.	O
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
big	O
model	O
achieves	O
a	O
BLEU	B-MetricName
score	O
of	O
41.0,	B-MetricValue
outperforming	O
all	O
of	O
the	O
previously	O
published	O
single	O
models,	O
at	O
less	O
than	O
1/4	O
the	O
training	O
cost	O
of	O
the	O
previous	O
state-of-the-art	O
model.	O
The	O
Transformer	O
(big)	O
model	O
trained	O
for	O
English-to-French	O
used	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
P	O
drop	B-HyperparameterName
=	O
0.1,	B-HyperparameterValue
instead	O
of	O
0.3.	B-HyperparameterValue
For	O
the	O
base	O
models,	O
we	O
used	O
a	O
single	O
model	O
obtained	O
by	O
averaging	O
the	O
last	O
5	O
checkpoints,	O
which	O
were	O
written	O
at	O
10-minute	O
intervals.	O
For	O
the	O
big	O
models,	O
we	O
averaged	O
the	O
last	O
20	O
checkpoints.	O
We	O
used	O
beam	B-MethodName
search	I-MethodName
with	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
4	B-HyperparameterValue
and	O
length	B-HyperparameterName
penalty	I-HyperparameterName
α	B-HyperparameterValue
=	I-HyperparameterValue
0.6	I-HyperparameterValue
.	O
These	O
hyperparameters	O
were	O
chosen	O
after	O
experimentation	O
on	O
the	O
development	O
set.	O
We	O
set	O
the	O
maximum	O
output	O
length	O
during	O
inference	O
to	O
input	O
length	O
+	O
50,	O
but	O
terminate	O
early	O
when	O
possible	O
.	O

We	O
employ	O
three	O
types	O
of	O
regularization	O
during	O
training:	O
Residual	O
Dropout	B-HyperparameterName
We	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
output	O
of	O
each	O
sub-layer,	O
before	O
it	O
is	O
added	O
to	O
the	O
sub-layer	O
input	O
and	O
normalized.	O
In	O
addition,	O
we	O
apply	O
dropout	B-HyperparameterName
to	O
the	O
sums	O
of	O
the	O
embeddings	O
and	O
the	O
positional	O
encodings	O
in	O
both	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
For	O
the	O
base	O
model,	O
we	O
use	O
a	O
rate	O
of	O
P	O
drop	B-HyperparameterName
=	O
0.1.	B-HyperparameterValue
Label	B-HyperparameterName
Smoothing	I-HyperparameterName
During	O
training,	O
we	O
employed	O
label	O
smoothing	O
of	O
value	O
ls	B-HyperparameterName
=	O
0.1	B-HyperparameterValue
.	O
This	O
hurts	O
perplexity,	B-MetricName
as	O
the	O
model	O
learns	O
to	O
be	O
more	O
unsure,	O
but	O
improves	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
score.	O
6	B-MetricValue
Results	O

We	O
used	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
β	O
1	O
=	O
0.9,	O
β	O
2	O
=	O
0.98	O
and	O
=	O
10	O
−9	O
.	O
We	O
varied	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
over	O
the	O
course	O
of	O
training,	O
according	O
to	O
the	O
formula:	O
lrate	O
=	O
d	O
−0.5	O
model	O
•	O
min(step_num	O
−0.5	O
,	O
step_num	O
•	O
warmup_steps	O
−1.5	O
)(3)	O
This	O
corresponds	O
to	O
increasing	O
the	O
learning	O
rate	O
linearly	O
for	O
the	O
first	O
warmup_steps	O
training	O
steps,	O
and	O
decreasing	O
it	O
thereafter	O
proportionally	O
to	O
the	O
inverse	O
square	O
root	O
of	O
the	O
step	O
number.	O
We	O
used	O
warmup_steps	B-HyperparameterName
=	O
4000.	B-HyperparameterValue

We	O
trained	O
our	O
models	O
on	O
one	O
machine	O
with	O
8	O
NVIDIA	O
P100	O
GPUs.	O
For	O
our	O
base	O
models	O
using	O
the	O
hyperparameters	O
described	O
throughout	O
the	O
paper,	O
each	O
training	O
step	O
took	O
about	O
0.4	O
seconds.	O
We	O
trained	O
the	O
base	O
models	O
for	O
a	O
total	O
of	O
100,000	B-HyperparameterValue
steps	B-HyperparameterName
or	O
12	O
hours.	O
For	O
our	O
big	O
models,(described	O
on	O
the	O
bottom	O
line	O
of	O
table	O
3),	O
step	O
time	O
was	O
1.0	O
seconds.	O
The	O
big	O
models	O
were	O
trained	O
for	O
300,000	B-HyperparameterValue
steps	B-HyperparameterName
(3.5	O
days).	O

We	O
trained	O
on	O
the	O
standard	O
WMT	B-DatasetName
2014	I-DatasetName
English-German	I-DatasetName
dataset	O
consisting	O
of	O
about	O
4.5	O
million	O
sentence	O
pairs.	O
Sentences	O
were	O
encoded	O
using	O
byte-pair	O
encoding	O
,	O
which	O
has	O
a	O
shared	O
sourcetarget	O
vocabulary	O
of	O
about	O
37000	O
tokens.	O
For	O
English-French,	O
we	O
used	O
the	O
significantly	O
larger	O
WMT	B-DatasetName
2014	I-DatasetName
English-French	I-DatasetName
dataset	O
consisting	O
of	O
36M	O
sentences	O
and	O
split	O
tokens	O
into	O
a	O
32000	O
word-piece	O
vocabulary	O
.	O
Sentence	O
pairs	O
were	O
batched	O
together	O
by	O
approximate	O
sequence	O
length.	O
Each	O
training	O
batch	O
contained	O
a	O
set	O
of	O
sentence	O
pairs	O
containing	O
approximately	O
25000	O
source	O
tokens	O
and	O
25000	O
target	O
tokens.	O

This	O
section	O
describes	O
the	O
training	O
regime	O
for	O
our	O
models.	O

In	O
this	O
section	O
we	O
compare	O
various	O
aspects	O
of	O
self-attention	O
layers	O
to	O
the	O
recurrent	O
and	O
convolutional	O
layers	O
commonly	O
used	O
for	O
mapping	O
one	O
variable-length	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
another	O
sequence	O
of	O
equal	O
length	O
(z	O
1	O
,	O
...,	O
z	O
n	O
),	O
with	O
x	O
i	O
,	O
z	O
i	O
∈	O
R	O
d	O
,	O
such	O
as	O
a	O
hidden	O
layer	O
in	O
a	O
typical	O
sequence	O
transduction	O
encoder	O
or	O
decoder.	O
Motivating	O
our	O
use	O
of	O
self-attention	O
we	O
consider	O
three	O
desiderata.	O
One	O
is	O
the	O
total	O
computational	O
complexity	O
per	O
layer.	O
Another	O
is	O
the	O
amount	O
of	O
computation	O
that	O
can	O
be	O
parallelized,	O
as	O
measured	O
by	O
the	O
minimum	O
number	O
of	O
sequential	O
operations	O
required.	O
The	O
third	O
is	O
the	O
path	O
length	O
between	O
long-range	O
dependencies	O
in	O
the	O
network.	O
Learning	O
long-range	O
dependencies	O
is	O
a	O
key	O
challenge	O
in	O
many	O
sequence	O
transduction	O
tasks.	O
One	O
key	O
factor	O
affecting	O
the	O
ability	O
to	O
learn	O
such	O
dependencies	O
is	O
the	O
length	O
of	O
the	O
paths	O
forward	O
and	O
backward	O
signals	O
have	O
to	O
traverse	O
in	O
the	O
network.	O
The	O
shorter	O
these	O
paths	O
between	O
any	O
combination	O
of	O
positions	O
in	O
the	O
input	O
and	O
output	O
sequences,	O
the	O
easier	O
it	O
is	O
to	O
learn	O
long-range	O
dependencies	O
.	O
Hence	O
we	O
also	O
compare	O
the	O
maximum	O
path	O
length	O
between	O
any	O
two	O
input	O
and	O
output	O
positions	O
in	O
networks	O
composed	O
of	O
the	O
different	O
layer	O
types.	O
As	O
noted	O
in	O
Table	O
1,	O
a	O
self-attention	O
layer	O
connects	O
all	O
positions	O
with	O
a	O
constant	O
number	O
of	O
sequentially	O
executed	O
operations,	O
whereas	O
a	O
recurrent	O
layer	O
requires	O
O(n)	O
sequential	O
operations.	O
In	O
terms	O
of	O
computational	O
complexity,	O
self-attention	O
layers	O
are	O
faster	O
than	O
recurrent	O
layers	O
when	O
the	O
sequence	O
length	O
n	O
is	O
smaller	O
than	O
the	O
representation	O
dimensionality	O
d,	O
which	O
is	O
most	O
often	O
the	O
case	O
with	O
sentence	O
representations	O
used	O
by	O
state-of-the-art	O
models	O
in	O
machine	O
translations,	O
such	O
as	O
word-piece	O
and	O
byte-pair	O
representations.	O
To	O
improve	O
computational	O
performance	O
for	O
tasks	O
involving	O
very	O
long	O
sequences,	O
self-attention	O
could	O
be	O
restricted	O
to	O
considering	O
only	O
a	O
neighborhood	O
of	O
size	O
r	O
in	O
the	O
input	O
sequence	O
centered	O
around	O
the	O
respective	O
output	O
position.	O
This	O
would	O
increase	O
the	O
maximum	O
path	O
length	O
to	O
O(n/r).	O
We	O
plan	O
to	O
investigate	O
this	O
approach	O
further	O
in	O
future	O
work.	O
A	O
single	O
convolutional	O
layer	O
with	O
kernel	O
width	O
k	O
<	O
n	O
does	O
not	O
connect	O
all	O
pairs	O
of	O
input	O
and	O
output	O
positions.	O
Doing	O
so	O
requires	O
a	O
stack	O
of	O
O(n/k)	O
convolutional	O
layers	O
in	O
the	O
case	O
of	O
contiguous	O
kernels,	O
or	O
O(log	O
k	O
(n))	O
in	O
the	O
case	O
of	O
dilated	O
convolutions	O
,	O
increasing	O
the	O
length	O
of	O
the	O
longest	O
paths	O
between	O
any	O
two	O
positions	O
in	O
the	O
network.	O
Convolutional	O
layers	O
are	O
generally	O
more	O
expensive	O
than	O
recurrent	O
layers,	O
by	O
a	O
factor	O
of	O
k.	O
Separable	O
convolutions	O
,	O
however,	O
decrease	O
the	O
complexity	O
considerably,	O
to	O
O(k	O
•	O
n	O
•	O
d	O
+	O
n	O
•	O
d	O
2	O
)	O
.	O
Even	O
with	O
k	O
=	O
n,	O
however,	O
the	O
complexity	O
of	O
a	O
separable	O
convolution	O
is	O
equal	O
to	O
the	O
combination	O
of	O
a	O
self-attention	O
layer	O
and	O
a	O
point-wise	O
feed-forward	O
layer,	O
the	O
approach	O
we	O
take	O
in	O
our	O
model.	O
As	O
side	O
benefit,	O
self-attention	O
could	O
yield	O
more	O
interpretable	O
models.	O
We	O
inspect	O
attention	O
distributions	O
from	O
our	O
models	O
and	O
present	O
and	O
discuss	O
examples	O
in	O
the	O
appendix.	O
Not	O
only	O
do	O
individual	O
attention	O
heads	O
clearly	O
learn	O
to	O
perform	O
different	O
tasks,	O
many	O
appear	O
to	O
exhibit	O
behavior	O
related	O
to	O
the	O
syntactic	O
and	O
semantic	O
structure	O
of	O
the	O
sentences.	O

Since	O
our	O
model	O
contains	O
no	O
recurrence	O
and	O
no	O
convolution,	O
in	O
order	O
for	O
the	O
model	O
to	O
make	O
use	O
of	O
the	O
order	O
of	O
the	O
sequence,	O
we	O
must	O
inject	O
some	O
information	O
about	O
the	O
relative	O
or	O
absolute	O
position	O
of	O
the	O
O(n	O
2	O
•	O
d)	O
O(1)	O
O(1)	O
Recurrent	O
O(n	O
•	O
d	O
2	O
)	O
O(n)	O
O(n)	O
Convolutional	O
O(k	O
•	O
n	O
•	O
d	O
2	O
)	O
O(1)	O
O(log	O
k	O
(n))	O
Self-Attention	O
(restricted)	O
O(r	O
•	O
n	O
•	O
d)	O
O(1)	O
O(n/r)	O
tokens	O
in	O
the	O
sequence.	O
To	O
this	O
end,	O
we	O
add	O
"positional	O
encodings"	O
to	O
the	O
input	O
embeddings	O
at	O
the	O
bottoms	O
of	O
the	O
encoder	O
and	O
decoder	O
stacks.	O
The	O
positional	O
encodings	O
have	O
the	O
same	O
dimension	O
d	O
model	O
as	O
the	O
embeddings,	O
so	O
that	O
the	O
two	O
can	O
be	O
summed.	O
There	O
are	O
many	O
choices	O
of	O
positional	O
encodings,	O
learned	O
and	O
fixed	O
.	O
In	O
this	O
work,	O
we	O
use	O
sine	O
and	O
cosine	O
functions	O
of	O
different	O
frequencies:	O
P	O
E	O
(pos,2i)	O
=	O
sin(pos/10000	O
2i/dmodel	O
)	O
P	O
E	O
(pos,2i+1)	O
=	O
cos(pos/10000	O
2i/dmodel	O
)	O
where	O
pos	O
is	O
the	O
position	O
and	O
i	O
is	O
the	O
dimension.	O
That	O
is,	O
each	O
dimension	O
of	O
the	O
positional	O
encoding	O
corresponds	O
to	O
a	O
sinusoid.	O
The	O
wavelengths	O
form	O
a	O
geometric	O
progression	O
from	O
2π	O
to	O
10000	O
•	O
2π.	O
We	O
chose	O
this	O
function	O
because	O
we	O
hypothesized	O
it	O
would	O
allow	O
the	O
model	O
to	O
easily	O
learn	O
to	O
attend	O
by	O
relative	O
positions,	O
since	O
for	O
any	O
fixed	O
offset	O
k,	O
P	O
E	O
pos+k	O
can	O
be	O
represented	O
as	O
a	O
linear	O
function	O
of	O
P	O
E	O
pos	O
.	O
We	O
also	O
experimented	O
with	O
using	O
learned	O
positional	O
embeddings	O
instead,	O
and	O
found	O
that	O
the	O
two	O
versions	O
produced	O
nearly	O
identical	O
results	O
(see	O
Table	O
3	O
row	O
(E)).	O
We	O
chose	O
the	O
sinusoidal	O
version	O
because	O
it	O
may	O
allow	O
the	O
model	O
to	O
extrapolate	O
to	O
sequence	O
lengths	O
longer	O
than	O
the	O
ones	O
encountered	O
during	O
training.	O

Similarly	O
to	O
other	O
sequence	O
transduction	O
models,	O
we	O
use	O
learned	O
embeddings	O
to	O
convert	O
the	O
input	O
tokens	O
and	O
output	O
tokens	O
to	O
vectors	O
of	O
dimension	O
d	O
model	O
.	O
We	O
also	O
use	O
the	O
usual	O
learned	O
linear	O
transformation	O
and	O
softmax	O
function	O
to	O
convert	O
the	O
decoder	O
output	O
to	O
predicted	O
next-token	O
probabilities.	O
In	O
our	O
model,	O
we	O
share	O
the	O
same	O
weight	O
matrix	O
between	O
the	O
two	O
embedding	O
layers	O
and	O
the	O
pre-softmax	O
linear	O
transformation,	O
similar	O
to	O
.	O
In	O
the	O
embedding	O
layers,	O
we	O
multiply	O
those	O
weights	O
by	O
√	O
d	O
model	O
.	O

In	O
addition	O
to	O
attention	O
sub-layers,	O
each	O
of	O
the	O
layers	O
in	O
our	O
encoder	O
and	O
decoder	O
contains	O
a	O
fully	O
connected	O
feed-forward	O
network,	O
which	O
is	O
applied	O
to	O
each	O
position	O
separately	O
and	O
identically.	O
This	O
consists	O
of	O
two	O
linear	O
transformations	O
with	O
a	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
in	O
between.	O
FFN(x)	O
=	O
max(0,	O
xW	O
1	O
+	O
b	O
1	O
)W	O
2	O
+	O
b	O
2	O
(2)	O
While	O
the	O
linear	O
transformations	O
are	O
the	O
same	O
across	O
different	O
positions,	O
they	O
use	O
different	O
parameters	O
from	O
layer	O
to	O
layer.	O
Another	O
way	O
of	O
describing	O
this	O
is	O
as	O
two	O
convolutions	O
with	O
kernel	B-HyperparameterName
size	I-HyperparameterName
1.	B-HyperparameterValue
The	O
dimensionality	O
of	O
input	O
and	O
output	O
is	O
d	O
model	O
=	O
512,	O
and	O
the	O
inner-layer	O
has	O
dimensionality	O
d	O
f	O
f	O
=	O
2048.	O

The	O
Transformer	B-MethodName
uses	O
multi-head	O
attention	O
in	O
three	O
different	O
ways:	O
•	O
In	O
"encoder-decoder	O
attention"	O
layers,	O
the	O
queries	O
come	O
from	O
the	O
previous	O
decoder	O
layer,	O
and	O
the	O
memory	O
keys	O
and	O
values	O
come	O
from	O
the	O
output	O
of	O
the	O
encoder.	O
This	O
allows	O
every	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
over	O
all	O
positions	O
in	O
the	O
input	O
sequence.	O
This	O
mimics	O
the	O
typical	O
encoder-decoder	O
attention	O
mechanisms	O
in	O
sequence-to-sequence	O
models	O
such	O
as	O
.	O
•	O
The	O
encoder	O
contains	O
self-attention	O
layers.	O
In	O
a	O
self-attention	O
layer	O
all	O
of	O
the	O
keys,	O
values	O
and	O
queries	O
come	O
from	O
the	O
same	O
place,	O
in	O
this	O
case,	O
the	O
output	O
of	O
the	O
previous	O
layer	O
in	O
the	O
encoder.	O
Each	O
position	O
in	O
the	O
encoder	O
can	O
attend	O
to	O
all	O
positions	O
in	O
the	O
previous	O
layer	O
of	O
the	O
encoder.	O
•	O
Similarly,	O
self-attention	O
layers	O
in	O
the	O
decoder	O
allow	O
each	O
position	O
in	O
the	O
decoder	O
to	O
attend	O
to	O
all	O
positions	O
in	O
the	O
decoder	O
up	O
to	O
and	O
including	O
that	O
position.	O
We	O
need	O
to	O
prevent	O
leftward	O
information	O
flow	O
in	O
the	O
decoder	O
to	O
preserve	O
the	O
auto-regressive	O
property.	O
We	O
implement	O
this	O
inside	O
of	O
scaled	O
dot-product	O
attention	O
by	O
masking	O
out	O
(setting	O
to	O
−∞)	O
all	O
values	O
in	O
the	O
input	O
of	O
the	O
softmax	O
which	O
correspond	O
to	O
illegal	O
connections.	O
See	O
Figure	O
2.	O

Instead	O
of	O
performing	O
a	O
single	O
attention	O
function	O
with	O
d	O
model	O
-dimensional	O
keys,	O
values	O
and	O
queries,	O
we	O
found	O
it	O
beneficial	O
to	O
linearly	O
project	O
the	O
queries,	O
keys	O
and	O
values	O
h	O
times	O
with	O
different,	O
learned	O
linear	O
projections	O
to	O
d	O
k	O
,	O
d	O
k	O
and	O
d	O
v	O
dimensions,	O
respectively.	O
On	O
each	O
of	O
these	O
projected	O
versions	O
of	O
queries,	O
keys	O
and	O
values	O
we	O
then	O
perform	O
the	O
attention	O
function	O
in	O
parallel,	O
yielding	O
d	O
v	O
-dimensional	O
output	O
values.	O
These	O
are	O
concatenated	O
and	O
once	O
again	O
projected,	O
resulting	O
in	O
the	O
final	O
values,	O
as	O
depicted	O
in	O
Figure	O
2.	O
Multi-head	O
attention	O
allows	O
the	O
model	O
to	O
jointly	O
attend	O
to	O
information	O
from	O
different	O
representation	O
subspaces	O
at	O
different	O
positions.	O
With	O
a	O
single	O
attention	O
head,	O
averaging	O
inhibits	O
this.	O
MultiHead(Q,	O
K,	O
V	O
)	O
=	O
Concat(head	O
1	O
,	O
...,	O
head	O
h	O
)W	O
O	O
where	O
head	O
i	O
=	O
Attention(QW	O
Q	O
i	O
,	O
KW	O
K	O
i	O
,	O
V	O
W	O
V	O
i	O
)	O
Where	O
the	O
projections	O
are	O
parameter	O
matrices	O
W	O
Q	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
K	O
i	O
∈	O
R	O
dmodel×d	O
k	O
,	O
W	O
V	O
i	O
∈	O
R	O
dmodel×dv	O
and	O
W	O
O	O
∈	O
R	O
hdv×dmodel	O
.	O
In	O
this	O
work	O
we	O
employ	O
h	O
=	O
8	O
parallel	O
attention	O
layers,	O
or	O
heads.	O
For	O
each	O
of	O
these	O
we	O
use	O
d	O
k	O
=	O
d	O
v	O
=	O
d	O
model	O
/h	O
=	O
64.	O
Due	O
to	O
the	O
reduced	O
dimension	O
of	O
each	O
head,	O
the	O
total	O
computational	O
cost	O
is	O
similar	O
to	O
that	O
of	O
single-head	O
attention	O
with	O
full	O
dimensionality.	O

We	O
call	O
our	O
particular	O
attention	O
"Scaled	O
Dot-Product	O
Attention"	O
(Figure	O
2).	O
The	O
input	O
consists	O
of	O
queries	O
and	O
keys	O
of	O
dimension	O
d	O
k	O
,	O
and	O
values	O
of	O
dimension	O
d	O
v	O
.	O
We	O
compute	O
the	O
dot	O
products	O
of	O
the	O
query	O
with	O
all	O
keys,	O
divide	O
each	O
by	O
√	O
d	O
k	O
,	O
and	O
apply	O
a	O
softmax	O
function	O
to	O
obtain	O
the	O
weights	O
on	O
the	O
values.	O
In	O
practice,	O
we	O
compute	O
the	O
attention	O
function	O
on	O
a	O
set	O
of	O
queries	O
simultaneously,	O
packed	O
together	O
into	O
a	O
matrix	O
Q.	O
The	O
keys	O
and	O
values	O
are	O
also	O
packed	O
together	O
into	O
matrices	O
K	O
and	O
V	O
.	O
We	O
compute	O
the	O
matrix	O
of	O
outputs	O
as:	O
Attention(Q,	O
K,	O
V	O
)	O
=	O
softmax(	O
QK	O
T	O
√	O
d	O
k	O
)V	O
(1)	O
The	O
two	O
most	O
commonly	O
used	O
attention	O
functions	O
are	O
additive	O
attention	O
,	O
and	O
dot-product	O
(multiplicative)	O
attention.	O
Dot-product	O
attention	O
is	O
identical	O
to	O
our	O
algorithm,	O
except	O
for	O
the	O
scaling	O
factor	O
of	O
1	O
√	O
d	O
k	O
.	O
Additive	O
attention	O
computes	O
the	O
compatibility	O
function	O
using	O
a	O
feed-forward	O
network	O
with	O
a	O
single	O
hidden	O
layer.	O
While	O
the	O
two	O
are	O
similar	O
in	O
theoretical	O
complexity,	O
dot-product	O
attention	O
is	O
much	O
faster	O
and	O
more	O
space-efficient	O
in	O
practice,	O
since	O
it	O
can	O
be	O
implemented	O
using	O
highly	O
optimized	O
matrix	O
multiplication	O
code.	O
While	O
for	O
small	O
values	O
of	O
d	O
k	O
the	O
two	O
mechanisms	O
perform	O
similarly,	O
additive	O
attention	O
outperforms	O
dot	O
product	O
attention	O
without	O
scaling	O
for	O
larger	O
values	O
of	O
d	O
k	O
.	O
We	O
suspect	O
that	O
for	O
large	O
values	O
of	O
d	O
k	O
,	O
the	O
dot	O
products	O
grow	O
large	O
in	O
magnitude,	O
pushing	O
the	O
softmax	O
function	O
into	O
regions	O
where	O
it	O
has	O
extremely	O
small	O
gradients	O
4	O
.	O
To	O
counteract	O
this	O
effect,	O
we	O
scale	O
the	O
dot	O
products	O
by	O
1	O
√	O
d	O
k	O
.	O

An	O
attention	O
function	O
can	O
be	O
described	O
as	O
mapping	O
a	O
query	O
and	O
a	O
set	O
of	O
key-value	O
pairs	O
to	O
an	O
output,	O
where	O
the	O
query,	O
keys,	O
values,	O
and	O
output	O
are	O
all	O
vectors.	O
The	O
output	O
is	O
computed	O
as	O
a	O
weighted	O
sum	O
of	O
the	O
values,	O
where	O
the	O
weight	O
assigned	O
to	O
each	O
value	O
is	O
computed	O
by	O
a	O
compatibility	O
function	O
of	O
the	O
query	O
with	O
the	O
corresponding	O
key.	O

Encoder:	O
The	O
encoder	O
is	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
Each	O
layer	O
has	O
two	O
sub-layers.	O
The	O
first	O
is	O
a	O
multi-head	O
self-attention	O
mechanism,	O
and	O
the	O
second	O
is	O
a	O
simple,	O
positionwise	O
fully	O
connected	O
feed-forward	O
network.	O
We	O
employ	O
a	O
residual	O
connection	O
around	O
each	O
of	O
the	O
two	O
sub-layers,	O
followed	O
by	O
layer	O
normalization	O
.	O
That	O
is,	O
the	O
output	O
of	O
each	O
sub-layer	O
is	O
LayerNorm(x	O
+	O
Sublayer(x)),	O
where	O
Sublayer(x)	O
is	O
the	O
function	O
implemented	O
by	O
the	O
sub-layer	O
itself.	O
To	O
facilitate	O
these	O
residual	O
connections,	O
all	O
sub-layers	O
in	O
the	O
model,	O
as	O
well	O
as	O
the	O
embedding	O
layers,	O
produce	O
outputs	O
of	O
dimension	O
d	O
model	O
=	O
512.	O
Decoder:	O
The	O
decoder	O
is	O
also	O
composed	O
of	O
a	O
stack	O
of	O
N	O
=	O
6	O
identical	O
layers.	O
In	O
addition	O
to	O
the	O
two	O
sub-layers	O
in	O
each	O
encoder	O
layer,	O
the	O
decoder	O
inserts	O
a	O
third	O
sub-layer,	O
which	O
performs	O
multi-head	O
attention	O
over	O
the	O
output	O
of	O
the	O
encoder	O
stack.	O
Similar	O
to	O
the	O
encoder,	O
we	O
employ	O
residual	O
connections	O
around	O
each	O
of	O
the	O
sub-layers,	O
followed	O
by	O
layer	O
normalization.	O
We	O
also	O
modify	O
the	O
self-attention	O
sub-layer	O
in	O
the	O
decoder	O
stack	O
to	O
prevent	O
positions	O
from	O
attending	O
to	O
subsequent	O
positions.	O
This	O
masking,	O
combined	O
with	O
fact	O
that	O
the	O
output	O
embeddings	O
are	O
offset	O
by	O
one	O
position,	O
ensures	O
that	O
the	O
predictions	O
for	O
position	O
i	O
can	O
depend	O
only	O
on	O
the	O
known	O
outputs	O
at	O
positions	O
less	O
than	O
i.	O

Most	O
competitive	O
neural	O
sequence	O
transduction	O
models	O
have	O
an	O
encoder-decoder	O
structure	O
.	O
Here,	O
the	O
encoder	O
maps	O
an	O
input	O
sequence	O
of	O
symbol	O
representations	O
(x	O
1	O
,	O
...,	O
x	O
n	O
)	O
to	O
a	O
sequence	O
of	O
continuous	O
representations	O
z	O
=	O
(z	O
1	O
,	O
...,	O
z	O
n	O
).	O
Given	O
z,	O
the	O
decoder	O
then	O
generates	O
an	O
output	O
sequence	O
(y	O
1	O
,	O
...,	O
y	O
m	O
)	O
of	O
symbols	O
one	O
element	O
at	O
a	O
time.	O
At	O
each	O
step	O
the	O
model	O
is	O
auto-regressive	O
,	O
consuming	O
the	O
previously	O
generated	O
symbols	O
as	O
additional	O
input	O
when	O
generating	O
the	O
next.	O
The	O
Transformer	B-MethodName
follows	O
this	O
overall	O
architecture	O
using	O
stacked	O
self-attention	O
and	O
point-wise,	O
fully	O
connected	O
layers	O
for	O
both	O
the	O
encoder	O
and	O
decoder,	O
shown	O
in	O
the	O
left	O
and	O
right	O
halves	O
of	O
Figure	O
1,	O
respectively.	O
Figure	O
1:	O
The	O
Transformer	B-MethodName
-model	O
architecture.	O

The	O
goal	O
of	O
reducing	O
sequential	O
computation	O
also	O
forms	O
the	O
foundation	O
of	O
the	O
Extended	O
Neural	O
GPU	O
,	O
ByteNet	B-MethodName
and	O
ConvS2S	B-MethodName
,	O
all	O
of	O
which	O
use	O
convolutional	O
neural	O
networks	O
as	O
basic	O
building	O
block,	O
computing	O
hidden	O
representations	O
in	O
parallel	O
for	O
all	O
input	O
and	O
output	O
positions.	O
In	O
these	O
models,	O
the	O
number	O
of	O
operations	O
required	O
to	O
relate	O
signals	O
from	O
two	O
arbitrary	O
input	O
or	O
output	O
positions	O
grows	O
in	O
the	O
distance	O
between	O
positions,	O
linearly	O
for	O
ConvS2S	B-MethodName
and	O
logarithmically	O
for	O
ByteNet.	B-MethodName
This	O
makes	O
it	O
more	O
difficult	O
to	O
learn	O
dependencies	O
between	O
distant	O
positions	O
.	O
In	O
the	O
Transformer	B-MethodName
this	O
is	O
reduced	O
to	O
a	O
constant	O
number	O
of	O
operations,	O
albeit	O
at	O
the	O
cost	O
of	O
reduced	O
effective	O
resolution	O
due	O
to	O
averaging	O
attention-weighted	O
positions,	O
an	O
effect	O
we	O
counteract	O
with	O
Multi-Head	O
Attention	O
as	O
described	O
in	O
section	O
3.2.	O
Self-attention,	O
sometimes	O
called	O
intra-attention	O
is	O
an	O
attention	O
mechanism	O
relating	O
different	O
positions	O
of	O
a	O
single	O
sequence	O
in	O
order	O
to	O
compute	O
a	O
representation	O
of	O
the	O
sequence.	O
Self-attention	O
has	O
been	O
used	O
successfully	O
in	O
a	O
variety	O
of	O
tasks	O
including	O
reading	O
comprehension,	O
abstractive	O
summarization,	O
textual	O
entailment	O
and	O
learning	O
task-independent	O
sentence	O
representations	O
.	O
End-to-end	O
memory	O
networks	O
are	O
based	O
on	O
a	O
recurrent	O
attention	O
mechanism	O
instead	O
of	O
sequencealigned	O
recurrence	O
and	O
have	O
been	O
shown	O
to	O
perform	O
well	O
on	O
simple-language	B-TaskName
question	I-TaskName
answering	I-TaskName
and	O
language	B-TaskName
modeling	I-TaskName
tasks	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge,	O
however,	O
the	O
Transformer	B-MethodName
is	O
the	O
first	O
transduction	O
model	O
relying	O
entirely	O
on	O
self-attention	O
to	O
compute	O
representations	O
of	O
its	O
input	O
and	O
output	O
without	O
using	O
sequencealigned	O
RNNs	O
or	O
convolution.	O
In	O
the	O
following	O
sections,	O
we	O
will	O
describe	O
the	O
Transformer,	B-MethodName
motivate	O
self-attention	O
and	O
discuss	O
its	O
advantages	O
over	O
models	O
such	O
as	O
and	O
.	O

Recurrent	O
neural	O
networks,	O
long	O
short-term	O
memory	O
and	O
gated	O
recurrent	O
neural	O
networks	O
in	O
particular,	O
have	O
been	O
firmly	O
established	O
as	O
state	O
of	O
the	O
art	O
approaches	O
in	O
sequence	O
modeling	O
and	O
transduction	O
problems	O
such	O
as	B-TaskName
language	I-TaskName
modeling	I-TaskName
and	O
machine	B-TaskName
translation	I-TaskName
.	O
Numerous	O
efforts	O
have	O
since	O
continued	O
to	O
push	O
the	O
boundaries	O
of	O
recurrent	O
language	O
models	O
and	O
encoder-decoder	O
architectures	O
.	O
Recurrent	O
models	O
typically	O
factor	O
computation	O
along	O
the	O
symbol	O
positions	O
of	O
the	O
input	O
and	O
output	O
sequences.	O
Aligning	O
the	O
positions	O
to	O
steps	O
in	O
computation	O
time,	O
they	O
generate	O
a	O
sequence	O
of	O
hidden	O
states	O
h	O
t	O
,	O
as	O
a	O
function	O
of	O
the	O
previous	O
hidden	O
state	O
h	O
t−1	O
and	O
the	O
input	O
for	O
position	O
t.	O
This	O
inherently	O
sequential	O
nature	O
precludes	O
parallelization	O
within	O
training	O
examples,	O
which	O
becomes	O
critical	O
at	O
longer	O
sequence	O
lengths,	O
as	O
memory	O
constraints	O
limit	O
batching	O
across	O
examples.	O
Recent	O
work	O
has	O
achieved	O
significant	O
improvements	O
in	O
computational	O
efficiency	O
through	O
factorization	O
tricks	O
and	O
conditional	O
computation	O
,	O
while	O
also	O
improving	O
model	O
performance	O
in	O
case	O
of	O
the	O
latter.	O
The	O
fundamental	O
constraint	O
of	O
sequential	O
computation,	O
however,	O
remains.	O
Attention	O
mechanisms	O
have	O
become	O
an	O
integral	O
part	O
of	O
compelling	O
sequence	O
modeling	O
and	O
transduction	O
models	O
in	O
various	O
tasks,	O
allowing	O
modeling	O
of	O
dependencies	O
without	O
regard	O
to	O
their	O
distance	O
in	O
the	O
input	O
or	O
output	O
sequences	O
.	O
In	O
all	O
but	O
a	O
few	O
cases	O
,	O
however,	O
such	O
attention	O
mechanisms	O
are	O
used	O
in	O
conjunction	O
with	O
a	O
recurrent	O
network.	O
In	O
this	O
work	O
we	O
propose	O
the	O
Transformer,	B-MethodName
a	O
model	O
architecture	O
eschewing	O
recurrence	O
and	O
instead	O
relying	O
entirely	O
on	O
an	O
attention	O
mechanism	O
to	O
draw	O
global	O
dependencies	O
between	O
input	O
and	O
output.	O
The	O
Transformer	B-MethodName
allows	O
for	O
significantly	O
more	O
parallelization	O
and	O
can	O
reach	O
a	O
new	O
state	O
of	O
the	O
art	O
in	O
translation	O
quality	O
after	O
being	O
trained	O
for	O
as	O
little	O
as	O
twelve	O
hours	O
on	O
eight	O
P100	O
GPUs.	O

The	O
dominant	O
sequence	O
transduction	O
models	O
are	O
based	O
on	O
complex	O
recurrent	O
or	O
convolutional	O
neural	O
networks	O
that	O
include	O
an	O
encoder	O
and	O
a	O
decoder.	O
The	O
best	O
performing	O
models	O
also	O
connect	O
the	O
encoder	O
and	O
decoder	O
through	O
an	O
attention	O
mechanism.	O
We	O
propose	O
a	O
new	O
simple	O
network	O
architecture,	O
the	O
Transformer,	B-MethodName
based	O
solely	O
on	O
attention	O
mechanisms,	O
dispensing	O
with	O
recurrence	O
and	O
convolutions	O
entirely.	O
Experiments	O
on	O
two	O
machine	B-TaskName
translation	I-TaskName
tasks	O
show	O
these	O
models	O
to	O
be	O
superior	O
in	O
quality	O
while	O
being	O
more	O
parallelizable	O
and	O
requiring	O
significantly	O
less	O
time	O
to	O
train.	O
Our	O
model	O
achieves	O
28.4	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
Englishto-German	I-DatasetName
translation	O
task,	O
improving	O
over	O
the	O
existing	O
best	O
results,	O
including	O
ensembles,	O
by	O
over	O
2	B-MetricValue
BLEU.	B-MetricName
On	O
the	O
WMT	B-DatasetName
2014	I-DatasetName
English-to-French	I-DatasetName
translation	O
task,	O
our	O
model	O
establishes	O
a	O
new	O
single-model	O
state-of-the-art	O
BLEU	B-MetricName
score	O
of	O
41.8	B-MetricValue
after	O
training	O
for	O
3.5	O
days	O
on	O
eight	O
GPUs,	O
a	O
small	O
fraction	O
of	O
the	O
training	O
costs	O
of	O
the	O
best	O
models	O
from	O
the	O
literature.	O
We	O
show	O
that	O
the	O
Transformer	B-MethodName
generalizes	O
well	O
to	O
other	O
tasks	O
by	O
applying	O
it	O
successfully	O
to	O
English	B-TaskName
constituency	I-TaskName
parsing	I-TaskName
both	O
with	O
large	O
and	O
limited	O
training	O
data.	O
*	O
Equal	O
contribution.	O
Listing	O
order	O
is	O
random.	O
Jakob	O
proposed	O
replacing	O
RNNs	O
with	O
self-attention	O
and	O
started	O
the	O
effort	O
to	O
evaluate	O
this	O
idea.	O
Ashish,	O
with	O
Illia,	O
designed	O
and	O
implemented	O
the	O
first	O
Transformer	B-MethodName
models	O
and	O
has	O
been	O
crucially	O
involved	O
in	O
every	O
aspect	O
of	O
this	O
work.	O
Noam	O
proposed	O
scaled	O
dot-product	O
attention,	O
multi-head	O
attention	O
and	O
the	O
parameter-free	O
position	O
representation	O
and	O
became	O
the	O
other	O
person	O
involved	O
in	O
nearly	O
every	O
detail.	O
Niki	O
designed,	O
implemented,	O
tuned	O
and	O
evaluated	O
countless	O
model	O
variants	O
in	O
our	O
original	O
codebase	O
and	O
tensor2tensor.	O
Llion	O
also	O
experimented	O
with	O
novel	O
model	O
variants,	O
was	O
responsible	O
for	O
our	O
initial	O
codebase,	O
and	O
efficient	O
inference	O
and	O
visualizations.	O
Lukasz	O
and	O
Aidan	O
spent	O
countless	O
long	O
days	O
designing	O
various	O
parts	O
of	O
and	O
implementing	O
tensor2tensor,	O
replacing	O
our	O
earlier	O
codebase,	O
greatly	O
improving	O
results	O
and	O
massively	O
accelerating	O
our	O
research.	O
†	O
Work	O
performed	O
while	O
at	O
Google	O
Brain.	O
‡	O
Work	O
performed	O
while	O
at	O
Google	O
Research.	O

We	O
also	O
evaluated	O
performance	O
on	O
WMT16	B-DatasetName
Romanian-English,	I-DatasetName
augmented	O
with	O
back-translation	O
data	O
from	O
Sennrich	O
et	O
al.	O
(2016).	O
We	O
use	O
a	O
6-layer	O
transformer	O
source	O
encoder	O
to	O
map	O
Romanian	O
into	O
a	O
representation	O
that	O
BART	B-MethodName
is	O
able	O
to	O
de-noise	O
into	O
English,	O
following	O
the	O
approach	O
introduced	O
in	O
§3.4.	O
Experiment	O
results	O
are	O
presented	O
in	O
Table	O
6.	O
We	O
compare	O
our	O
results	O
against	O
a	O
baseline	O
Transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017)	O
with	O
Transformerlarge	O
settings	O
(the	O
baseline	O
row).	O
We	O
show	O
the	O
performance	O
of	O
both	O
steps	O
of	O
our	O
model	O
in	O
the	O
fixed	O
BART	B-MethodName
and	O
tuned	O
BART	B-MethodName
rows.	O
For	O
each	O
row	O
we	O
experiment	O
on	O
the	O
original	O
WMT16	B-DatasetName
Romanian-English	I-DatasetName
augmented	O
with	O
back-translation	O
data.	O
We	O
use	O
a	O
beam	B-HyperparameterName
width	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
α	B-HyperparameterValue
=	I-HyperparameterValue
1.	I-HyperparameterValue
Preliminary	O
results	O
suggested	O
that	O
our	O
approach	O
was	O
less	O
effective	O
without	O
back-translation	O
data,	O
and	O
prone	O
to	O
overfitting-future	O
work	O
should	O
explore	O
additional	O
regularization	O
techniques.	O

We	O
also	O
experiment	O
with	O
several	O
text	O
generation	O
tasks.	O
BART	B-MethodName
is	O
fine-tuned	O
as	O
a	O
standard	O
sequence-to-sequence	O
model	O
from	O
the	O
input	O
to	O
the	O
output	O
text.	O
During	O
finetuning	O
we	O
use	O
a	O
label	O
smoothed	O
cross	O
entropy	O
loss	O
(Pereyra	O
et	O
al.,	O
2017),	O
with	O
the	O
smoothing	B-HyperparameterName
parameter	I-HyperparameterName
set	O
to	O
0.1.	B-HyperparameterValue
During	O
generation,	O
we	O
set	O
beam	B-HyperparameterName
size	I-HyperparameterName
as	O
5,	B-HyperparameterValue
remove	O
duplicated	O
trigrams	O
in	O
beam	O
search,	O
and	O
tuned	O
the	O
model	O
with	O
min-len,	O
max-len,	O
length	O
penalty	O
on	O
the	O
validation	O
set	O
(Fan	O
et	O
al.,	O
2017	O
Summarization	O
To	O
provide	O
a	O
comparison	O
with	O
the	O
state-of-the-art	O
in	O
summarization,	O
we	O
present	O
results	O
on	O
two	O
summarization	O
datasets,	O
CNN/DailyMail	B-DatasetName
and	O
XSum,	B-DatasetName
which	O
have	O
distinct	O
properties.	O
Summaries	O
in	O
the	O
CNN/DailyMail	B-DatasetName
tend	O
to	O
resemble	O
source	O
sentences.	O
Extractive	O
models	O
do	O
well	O
here,	O
and	O
even	O
the	O
baseline	O
of	O
the	O
first-three	O
source	O
sentences	O
is	O
highly	O
competitive.	O
Nevertheless,	O
BART	O
outperforms	O
all	O
existing	O
work.	O
In	O
contrast,	O
XSum	B-MethodName
is	O
highly	O
abstractive,	O
and	O
extractive	O
models	O
perform	O
poorly.	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work,	O
which	O
leverages	O
BERT,	B-MethodName
by	O
roughly	O
6.0	B-MetricValue
points	I-MetricValue
on	O
all	O
ROUGE	B-MetricName
metrics-representing	O
a	O
significant	O
advance	O
in	O
performance	O
on	O
this	O
problem.	O
Qualitatively,	O
sample	O
quality	O
is	O
high	O
(see	O
§6).	O
Dialogue	O
We	O
evaluate	O
dialogue	O
response	O
generation	O
on	O
CONVAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
in	O
which	O
agents	O
must	O
generate	O
responses	O
conditioned	O
on	O
both	O
the	O
previous	O
context	O
and	O
a	O
textually-specified	O
persona.	O
BART	B-MethodName
outperforms	O
previous	O
work	O
on	O
two	O
automated	O
metrics.	O
Abstractive	B-TaskName
QA	I-TaskName
We	O
use	O
the	O
recently	O
proposed	O
ELI5	B-DatasetName
dataset	O
to	O
test	O
the	O
model's	O
ability	O
to	O
generate	O
long	O
freeform	O
answers.	O
We	O
find	O
BART	B-MethodName
outperforms	O
the	O
best	O
previous	O
work	O
by	O
1.2	B-MetricValue
ROUGE-L,	B-MetricName
but	O
the	O
dataset	O
remains	O
a	O
challenging,	O
because	O
answers	O
are	O
only	O
weakly	O
specified	O
by	O
the	O
question.	O

Table	O
2	O
compares	O
the	O
performance	O
of	O
BART	B-MethodName
with	O
several	O
recent	O
approaches	O
on	O
the	O
well-studied	O
SQuAD	B-TaskName
and	O
GLUE	B-TaskName
tasks	O
(Warstadt	O
et	O
al.,	O
2018;Socher	O
et	O
al.,	O
2013;Dolan	O
&	O
Brockett,	O
2005;Agirre	O
et	O
al.,	O
2007;Williams	O
et	O
al.,	O
2018;Dagan	O
et	O
al.,	O
2006;Levesque	O
et	O
al.,	O
2011).	O
The	O
most	O
directly	O
comparable	O
baseline	O
is	O
RoBERTa,	B-MethodName
which	O
was	O
pre-trained	O
with	O
the	O
same	O
resources,	O
but	O
a	O
different	O
objective.	O
Overall,	O
BART	B-MethodName
performs	O
similarly,	O
with	O
only	O
small	O
differences	O
between	O
the	O
models	O
on	O
most	O
tasks.	O
suggesting	O
that	O
BART's	B-MethodName
improvements	O
on	O
generation	O
tasks	O
do	O
not	O
come	O
at	O
the	O
expense	O
of	O
classification	O
performance.	O

We	O
pre-train	O
a	O
large	O
model	O
with	O
12	O
layers	O
in	O
each	O
of	O
the	O
encoder	O
and	O
decoder,	O
and	O
a	O
hidden	O
size	O
of	O
1024.	O
Following	O
RoBERTa	B-MethodName
,	O
we	O
use	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8000,	B-HyperparameterValue
and	O
train	O
the	O
model	O
for	O
500000	B-HyperparameterValue
steps.	B-HyperparameterName
Documents	O
are	O
tokenized	O
with	O
the	O
same	O
byte-pair	O
encoding	O
as	O
GPT-2	B-MethodName
(Radford	O
et	O
al.,	O
2019).	O
Based	O
on	O
the	O
results	O
in	O
Section	O
§4,	O
we	O
use	O
a	O
combination	O
of	O
text	O
infilling	O
and	O
sentence	O
permutation.	O
We	O
mask	O
30%	B-MetricValue
of	O
tokens	O
in	O
each	O
document,	O
and	O
permute	O
all	O
sentences.	O
Although	O
sentence	O
permutation	O
only	O
shows	O
significant	O
additive	O
gains	O
on	O
the	O
CNN/DM	B-DatasetName
summarization	I-DatasetName
dataset,	O
we	O
hypothesised	O
that	O
larger	O
pre-trained	O
models	O
may	O
be	O
better	O
able	O
to	O
learn	O
from	O
this	O
task.	O
To	O
help	O
the	O
model	O
better	O
fit	O
the	O
data,	O
we	O
disabled	O
dropout	B-HyperparameterName
for	O
the	O
final	O
10%	B-MetricValue
of	O
training	O
steps.	O
We	O
use	O
the	O
same	O
pre-training	O
data	O
as	O
,	O
consisting	O
of	O
160Gb	O
of	O
news,	O
books,	O
stories,	O
and	O
web	O
text.	O

Recent	O
work	O
has	O
shown	O
that	O
downstream	O
performance	O
can	O
dramatically	O
improve	O
when	O
pre-training	O
is	O
scaled	O
to	O
large	O
batch	O
sizes	O
(Yang	O
et	O
al.,	O
2019;	O
and	O
corpora.	O
To	O
test	O
how	O
well	O
BART	B-MethodName
performs	O
in	O
this	O
regime,	O
and	O
to	O
create	O
a	O
useful	O
model	O
for	O
downstream	O
tasks,	O
we	O
trained	O
BART	B-MethodName
using	O
the	O
same	O
scale	O
as	O
the	O
RoBERTa	B-MethodName
model.	O

BART	B-MethodName
shows	O
large	O
improvements	O
on	O
summarization	O
metrics,	O
of	O
up	O
to	O
6	O
points	O
over	O
the	O
prior	O
state-of-the-art.	O
To	O
understand	O
BART's	B-MethodName
performance	O
beyond	O
automated	O
metrics,	O
we	O
analyse	O
its	O
generations	O
qualitatively.	O
Table	O
7	O
shows	O
example	O
summaries	O
generated	O
by	O
BART.	B-MethodName
Examples	O
are	O
taken	O
from	O
WikiNews	B-DatasetName
articles	I-DatasetName
published	O
after	O
the	O
creation	O
of	O
the	O
pre-training	O
corpus,	O
to	O
eliminate	O
the	O
possibility	O
of	O
the	O
events	O
described	O
being	O
present	O
in	O
the	O
model's	O
training	O
data.	O
Following	O
Narayan	O
et	O
al.	O
(2018),	O
we	O
remove	O
the	O
first	O
sentence	O
of	O
the	O
article	O
prior	O
to	O
summarizing	O
it,	O
so	O
there	O
is	O
no	O
easy	O
extractive	O
summary	O
of	O
the	O
document.	O
Unsurprisingly,	O
model	O
output	O
is	O
fluent	O
and	O
grammatical	O
English.	O
However,	O
model	O
output	O
is	O
also	O
highly	O
abstractive,	O
with	O
few	O
phrases	O
copied	O
from	O
the	O
input.	O
The	O
output	O
is	O
also	O
generally	O
factually	O
accurate,	O
and	O
integrates	O
supporting	O
evidence	O
from	O
across	O
the	O
input	O
document	O
with	O
background	O
knowledge	O
(for	O
example,	O
correctly	O
completing	O
names,	O
or	O
inferring	O
that	O
PG&E	O
operates	O
in	O
California).	O
In	O
the	O
first	O
example,	O
inferring	O
that	O
fish	O
are	O
protecting	O
reefs	O
from	O
global	O
warming	O
requires	O
non-trivial	O
inference	O
from	O
the	O
text.	O
However,	O
the	O
claim	O
that	O
the	O
work	O
was	O
published	O
in	O
Science	O
is	O
not	O
supported	O
by	O
the	O
source.	O
These	O
samples	O
demonstrate	O
that	O
the	O
BART	B-MethodName
pretraining	O
has	O
learned	O
a	O
strong	O
combination	O
of	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
and	I-TaskName
generation.	I-TaskName

Early	O
methods	O
for	O
pretraining	O
were	O
based	O
on	O
language	O
models.	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018)	O
only	O
models	O
leftward	O
context,	O
which	O
is	O
problematic	O
for	O
some	O
tasks.	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018)	O
concatenates	O
left-only	O
and	O
right-only	O
representations,	O
but	O
does	O
not	O
pre-train	O
interactions	O
between	O
these	O
features.	O
Radford	O
et	O
al.	O
(2019)	O
demonstrated	O
that	O
very	O
large	O
language	O
models	O
can	O
act	O
as	O
unsupervised	O
multitask	O
models.	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
introduced	O
masked	O
language	O
modelling,	O
which	O
allows	O
pre-training	O
to	O
learn	O
interactions	O
between	O
left	O
and	O
right	O
context	O
words.	O
Recent	O
work	O
has	O
shown	O
that	O
very	O
strong	O
performance	O
can	O
be	O
achieved	O
by	O
training	O
for	O
longer	O
,	O
by	O
tying	O
parameters	O
across	O
layers	O
(Lan	O
et	O
al.,	O
2019),	O
and	O
by	O
masking	O
spans	O
instead	O
of	O
words	O
.	O
Predictions	O
are	O
not	O
made	O
auto-regressively,	O
reducing	O
the	O
effectiveness	O
of	O
BERT	B-MethodName
for	O
generation	O
tasks.	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019)	O
fine-tunes	O
BERT	B-MethodName
with	O
an	O
ensemble	O
of	O
masks,	O
some	O
of	O
which	O
allow	O
only	O
leftward	O
context.	O
Like	O
BART,	B-MethodName
this	O
allows	O
UniLM	B-MethodName
to	O
be	O
used	O
for	O
both	O
generative	O
and	O
discriminative	O
tasks.	O
A	O
difference	O
is	O
that	O
UniLM	B-MethodName
predictions	O
are	O
conditionally	O
independent,	O
whereas	O
BART's	B-MethodName
are	O
autoregressive.	O
BART	B-MethodName
reduces	O
the	O
mismatch	O
between	O
pre-training	O
and	O
generation	O
tasks,	O
because	O
the	O
decoder	O
is	O
always	O
trained	O
on	O
uncorrupted	O
context.	O
MASS	B-MethodName
(Song	O
et	O
al.,	O
2019)	O
is	O
perhaps	O
the	O
most	O
similar	O
model	O
to	O
BART.	B-MethodName
An	O
input	O
sequence	O
where	O
a	O
contiguous	O
span	O
of	O
tokens	O
is	O
masked	O
is	O
mapped	O
to	O
a	O
sequence	O
consisting	O
of	O
the	O
missing	O
tokens.	O
MASS	B-MethodName
is	O
less	O
effective	O
for	O
discriminative	O
tasks,	O
because	O
disjoint	O
sets	O
of	O
tokens	O
are	O
fed	O
into	O
the	O
encoder	O
and	O
decoder.	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
extends	O
BERT	B-MethodName
by	O
pre-Source	O
Document	O
(abbreviated)	O

The	O
researchers	O
examined	O
three	O
types	O
of	O
coral	O
in	O
reefs	O
off	O
the	O
coast	O
of	O
Fiji	O
...	O
The	O
researchers	O
found	O
when	O
fish	O
were	O
plentiful,	O
they	O
would	O
eat	O
algae	O
and	O
seaweed	O
off	O
the	O
corals,	O
which	O
appeared	O
to	O
leave	O
them	O
more	O
resistant	O
to	O
the	O
bacterium	O
Vibrio	O
coralliilyticus,	O
a	O
bacterium	O
associated	O
with	O
bleaching.	O
The	O
researchers	O
suggested	O
the	O
algae,	O
like	O
warming	O
temperatures,	O
might	O
render	O
the	O
corals'	O
chemical	O
defenses	O
less	O
effective,	O
and	O
the	O
fish	O
were	O
protecting	O
the	O
coral	O
by	O
removing	O
the	O
algae.	O
Fisheries	O
off	O
the	O
coast	O
of	O
Fiji	O
are	O
protecting	O
coral	O
reefs	O
from	O
the	O
effects	O
of	O
global	O
warming,	O
according	O
to	O
a	O
study	O
in	O
the	O
journal	O
Science.	O
Sacoolas,	O
who	O
has	O
immunity	O
as	O
a	O
diplomat's	O
wife,	O
was	O
involved	O
in	O
a	O
traffic	O
collision	O
...	O
Prime	O
Minister	O
Johnson	O
was	O
questioned	O
about	O
the	O
case	O
while	O
speaking	O
to	O
the	O
press	O
at	O
a	O
hospital	O
in	O
Watford.	O
He	O
said,	O
"I	O
hope	O
that	O
Anne	O
Sacoolas	O
will	O
come	O
back	O
...	O
if	O
we	O
can't	O
resolve	O
it	O
then	O
of	O
course	O
I	O
will	O
be	O
raising	O
it	O
myself	O
personally	O
with	O
the	O
White	O
House."	O
Boris	O
Johnson	O
has	O
said	O
he	O
will	O
raise	O
the	O
issue	O
of	O
US	O
diplomat	O
Anne	O
Sacoolas'	O
diplomatic	O
immunity	O
with	O
the	O
White	O
House.	O
PG&E	O
stated	O
it	O
scheduled	O
the	O
blackouts	O
in	O
response	O
to	O
forecasts	O
for	O
high	O
winds	O
amid	O
dry	O
conditions.	O
The	O
aim	O
is	O
to	O
reduce	O
the	O
risk	O
of	O
wildfires.	O
Nearly	O
800	O
thousand	O
customers	O
were	O
scheduled	O
to	O
be	O
affected	O
by	O
the	O
shutoffs	O
which	O
were	O
expected	O
to	O
last	O
through	O
at	O
least	O
midday	O
tomorrow.	O
Power	O
has	O
been	O
turned	O
off	O
to	O
millions	O
of	O
customers	O
in	O
California	O
as	O
part	O
of	O
a	O
power	O
shutoff	O
plan.	O
dicting	O
masked	O
tokens	O
auto-regressively	O
in	O
a	O
permuted	O
order.	O
This	O
objective	O
allows	O
predictions	O
to	O
condition	O
on	O
both	O
left	O
and	O
right	O
context.	O
In	O
contrast,	O
the	O
BART	B-MethodName
decoder	O
works	O
left-to-right	O
during	O
pre-training,	O
matching	O
the	O
setting	O
during	O
generation.	O
Several	O
papers	O
have	O
explored	O
using	O
pre-trained	O
representations	O
to	O
improve	O
machine	B-TaskName
translation.	I-TaskName
The	O
largest	O
improvements	O
have	O
come	O
from	O
pre-training	O
on	O
both	O
source	O
and	O
target	O
languages	O
(Song	O
et	O
al.,	O
2019;Lample	O
&	O
Conneau,	O
2019),	O
but	O
this	O
requires	O
pretraining	O
on	O
all	O
languages	O
of	O
interest.	O
Other	O
work	O
has	O
shown	O
that	O
encoders	O
can	O
be	O
improved	O
using	O
pre-trained	O
representations	O
(Edunov	O
et	O
al.,	O
2019),	O
but	O
gains	O
in	O
decoders	O
are	O
more	O
limited.	O
We	O
show	O
how	O
BART	B-MethodName
can	O
be	O
used	O
to	O
improve	O
machine	O
translation	O
decoders.	O

We	O
introduced	O
BART,	B-MethodName
a	O
pre-training	O
approach	O
that	O
learns	O
to	O
map	O
corrupted	O
documents	O
to	O
the	O
original.	O
BART	B-MethodName
achieves	O
similar	O
performance	O
to	O
RoBERTa	B-MethodName
on	O
discriminative	O
tasks,	O
while	O
achieving	O
new	O
state-of-theart	O
results	O
on	O
a	O
number	O
of	O
text	B-TaskName
generation	I-TaskName
tasks.	O
Future	O
work	O
should	O
explore	O
new	O
methods	O
for	O
corrupting	O
documents	O
for	O
pre-training,	O
perhaps	O
tailoring	O
them	O
to	O
specific	O
end	O
tasks.	O

The	O
Masked	O
Language	O
Model	O
and	O
the	O
Permuted	O
Language	O
Model	O
perform	O
less	O
well	O
than	O
others	O
on	O
generation,	O
and	O
are	O
the	O
only	O
models	O
we	O
consider	O
that	O
do	O
not	O
include	O
left-to-right	O
auto-regressive	O
language	O
modelling	O
during	O
pre-training.	O
Bidirectional	O
encoders	O
are	O
crucial	O
for	O
SQuAD	B-DatasetName
As	O
noted	O
in	O
previous	O
work	O
(Devlin	O
et	O
al.,	O
2019),	O
just	O
left-to-right	O
decoder	O
performs	O
poorly	O
on	O
SQuAD,	B-DatasetName
because	O
future	O
context	O
is	O
crucial	O
in	O
classification	O
decisions.	O
However,	O
BART	B-MethodName
achieves	O
similar	O
performance	O
with	O
only	O
half	O
the	O
number	O
of	O
bidirectional	O
layers.	O
The	O
pre-training	O
objective	O
is	O
not	O
the	O
only	O
important	O
factor	O
Our	O
Permuted	O
Language	O
Model	O
performs	O
less	O
well	O
than	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019).	O
Some	O
of	O
this	O
difference	O
is	O
likely	O
due	O
to	O
not	O
including	O
other	O
architectural	O
improvements,	O
such	O
as	O
relative-position	O
embeddings	O
or	O
segment-level	O
recurrence.	O
Pure	O
language	O
models	O
perform	O
best	O
on	O
ELI5	B-DatasetName
The	O
ELI5	B-DatasetName
dataset	O
is	O
an	O
outlier,	O
with	O
much	O
higher	O
perplexities	B-MetricName
than	O
other	O
tasks,	O
and	O
is	O
the	O
only	O
generation	O
task	O
where	O
other	O
models	O
outperform	O
BART.	B-MethodName
A	O
pure	O
language	O
model	O
performs	O
best,	O
suggesting	O
that	O
BART	B-MethodName
is	O
less	O
effective	O
when	O
the	O
output	O
is	O
only	O
loosely	O
constrained	O
by	O
the	O
input.	O
BART	B-MethodName
achieves	O
the	O
most	O
consistently	O
strong	O
performance.	O
With	O
the	O
exception	O
of	O
ELI5,	B-DatasetName
BART	B-MethodName
models	O
using	O
text-infilling	O
perform	O
well	O
on	O
all	O
tasks.	O

SQuAD	B-MethodName
(Rajpurkar	O
et	O
al.,	O
2016)a	O
an	O
extractive	O
question	O
answering	O
task	O
on	O
Wikipedia	O
paragraphs.	O
Answers	O
are	O
text	O
spans	O
extracted	O
from	O
a	O
given	O
document	O
context.	O
Similar	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
use	O
concatenated	O
question	O
and	O
context	O
as	O
input	O
to	O
the	O
encoder	O
of	O
BART,	B-MethodName
and	O
additionally	O
pass	O
them	O
to	O
the	O
decoder.	O
The	O
model	O
includes	O
classifiers	O
to	O
predict	O
the	O
start	O
and	O
end	O
indices	O
of	O
each	O
token.	O
MNLI	B-DatasetName
(Williams	O
et	O
al.,	O
2017),	O
a	O
bitext	B-TaskName
classification	I-TaskName
task	I-TaskName
to	O
predict	O
whether	O
one	O
sentence	O
entails	O
another.	O
The	O
fine-tuned	O
model	O
concatenates	O
the	O
two	O
sentences	O
with	O
appended	O
an	O
EOS	O
token,	O
and	O
passes	O
them	O
to	O
both	O
the	O
BART	B-MethodName
encoder	O
and	O
decoder.	O
In	O
contrast	O
to	O
BERT,	B-MethodName
the	O
representation	O
of	O
the	O
EOS	O
token	O
is	O
used	O
to	O
classify	O
the	O
sentences	O
relations.	O
ELI5	B-DatasetName
(Fan	O
et	O
al.,	O
2019),	O
a	O
long-form	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
dataset.	O
Models	O
generate	O
answers	O
conditioned	O
on	O
the	O
concatenation	O
of	O
a	O
question	O
and	O
supporting	O
documents.	O
XSum	B-DatasetName
(Narayan	O
et	O
al.,	O
2018),	O
a	O
news	O
summarization	O
dataset	O
with	O
highly	O
abstractive	O
summaries.	O
ConvAI2	B-DatasetName
(Dinan	O
et	O
al.,	O
2019),	O
a	O
dialogue	O
response	O
generation	O
task,	O
conditioned	O
on	O
context	O
and	O
a	O
persona.	O
(Hermann	O
et	O
al.,	O
2015),	O
a	O
news	O
summarization	O
dataset.	O
Summaries	O
here	O
are	O
typically	O
closely	O
related	O
to	O
source	O
sentences.	O

While	O
many	O
pre-training	O
objectives	O
have	O
been	O
proposed,	O
fair	O
comparisons	O
between	O
these	O
have	O
been	O
difficult	O
to	O
perform,	O
at	O
least	O
in	O
part	O
due	O
to	O
differences	O
in	O
training	O
data,	O
training	O
resources,	O
architectural	O
differences	O
between	O
models,	O
and	O
fine-tuning	O
procedures.	O
We	O
re-implement	O
strong	O
pre-training	O
approaches	O
recently	O
proposed	O
for	O
discriminative	B-TaskName
and	I-TaskName
generation	I-TaskName
tasks.	I-TaskName
We	O
aim,	O
as	O
much	O
as	O
possible,	O
to	O
control	O
for	O
differences	O
unrelated	O
to	O
the	O
pre-training	O
objective.	O
However,	O
we	O
do	O
make	O
minor	O
changes	O
to	O
the	O
learning	O
rate	O
and	O
usage	O
of	O
layer	O
normalisation	O
in	O
order	O
to	O
improve	O
performance	O
(tuning	O
these	O
separately	O
for	O
each	O
objective).	O
For	O
reference,	O
we	O
compare	O
our	O
implementations	O
with	O
published	O
numbers	O
from	O
BERT,	B-MethodName
which	O
was	O
also	O
trained	O
for	O
1M	B-HyperparameterValue
steps	B-HyperparameterName
on	O
a	O
combination	O
of	O
books	O
and	O
Wikipedia	O
data.	O
We	O
compare	O
the	O
following	O
approaches:	O
Language	O
Model	O
Similarly	O
to	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
we	O
train	O
a	O
left-to-right	O
Transformer	O
language	O
model.	O
This	O
model	O
is	O
equivalent	O
to	O
the	O
BART	B-MethodName
decoder,	O
without	O
cross-attention.	O
Permuted	O
Language	O
Model	O
Based	O
on	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
we	O
sample	O
1/6	O
of	O
the	O
tokens,	O
and	O
generate	O
them	O
in	O
a	O
random	O
order	O
autoregressively.	O
For	O
consistency	O
with	O
other	O
models,	O
we	O
do	O
not	O
implement	O
the	O
relative	O
positional	O
embeddings	O
or	O
attention	O
across	O
segments	O
from	O
XLNet.	B-MethodName
Masked	O
Language	O
Model	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
we	O
replace	O
15%	B-MetricValue
of	O
tokens	O
with	O
[MASK]	O
symbols,	O
and	O
train	O
the	O
model	O
to	O
independently	O
predict	O
the	O
original	O
tokens.	O
Multitask	O
Masked	O
Language	O
Model	O
As	O
in	O
UniLM	B-MethodName
(Dong	O
et	O
al.,	O
2019),	O
we	O
train	O
a	O
Masked	O
Language	O
Model	O
with	O
additional	O
self-attention	O
masks.	O
Self	O
attention	O
masks	O
are	O
chosen	O
randomly	O
in	O
with	O
the	O
follow	O
proportions:	O
1/6	O
left-to-right,	O
1/6	O
right-to-left,	O
1/3	O
unmasked,	O
and	O
1/3	O
with	O
the	O
first	O
50%	B-MetricValue
of	O
tokens	O
unmasked	O
and	O
a	O
left-to-right	O
mask	O
for	O
the	O
remainder.	O
Masked	B-MethodName
Seq-to-Seq	I-MethodName
Inspired	I-MethodName
by	I-MethodName
MASS	I-MethodName
(Song	O
et	O
al.,	O
2019),	O
we	O
mask	O
a	O
span	O
containing	O
50%	B-MetricValue
of	O
tokens,	O
and	O
train	O
a	O
sequence	O
to	O
sequence	O
model	O
to	O
predict	O
the	O
masked	O
tokens.	O
For	O
the	O
Permuted	O
LM,	O
Masked	B-MethodName
LM	I-MethodName
and	O
Multitask	B-MethodName
Masked	I-MethodName
LM,	I-MethodName
we	O
use	O
two-stream	O
attention	O
(Yang	O
et	O
al.,	O
2019)	O
to	O
efficiently	O
compute	O
likelihoods	O
of	O
the	O
output	O
part	O
of	O
the	O
sequence	O
(using	O
a	O
diagonal	O
self-attention	O
mask	O
on	O
the	O
output	O
to	O
predict	O
words	O
left-to-right).	O
We	O
experiment	O
with	O
(1)	O
treating	O
the	O
task	O
as	O
a	O
standard	O
sequence-to-sequence	O
problem,	O
where	O
the	O
source	O
input	O
to	O
the	O
encoder	O
and	O
the	O
target	O
is	O
the	O
decoder	O
output,	O
or	O
(2)	O
adding	O
the	O
source	O
as	O
prefix	O
to	O
the	O
target	O
in	O
the	O
decoder,	O
with	O
a	O
loss	O
only	O
on	O
the	O
target	O
part	O
of	O
the	O
sequence.	O
We	O
find	O
the	O
former	O
works	O
better	O
for	O
BART	B-MethodName
models,	O
and	O
the	O
latter	O
for	O
other	O
models.	O
To	O
most	O
directly	O
compare	O
our	O
models	O
on	O
their	O
ability	O
to	O
model	O
their	O
fine-tuning	O
objective	O
(the	O
log	O
likelihood	O
of	O
the	O
human	O
text),	O
we	O
report	O
perplexity	B-MetricName
in	O
Table	O
1.	O

BART	B-MethodName
supports	O
a	O
much	O
wider	O
range	O
of	O
noising	O
schemes	O
during	O
pre-training	O
than	O
previous	O
work.	O
We	O
compare	O
a	O
range	O
of	O
options	O
using	O
base-size	O
models	O
(6	O
encoder	O
and	O
6	B-HyperparameterValue
decoder	B-HyperparameterName
layers,	I-HyperparameterName
with	O
a	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	O
768),	B-HyperparameterValue
evaluated	O
on	O
a	O
representative	O
subset	O
of	O
the	O
tasks	O
we	O
will	O
consider	O
for	O
the	O
full	O
large	O
scale	O
experiments	O
in	O
§5.	O

We	O
also	O
explore	O
using	O
BART	B-MethodName
to	O
improve	O
machine	O
translation	O
decoders	O
for	O
translating	O
into	O
English.	O
Previous	O
work	O
Edunov	O
et	O
al.	O
(2019)	O
has	O
shown	O
that	O
models	O
can	O
be	O
improved	O
by	O
incorporating	O
pre-trained	O
encoders,	O
but	O
gains	O
from	O
using	O
pre-trained	O
language	O
models	O
in	O
decoders	O
have	O
been	O
limited.	O
We	O
show	O
that	O
it	O
is	O
possible	O
to	O
use	O
the	O
entire	O
BART	B-MethodName
model	O
(both	O
encoder	O
and	O
decoder)	O
as	O
a	O
single	O
pretrained	O
decoder	O
for	O
machine	B-TaskName
translation,	I-TaskName
by	O
adding	O
a	O
new	O
set	O
of	O
encoder	O
parameters	O
that	O
are	O
learned	O
from	O
bitext	O
(see	O
Figure	O
3b).	O
More	O
precisely,	O
we	O
replace	O
BART's	B-MethodName
encoder	O
embedding	O
layer	O
with	O
a	O
new	O
randomly	O
initialized	O
encoder.	O
The	O
model	O
is	O
trained	O
end-to-end,	O
which	O
trains	O
the	O
new	O
encoder	O
to	O
map	O
foreign	O
words	O
into	O
an	O
input	O
that	O
BART	B-MethodName
can	O
de-noise	O
to	O
English.	O
The	O
new	O
encoder	O
can	O
use	O
a	O
separate	O
vocabulary	O
from	O
the	O
original	O
BART	B-MethodName
model.	O
We	O
train	O
the	O
source	O
encoder	O
in	O
two	O
steps,	O
in	O
both	O
cases	O
backpropagating	O
the	O
cross-entropy	B-MetricName
loss	O
from	O
the	O
output	O
of	O
the	O
BART	B-MethodName
model.	O
In	O
the	O
first	O
step,	O
we	O
freeze	O
most	O
of	O
BART	B-MethodName
parameters	O
and	O
only	O
update	O
the	O
randomly	O
initialized	O
source	O
encoder,	O
the	O
BART	B-MethodName
positional	O
embeddings,	O
and	O
the	O
self-attention	O
input	O
projection	O
matrix	O
of	O
BART's	B-MethodName
encoder	O
first	O
layer.	O
In	O
the	O
second	O
step,	O
we	O
train	O
all	O
model	O
parameters	O
for	O
a	O
small	O
number	O
of	O
iterations.	O

Because	O
BART	B-MethodName
has	O
an	O
autoregressive	O
decoder,	O
it	O
can	O
be	O
directly	O
fine	O
tuned	O
for	O
sequence	O
generation	O
tasks	O
such	O
as	O
abstractive	B-TaskName
question	I-TaskName
answering	I-TaskName
and	I-TaskName
summarization.	I-TaskName
In	O
both	O
of	O
these	O
tasks,	O
information	O
is	O
copied	O
from	O
the	O
input	O
but	O
manipulated,	O
which	O
is	O
closely	O
related	O
to	O
the	O
denoising	O
pre-training	O
objective.	O
Here,	O
the	O
encoder	O
input	O
is	O
the	O
input	O
sequence,	O
and	O
the	O
decoder	O
generates	O
outputs	O
autoregressively.	O

For	O
token	B-TaskName
classification	I-TaskName
tasks,	I-TaskName
such	O
as	O
answer	O
endpoint	B-TaskName
classification	I-TaskName
for	O
SQuAD,	B-DatasetName
we	O
feed	O
the	O
complete	O
document	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
use	O
the	O
top	O
hidden	O
state	O
of	O
the	O
decoder	O
as	O
a	O
representation	O
for	O
each	O
word.	O
This	O
representation	O
is	O
used	O
to	O
classify	O
the	O
token.	O

For	O
sequence	B-TaskName
classification	I-TaskName
tasks,	O
the	O
same	O
input	O
is	O
fed	O
into	O
the	O
encoder	O
and	O
decoder,	O
and	O
the	O
final	O
hidden	O
state	O
of	O
the	O
final	O
decoder	O
token	O
is	O
fed	O
into	O
new	O
multi-class	O
linear	O
classifier.	O
This	O
approach	O
is	O
related	O
to	O
the	O
CLS	O
token	O
in	O
BERT;	B-MethodName
however	O
we	O
add	O
the	O
additional	O
token	O
to	O
the	O
end	O
so	O
that	O
representation	O
for	O
the	O
token	O
in	O
the	O
decoder	O
can	O
attend	O
to	O
decoder	O
states	O
from	O
the	O
complete	O
input	O
(Figure	O
3a).	O

The	O
representations	O
produced	O
by	O
BART	B-MethodName
can	O
be	O
used	O
in	O
several	O
ways	O
for	O
downstream	O
applications.	O

BART	B-MethodName
is	O
trained	O
by	O
corrupting	O
documents	O
and	O
then	O
optimizing	O
a	O
reconstruction	B-MetricName
loss-the	I-MetricName
cross-entropy	B-MetricName
between	O
the	O
decoder's	O
output	O
and	O
the	O
original	O
document.	O
Unlike	O
existing	O
denoising	O
autoencoders,	O
which	O
are	O
tailored	O
to	O
specific	O
noising	O
schemes,	O
BART	B-MethodName
allows	O
us	O
to	O
apply	O
any	O
type	O
of	O
document	O
corruption.	O
In	O
the	O
extreme	O
case,	O
where	O
all	O
information	O
about	O
the	O
source	O
is	O
lost,	O
BART	B-MethodName
is	O
equivalent	O
to	O
a	O
language	O
model.	O
We	O
experiment	O
with	O
several	O
previously	O
proposed	O
and	O
novel	O
transformations,	O
but	O
we	O
believe	O
there	O
is	O
a	O
significant	O
potential	O
for	O
development	O
of	O
other	O
new	O
alternatives.	O
The	O
transformations	O
we	O
used	O
are	O
summarized	O
below,	O
and	O
examples	O
are	O
shown	O
in	O
Figure	O
2.	O
Token	O
Masking	O
Following	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
random	O
tokens	O
are	O
sampled	O
and	O
replaced	O
with	O
[MASK]	O
elements.	O
Token	O
Deletion	O
Random	O
tokens	O
are	O
deleted	O
from	O
the	O
input.	O
In	O
contrast	O
to	O
token	O
masking,	O
the	O
model	O
must	O
decide	O
which	O
positions	O
are	O
missing	O
inputs.	O
Text	O
Infilling	O
A	O
number	O
of	O
text	O
spans	O
are	O
sampled,	O
with	O
span	O
lengths	O
drawn	O
from	O
a	O
Poisson	O
distribution	O
(λ	O
=	O
3).	O
Each	O
span	O
is	O
replaced	O
with	O
a	O
single	O
[MASK]	O
token.	O
0-length	O
spans	O
correspond	O
to	O
the	O
insertion	O
of	O
[MASK]	O
tokens.	O
Text	O
infilling	O
is	O
inspired	O
by	O
Span-BERT	B-MethodName
,	O
but	O
SpanBERT	B-MethodName
samples	O
span	O
lengths	O
from	O
a	O
different	O
(clamped	O
geometric)	O
distribution,	O
and	O
replaces	O
each	O
span	O
with	O
a	O
sequence	O
of	O
[MASK]	O
tokens	O
of	O
exactly	O
the	O
same	O
length.	O
Text	O
infilling	O
teaches	O
the	O
model	O
to	O
predict	O
how	O
many	O
tokens	O
are	O
missing	O
from	O
a	O
span.	O
Sentence	O
Permutation	O
A	O
document	O
is	O
divided	O
into	O
sentences	O
based	O
on	O
full	O
stops,	O
and	O
these	O
sentences	O
are	O
shuffled	O
in	O
a	O
random	O
order.	O
Document	O
Rotation	O
A	O
token	O
is	O
chosen	O
uniformly	O
at	O
random,	O
and	O
the	O
document	O
is	O
rotated	O
so	O
that	O
it	O
begins	O
with	O
that	O
token.	O
This	O
task	O
trains	O
the	O
model	O
to	O
identify	O
the	O
start	O
of	O
the	O
document.	O

BART	B-MethodName
uses	O
the	O
standard	O
sequence-to-sequence	O
Transformer	O
architecture	O
from	O
(Vaswani	O
et	O
al.,	O
2017),	O
except,	O
following	O
GPT,	B-MethodName
that	O
we	O
modify	O
ReLU	B-HyperparameterValue
activation	B-HyperparameterName
functions	I-HyperparameterName
to	O
GeLUs	B-HyperparameterValue
(Hendrycks	O
&	O
Gimpel,	O
2016)	O
and	O
initialise	O
parameters	O
from	O
N	O
(0,	O
0.02).	O
For	O
our	O
base	O
model,	O
we	O
use	O
6	O
layers	O
in	O
the	O
encoder	O
and	O
de-coder,	O
and	O
for	O
our	O
large	O
model	O
we	O
use	O
12	O
layers	O
in	O
each.	O
The	O
architecture	O
is	O
closely	O
related	O
to	O
that	O
used	O
in	O
BERT,	B-MethodName
with	O
the	O
following	O
differences:	O
(1)	O
each	O
layer	O
of	O
the	O
decoder	O
additionally	O
performs	O
cross-attention	O
over	O
the	O
final	O
hidden	O
layer	O
of	O
the	O
encoder	O
(as	O
in	O
the	O
transformer	O
sequence-to-sequence	O
model);	O
and	O
(2)	O
BERT	B-MethodName
uses	O
an	O
additional	O
feed-forward	O
network	O
before	O
wordprediction,	O
which	O
BART	B-MethodName
does	O
not.	O
In	O
total,	O
BART	B-MethodName
contains	O
roughly	O
10%	B-MetricValue
more	O
parameters	O
than	O
the	O
equivalently	O
sized	O
BERT	B-MethodName
model.	O

BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
that	O
maps	O
a	O
corrupted	O
document	O
to	O
the	O
original	O
document	O
it	O
was	O
derived	O
from.	O
It	O
is	O
implemented	O
as	O
a	O
sequence-to-sequence	O
model	O
with	O
a	O
bidirectional	O
encoder	O
over	O
corrupted	O
text	O
and	O
a	O
left-to-right	O
autoregressive	O
decoder.	O
For	O
pre-training,	O
we	O
optimize	O
the	O
negative	B-MetricName
log	I-MetricName
likelihood	I-MetricName
of	O
the	O
original	O
document.	O

Bidirectional	O
Encoder	O
A	O
B	O
C	O
D	O
E	O
A	O
_	O
B	O
_	O
E	O
<s>	O
A	O
B	O
C	O
D(	O
c)	O
BART:	O
Inputs	O
to	O
the	O
encoder	O
need	O
not	O
be	O
aligned	O
with	O
decoder	O
outputs,	O
allowing	O
arbitary	O
noise	O
transformations.	O
Here,	O
a	O
document	O
has	O
been	O
corrupted	O
by	O
replacing	O
spans	O
of	O
text	O
with	O
mask	O
symbols.	O
The	O
corrupted	O
document	O
(left)	O
is	O
encoded	O
with	O
a	O
bidirectional	O
model,	O
and	O
then	O
the	O
likelihood	O
of	O
the	O
original	O
document	O
(right)	O
is	O
calculated	O
with	O
an	O
autoregressive	O
decoder.	O
For	O
fine-tuning,	O
an	O
uncorrupted	O
document	O
is	O
input	O
to	O
both	O
the	O
encoder	O
and	O
decoder,	O
and	O
we	O
use	O
representations	O
from	O
the	O
final	O
hidden	O
state	O
of	O
the	O
decoder.	O
Figure	O
1:	O
A	O
schematic	O
comparison	O
of	O
BART	B-MethodName
with	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018).	O
English,	O
by	O
propagation	O
through	O
BART,	B-MethodName
thereby	O
using	O
BART	B-MethodName
as	O
a	O
pre-trained	O
target-side	O
language	O
model.	O
This	O
approach	O
improves	O
performance	O
over	O
a	O
strong	O
back-translation	O
MT	O
baseline	O
by	O
1.1	B-MetricValue
BLEU	B-MetricName
on	O
the	O
WMT	B-DatasetName
Romanian-English	I-DatasetName
benchmark.	O
To	O
better	O
understand	O
these	O
effects,	O
we	O
also	O
report	O
an	O
ablation	O
analysis	O
that	O
replicates	O
other	O
recently	O
proposed	O
training	O
objectives.	O
This	O
study	O
allows	O
us	O
to	O
carefully	O
control	O
for	O
a	O
number	O
of	O
factors,	O
including	O
data	O
and	O
optimization	O
parameters,	O
which	O
have	O
been	O
shown	O
to	O
be	O
as	O
important	O
for	O
overall	O
performance	O
as	O
the	O
selection	O
of	O
training	O
objectives	O
.	O
We	O
find	O
that	O
BART	B-MethodName
exhibits	O
the	O
most	O
consistently	O
strong	O
performance	O
across	O
the	O
full	O
range	O
of	O
tasks	O
we	O
consider.	O

A	O
B	O
C	O
D	O
E	O
<s>	O
A	O
B	O
C	O
D	O
(b)	O
GPT:	B-MethodName
Tokens	O
are	O
predicted	O
auto-regressively,	O
meaning	O
GPT	B-MethodName
can	O
be	O
used	O
for	O
generation.	O
However	O
words	O
can	O
only	O
condition	O
on	O
leftward	O
context,	O
so	O
it	O
cannot	O
learn	O
bidirectional	O
interactions.	O

Self-supervised	O
methods	O
have	O
achieved	O
remarkable	O
success	O
in	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
(Mikolov	O
et	O
al.,	O
2013;Peters	O
et	O
al.,	O
2018;Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;.	O
The	O
most	O
successful	O
approaches	O
have	O
been	O
variants	O
of	O
masked	O
language	O
models,	O
which	O
are	O
denoising	O
autoencoders	O
that	O
are	O
trained	O
to	O
reconstruct	O
text	O
where	O
a	O
random	O
subset	O
of	O
the	O
words	O
has	O
been	O
masked	O
out.	O
Recent	O
work	O
has	O
shown	O
gains	O
by	O
improving	O
the	O
distribution	O
of	O
masked	O
tokens	O
,	O
the	O
order	O
in	O
which	O
masked	O
tokens	O
are	O
predicted	O
(Yang	O
et	O
al.,	O
2019),	O
and	O
the	O
available	O
context	O
for	O
replacing	O
masked	O
tokens	O
(Dong	O
et	O
al.,	O
2019).	O
However,	O
these	O
methods	O
typically	O
focus	O
on	O
particular	O
types	O
of	O
end	O
tasks	O
(e.g.	O
span	B-TaskName
prediction,	I-TaskName
generation,	B-TaskName
etc.),	O
limiting	O
their	O
applicability.	O
In	O
this	O
paper,	O
we	O
present	O
BART,	B-MethodName
which	O
pre-trains	O
a	O
model	O
combining	O
Bidirectional	O
and	O
Auto-Regressive	O
Transformers.	O
BART	B-MethodName
is	O
a	O
denoising	O
autoencoder	O
built	O
with	O
a	O
sequence-to-sequence	O
model	O
that	O
is	O
applicable	O
to	O
a	O
very	O
wide	O
range	O
of	O
end	O
tasks.	O
Pretraining	O
has	O
two	O
stages	O
(1)	O
text	O
is	O
corrupted	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
a	O
sequence-to-sequence	O
model	O
is	O
learned	O
to	O
reconstruct	O
the	O
original	O
text.	O
BART	B-MethodName
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes	O
(see	O
Figure	O
1).	O
A	O
key	O
advantage	O
of	O
this	O
setup	O
is	O
the	O
noising	O
flexibility;	O
arbitrary	O
transformations	O
can	O
be	O
applied	O
to	O
the	O
original	O
text,	O
including	O
changing	O
its	O
length.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
arbitrary	O
length	O
spans	O
of	O
text	O
(including	O
zero	O
length)	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
This	O
approach	O
generalizes	O
the	O
original	O
word	O
masking	O
and	O
next	O
sentence	O
prediction	O
objectives	O
in	O
BERT	B-MethodName
by	O
forcing	O
the	O
model	O
to	O
reason	O
more	O
about	O
overall	O
sentence	O
length	O
and	O
make	O
longer	O
range	O
transformations	O
to	O
the	O
input.	O
BART	B-MethodName
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	O
generation	O
but	O
also	O
works	O
well	O
for	O
comprehension	O
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	B-MethodName
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2018)	O
and	O
SQuAD	B-DatasetName
(Rajpurkar	O
et	O
al.,	O
2016),	O
and	O
achieves	O
new	O
state-of-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	O
dialogue,	O
question	O
answering,	O
and	O
summarization	O
tasks.	O
For	O
example,	O
it	O
improves	O
performance	O
by	O
6	B-MetricValue
ROUGE	B-MetricName
over	O
previous	O
work	O
on	O
XSum	O
(Narayan	O
et	O
al.,	O
2018).	O
BART	B-MethodName
also	O
opens	O
up	O
new	O
ways	O
of	O
thinking	O
about	O
fine	O
tuning.	O
We	O
present	O
a	O
new	O
scheme	O
for	O
machine	O
translation	O
where	O
a	O
BART	B-MethodName
model	O
is	O
stacked	O
above	O
a	O
few	O
additional	O
transformer	O
layers.	O
These	O
layers	O
are	O
trained	O
to	O
essentially	O
translate	O
the	O
foreign	O
language	O
to	O
noised	O
Bidirectional	O
Encoder	O
A	O
_	O
C	O
_	O
E	O
B	O
D	O
(a)	O
BERT:	B-MethodName
Random	O
tokens	O
are	O
replaced	O
with	O
masks,	O
and	O
the	O
document	O
is	O
encoded	O
bidirectionally.	O
Missing	O
tokens	O
are	O
predicted	O
independently,	O
so	O
BERT	B-MethodName
cannot	O
easily	O
be	O
used	O
for	O
generation.	O

We	O
present	O
BART,	B-MethodName
a	O
denoising	O
autoencoder	O
for	O
pretraining	O
sequence-to-sequence	O
models.	O
BART	B-MethodName
is	O
trained	O
by	O
(1)	O
corrupting	O
text	O
with	O
an	O
arbitrary	O
noising	O
function,	O
and	O
(2)	O
learning	O
a	O
model	O
to	O
reconstruct	O
the	O
original	O
text.	O
It	O
uses	O
a	O
standard	O
Tranformer-based	O
neural	O
machine	O
translation	O
architecture	O
which,	O
despite	O
its	O
simplicity,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	B-MethodName
(due	O
to	O
the	O
bidirectional	O
encoder),	O
GPT	B-MethodName
(with	O
the	O
left-to-right	O
decoder),	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in-filling	O
scheme,	O
where	O
spans	O
of	O
text	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token.	O
BART	O
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	B-TaskName
generation	I-TaskName
but	O
also	O
works	O
well	O
for	O
comprehension	B-TaskName
tasks.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	O
with	O
comparable	O
training	O
resources	O
on	O
GLUE	B-DatasetName
and	O
SQuAD,	B-DatasetName
achieves	O
new	O
stateof-the-art	O
results	O
on	O
a	O
range	O
of	O
abstractive	B-TaskName
dialogue,	I-TaskName
question	B-TaskName
answering,	I-TaskName
and	O
summarization	O
tasks,	O
with	O
gains	O
of	O
up	O
to	O
6	B-MetricValue
ROUGE.	B-MetricName
BART	B-MethodName
also	O
provides	O
a	O
1.1	B-MetricValue
BLEU	B-MetricName
increase	O
over	O
a	O
back-translation	O
system	O
for	O
machine	O
translation,	B-TaskName
with	O
only	O
target	O
language	O
pretraining.	O
We	O
also	O
report	O
ablation	O
experiments	O
that	O
replicate	O
other	O
pretraining	O
schemes	O
within	O
the	O
BART	B-MethodName
framework,	O
to	O
better	O
measure	O
which	O
factors	O
most	O
influence	O
end-task	O
performance.	O

Task-specific	O
distillation	O
Most	O
of	O
the	O
prior	O
works	O
focus	O
on	O
building	O
task-specific	O
distillation	O
setups.	O
Tang	O
et	O
al.	O
transfer	O
fine-tune	O
classification	O
model	O
BERT	B-MethodName
to	O
an	O
LSTM-based	B-MethodName
classifier.	O
Chatterjee	O
distill	B-MethodName
BERT	I-MethodName
model	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
in	O
a	O
smaller	O
Transformer	B-MethodName
model	O
previously	O
initialized	O
from	O
BERT.	B-MethodName
In	O
the	O
present	O
work,	O
we	O
found	O
it	O
beneficial	O
to	O
use	O
a	O
general-purpose	O
pre-training	O
distillation	O
rather	O
than	O
a	O
task-specific	O
distillation.	O
Turc	O
et	O
al.	O
use	O
the	O
original	O
pretraining	O
objective	O
to	O
train	O
smaller	O
student,	O
then	O
fine-tuned	O
via	O
distillation.	B-MethodName
As	O
shown	O
in	O
the	O
ablation	O
study,	O
we	O
found	O
it	O
beneficial	O
to	O
leverage	O
the	O
teacher's	O
knowledge	O
to	O
pre-train	O
with	O
additional	O
distillation	O
signal.	O
Multi-distillation	O
Yang	O
et	O
al.	O
combine	O
the	O
knowledge	O
of	O
an	O
ensemble	O
of	O
teachers	O
using	O
multi-task	O
learning	O
to	O
regularize	O
the	O
distillation.	O
The	O
authors	O
apply	O
Multi-Task	O
Knowledge	O
Distillation	O
to	O
learn	O
a	O
compact	O
question	O
answering	O
model	O
from	O
a	O
set	O
of	O
large	O
question	B-TaskName
answering	I-TaskName
models.	O
An	O
application	O
of	O
multi-distillation	O
is	O
multi-linguality:	B-TaskName
Tsai	O
et	O
al.	O
adopts	O
a	O
similar	O
approach	O
to	O
us	O
by	O
pre-training	O
a	O
multilingual	O
model	O
from	O
scratch	O
solely	O
through	O
distillation.	O
However,	O
as	O
shown	O
in	O
the	O
ablation	O
study,	O
leveraging	O
the	O
teacher's	O
knowledge	O
with	O
initialization	O
and	O
additional	O
losses	O
leads	O
to	O
substantial	O
gains.	O
Other	O
compression	O
techniques	O
have	O
been	O
studied	O
to	O
compress	O
large	O
models.	O
Recent	O
developments	O
in	O
weights	O
pruning	O
reveal	O
that	O
it	O
is	O
possible	O
to	O
remove	O
some	O
heads	O
in	O
the	O
self-attention	O
at	O
test	O
time	O
without	O
significantly	O
degrading	O
the	O
performance	O
Michel	O
et	O
al.	O
.	O
Some	O
layers	O
can	O
be	O
reduced	O
to	O
one	O
head.	O
A	O
separate	O
line	O
of	O
study	O
leverages	O
quantization	O
to	O
derive	O
smaller	O
models	O
(Gupta	O
et	O
al.	O
).	O
Pruning	O
and	O
quantization	O
are	O
orthogonal	O
to	O
the	O
present	O
work.	O

We	O
introduced	O
DistilBERT,	B-MethodName
a	O
general-purpose	O
pre-trained	O
version	O
of	O
BERT,	B-MethodName
40%	B-MetricValue
smaller,	O
60%	B-MetricValue
faster,	O
that	O
retains	O
97%	B-MetricValue
of	O
the	O
language	O
understanding	O
capabilities.	O
We	O
showed	O
that	O
a	O
general-purpose	O
language	O
model	O
can	O
be	O
successfully	O
trained	O
with	O
distillation	O
and	O
analyzed	O
the	O
various	O
components	O
with	O
an	O
ablation	O
study.	O
We	O
further	O
demonstrated	O
that	O
DistilBERT	B-MethodName
is	O
a	O
compelling	O
option	O
for	O
edge	O
applications.	O

In	O
this	O
section,	O
we	O
investigate	O
the	O
influence	O
of	O
various	O
components	O
of	O
the	O
triple	O
loss	O
and	O
the	O
student	O
initialization	O
on	O
the	O
performances	O
of	O
the	O
distilled	O
model.	O
We	O
report	O
the	O
macro-score	O
on	O
GLUE.	B-DatasetName
Table	O
4	O
presents	O
the	O
deltas	O
with	O
the	O
full	O
triple	O
loss:	O
removing	O
the	O
Masked	B-MetricName
Language	I-MetricName
Modeling	I-MetricName
loss	I-MetricName
has	O
little	O
impact	O
while	O
the	O
two	O
distillation	O
losses	O
account	O
for	O
a	O
large	O
portion	O
of	O
the	O
performance.	O

To	O
further	O
investigate	O
the	O
speed-up/size	O
trade-off	O
of	O
DistilBERT,	B-MethodName
we	O
compare	O
(in	O
Table	O
3)	O
the	O
number	O
of	O
parameters	O
of	O
each	O
model	O
along	O
with	O
the	O
inference	O
time	O
needed	O
to	O
do	O
a	O
full	O
pass	O
on	O
the	O
STS-B	B-DatasetName
development	O
set	O
on	O
CPU	O
(Intel	O
Xeon	O
E5-2690	O
v3	O
Haswell	O
@2.9GHz)	O
using	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
1.	B-HyperparameterValue
DistilBERT	B-MethodName
has	O
40%	B-MetricValue
fewer	O
parameters	O
than	O
BERT	B-MethodName
and	O
is	O
60%	B-MetricValue
faster	O
than	O
BERT.	B-MethodName
On	O
device	O
computation	O
We	O
studied	O
whether	O
DistilBERT	B-MethodName
could	O
be	O
used	O
for	O
on-the-edge	O
applications	O
by	O
building	O
a	O
mobile	O
application	O
for	O
question	O
answering.	O
We	O
compare	O
the	O
average	O
inference	O
time	O
on	O
a	O
recent	O
smartphone	O
(iPhone	O
7	O
Plus)	O
against	O
our	O
previously	O
trained	O
question	O
answering	O
model	O
based	O
on	O
BERT-base.	B-MethodName
Excluding	O
the	O
tokenization	O
step,	O
DistilBERT	B-MethodName
is	O
71%	B-MetricValue
faster	O
than	O
BERT,	B-MethodName
and	O
the	O
whole	O
model	O
weighs	O
207	O
MB	O
(which	O
could	O
be	O
further	O
reduced	O
with	O
quantization).	O
Our	O
code	O
is	O
available	O
5	O
.	O

Downstream	O
tasks	O
We	O
further	O
study	O
the	O
performances	O
of	O
DistilBERT	B-MethodName
on	O
several	O
downstream	O
tasks	O
under	O
efficient	O
inference	O
constraints:	O
a	O
classification	O
task	O
(IMDb	O
sentiment	O
classification	O
-	O
Maas	O
et	O
al.	O
)	O
and	O
a	O
question	O
answering	O
task	O
(SQuAD	B-DatasetName
v1.1	O
-Rajpurkar	O
et	O
al.	O
).	O
As	O
shown	O
in	O
Table	O
2,	O
DistilBERT	B-MethodName
is	O
only	O
0.6%	B-MetricValue
point	O
behind	O
BERT	B-MethodName
in	O
test	O
accuracy	B-MetricName
on	O
the	O
IMDb	B-DatasetName
benchmark	O
while	O
being	O
40%	B-MetricValue
smaller.	O
On	O
SQuAD,	B-DatasetName
DistilBERT	B-MethodName
is	O
within	O
3.9	B-MetricValue
points	I-MetricValue
of	O
the	O
full	O
BERT.	B-MethodName
We	O
also	O
studied	O
whether	O
we	O
could	O
add	O
another	O
step	O
of	O
distillation	B-MethodName
during	O
the	O
adaptation	O
phase	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
on	O
SQuAD	B-DatasetName
using	O
a	O
BERT	B-MethodName
model	O
previously	O
fine-tuned	O
on	O
SQuAD	B-DatasetName
as	O
a	O
∅	O
-L	O
cos	O
-L	O
mlm	O
-2.96	O
L	O
ce	O
-∅	O
-L	O
mlm	O
-1.46	O
L	O
ce	O
-L	O
cos	O
-∅	O
-0.	O
31	O
Triple	B-MetricName
loss	I-MetricName
+	O
random	O
weights	O
initialization	O
-3.69	O
teacher	O
for	O
an	O
additional	O
term	O
in	O
the	O
loss	O
(knowledge	O
distillation).	O
In	O
this	O
setting,	O
there	O
are	O
thus	O
two	O
successive	O
steps	O
of	O
distillation,	B-MethodName
one	O
during	O
the	O
pre-training	O
phase	O
and	O
one	O
during	O
the	O
adaptation	O
phase.	O
In	O
this	O
case,	O
we	O
were	O
able	O
to	O
reach	O
interesting	O
performances	O
given	O
the	O
size	O
of	O
the	O
model:	O
79.8	O
F1	O
and	O
70.4	O
EM,	O
i.e.	O
within	O
3	O
points	O
of	O
the	O
full	O
model.	O

General	O
Language	O
Understanding	O
We	O
assess	O
the	O
language	O
understanding	O
and	O
generalization	O
capabilities	O
of	B-MethodName
DistilBERT	I-MethodName
on	O
the	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	I-DatasetName
[Wang	O
et	O
al.,	O
2018],	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
We	O
report	O
scores	O
on	O
the	O
development	O
sets	O
for	O
each	O
task	O
by	O
fine-tuning	O
DistilBERT	B-MethodName
without	O
the	O
use	O
of	O
ensembling	O
or	O
multi-tasking	O
scheme	O
for	O
fine-tuning	O
(which	O
are	O
mostly	O
orthogonal	O
to	O
the	O
present	O
work).	O
We	O
compare	O
the	O
results	O
to	O
the	O
baseline	O
provided	O
by	O
the	O
authors	O
of	O
GLUE:	B-DatasetName
an	O
ELMo	B-MethodName
(Peters	O
et	O
al.	O
)	O
encoder	O
followed	O
by	O
two	O
BiLSTMs.	B-MethodName
4	O
The	O
results	O
on	O
each	O
of	O
the	O
9	O
tasks	O
are	O
showed	O
on	O
Table	O
1	O
along	O
with	O
the	O
macro-score	O
(average	O
of	O
individual	O
scores).	O
Among	O
the	O
9	O
tasks,	O
DistilBERT	B-MethodName
is	O
always	O
on	O
par	O
or	O
improving	O
over	O
the	O
ELMo	B-MethodName
baseline	O
(up	O
to	O
19	B-MetricValue
points	I-MetricValue
of	O
accuracy	B-MetricName
on	O
STS-B).	B-DatasetName
DistilBERT	B-MethodName
also	O
compares	O
surprisingly	O
well	O
to	O
BERT,	B-MethodName
retaining	O
97%	B-MetricValue
of	O
the	O
performance	B-MetricName
with	O
40%	B-MetricValue
fewer	O
parameters.	O

In	O
addition	O
to	O
the	O
previously	O
described	O
optimization	O
and	O
architectural	O
choices,	O
an	O
important	O
element	O
in	O
our	O
training	O
procedure	O
is	O
to	O
find	O
the	O
right	O
initialization	O
for	O
the	O
sub-network	O
to	O
converge.	O
Taking	O
advantage	O
of	O
the	O
common	O
dimensionality	O
between	O
teacher	O
and	O
student	O
networks,	O
we	O
initialize	O
the	O
student	O
from	O
the	O
teacher	O
by	O
taking	O
one	O
layer	O
out	O
of	O
two.	O

Knowledge	B-MethodName
distillation	I-MethodName
[Bucila	O
et	O
al.,	O
2006,	O
Hinton	O
et	O
al.,	O
2015]	O
is	O
a	O
compression	O
technique	O
in	O
which	O
a	O
compact	O
model	O
-the	O
student	O
-is	O
trained	O
to	O
reproduce	O
the	O
behaviour	O
of	O
a	O
larger	O
model	O
-the	O
teacheror	O
an	O
ensemble	O
of	O
models.	O
In	O
supervised	O
learning,	O
a	O
classification	O
model	O
is	O
generally	O
trained	O
to	O
predict	O
an	O
instance	O
class	O
by	O
maximizing	O
the	O
estimated	O
probability	O
of	O
gold	O
labels.	O
A	O
standard	O
training	O
objective	O
thus	O
involves	O
minimizing	O
the	O
cross-entropy	O
between	O
the	O
model's	O
predicted	O
distribution	O
and	O
the	O
one-hot	O
empirical	O
distribution	O
of	O
training	O
labels.	O
A	O
model	O
performing	O
well	O
on	O
the	O
training	O
set	O
will	O
predict	O
an	O
output	O
distribution	O
with	O
high	O
probability	O
on	O
the	O
correct	O
class	O
and	O
with	O
near-zero	O
probabilities	O
on	O
other	O
classes.	O
But	O
some	O
of	O
these	O
"near-zero"	O
probabilities	O
are	O
larger	O
than	O
others	O
and	O
reflect,	O
in	O
part,	O
the	O
generalization	O
capabilities	O
of	O
the	O
model	O
and	O
how	O
well	O
it	O
will	O
perform	O
on	O
the	O
test	O
set	O
3	O
.	O
Training	O
loss	O
The	O
student	O
is	O
trained	O
with	O
a	O
distillation	B-MetricName
loss	I-MetricName
over	O
the	O
soft	O
target	O
probabilities	O
of	O
the	O
teacher:	O
L	O
ce	O
=	O
i	O
t	O
i	O
*	O
log(s	O
i	O
)	O
where	O
t	O
i	O
(resp.	O
s	O
i	O
)	O
is	O
a	O
probability	O
estimated	O
by	O
the	O
teacher	O
(resp.	O
the	O
student).	O
This	O
objective	O
results	O
in	O
a	O
rich	O
training	O
signal	O
by	O
leveraging	O
the	O
full	O
teacher	O
distribution.	O
Following	O
Hinton	O
et	O
al.	O
we	O
used	O
a	O
softmax-temperature:	B-HyperparameterName
p	O
i	O
=	O
exp(zi/T	O
)	O
j	O
exp(zj	O
/T	O
)	O
where	O
T	B-HyperparameterName
controls	O
the	O
smoothness	O
of	O
the	O
output	O
distribution	O
and	O
z	O
i	O
is	O
the	O
model	O
score	O
for	O
the	O
class	O
i.	O
The	O
same	O
temperature	O
T	O
is	O
applied	O
to	O
the	O
student	O
and	O
the	O
teacher	O
at	O
training	O
time,	O
while	O
at	O
inference,	O
T	B-HyperparameterName
is	O
set	O
to	O
1	B-HyperparameterValue
to	O
recover	O
a	O
standard	O
softmax.	O
The	O
final	O
training	O
objective	O
is	O
a	O
linear	O
combination	O
of	O
the	O
distillation	B-MetricName
loss	I-MetricName
L	I-MetricName
ce	O
with	O
the	O
supervised	O
training	O
loss,	O
in	O
our	O
case	O
the	O
masked	B-MetricName
language	I-MetricName
modeling	I-MetricName
loss	I-MetricName
L	O
mlm	O
[Devlin	O
et	O
al.,	O
2018].	O
We	O
found	O
it	O
beneficial	O
to	O
add	O
a	O
cosine	B-MetricName
embedding	I-MetricName
loss	I-MetricName
(L	I-MetricName
cos	I-MetricName
)	I-MetricName
which	O
will	O
tend	O
to	O
align	O
the	O
directions	O
of	O
the	O
student	O
and	O
teacher	O
hidden	O
states	O
vectors.	O
3	O
DistilBERT:	B-MethodName
a	O
distilled	O
version	O
of	O
BERT	B-MethodName
Student	O
architecture	O
In	O
the	O
present	O
work,	O
the	O
student	O
-DistilBERT	B-MethodName
-has	O
the	O
same	O
general	O
architecture	O
as	O
BERT.	B-MethodName
The	O
token-type	O
embeddings	O
and	O
the	O
pooler	O
are	O
removed	O
while	O
the	O
number	O
of	O
layers	O
is	O
reduced	O
by	O
a	O
factor	O
of	O
2.	O
Most	O
of	O
the	O
operations	O
used	O
in	O
the	O
Transformer	B-MethodName
architecture	O
(linear	O
layer	O
and	O
layer	O
normalisation)	O
are	O
highly	O
optimized	O
in	O
modern	O
linear	O
algebra	O
frameworks	O
and	O
our	O
investigations	O
showed	O
that	O
variations	O
on	O
the	O
last	O
dimension	O
of	O
the	O
tensor	O
(hidden	O
size	O
dimension)	O
have	O
a	O
smaller	O
impact	O
on	O
computation	O
efficiency	O
(for	O
a	O
fixed	O
parameters	O
budget)	O
than	O
variations	O
on	O
other	O
factors	O
like	O
the	O
number	O
of	O
layers.	O
Thus	O
we	O
focus	O
on	O
reducing	O
the	O
number	O
of	O
layers.	O

The	O
last	O
two	O
years	O
have	O
seen	O
the	O
rise	O
of	O
Transfer	B-MethodName
Learning	I-MethodName
approaches	O
in	O
Natural	O
Language	O
Processing	O
(NLP)	O
with	O
large-scale	O
pre-trained	O
language	O
models	O
becoming	O
a	O
basic	O
tool	O
in	O
many	O
NLP	O
tasks	O
[Devlin	O
et	O
al.,	O
2018,	O
Radford	O
et	O
al.,	O
2019.	O
While	O
these	O
models	O
lead	O
to	O
significant	O
improvement,	O
they	O
often	O
have	O
several	O
hundred	O
million	O
parameters	O
and	O
current	O
research	O
1	O
on	O
pre-trained	O
models	O
indicates	O
that	O
training	O
even	O
larger	O
models	O
still	O
leads	O
to	O
better	O
performances	O
on	O
downstream	O
tasks.	O
The	O
trend	O
toward	O
bigger	O
models	O
raises	O
several	O
concerns.	O
First	O
is	O
the	O
environmental	O
cost	O
of	O
exponentially	O
scaling	O
these	O
models'	O
computational	O
requirements	O
as	O
mentioned	O
in	O
Schwartz	O
et	O
al.	O
,	O
Strubell	O
et	O
al.	O
.	O
Second,	O
while	O
operating	O
these	O
models	O
on-device	O
in	O
real-time	O
has	O
the	O
potential	O
to	O
enable	O
novel	O
and	O
interesting	O
language	O
processing	O
applications,	O
the	O
growing	O
computational	O
and	O
memory	O
requirements	O
of	O
these	O
models	O
may	O
hamper	O
wide	O
adoption.	O
In	O
this	O
paper,	O
we	O
show	O
that	O
it	O
is	O
possible	O
to	O
reach	O
similar	O
performances	O
on	O
many	O
downstream-tasks	O
using	O
much	O
smaller	O
language	O
models	O
pre-trained	O
with	O
knowledge	O
distillation,	B-MethodName
resulting	O
in	O
models	O
that	O
are	O
lighter	O
and	O
faster	O
at	O
inference	O
time,	O
while	O
also	O
requiring	O
a	O
smaller	O
computational	O
training	O
budget.	O
Our	O
general-purpose	O
pre-trained	O
models	O
can	O
be	O
fine-tuned	O
with	O
good	O
performances	O
on	O
several	O
downstream	O
tasks,	O
keeping	O
the	O
flexibility	O
of	O
larger	O
models.	O
We	O
also	O
show	O
that	O
our	O
compressed	O
models	O
are	O
small	O
enough	O
to	O
run	O
on	O
the	O
edge,	O
e.g.	O
on	O
mobile	O
devices.	O
Using	O
a	O
triple	O
loss,	O
we	O
show	O
that	O
a	O
40%	B-MetricValue
smaller	O
Transformer	O
(Vaswani	O
et	O
al.	O
)	O
pre-trained	O
through	O
distillation	B-MethodName
via	O
the	O
supervision	O
of	O
a	O
bigger	O
Transformer	B-MethodName
language	O
model	O
can	O
achieve	O
similar	O
performance	O
on	O
a	O
variety	O
of	O
downstream	O
tasks,	O
while	O
being	O
60%	O
faster	O
at	O
inference	O
time.	O
Further	O
ablation	O
studies	O
indicate	O
that	O
all	O
the	O
components	O
of	O
the	O
triple	O
loss	O
are	O
important	O
for	O
best	O
performances.	O
We	O
have	O
made	O
the	O
trained	O
weights	O
available	O
along	O
with	O
the	O
training	O
code	O
in	O
the	O
Transformers	O
2	O
library	O
from	O
HuggingFace	O
[Wolf	O
et	O
al.,	O
2019].	O

As	O
Transfer	B-MethodName
Learning	I-MethodName
from	O
large-scale	O
pre-trained	O
models	O
becomes	O
more	O
prevalent	O
in	O
Natural	O
Language	O
Processing	O
(NLP),	O
operating	O
these	O
large	O
models	O
in	O
on-theedge	O
and/or	O
under	O
constrained	O
computational	O
training	O
or	O
inference	O
budgets	O
remains	O
challenging.	O
In	O
this	O
work,	O
we	O
propose	O
a	O
method	O
to	O
pre-train	O
a	O
smaller	O
generalpurpose	O
language	O
representation	O
model,	O
called	O
DistilBERT,	B-MethodName
which	O
can	O
then	O
be	O
finetuned	O
with	O
good	O
performances	O
on	O
a	O
wide	O
range	O
of	O
tasks	O
like	O
its	O
larger	O
counterparts.	O
While	O
most	O
prior	O
work	O
investigated	O
the	O
use	O
of	O
distillation	B-MethodName
for	O
building	O
task-specific	O
models,	O
we	O
leverage	O
knowledge	O
distillation	B-MethodName
during	O
the	O
pre-training	O
phase	O
and	O
show	O
that	O
it	O
is	O
possible	O
to	O
reduce	O
the	O
size	O
of	O
a	O
BERT	B-MethodName
model	O
by	O
40%,	B-MetricValue
while	O
retaining	O
97%	B-MetricValue
of	O
its	O
language	O
understanding	O
capabilities	O
and	O
being	O
60%	B-MetricValue
faster.	O
To	O
leverage	O
the	O
inductive	O
biases	O
learned	O
by	O
larger	O
models	O
during	O
pre-training,	O
we	O
introduce	O
a	O
triple	O
loss	O
combining	O
language	O
modeling,	O
distillation	O
and	O
cosine-distance	O
losses.	O
Our	O
smaller,	O
faster	O
and	O
lighter	O
model	O
is	O
cheaper	O
to	O
pre-train	O
and	O
we	O
demonstrate	O
its	O
capabilities	O
for	O
on-device	O
computations	O
in	O
a	O
proof-of-concept	O
experiment	O
and	O
a	O
comparative	O
on-device	O
study.	O

Byte-Pair	B-MethodName
Encoding	I-MethodName
(BPE)	I-MethodName
(Sennrich	O
et	O
al.,	O
2016)	O
is	O
a	O
hybrid	O
between	O
character-and	O
word-level	O
representations	O
that	O
allows	O
handling	O
the	O
large	O
vocabularies	O
common	O
in	O
natural	O
language	O
corpora.	O
Instead	O
of	O
full	O
words,	O
BPE	O
relies	O
on	O
subwords	O
units,	O
which	O
are	O
extracted	O
by	O
performing	O
statistical	O
analysis	O
of	O
the	O
training	O
corpus.	O
BPE	O
vocabulary	O
sizes	O
typically	O
range	O
from	O
10K-100K	O
subword	O
units.	O
However,	O
unicode	O
characters	O
can	O
account	O
for	O
a	O
sizeable	O
portion	O
of	O
this	O
vocabulary	O
when	O
modeling	O
large	O
and	O
diverse	O
corpora,	O
such	O
as	O
the	O
ones	O
considered	O
in	O
this	O
work.	O
Radford	O
et	O
al.	O
(2019)	O
introduce	O
a	O
clever	O
implementation	O
of	O
BPE	O
that	O
uses	O
bytes	O
instead	O
of	O
unicode	O
characters	O
as	O
the	O
base	O
subword	O
units.	O
Using	O
bytes	O
makes	O
it	O
possible	O
to	O
learn	O
a	O
subword	O
vocabulary	O
of	O
a	O
modest	O
size	O
(50K	O
units)	O
that	O
can	O
still	O
encode	O
any	O
input	O
text	O
without	O
introducing	O
any	O
"unknown"	O
tokens.	O
8	O
Large	O
batch	O
training	O
can	O
improve	O
training	O
efficiency	O
even	O
without	O
large	O
scale	O
parallel	O
hardware	O
through	O
gradient	O
accumulation,	O
whereby	O
gradients	O
from	O
multiple	O
mini-batches	O
are	O
accumulated	O
locally	O
before	O
each	O
optimization	O
step.	O
This	O
functionality	O
is	O
supported	O
natively	O
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
The	O
original	O
BERT	B-MethodName
implementation	O
(Devlin	O
et	O
al.,	O
2019)	O
uses	O
a	O
character-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
of	O
size	O
30K,	B-HyperparameterValue
which	O
is	O
learned	O
after	O
preprocessing	O
the	O
input	O
with	O
heuristic	O
tokenization	O
rules.	O
Following	O
Radford	O
et	O
al.	O
(2019),	O
we	O
instead	O
consider	O
training	O
BERT	B-MethodName
with	O
a	O
larger	O
byte-level	O
BPE	B-HyperparameterName
vocabulary	I-HyperparameterName
containing	O
50K	B-HyperparameterValue
subword	I-HyperparameterValue
units,	I-HyperparameterValue
without	O
any	O
additional	O
preprocessing	O
or	O
tokenization	O
of	O
the	O
input.	O
This	O
adds	O
approximately	O
15M	O
and	O
20M	O
additional	O
parameters	O
for	O
BERT	B-MethodName
BASE	I-MethodName
and	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
respectively.	O
Early	O
experiments	O
revealed	O
only	O
slight	O
differences	O
between	O
these	O
encodings,	O
with	O
the	O
Radford	O
et	O
al.	O
(	O
2019)	O
BPE	B-MethodName
achieving	O
slightly	O
worse	O
end-task	O
performance	O
on	O
some	O
tasks.	O
Nevertheless,	O
we	O
believe	O
the	O
advantages	O
of	O
a	O
universal	O
encoding	O
scheme	O
outweighs	O
the	O
minor	O
degredation	O
in	O
performance	O
and	O
use	O
this	O
encoding	O
in	O
the	O
remainder	O
of	O
our	O
experiments.	O
A	O
more	O
detailed	O
comparison	O
of	O
these	O
encodings	O
is	O
left	O
to	O
future	O
work.	O

In	O
the	O
previous	O
section	O
we	O
propose	O
modifications	O
to	O
the	O
BERT	B-MethodName
pretraining	O
procedure	O
that	O
improve	O
end-task	O
performance.	O
We	O
now	O
aggregate	O
these	O
improvements	O
and	O
evaluate	O
their	O
combined	O
impact.	O
We	O
call	O
this	O
configuration	O
RoBERTa	B-MethodName
for	O
Robustly	B-MethodName
optimized	I-MethodName
BERT	I-MethodName
approach.	O
Specifically,	O
RoBERTa	B-MethodName
is	O
trained	O
with	O
dynamic	O
masking	O
(Section	O
4.1),	O
FULL-SENTENCES	O
without	O
NSP	O
loss	O
(Section	O
4.2),	O
large	O
mini-batches	O
(Section	O
4.3)	O
and	O
a	O
larger	O
byte-level	O
BPE	O
(Section	O
4.4).	O
Additionally,	O
we	O
investigate	O
two	O
other	O
important	O
factors	O
that	O
have	O
been	O
under-emphasized	O
in	O
previous	O
work:	O
(1)	O
the	O
data	O
used	O
for	O
pretraining,	O
and	O
(2)	O
the	O
number	O
of	O
training	O
passes	O
through	O
the	O
data.	O
For	O
example,	O
the	O
recently	O
proposed	O
XLNet	B-MethodName
architecture	O
(Yang	O
et	O
al.,	O
2019)	O
is	O
pretrained	O
using	O
nearly	O
10	O
times	O
more	O
data	O
than	O
the	O
original	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
It	O
is	O
also	O
trained	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
eight	O
times	O
larger	O
for	O
half	O
as	O
many	O
optimization	O
steps,	O
thus	O
seeing	O
four	O
times	O
as	O
many	O
sequences	O
in	O
pretraining	O
compared	O
to	O
BERT.	B-MethodName
To	O
help	O
disentangle	O
the	O
importance	O
of	O
these	O
factors	O
from	O
other	O
modeling	O
choices	O
(e.g.,	O
the	O
pretraining	O
objective),	O
we	O
begin	O
by	O
training	O
RoBERTa	B-MethodName
following	O
the	O
BERT	B-MethodName
LARGE	I-MethodName
architecture	O
(L	B-HyperparameterName
=	O
24,	B-HyperparameterValue
H	B-HyperparameterName
=	O
1024,	B-HyperparameterValue
A	B-HyperparameterName
=	O
16,	B-HyperparameterValue
355M	O
parameters).	O
We	O
pretrain	O
for	O
100K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
a	O
comparable	O
BOOK-CORPUS	B-DatasetName
plus	O
WIKIPEDIA	B-DatasetName
dataset	O
as	O
was	O
used	O
in	O
Yang	O
et	O
al.	O
(2019),	O
respectively.	O
Complete	O
results	O
on	O
all	O
GLUE	B-DatasetName
tasks	O
can	O
be	O
found	O
in	O
the	O
Appendix.	O
Devlin	O
et	O
al.	O
(2019).	O
We	O
pretrain	O
our	O
model	O
using	O
1024	O
V100	O
GPUs	O
for	O
approximately	O
one	O
day.	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
4.	O
When	O
controlling	O
for	O
training	O
data,	O
we	O
observe	O
that	O
RoBERTa	B-MethodName
provides	O
a	O
large	O
improvement	O
over	O
the	O
originally	O
reported	O
BERT	B-MethodName
LARGE	I-MethodName
results,	O
reaffirming	O
the	O
importance	O
of	O
the	O
design	O
choices	O
we	O
explored	O
in	O
Section	O
4.	O
Next,	O
we	O
combine	O
this	O
data	O
with	O
the	O
three	O
additional	O
datasets	O
described	O
in	O
Section	O
3.2.	O
We	O
train	O
RoBERTa	B-MethodName
over	O
the	O
combined	O
data	O
with	O
the	O
same	O
number	O
of	O
training	B-HyperparameterName
steps	I-HyperparameterName
as	O
before	O
(100K).	B-HyperparameterValue
In	O
total,	O
we	O
pretrain	O
over	O
160GB	O
of	O
text.	O
We	O
observe	O
further	O
improvements	O
in	O
performance	O
across	O
all	O
downstream	O
tasks,	O
validating	O
the	O
importance	O
of	O
data	O
size	O
and	O
diversity	O
in	O
pretraining.	O
9	O
Finally,	O
we	O
pretrain	O
RoBERTa	B-MethodName
for	O
significantly	O
longer,	O
increasing	O
the	O
number	O
of	O
pretraining	B-HyperparameterName
steps	I-HyperparameterName
from	O
100K	B-HyperparameterValue
to	I-HyperparameterValue
300K,	I-HyperparameterValue
and	O
then	O
further	O
to	O
500K.	B-HyperparameterValue
We	O
again	O
observe	O
significant	O
gains	O
in	O
downstream	O
task	O
performance,	O
and	O
the	O
300K	O
and	O
500K	O
step	O
models	O
outperform	O
XLNet	B-MethodName
LARGE	I-MethodName
across	O
most	O
tasks.	O
We	O
note	O
that	O
even	O
our	O
longest-trained	O
model	O
does	O
not	O
appear	O
to	O
overfit	O
our	O
data	O
and	O
would	O
likely	O
benefit	O
from	O
additional	O
training.	O
In	O
the	O
rest	O
of	O
the	O
paper,	O
we	O
evaluate	O
our	O
best	O
RoBERTa	B-MethodName
model	O
on	O
the	O
three	O
different	O
benchmarks:	O
GLUE,	B-DatasetName
SQuaD	B-DatasetName
and	O
RACE.	B-DatasetName
Specifically	O
we	O
consider	O
RoBERTa	B-MethodName
trained	O
for	O
500K	B-HyperparameterValue
steps	B-HyperparameterName
over	O
all	O
five	O
of	O
the	O
datasets	O
introduced	O
in	O
Section	O
3.2.	O

For	O
GLUE	B-DatasetName
we	O
consider	O
two	O
finetuning	O
settings.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev)	O
we	O
finetune	O
RoBERTa	B-MethodName
separately	O
for	O
each	O
of	O
the	O
GLUE	B-DatasetName
tasks,	O
using	O
only	O
the	O
training	O
data	O
for	O
the	O
corresponding	O
task.	O
We	O
consider	O
a	O
limited	O
hyperparameter	O
sweep	O
for	O
each	O
task,	O
with	O
batch	B-HyperparameterName
sizes	I-HyperparameterName
∈	O
{16,	B-HyperparameterValue
32}	I-HyperparameterValue
and	O
learning	B-HyperparameterName
rates	I-HyperparameterName
∈	O
{1e−5,	B-HyperparameterValue
2e−5,	I-HyperparameterValue
3e−5},	I-HyperparameterValue
with	O
a	O
linear	B-HyperparameterName
warmup	I-HyperparameterName
for	O
the	O
first	O
6%	B-HyperparameterValue
of	O
steps	O
followed	O
by	O
a	O
linear	B-HyperparameterName
decay	I-HyperparameterName
to	O
0.	O
We	O
finetune	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
perform	O
early	O
stopping	O
based	O
on	O
each	O
task's	O
evaluation	O
metric	O
on	O
the	O
dev	O
set.	O
The	O
rest	O
of	O
the	O
hyperparameters	O
remain	O
the	O
same	O
as	O
during	O
pretraining.	O
In	O
this	O
setting,	O
we	O
report	O
the	O
median	O
development	O
set	O
results	O
for	O
each	O
task	O
over	O
five	O
random	O
initializations,	O
without	O
model	O
ensembling.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
compare	O
RoBERTa	B-MethodName
to	O
other	O
approaches	O
on	O
the	O
test	O
set	O
via	O
the	O
GLUE	B-DatasetName
leaderboard.	O
While	O
many	O
submissions	O
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
depend	O
on	O
multitask	O
finetuning,	O
our	O
submission	O
depends	O
only	O
on	O
single-task	O
finetuning.	O
For	O
RTE,	B-DatasetName
STS	B-DatasetName
and	O
MRPC	B-DatasetName
we	O
found	O
it	O
helpful	O
to	O
finetune	O
starting	O
from	O
the	O
MNLI	O
single-task	O
model,	O
rather	O
than	O
the	O
baseline	O
pretrained	O
RoBERTa.	O
We	O
explore	O
a	O
slightly	O
wider	O
hyperparameter	O
space,	O
described	O
in	O
the	O
Appendix,	O
and	O
ensemble	O
between	O
5	O
and	O
7	O
models	O
per	O
task.	O
Task-specific	O
modifications	O
Two	O
of	O
the	O
GLUE	B-DatasetName
tasks	O
require	O
task-specific	O
finetuning	O
approaches	O
to	O
achieve	O
competitive	O
leaderboard	O
results.	O
QNLI:	B-DatasetName
Recent	O
submissions	O
on	O
the	O
GLUE	B-DatasetName
leaderboard	O
adopt	O
a	O
pairwise	O
ranking	O
formulation	O
for	O
the	O
QNLI	B-DatasetName
task,	O
in	O
which	O
candidate	O
answers	O
are	O
mined	O
from	O
the	O
training	O
set	O
and	O
compared	O
to	O
one	O
another,	O
and	O
a	O
single	O
(question,	O
candidate)	O
pair	O
is	O
classified	O
as	O
positive	O
(Liu	O
et	O
al.,	O
2019b,a;Yang	O
et	O
al.,	O
2019).	O
This	O
formulation	O
significantly	O
simplifies	O
the	O
task,	O
but	O
is	O
not	O
directly	O
comparable	O
to	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
Following	O
recent	O
work,	O
we	O
adopt	O
the	O
ranking	O
approach	O
for	O
our	O
test	O
submission,	O
but	O
for	O
direct	O
comparison	O
with	O
BERT	B-MethodName
we	O
report	O
development	O
set	O
results	O
based	O
on	O
a	O
pure	O
classification	O
approach.	O
WNLI:	B-DatasetName
We	O
found	O
the	O
provided	O
NLI-format	O
data	O
to	O
be	O
challenging	O
to	O
work	O
with.	O
Instead	O
we	O
use	O
the	O
reformatted	O
WNLI	B-DatasetName
data	O
from	O
Super-GLUE	B-DatasetName
(Wang	O
et	O
al.,	O
2019a),	O
which	O
indicates	O
the	O
span	O
of	O
the	O
query	O
pronoun	O
and	O
referent.	O
We	O
finetune	O
RoBERTa	B-MethodName
using	O
the	O
margin	O
ranking	O
loss	O
from	O
Kocijan	O
et	O
al.	O
(2019).	O
For	O
a	O
given	O
input	O
sentence,	O
we	O
use	O
spaCy	O
(Honnibal	O
and	O
Montani,	O
2017)	O
to	O
extract	O
additional	O
candidate	O
noun	O
phrases	O
from	O
the	O
sentence	O
and	O
finetune	O
our	O
model	O
so	O
that	O
it	O
assigns	O
higher	O
scores	O
to	O
positive	O
referent	O
phrases	O
than	O
for	O
any	O
of	O
the	O
generated	O
negative	O
candidate	O
phrases.	O
One	O
unfortunate	O
consequence	O
of	O
this	O
formulation	O
is	O
that	O
we	O
can	O
only	O
make	O
use	O
Results	O
We	O
present	O
our	O
results	O
in	O
Table	O
5.	O
In	O
the	O
first	O
setting	O
(single-task,	O
dev),	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
all	O
9	O
of	O
the	O
GLUE	O
task	O
development	O
sets.	O
Crucially,	O
RoBERTa	B-MethodName
uses	O
the	O
same	O
masked	O
language	O
modeling	O
pretraining	O
objective	O
and	O
architecture	O
as	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
yet	O
consistently	O
outperforms	O
both	O
BERT	B-MethodName
LARGE	I-MethodName
and	O
XLNet	B-MethodName
LARGE	I-MethodName
.	O
This	O
raises	O
questions	O
about	O
the	O
relative	O
importance	O
of	O
model	O
architecture	O
and	O
pretraining	O
objective,	O
compared	O
to	O
more	O
mundane	O
details	O
like	O
dataset	O
size	O
and	O
training	O
time	O
that	O
we	O
explore	O
in	O
this	O
work.	O
In	O
the	O
second	O
setting	O
(ensembles,	O
test),	O
we	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
GLUE	B-DatasetName
leaderboard	O
and	O
achieve	O
state-of-the-art	O
results	O
on	O
4	O
out	O
of	O
9	O
tasks	O
and	O
the	O
highest	O
average	O
score	O
to	O
date.	O
This	O
is	O
especially	O
exciting	O
because	O
RoBERTa	B-MethodName
does	O
not	O
depend	O
on	O
multi-task	O
finetuning,	O
unlike	O
most	O
of	O
the	O
other	O
top	O
submissions.	O
We	O
expect	O
future	O
work	O
may	O
further	O
improve	O
these	O
results	O
by	O
incorporating	O
more	O
sophisticated	O
multi-task	O
finetuning	O
procedures.	O

We	O
adopt	O
a	O
much	O
simpler	O
approach	O
for	O
SQuAD	B-DatasetName
compared	O
to	O
past	O
work.	O
In	O
particular,	O
while	O
both	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
and	O
XL-Net	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
XLNet,	B-MethodName
while	O
we	O
use	O
the	O
same	O
learning	O
rate	O
for	O
all	O
layers.	O
For	O
SQuAD	B-DatasetName
v1.1	I-DatasetName
we	O
follow	O
the	O
same	O
finetuning	O
procedure	O
as	O
Devlin	O
et	O
al.	O
(2019).	O
For	O
SQuAD	B-DatasetName
v2.0,	I-DatasetName
we	O
additionally	O
classify	O
whether	O
a	O
given	O
question	O
is	O
answerable;	O
we	O
train	O
this	O
classifier	O
jointly	O
with	O
the	O
span	O
predictor	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O

We	O
present	O
our	O
results	O
in	O
Table	O
6.	O
On	O
the	O
SQuAD	O
v1.1	O
development	O
set,	O
RoBERTa	B-MethodName
matches	O
the	O
state-of-the-art	O
set	O
by	O
XLNet.	B-MethodName
On	O
the	O
SQuAD	B-DatasetName
v2.0	I-DatasetName
development	O
set,	O
RoBERTa	B-DatasetName
sets	O
a	O
new	O
state-of-the-art,	O
improving	O
over	O
XLNet	B-DatasetName
by	B-MetricValue
0.4	I-MetricValue
points	I-MetricValue
(EM)	B-MetricName
and	O
0.6	B-MetricValue
points	I-MetricValue
(F1).	B-MetricName
We	O
also	O
submit	O
RoBERTa	B-MethodName
to	O
the	O
public	O
SQuAD	B-DatasetName
2.0	I-DatasetName
leaderboard	O
and	O
evaluate	O
its	O
performance	O
relative	O
to	O
other	O
systems.	O
Most	O
of	O
the	O
top	O
systems	O
build	O
upon	O
either	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
or	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019),	O
both	O
of	O
which	O
rely	O
on	O
additional	O
external	O
training	O
data.	O
In	O
contrast,	O
our	O
submission	O
does	O
not	O
use	O
any	O
additional	O
data.	O
Our	O
single	B-MethodName
RoBERTa	I-MethodName
model	O
outperforms	O
all	O
but	O
one	O
of	O
the	O
single	O
model	O
submissions,	O
and	O
is	O
the	O
top	O
scoring	O
system	O
among	O
those	O
that	O
do	O
not	O
rely	O
on	O
data	O
augmentation.	O

In	O
RACE,	B-DatasetName
systems	O
are	O
provided	O
with	O
a	O
passage	O
of	O
text,	O
an	O
associated	O
question,	O
and	O
four	O
candidate	O
answers.	O
Systems	O
are	O
required	O
to	O
classify	O
which	O
of	O
the	O
four	O
candidate	O
answers	O
is	O
correct.	O
We	O
modify	O
RoBERTa	B-MethodName
for	O
this	O
task	O
by	O
concate-	O
Yang	O
et	O
al.	O
(2019).	O
nating	O
each	O
candidate	O
answer	O
with	O
the	O
corresponding	O
question	O
and	O
passage.	O
We	O
then	O
encode	O
each	O
of	O
these	O
four	O
sequences	O
and	O
pass	O
the	O
resulting	O
[CLS]	O
representations	O
through	O
a	O
fully-connected	O
layer,	O
which	O
is	O
used	O
to	O
predict	O
the	O
correct	O
answer.	O
We	O
truncate	O
question-answer	O
pairs	O
that	O
are	O
longer	O
than	O
128	O
tokens	O
and,	O
if	O
needed,	O
the	O
passage	O
so	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Results	O
on	O
the	O
RACE	B-DatasetName
test	O
sets	O
are	O
presented	O
in	O
Table	O
7.	O
RoBERTa	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
both	O
middle-school	O
and	O
high-school	O
settings.	O

Past	O
work	O
in	O
Neural	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
has	O
shown	O
that	O
training	O
with	O
very	O
large	O
mini-batches	B-HyperparameterName
can	O
both	O
improve	O
optimization	O
speed	O
and	O
end-task	O
performance	O
when	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
increased	O
appropriately	O
(Ott	O
et	O
al.,	O
2018).	O
Recent	O
work	O
has	O
shown	O
that	O
BERT	B-MethodName
is	O
also	O
amenable	O
to	O
large	O
batch	O
training	O
(You	O
et	O
al.,	O
2019	O

In	O
the	O
original	O
BERT	B-MethodName
pretraining	O
procedure,	O
the	O
model	O
observes	O
two	O
concatenated	O
document	O
segments,	O
which	O
are	O
either	O
sampled	O
contiguously	O
from	O
the	O
same	O
document	O
(with	O
p	B-MetricName
=	O
0.5)	B-MetricValue
or	O
from	O
distinct	O
documents.	O
In	O
addition	O
to	O
the	O
masked	O
language	O
modeling	O
objective,	O
the	O
model	O
is	O
trained	O
to	O
predict	O
whether	O
the	O
observed	O
document	O
segments	O
come	O
from	O
the	O
same	O
or	O
distinct	O
documents	O
via	O
an	O
auxiliary	O
Next	O
Sentence	O
Prediction	O
(NSP)	O
loss.	O
The	O
NSP	O
loss	O
was	O
hypothesized	O
to	O
be	O
an	O
important	O
factor	O
in	O
training	O
the	O
original	O
BERT	B-MethodName
model.	O
Devlin	O
et	O
al.	O
(2019)	O
observe	O
that	O
removing	O
NSP	O
hurts	O
performance,	O
with	O
significant	O
performance	O
degradation	O
on	O
QNLI,	B-DatasetName
MNLI,	B-DatasetName
and	O
SQuAD	B-DatasetName
1.1.	I-DatasetName
However,	O
some	O
recent	O
work	O
has	O
questioned	O
the	O
necessity	O
of	O
the	O
NSP	O
loss	O
(Lample	O
and	O
Conneau,	O
2019;Yang	O
et	O
al.,	O
2019;Joshi	O
et	O
al.,	O
2019).	O
To	O
better	O
understand	O
this	O
discrepancy,	O
we	O
compare	O
several	O
alternative	O
training	O
formats:	O
•	O
SEGMENT-PAIR+NSP:	O
This	O
follows	O
the	O
original	O
input	O
format	O
used	O
in	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
with	O
the	O
NSP	O
loss.	O
Each	O
input	O
has	O
a	O
pair	O
of	O
segments,	O
which	O
can	O
each	O
contain	O
multiple	O
natural	O
sentences,	O
but	O
the	O
total	O
combined	O
length	O
must	O
be	O
less	O
than	O
512	O
tokens.	O
•	O
SENTENCE-PAIR+NSP:	O
Each	O
input	O
contains	O
a	O
pair	O
of	O
natural	O
sentences,	O
either	O
sampled	O
from	O
a	O
contiguous	O
portion	O
of	O
one	O
document	O
or	O
from	O
separate	O
documents.	O
Since	O
these	O
inputs	O
are	O
significantly	O
shorter	O
than	O
512	O
tokens,	O
we	O
increase	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
so	O
that	O
the	O
total	O
number	O
of	O
tokens	O
remains	O
similar	O
to	O
SEGMENT-PAIR+NSP.	O
We	O
retain	O
the	O
NSP	O
loss.	O
•	O
FULL-SENTENCES:	O
Each	O
input	O
is	O
packed	O
with	O
full	O
sentences	O
sampled	O
contiguously	O
from	O
one	O
or	O
more	O
documents,	O
such	O
that	O
the	O
total	O
length	O
is	O
at	O
most	O
512	O
tokens.	O
Inputs	O
may	O
cross	O
document	O
boundaries.	O
When	O
we	O
reach	O
the	O
end	O
of	O
one	O
document,	O
we	O
begin	O
sampling	O
sentences	O
from	O
the	O
next	O
document	O
and	O
add	O
an	O
extra	O
separator	O
token	O
between	O
documents.	O
We	O
remove	O
the	O
NSP	O
loss.	O
•	O
DOC-SENTENCES:	O
Inputs	O
are	O
constructed	O
similarly	O
to	O
FULL-SENTENCES,	O
except	O
that	O
they	O
may	O
not	O
cross	O
document	O
boundaries.	O
Inputs	O
sampled	O
near	O
the	O
end	O
of	O
a	O
document	O
may	O
be	O
shorter	O
than	O
512	O
tokens,	O
so	O
we	O
dynamically	O
increase	O
the	O
batch	O
size	O
in	O
these	O
cases	O
to	O
achieve	O
a	O
similar	O
number	O
of	O
total	O
tokens	O
as	O
FULL-SENTENCES.	O
We	O
remove	O
the	O
NSP	O
loss.	O
Results	O
Table	O
2	O
shows	O
results	O
for	O
the	O
four	O
different	O
settings.	O
We	O
first	O
compare	O
the	O
original	O
SEGMENT-PAIR	O
input	O
format	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
the	O
SENTENCE-PAIR	O
format;	O
both	O
formats	O
retain	O
the	O
NSP	O
loss,	O
but	O
the	O
latter	O
uses	O
single	O
sentences.	O
We	O
find	O
that	O
using	O
individual	O
sentences	O
hurts	O
performance	O
on	O
downstream	O
tasks,	O
which	O
we	O
hypothesize	O
is	O
because	O
the	O
model	O
is	O
not	O
able	O
to	O
learn	O
long-range	O
dependencies.	O
We	O
next	O
compare	O
training	O
without	O
the	O
NSP	O
loss	O
and	O
training	O
with	O
blocks	O
of	O
text	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES).	O
We	O
find	O
that	O
this	O
setting	O
outperforms	O
the	O
originally	O
published	O
BERT	B-MethodName
BASE	I-MethodName
results	O
and	O
that	O
removing	O
the	O
NSP	O
loss	O
matches	O
or	O
slightly	O
improves	O
downstream	O
task	O
performance,	O
in	O
contrast	O
to	O
Devlin	O
et	O
al.	O
(2019).	O
It	O
is	O
possible	O
that	O
the	O
original	O
BERT	B-MethodName
implementation	O
may	O
only	O
have	O
removed	O
the	O
loss	O
term	O
while	O
still	O
retaining	O
the	O
SEGMENT-PAIR	O
input	O
format.	O
Finally	O
we	O
find	O
that	O
restricting	O
sequences	O
to	O
come	O
from	O
a	O
single	O
document	O
(DOC-SENTENCES)	O
performs	O
slightly	O
better	O
than	O
packing	O
sequences	O
from	O
multiple	O
documents	O
(FULL-SENTENCES).	O
However,	O
because	O
the	O
DOC-SENTENCES	O
format	O
results	O
in	O
variable	O
batch	O
sizes,	O
we	O
use	O
FULL-SENTENCES	O
in	O
the	O
remainder	O
of	O
our	O
experiments	O
for	O
easier	O
comparison	O
with	O
related	O
work.	O

As	O
discussed	O
in	O
Section	O
2,	O
BERT	B-MethodName
relies	O
on	O
randomly	O
masking	O
and	O
predicting	O
tokens.	O
The	O
original	O
BERT	B-MethodName
implementation	O
performed	O
masking	O
once	O
during	O
data	O
preprocessing,	O
resulting	O
in	O
a	O
single	O
static	O
mask.	O
To	O
avoid	O
using	O
the	O
same	O
mask	O
for	O
each	O
training	O
instance	O
in	O
every	O
epoch,	O
training	O
data	O
was	O
duplicated	O
10	O
times	O
so	O
that	O
each	O
sequence	O
is	O
masked	O
in	O
10	O
different	O
ways	O
over	O
the	O
40	B-HyperparameterValue
epochs	B-HyperparameterName
of	O
training.	O
Thus,	O
each	O
training	O
sequence	O
was	O
seen	O
with	O
the	O
same	O
mask	O
four	O
times	O
during	O
training.	O
We	O
compare	O
this	O
strategy	O
with	O
dynamic	O
masking	O
where	O
we	O
generate	O
the	O
masking	O
pattern	O
every	O
time	O
we	O
feed	O
a	O
sequence	O
to	O
the	O
model.	O
This	O
becomes	O
crucial	O
when	O
pretraining	O
for	O
more	O
steps	O
or	O
with	O
larger	O
datasets.	O
Results	O
Table	O
1	O
compares	O
the	O
published	O
BERT	O
BASE	O
results	O
from	O
Devlin	O
et	O
al.	O
(2019)	O
to	O
our	O
reimplementation	O
with	O
either	O
static	O
or	O
dynamic	O
masking.	O
We	O
find	O
that	O
our	O
reimplementation	O
with	O
static	O
masking	O
performs	O
similar	O
to	O
the	O
original	O
BERT	B-MethodName
model,	O
and	O
dynamic	O
masking	O
is	O
comparable	O
or	O
slightly	O
better	O
than	O
static	O
masking.	O
Given	O
these	O
results	O
and	O
the	O
additional	O
efficiency	O
benefits	O
of	O
dynamic	O
masking,	O
we	O
use	O
dynamic	O
masking	O
in	O
the	O
remainder	O
of	O
the	O
experiments.	O

Pretraining	O
methods	O
have	O
been	O
designed	O
with	O
different	O
training	O
objectives,	O
including	O
language	O
modeling	O
(Dai	O
and	O
Le,	O
2015;Peters	O
et	O
al.,	O
2018;Howard	O
and	O
Ruder,	O
2018),	O
machine	B-TaskName
translation	I-TaskName
(McCann	O
et	O
al.,	O
2017),	O
and	O
masked	O
language	O
modeling	O
(Devlin	O
et	O
al.,	O
2019;Lample	O
and	O
Conneau,	O
2019).	O
Many	O
recent	O
papers	O
have	O
used	O
a	O
basic	O
recipe	O
of	O
finetuning	O
models	O
for	O
each	O
end	O
task	O
(Howard	O
and	O
Ruder,	O
2018;Radford	O
et	O
al.,	O
2018),	O
and	O
pretraining	O
with	O
some	O
variant	O
of	O
a	O
masked	O
language	O
model	O
objective.	O
However,	O
newer	O
methods	O
have	O
improved	O
performance	O
by	O
multi-task	O
fine	O
tuning	O
(Dong	O
et	O
al.,	O
2019),	O
incorporating	O
entity	O
embeddings	O
(Sun	O
et	O
al.,	O
2019),	O
span	O
prediction	O
(Joshi	O
et	O
al.,	O
2019),	O
and	O
multiple	O
variants	O
of	O
autoregressive	O
pretraining	O
Chan	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019).	O
Performance	O
is	O
also	O
typically	O
improved	O
by	O
training	O
bigger	O
models	O
on	O
more	O
data	O
(Devlin	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Radford	O
et	O
al.,	O
2019).	O
Our	O
goal	O
was	O
to	O
replicate,	O
simplify,	O
and	O
better	O
tune	O
the	O
training	O
of	O
BERT,	B-MethodName
as	O
a	O
reference	O
point	O
for	O
better	O
understanding	O
the	O
relative	O
performance	O
of	O
all	O
of	O
these	O
methods.	O
We	O
carefully	O
evaluate	O
a	O
number	O
of	O
design	O
decisions	O
when	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
find	O
that	O
performance	O
can	O
be	O
substantially	O
improved	O
by	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches	O
over	O
more	O
data;	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
training	O
on	O
longer	O
sequences;	O
and	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
Our	O
improved	O
pretraining	O
procedure,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD,	B-DatasetName
without	O
multi-task	O
finetuning	O
for	O
GLUE	B-DatasetName
or	O
additional	O
data	O
for	O
SQuAD.	B-DatasetName
These	O
results	O
illustrate	O
the	O
importance	O
of	O
these	O
previously	O
overlooked	O
design	O
decisions	O
and	O
suggest	O
that	O
BERT's	B-MethodName
pretraining	O
objective	O
remains	O
competitive	O
with	O
recently	O
proposed	O
alternatives.	O
We	O
additionally	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
release	O
our	O
models	O
and	O
code	O
for	O
pretraining	O
and	O
finetuning	O
at:	O

This	O
section	O
explores	O
and	O
quantifies	O
which	O
choices	O
are	O
important	O
for	O
successfully	O
pretraining	O
BERT	B-MethodName
models.	O
We	O
keep	O
the	O
model	O
architecture	O
fixed.	O
7	O
Specifically,	O
we	O
begin	O
by	O
training	O
BERT	B-MethodName
models	O
with	O
the	O
same	O
configuration	O
as	O
BERT	B-MethodName
BASE	I-MethodName
(L	B-HyperparameterName
=	O
12,	B-HyperparameterValue
H	B-HyperparameterName
=	O
768,	B-HyperparameterValue
A	B-HyperparameterName
=	O
12,	B-HyperparameterValue
110M	B-MetricValue
params).	I-MetricValue

Following	O
previous	O
work,	O
we	O
evaluate	O
our	O
pretrained	O
models	O
on	O
downstream	O
tasks	O
using	O
the	O
following	O
three	O
benchmarks.	O
GLUE	B-DatasetName
The	B-DatasetName
General	I-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(GLUE)	I-DatasetName
benchmark	O
(Wang	O
et	O
al.,	O
2019b)	O
is	O
a	O
collection	O
of	O
9	O
datasets	O
for	O
evaluating	O
natural	O
language	O
understanding	O
systems.	O
6	O
Tasks	O
are	O
framed	O
as	O
either	O
single-sentence	O
classification	O
or	O
sentence-pair	O
classification	O
tasks.	O
The	O
GLUE	B-DatasetName
organizers	O
provide	O
training	O
and	O
development	O
data	O
splits	O
as	O
well	O
as	O
a	O
submission	O
server	O
and	O
leaderboard	O
that	O
allows	O
participants	O
to	O
evaluate	O
and	O
compare	O
their	O
systems	O
on	O
private	O
held-out	O
test	O
data.	O
For	O
the	O
replication	O
study	O
in	O
Section	O
4,	O
we	O
report	O
results	O
on	O
the	O
development	O
sets	O
after	O
finetuning	O
the	O
pretrained	O
models	O
on	O
the	O
corresponding	O
singletask	O
training	O
data	O
(i.e.,	O
without	O
multi-task	O
training	O
or	O
ensembling).	O
Our	O
finetuning	O
procedure	O
follows	O
the	O
original	O
BERT	B-MethodName
paper	O
(Devlin	O
et	O
al.,	O
2019).	O
In	O
Section	O
5	O
we	O
additionally	O
report	O
test	O
set	O
results	O
obtained	O
from	O
the	O
public	O
leaderboard.	O
These	O
results	O
depend	O
on	O
a	O
several	O
task-specific	O
modifications,	O
which	O
we	O
describe	O
in	O
Section	O
5.1.	O
SQuAD	B-DatasetName
The	B-DatasetName
Stanford	I-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(SQuAD)	I-DatasetName
provides	O
a	O
paragraph	O
of	O
context	O
and	O
a	O
question.	O
The	O
task	O
is	O
to	O
answer	O
the	O
question	O
by	O
extracting	O
the	O
relevant	O
span	O
from	O
the	O
context.	O
We	O
evaluate	O
on	O
two	O
versions	O
of	O
SQuAD:	B-DatasetName
V1.1	O
and	O
V2.0	O
(Rajpurkar	O
et	O
al.,	O
2016(Rajpurkar	O
et	O
al.,	O
,	O
2018.	O
In	O
V1.1	O
the	O
context	O
always	O
contains	O
an	O
answer,	O
whereas	O
in	O
V2.0	O
some	O
questions	O
are	O
not	O
answered	O
in	O
the	O
provided	O
context,	O
making	O
the	O
task	O
more	O
challenging.	O
For	O
SQuAD	B-DatasetName
V1.1	I-DatasetName
we	O
adopt	O
the	O
same	O
span	O
prediction	O
method	O
as	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019).	O
For	O
SQuAD	B-DatasetName
V2.0,	I-DatasetName
we	O
add	O
an	O
additional	O
binary	O
classifier	O
to	O
predict	O
whether	O
the	O
question	O
is	O
answerable,	O
which	O
we	O
train	O
jointly	O
by	O
summing	O
the	O
classification	O
and	O
span	O
loss	O
terms.	O
During	O
evaluation,	O
we	O
only	O
predict	O
span	O
indices	O
on	O
pairs	O
that	O
are	O
classified	O
as	O
answerable.	O
RACE	B-DatasetName
The	B-DatasetName
ReAding	I-DatasetName
Comprehension	I-DatasetName
from	I-DatasetName
Examinations	I-DatasetName
(RACE)	I-DatasetName
(Lai	O
et	O
al.,	O
2017)	O
task	O
is	O
a	O
large-scale	O
reading	O
comprehension	O
dataset	O
with	O
more	O
than	O
28,000	O
passages	O
and	O
nearly	O
100,000	O
questions.	O
The	O
dataset	O
is	O
collected	O
from	O
English	O
examinations	O
in	O
China,	O
which	O
are	O
designed	O
for	O
middle	O
and	O
high	O
school	O
students.	O
In	O
RACE,	B-DatasetName
each	O
passage	O
is	O
associated	O
with	O
multiple	O
questions.	O
For	O
every	O
question,	O
the	O
task	O
is	O
to	O
select	O
one	O
correct	O
answer	O
from	O
four	O
options.	O
RACE	B-DatasetName
has	O
significantly	O
longer	O
context	O
than	O
other	O
popular	O
reading	O
comprehension	O
datasets	O
and	O
the	O
proportion	O
of	O
questions	O
that	O
requires	O
reasoning	O
is	O
very	O
large.	O

BERT-style	B-MethodName
pretraining	O
crucially	O
relies	O
on	O
large	O
quantities	O
of	O
text.	O
demonstrate	O
that	O
increasing	O
data	O
size	O
can	O
result	O
in	O
improved	O
end-task	O
performance.	O
Several	O
efforts	O
have	O
trained	O
on	O
datasets	O
larger	O
and	O
more	O
diverse	O
than	O
the	O
original	O
BERT	B-MethodName
(Radford	O
et	O
al.,	O
2019;Yang	O
et	O
al.,	O
2019;Zellers	O
et	O
al.,	O
2019).	O
Unfortunately,	O
not	O
all	O
of	O
the	O
additional	O
datasets	O
can	O
be	O
publicly	O
released.	O
For	O
our	O
study,	O
we	O
focus	O
on	O
gathering	O
as	O
much	O
data	O
as	O
possible	O
for	O
experimentation,	O
allowing	O
us	O
to	O
match	O
the	O
overall	O
quality	O
and	O
quantity	O
of	O
data	O
as	O
appropriate	O
for	O
each	O
comparison.	O
We	O
consider	O
five	O
English-language	O
corpora	O
of	O
varying	O
sizes	O
and	O
domains,	O
totaling	O
over	O
160GB	O
of	O
uncompressed	O
text.	O
We	O
use	O
the	O
following	O
text	O
corpora:	O
•	O
BOOKCORPUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA.	I-DatasetName
This	O
is	O
the	O
original	O
data	O
used	O
to	O
train	O
BERT.	B-MethodName
(16GB).	O
•	O
CC-NEWS,	B-DatasetName
which	O
we	O
collected	O
from	O
the	O
English	O
portion	O
of	O
the	O
CommonCrawl	B-DatasetName
News	I-DatasetName
dataset	I-DatasetName
(Nagel,	O
2016).	O
The	O
data	O
contains	O
63	B-HyperparameterName
million	I-HyperparameterName
English	I-HyperparameterName
news	I-HyperparameterName
articles	I-HyperparameterName
crawled	O
between	O
September	O
2016	O
and	O
February	O
2019.	O
(76GB	O
after	O
filtering).	O
4	O
•	O
OPENWEBTEXT	B-DatasetName
(Gokaslan	O
and	O
Cohen,	O
2019),	O
an	O
open-source	O
recreation	O
of	O
the	O
WebText	O
cor-pus	O
described	O
in	O
Radford	O
et	O
al.	O
(2019).	O
The	O
text	O
is	O
web	O
content	O
extracted	O
from	O
URLs	O
shared	O
on	O
Reddit	O
with	O
at	O
least	O
three	O
upvotes.	O
(38GB).	O
5	O
•	O
STORIES,	B-DatasetName
a	O
dataset	O
introduced	O
in	O
Trinh	O
and	O
Le	O
(2018)	O
containing	O
a	O
subset	O
of	O
CommonCrawl	B-DatasetName
data	O
filtered	O
to	O
match	O
the	O
story-like	O
style	O
of	O
Winograd	O
schemas.	O
(31GB).	O

We	O
reimplement	O
BERT	B-MethodName
in	O
FAIRSEQ	O
(Ott	O
et	O
al.,	O
2019).	O
We	O
primarily	O
follow	O
the	O
original	O
BERT	B-MethodName
optimization	O
hyperparameters,	O
given	O
in	O
Section	O
2,	O
except	O
for	O
the	O
peak	O
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
number	B-HyperparameterName
of	I-HyperparameterName
warmup	I-HyperparameterName
steps,	I-HyperparameterName
which	O
are	O
tuned	O
separately	O
for	O
each	O
setting.	O
We	O
additionally	O
found	O
training	O
to	O
be	O
very	O
sensitive	O
to	O
the	O
Adam	B-HyperparameterValue
epsilon	O
term,	O
and	O
in	O
some	O
cases	O
we	O
obtained	O
better	O
performance	O
or	O
improved	O
stability	O
after	O
tuning	O
it.	O
Similarly,	O
we	O
found	O
setting	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.98	B-HyperparameterValue
to	O
improve	O
stability	O
when	O
training	O
with	O
large	O
batch	O
sizes.	O
We	O
pretrain	O
with	O
sequences	O
of	O
at	O
most	O
T	O
=	O
512	O
tokens.	O
Unlike	O
Devlin	O
et	O
al.	O
(2019),	O
we	O
do	O
not	O
randomly	O
inject	O
short	O
sequences,	O
and	O
we	O
do	O
not	O
train	O
with	O
a	O
reduced	O
sequence	O
length	O
for	O
the	O
first	O
90%	B-MetricValue
of	O
updates.	O
We	O
train	O
only	O
with	O
full-length	O
sequences.	O
We	O
train	O
with	O
mixed	O
precision	O
floating	O
point	O
arithmetic	O
on	O
DGX-1	O
machines,	O
each	O
with	O
8	O
×	O
32GB	O
Nvidia	O
V100	O
GPUs	O
interconnected	O
by	O
Infiniband	O
(Micikevicius	O
et	O
al.,	O
2018).	O

In	O
this	O
section,	O
we	O
describe	O
the	O
experimental	O
setup	O
for	O
our	O
replication	O
study	O
of	O
BERT.	B-MethodName

BERT	B-MethodName
is	O
trained	O
on	O
a	O
combination	O
of	O
BOOKCOR-PUS	B-DatasetName
(Zhu	O
et	O
al.,	O
2015)	O
plus	O
English	B-DatasetName
WIKIPEDIA,	I-DatasetName
which	O
totals	O
16GB	O
of	O
uncompressed	O
text.	O
3	O

BERT	B-MethodName
is	O
optimized	O
with	O
Adam	B-HyperparameterValue
(Kingma	O
and	O
Ba,	O
2015)	O
using	O
the	O
following	O
parameters:	O
β	B-HyperparameterName
1	I-HyperparameterName
=	O
0.9,	B-HyperparameterValue
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.999,	B-HyperparameterValue
ǫ	B-HyperparameterName
=	O
1e-6	B-HyperparameterValue
and	O
L	B-HyperparameterName
2	I-HyperparameterName
weight	I-HyperparameterName
decay	I-HyperparameterName
of	O
0.01.	B-HyperparameterValue
The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
warmed	O
up	O
over	O
the	O
first	O
10,000	O
steps	O
to	O
a	O
peak	O
value	O
of	O
1e-4,	B-HyperparameterValue
and	O
then	O
linearly	O
decayed.	O
BERT	B-MethodName
trains	O
with	O
a	O
dropout	B-HyperparameterName
of	O
0.1	B-HyperparameterValue
on	O
all	O
layers	O
and	O
attention	O
weights,	O
and	O
a	O
GELU	B-HyperparameterValue
activation	B-HyperparameterName
function	I-HyperparameterName
(Hendrycks	O
and	O
Gimpel,	O
2016).	O
Models	O
are	O
pretrained	O
for	O
S	B-HyperparameterName
=	O
1,000,000	B-HyperparameterValue
updates,	O
with	O
minibatches	B-HyperparameterName
containing	O
B	O
=	O
256	B-HyperparameterValue
sequences	O
of	O
maximum	O
length	O
T	O
=	O
512	O
tokens.	O

During	O
pretraining,	O
BERT	B-MethodName
uses	O
two	O
objectives:	O
masked	O
language	O
modeling	O
and	O
next	B-TaskName
sentence	I-TaskName
prediction.	I-TaskName
Masked	O
Language	O
Model	O
(MLM)	O
A	O
random	O
sample	O
of	O
the	O
tokens	O
in	O
the	O
input	O
sequence	O
is	O
selected	O
and	O
replaced	O
with	O
the	O
special	O
token	O
[MASK	O
].	O
The	O
MLM	O
objective	O
is	O
a	O
cross-entropy	B-MetricName
loss	O
on	O
predicting	O
the	O
masked	O
tokens.	O
BERT	B-MethodName
uniformly	O
selects	O
15%	B-MetricValue
of	O
the	O
input	O
tokens	O
for	O
possible	O
replacement.	O
Of	O
the	O
selected	O
tokens,	O
80%	B-MetricValue
are	O
replaced	O
with	O
[MASK	O
],	O
10%	B-MetricValue
are	O
left	O
unchanged,	O
and	O
10%	B-MetricValue
are	O
replaced	O
by	O
a	O
randomly	O
selected	O
vocabulary	O
token.	O
In	O
the	O
original	O
implementation,	O
random	O
masking	O
and	O
replacement	O
is	O
performed	O
once	O
in	O
the	O
beginning	O
and	O
saved	O
for	O
the	O
duration	O
of	O
training,	O
although	O
in	O
practice,	O
data	O
is	O
duplicated	O
so	O
the	O
mask	O
is	O
not	O
always	O
the	O
same	O
for	O
every	O
training	O
sentence	O
(see	O
Section	O
4.1).	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
(NSP)	O
NSP	B-TaskName
is	O
a	O
binary	O
classification	O
loss	O
for	O
predicting	O
whether	O
two	O
segments	O
follow	O
each	O
other	O
in	O
the	O
original	O
text.	O
Positive	O
examples	O
are	O
created	O
by	O
taking	O
consecutive	O
sentences	O
from	O
the	O
text	O
corpus.	O
Negative	O
examples	O
are	O
created	O
by	O
pairing	O
segments	O
from	O
different	O
documents.	O
Positive	O
and	O
negative	O
examples	O
are	O
sampled	O
with	O
equal	O
probability.	O
The	O
NSP	B-TaskName
objective	O
was	O
designed	O
to	O
improve	O
performance	O
on	O
downstream	O
tasks,	O
such	O
as	O
Natural	O
Language	O
Inference	O
(Bowman	O
et	O
al.,	O
2015),	O
which	O
require	O
reasoning	O
about	O
the	O
relationships	O
between	O
pairs	O
of	O
sentences.	O

BERT	B-MethodName
uses	O
the	O
now	O
ubiquitous	O
transformer	O
architecture	O
(Vaswani	O
et	O
al.,	O
2017),	O
which	O
we	O
will	O
not	O
review	O
in	O
detail.	O
We	O
use	O
a	O
transformer	O
architecture	O
with	O
L	O
layers.	O
Each	O
block	O
uses	O
A	O
self-attention	O
heads	O
and	O
hidden	O
dimension	O
H.	O

BERT	B-MethodName
takes	O
as	O
input	O
a	O
concatenation	O
of	O
two	O
segments	O
(sequences	O
of	O
tokens),	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
and	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
.	O
Segments	O
usually	O
consist	O
of	O
more	O
than	O
one	O
natural	O
sentence.	O
The	O
two	O
segments	O
are	O
presented	O
as	O
a	O
single	O
input	O
sequence	O
to	O
BERT	B-MethodName
with	O
special	O
tokens	O
delimiting	O
them:	O
[CLS	O
],	O
x	O
1	O
,	O
.	O
.	O
.	O
,	O
x	O
N	O
,	O
[SEP	O
],	O
y	O
1	O
,	O
.	O
.	O
.	O
,	O
y	O
M	O
,	O
[EOS	O
].	O
M	O
and	O
N	O
are	O
constrained	O
such	O
that	O
M	O
+	O
N	O
<	O
T	O
,	O
where	O
T	O
is	O
a	O
parameter	O
that	O
controls	O
the	O
maximum	O
sequence	O
length	O
during	O
training.	O
The	O
model	O
is	O
first	O
pretrained	O
on	O
a	O
large	O
unlabeled	O
text	O
corpus	O
and	O
subsequently	O
finetuned	O
using	O
end-task	O
labeled	O
data.	O

In	O
this	O
section,	O
we	O
give	O
a	O
brief	O
overview	O
of	O
the	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019)	O
pretraining	O
approach	O
and	O
some	O
of	O
the	O
training	O
choices	O
that	O
we	O
will	O
examine	O
experimentally	O
in	O
the	O
following	O
section.	O

Self-training	O
methods	O
such	O
as	O
ELMo	B-MethodName
(Peters	O
et	O
al.,	O
2018),	O
GPT	B-MethodName
(Radford	O
et	O
al.,	O
2018),	O
BERT	B-MethodName
(Devlin	O
et	O
al.,	O
2019),	O
XLM	B-MethodName
(Lample	O
and	O
Conneau,	O
2019),	O
and	O
XLNet	B-MethodName
(Yang	O
et	O
al.,	O
2019)	O
have	O
brought	O
significant	O
performance	O
gains,	O
but	O
it	O
can	O
be	O
challenging	O
to	O
determine	O
which	O
aspects	O
of	O
the	O
methods	O
contribute	O
the	O
most.	O
Training	O
is	O
computationally	O
expensive,	O
limiting	O
the	O
amount	O
of	O
tuning	O
that	O
can	O
be	O
done,	O
and	O
is	O
often	O
done	O
with	O
private	O
training	O
data	O
of	O
varying	O
sizes,	O
limiting	O
our	O
ability	O
to	O
measure	O
the	O
effects	O
of	O
the	O
modeling	O
advances.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019),	O
which	O
includes	O
a	O
careful	O
evaluation	O
of	O
the	O
effects	O
of	O
hyperparmeter	O
tuning	O
and	O
training	O
set	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained	O
and	O
propose	O
an	O
improved	O
recipe	O
for	O
training	O
BERT	B-MethodName
models,	O
which	O
we	O
call	O
RoBERTa,	B-MethodName
that	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
all	O
of	O
the	O
post-BERT	O
methods.	O
Our	O
modifications	O
are	O
simple,	O
they	O
include:	O
(1)	O
training	O
the	O
model	O
longer,	O
with	O
bigger	O
batches,	O
over	O
more	O
data;	O
(2)	O
removing	O
the	O
next	O
sentence	O
prediction	O
objective;	O
(3)	O
training	O
on	O
longer	O
sequences;	O
and	O
(4)	O
dynamically	O
changing	O
the	O
masking	O
pattern	O
applied	O
to	O
the	O
training	O
data.	O
We	O
also	O
collect	O
a	O
large	O
new	O
dataset	O
(CC-NEWS)	B-DatasetName
of	O
comparable	O
size	O
to	O
other	O
privately	O
used	O
datasets,	O
to	O
better	O
control	O
for	O
training	O
set	O
size	O
effects.	O
When	O
controlling	O
for	O
training	O
data,	O
our	O
improved	O
training	O
procedure	O
improves	O
upon	O
the	O
published	O
BERT	B-MethodName
results	O
on	O
both	O
GLUE	B-DatasetName
and	O
SQuAD.	B-DatasetName
When	O
trained	O
for	O
longer	O
over	O
additional	O
data,	O
our	O
model	O
achieves	O
a	O
score	O
of	O
88.5	B-MetricValue
on	O
the	O
public	O
GLUE	B-DatasetName
leaderboard,	O
matching	O
the	O
88.4	B-MetricValue
reported	O
by	O
Yang	O
et	O
al.	O
(2019).	O
Our	O
model	O
establishes	O
a	O
new	O
state-of-the-art	O
on	O
4/9	O
of	O
the	O
GLUE	B-DatasetName
tasks:	O
MNLI,	B-DatasetName
QNLI,	B-DatasetName
RTE	B-DatasetName
and	O
STS-B.	B-DatasetName
We	O
also	O
match	O
state-of-the-art	O
results	O
on	O
SQuAD	B-DatasetName
and	O
RACE.	B-DatasetName
Overall,	O
we	O
re-establish	O
that	O
BERT's	B-MethodName
masked	O
language	O
model	O
training	O
objective	O
is	O
competitive	O
with	O
other	O
recently	O
proposed	O
training	O
objectives	O
such	O
as	O
perturbed	O
autoregressive	O
language	O
modeling	O
(Yang	O
et	O
al.,	O
2019).	O
2	O
In	O
summary,	O
the	O
contributions	O
of	O
this	O
paper	O
are:	O
(1)	O
We	O
present	O
a	O
set	O
of	O
important	O
BERT	B-MethodName
design	O
choices	O
and	O
training	O
strategies	O
and	O
introduce	O
alternatives	O
that	O
lead	O
to	O
better	O
downstream	O
task	O
performance;	O
(2)	O
We	O
use	O
a	O
novel	O
dataset,	O
CC-NEWS,	B-DatasetName
and	O
confirm	O
that	O
using	O
more	O
data	O
for	O
pretraining	O
further	O
improves	O
performance	O
on	O
downstream	O
tasks;	O
(3)	O
Our	O
training	O
improvements	O
show	O
that	O
masked	O
language	O
model	O
pretraining,	O
under	O
the	O
right	O
design	O
choices,	O
is	O
competitive	O
with	O
all	O
other	O
recently	O
published	O
methods.	O
We	O
release	O
our	O
model,	O
pretraining	O
and	O
fine-tuning	O
code	O
implemented	O
in	O
PyTorch	O
(Paszke	O
et	O
al.,	O
2017).	O

Language	O
model	O
pretraining	O
has	O
led	O
to	O
significant	O
performance	O
gains	O
but	O
careful	O
comparison	O
between	O
different	O
approaches	O
is	O
challenging.	O
Training	O
is	O
computationally	O
expensive,	O
often	O
done	O
on	O
private	O
datasets	O
of	O
different	O
sizes,	O
and,	O
as	O
we	O
will	O
show,	O
hyperparameter	O
choices	O
have	O
significant	O
impact	O
on	O
the	O
final	O
results.	O
We	O
present	O
a	O
replication	O
study	O
of	O
BERT	B-MethodName
pretraining	O
(Devlin	O
et	O
al.,	O
2019)	O
that	O
carefully	O
measures	O
the	O
impact	O
of	O
many	O
key	O
hyperparameters	O
and	O
training	O
data	O
size.	O
We	O
find	O
that	O
BERT	B-MethodName
was	O
significantly	O
undertrained,	O
and	O
can	O
match	O
or	O
exceed	O
the	O
performance	O
of	O
every	O
model	O
published	O
after	O
it.	O
Our	O
best	O
model	O
achieves	O
state-of-the-art	O
results	O
on	O
GLUE,	B-DatasetName
RACE	B-DatasetName
and	O
SQuAD.	B-DatasetName
These	O
results	O
highlight	O
the	O
importance	O
of	O
previously	O
overlooked	O
design	O
choices,	O
and	O
raise	O
questions	O
about	O
the	O
source	O
of	O
recently	O
reported	O
improvements.	O
We	O
release	O
our	O
models	O
and	O
code.	O
1	O

In	O
Table	O
8	O
we	O
present	O
the	O
full	O
set	O
of	O
development	O
set	O
results	O
for	O
RoBERTa.	B-MethodName
We	O
present	O
results	O
for	O
a	O
LARGE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
LARGE	I-MethodName
,	O
as	O
well	O
as	O
a	O
BASE	O
configuration	O
that	O
follows	O
BERT	B-MethodName
BASE	I-MethodName
.	O

Appendix	O
for	O
"RoBERTa:	B-MethodName
A	O
Robustly	O
Optimized	O
BERT	O
Pretraining	O
Approach"	O