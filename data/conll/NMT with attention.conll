-DOCSTART- -X- O
In -X- _ O
Table -X- _ O
1, -X- _ O
we -X- _ O
list -X- _ O
the -X- _ O
translation -X- _ O
performances -X- _ O
measured -X- _ O
in -X- _ O
BLEU -X- _ B-MetricName
score. -X- _ O
It -X- _ O
is -X- _ O
clear -X- _ O
from -X- _ O
the -X- _ O
table -X- _ O
that -X- _ O
in -X- _ O
all -X- _ O
the -X- _ O
cases, -X- _ O
the -X- _ O
proposed -X- _ O
RNNsearch -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
conventional -X- _ O
RNNencdec. -X- _ B-MethodName
More -X- _ O
importantly, -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
RNNsearch -X- _ B-MethodName
is -X- _ O
as -X- _ O
high -X- _ O
as -X- _ O
that -X- _ O
of -X- _ O
the -X- _ O
conventional -X- _ O
phrase-based -X- _ O
translation -X- _ O
system -X- _ O
(Moses), -X- _ O
when -X- _ O
only -X- _ O
the -X- _ O
sentences -X- _ O
consisting -X- _ O
of -X- _ O
known -X- _ O
words -X- _ O
are -X- _ O
considered. -X- _ O
This -X- _ O
is -X- _ O
a -X- _ O
significant -X- _ O
achievement, -X- _ O
considering -X- _ O
that -X- _ O
Moses -X- _ O
uses -X- _ O
a -X- _ O
separate -X- _ O
monolingual -X- _ O
corpus -X- _ O
(418M -X- _ O
words) -X- _ O
in -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
parallel -X- _ O
corpora -X- _ O
we -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
RNNsearch -X- _ B-MethodName
and -X- _ O
RNNencdec. -X- _ B-MethodName
(a -X- _ O
) -X- _ O
(b) -X- _ O
(c) -X- _ O
(d) -X- _ O
Figure -X- _ O
3: -X- _ O
Four -X- _ O
sample -X- _ O
alignments -X- _ O
found -X- _ O
by -X- _ O
RNNsearch-50. -X- _ B-MethodName
The -X- _ O
x-axis -X- _ O
and -X- _ O
y-axis -X- _ O
of -X- _ O
each -X- _ O
plot -X- _ O
correspond -X- _ O
to -X- _ O
the -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
(English) -X- _ O
and -X- _ O
the -X- _ O
generated -X- _ O
translation -X- _ O
(French), -X- _ O
respectively. -X- _ O
Each -X- _ O
pixel -X- _ O
shows -X- _ O
the -X- _ O
weight -X- _ O
↵ -X- _ O
ij -X- _ O
of -X- _ O
the -X- _ O
annotation -X- _ O
of -X- _ O
the -X- _ O
j-th -X- _ O
source -X- _ O
word -X- _ O
for -X- _ O
the -X- _ O
i-th -X- _ O
target -X- _ O
word -X- _ O
(see -X- _ O
Eq. -X- _ O
( -X- _ O
6)), -X- _ O
in -X- _ O
grayscale -X- _ O
(0: -X- _ O
black, -X- _ O
1: -X- _ O
white). -X- _ O
(a) -X- _ O
an -X- _ O
arbitrary -X- _ O
sentence. -X- _ O
(b-d) -X- _ O
three -X- _ O
randomly -X- _ O
selected -X- _ O
samples -X- _ O
among -X- _ O
the -X- _ O
sentences -X- _ B-HyperparameterName
without -X- _ I-HyperparameterName
any -X- _ I-HyperparameterName
unknown -X- _ I-HyperparameterName
words -X- _ I-HyperparameterName
and -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
between -X- _ O
10 -X- _ B-HyperparameterValue
and -X- _ I-HyperparameterValue
20 -X- _ I-HyperparameterValue
words -X- _ I-HyperparameterValue
from -X- _ O
the -X- _ O
test -X- _ O
set. -X- _ O
One -X- _ O
of -X- _ O
the -X- _ O
motivations -X- _ O
behind -X- _ O
the -X- _ O
proposed -X- _ O
approach -X- _ O
was -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
fixed-length -X- _ O
context -X- _ O
vector -X- _ O
in -X- _ O
the -X- _ O
basic -X- _ O
encoder-decoder -X- _ O
approach. -X- _ O
We -X- _ O
conjectured -X- _ O
that -X- _ O
this -X- _ O
limitation -X- _ O
may -X- _ O
make -X- _ O
the -X- _ O
basic -X- _ O
encoder-decoder -X- _ O
approach -X- _ O
to -X- _ O
underperform -X- _ O
with -X- _ O
long -X- _ O
sentences. -X- _ O
In -X- _ O
Fig. -X- _ O
2, -X- _ O
we -X- _ O
see -X- _ O
that -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
RNNencdec -X- _ B-MethodName
dramatically -X- _ O
drops -X- _ O
as -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
sentences -X- _ O
increases. -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
hand, -X- _ O
both -X- _ O
RNNsearch-30 -X- _ B-MethodName
and -X- _ O
RNNsearch-50 -X- _ B-MethodName
are -X- _ O
more -X- _ O
robust -X- _ O
to -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
sentences. -X- _ O
RNNsearch-50, -X- _ B-MethodName
especially, -X- _ O
shows -X- _ O
no -X- _ O
performance -X- _ O
deterioration -X- _ O
even -X- _ O
with -X- _ O
sentences -X- _ O
of -X- _ O
length -X- _ O
50 -X- _ O
or -X- _ O
more. -X- _ O
This -X- _ O
superiority -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
over -X- _ O
the -X- _ O
basic -X- _ O
encoder-decoder -X- _ O
is -X- _ O
further -X- _ O
confirmed -X- _ O
by -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
RNNsearch-30 -X- _ B-MethodName
even -X- _ O
outperforms -X- _ O
RNNencdec-50 -X- _ B-MethodName
(see -X- _ O
Table -X- _ O
1). -X- _ O
tokens -X- _ O
when -X- _ O
only -X- _ O
the -X- _ O
sentences -X- _ O
having -X- _ O
no -X- _ O
unknown -X- _ O
words -X- _ O
were -X- _ O
evaluated -X- _ O
(last -X- _ O
column). -X- _ O

We -X- _ O
train -X- _ O
two -X- _ O
types -X- _ O
of -X- _ O
models. -X- _ O
The -X- _ O
first -X- _ O
one -X- _ O
is -X- _ O
an -X- _ O
RNN -X- _ B-MethodName
Encoder-Decoder -X- _ I-MethodName
(RNNencdec, -X- _ B-MethodName
Cho -X- _ O
et -X- _ O
al., -X- _ O
2014a), -X- _ O
and -X- _ O
the -X- _ O
other -X- _ O
is -X- _ O
the -X- _ O
proposed -X- _ O
model, -X- _ O
to -X- _ O
which -X- _ O
we -X- _ O
refer -X- _ O
as -X- _ O
RNNsearch. -X- _ B-MethodName
We -X- _ O
train -X- _ O
each -X- _ O
model -X- _ O
twice: -X- _ O
first -X- _ O
with -X- _ O
the -X- _ O
sentences -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
up -X- _ O
to -X- _ O
30 -X- _ B-HyperparameterValue
words -X- _ I-HyperparameterValue
(RNNencdec-30, -X- _ B-MethodName
RNNsearch-30) -X- _ B-MethodName
and -X- _ O
then -X- _ O
with -X- _ O
the -X- _ O
sentences -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
up -X- _ O
to -X- _ O
50 -X- _ B-HyperparameterValue
word -X- _ I-HyperparameterValue
(RNNencdec-50, -X- _ B-MethodName
RNNsearch-50). -X- _ B-MethodName
The -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
of -X- _ O
the -X- _ O
RNNencdec -X- _ B-MethodName
have -X- _ O
1000 -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
units -X- _ I-HyperparameterName
each. -X- _ O
7 -X- _ O
The -X- _ O
encoder -X- _ O
of -X- _ O
the -X- _ O
RNNsearch -X- _ B-MethodName
consists -X- _ O
of -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
recurrent -X- _ O
neural -X- _ O
networks -X- _ O
(RNN) -X- _ B-MethodName
each -X- _ O
having -X- _ O
1000 -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
units. -X- _ I-HyperparameterName
Its -X- _ O
decoder -X- _ O
has -X- _ O
1000 -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
units. -X- _ I-HyperparameterName
In -X- _ O
both -X- _ O
cases, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
multilayer -X- _ O
network -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
maxout -X- _ O
(Goodfellow -X- _ O
et -X- _ O
al., -X- _ O
2013) -X- _ O
hidden -X- _ O
layer -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
each -X- _ O
target -X- _ O
word -X- _ O
(Pascanu -X- _ O
et -X- _ O
al., -X- _ O
2014). -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
minibatch -X- _ O
stochastic -X- _ O
gradient -X- _ O
descent -X- _ O
(SGD) -X- _ O
algorithm -X- _ O
together -X- _ O
with -X- _ O
Adadelta -X- _ O
(Zeiler, -X- _ O
2012) -X- _ O
to -X- _ O
train -X- _ O
each -X- _ O
model. -X- _ O
Each -X- _ O
SGD -X- _ O
update -X- _ O
direction -X- _ O
is -X- _ O
computed -X- _ O
using -X- _ O
a -X- _ O
minibatch -X- _ B-HyperparameterName
of -X- _ O
80 -X- _ B-HyperparameterValue
sentences. -X- _ I-HyperparameterValue
We -X- _ O
trained -X- _ O
each -X- _ O
model -X- _ O
for -X- _ O
approximately -X- _ O
5 -X- _ O
days. -X- _ O
Once -X- _ O
a -X- _ O
model -X- _ O
is -X- _ O
trained, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
beam -X- _ O
search -X- _ O
to -X- _ O
find -X- _ O
a -X- _ O
translation -X- _ O
that -X- _ O
approximately -X- _ O
maximizes -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
(see, -X- _ O
e.g., -X- _ O
Graves, -X- _ O
2012;Boulanger-Lewandowski -X- _ O
et -X- _ O
al., -X- _ O
2013). -X- _ O
Sutskever -X- _ O
et -X- _ O
al. -X- _ O
(2014) -X- _ O
used -X- _ O
this -X- _ O
approach -X- _ O
to -X- _ O
generate -X- _ O
translations -X- _ O
from -X- _ O
their -X- _ O
neural -X- _ B-MethodName
machine -X- _ I-MethodName
translation -X- _ I-MethodName
model. -X- _ O
For -X- _ O
more -X- _ O
details -X- _ O
on -X- _ O
the -X- _ O
architectures -X- _ O
of -X- _ O
the -X- _ O
models -X- _ O
and -X- _ O
training -X- _ O
procedure -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
experiments, -X- _ O
see -X- _ O
Appendices -X- _ O
A -X- _ O
and -X- _ O
B. -X- _ O

We -X- _ O
evaluate -X- _ O
the -X- _ O
proposed -X- _ O
approach -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
English-to-French -X- _ B-TaskName
translation. -X- _ I-TaskName
We -X- _ O
use -X- _ O
the -X- _ O
bilingual, -X- _ B-DatasetName
parallel -X- _ I-DatasetName
corpora -X- _ I-DatasetName
provided -X- _ I-DatasetName
by -X- _ I-DatasetName
ACL -X- _ I-DatasetName
WMT -X- _ I-DatasetName
'14. -X- _ I-DatasetName
3 -X- _ O
As -X- _ O
a -X- _ O
comparison, -X- _ O
we -X- _ O
also -X- _ O
report -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
an -X- _ O
RNN -X- _ B-MethodName
Encoder-Decoder -X- _ I-MethodName
which -X- _ O
was -X- _ O
proposed -X- _ O
recently -X- _ O
by -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
(2014a). -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
training -X- _ O
procedures -X- _ O
and -X- _ O
the -X- _ O
same -X- _ O
dataset -X- _ O
for -X- _ O
both -X- _ O
models. -X- _ O
4 -X- _ O
4.1 -X- _ O
DATASET -X- _ O
WMT -X- _ B-DatasetName
'14 -X- _ I-DatasetName
contains -X- _ O
the -X- _ O
following -X- _ O
English-French -X- _ O
parallel -X- _ O
corpora: -X- _ O
Europarl -X- _ O
(61M -X- _ O
words), -X- _ O
news -X- _ O
commentary -X- _ O
(5.5M), -X- _ O
UN -X- _ O
(421M) -X- _ O
and -X- _ O
two -X- _ O
crawled -X- _ O
corpora -X- _ O
of -X- _ O
90M -X- _ O
and -X- _ O
272.5M -X- _ O
words -X- _ O
respectively, -X- _ O
totaling -X- _ O
850M -X- _ O
words. -X- _ O
Following -X- _ O
the -X- _ O
procedure -X- _ O
described -X- _ O
in -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
(2014a), -X- _ O
we -X- _ O
reduce -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
combined -X- _ O
corpus -X- _ O
to -X- _ O
have -X- _ O
348M -X- _ O
words -X- _ O
using -X- _ O
the -X- _ O
data -X- _ O
selection -X- _ O
method -X- _ O
by -X- _ O
Axelrod -X- _ O
et -X- _ O
al. -X- _ O
(2011). -X- _ O
5 -X- _ O
We -X- _ O
do -X- _ O
not -X- _ O
use -X- _ O
any -X- _ O
monolingual -X- _ O
data -X- _ O
other -X- _ O
than -X- _ O
the -X- _ O
mentioned -X- _ O
parallel -X- _ O
corpora, -X- _ O
although -X- _ O
it -X- _ O
may -X- _ O
be -X- _ O
possible -X- _ O
to -X- _ O
use -X- _ O
a -X- _ O
much -X- _ O
larger -X- _ O
monolingual -X- _ O
corpus -X- _ O
to -X- _ O
pretrain -X- _ O
an -X- _ O
encoder. -X- _ O
We -X- _ O
concatenate -X- _ O
news-test- -X- _ O
After -X- _ O
a -X- _ O
usual -X- _ O
tokenization -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
shortlist -X- _ O
of -X- _ O
30,000 -X- _ B-HyperparameterValue
most -X- _ B-HyperparameterName
frequent -X- _ I-HyperparameterName
words -X- _ I-HyperparameterName
in -X- _ O
each -X- _ O
language -X- _ O
to -X- _ O
train -X- _ O
our -X- _ O
models. -X- _ O
Any -X- _ O
word -X- _ O
not -X- _ O
included -X- _ O
in -X- _ O
the -X- _ O
shortlist -X- _ O
is -X- _ O
mapped -X- _ O
to -X- _ O
a -X- _ O
special -X- _ O
token -X- _ O
([UNK]). -X- _ O
We -X- _ O
do -X- _ O
not -X- _ O
apply -X- _ O
any -X- _ O
other -X- _ O
special -X- _ O
preprocessing, -X- _ O
such -X- _ O
as -X- _ O
lowercasing -X- _ O
or -X- _ O
stemming, -X- _ O
to -X- _ O
the -X- _ O
data. -X- _ O

The -X- _ O
usual -X- _ O
RNN, -X- _ B-MethodName
described -X- _ O
in -X- _ O
Eq. -X- _ O
(1), -X- _ O
reads -X- _ O
an -X- _ O
input -X- _ O
sequence -X- _ O
x -X- _ O
in -X- _ O
order -X- _ O
starting -X- _ O
from -X- _ O
the -X- _ O
first -X- _ O
symbol -X- _ O
x -X- _ O
1 -X- _ O
to -X- _ O
the -X- _ O
last -X- _ O
one -X- _ O
x -X- _ O
Tx -X- _ O
. -X- _ O
However, -X- _ O
in -X- _ O
the -X- _ O
proposed -X- _ O
scheme, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
the -X- _ O
annotation -X- _ O
of -X- _ O
each -X- _ O
word -X- _ O
to -X- _ O
summarize -X- _ O
not -X- _ O
only -X- _ O
the -X- _ O
preceding -X- _ O
words, -X- _ O
but -X- _ O
also -X- _ O
the -X- _ O
following -X- _ O
words. -X- _ O
Hence, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
use -X- _ O
a -X- _ O
bidirectional -X- _ B-MethodName
RNN -X- _ I-MethodName
(BiRNN, -X- _ B-MethodName
Schuster -X- _ O
and -X- _ O
Paliwal, -X- _ O
1997), -X- _ O
which -X- _ O
has -X- _ O
been -X- _ O
successfully -X- _ O
used -X- _ O
recently -X- _ O
in -X- _ O
speech -X- _ O
recognition -X- _ O
(see, -X- _ O
e.g., -X- _ O
Graves -X- _ O
et -X- _ O
al., -X- _ O
2013). -X- _ O
A -X- _ O
BiRNN -X- _ B-MethodName
consists -X- _ O
of -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
RNN's. -X- _ B-MethodName
The -X- _ O
forward -X- _ O
RNN -X- _ B-MethodName
! -X- _ O
f -X- _ O
reads -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
as -X- _ O
it -X- _ O
is -X- _ O
ordered -X- _ O
(from -X- _ O
x -X- _ O
1 -X- _ O
to -X- _ O
x -X- _ O
Tx -X- _ O
) -X- _ O
and -X- _ O
calculates -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
forward -X- _ O
hidden -X- _ O
states -X- _ O
( -X- _ O
! -X- _ O
h -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
! -X- _ O
h -X- _ O
Tx -X- _ O
). -X- _ O
The -X- _ O
backward -X- _ O
RNN -X- _ B-MethodName
f -X- _ O
reads -X- _ O
the -X- _ O
sequence -X- _ O
in -X- _ O
the -X- _ O
reverse -X- _ O
order -X- _ O
(from -X- _ O
x -X- _ O
Tx -X- _ O
to -X- _ O
x -X- _ O
1 -X- _ O
), -X- _ O
resulting -X- _ O
in -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
backward -X- _ O
hidden -X- _ O
states -X- _ O
( -X- _ O
h -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
h -X- _ O
Tx -X- _ O
). -X- _ O
We -X- _ O
obtain -X- _ O
an -X- _ O
annotation -X- _ O
for -X- _ O
each -X- _ O
word -X- _ O
x -X- _ O
j -X- _ O
by -X- _ O
concatenating -X- _ O
the -X- _ O
forward -X- _ O
hidden -X- _ O
state -X- _ O
! -X- _ O
h -X- _ O
j -X- _ O
and -X- _ O
the -X- _ O
backward -X- _ O
one -X- _ O
h -X- _ O
j -X- _ O
, -X- _ O
i.e., -X- _ O
h -X- _ O
j -X- _ O
= -X- _ O
h -X- _ O
! -X- _ O
h -X- _ O
> -X- _ O
j -X- _ O
; -X- _ O
h -X- _ O
> -X- _ O
j -X- _ O
i -X- _ O
> -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
way, -X- _ O
the -X- _ O
annotation -X- _ O
h -X- _ O
j -X- _ O
contains -X- _ O
the -X- _ O
summaries -X- _ O
of -X- _ O
both -X- _ O
the -X- _ O
preceding -X- _ O
words -X- _ O
and -X- _ O
the -X- _ O
following -X- _ O
words. -X- _ O
Due -X- _ O
to -X- _ O
the -X- _ O
tendency -X- _ O
of -X- _ O
RNNs -X- _ B-MethodName
to -X- _ O
better -X- _ O
represent -X- _ O
recent -X- _ O
inputs, -X- _ O
the -X- _ O
annotation -X- _ O
h -X- _ O
j -X- _ O
will -X- _ O
be -X- _ O
focused -X- _ O
on -X- _ O
the -X- _ O
words -X- _ O
around -X- _ O
x -X- _ O
j -X- _ O
. -X- _ O
This -X- _ O
sequence -X- _ O
of -X- _ O
annotations -X- _ O
is -X- _ O
used -X- _ O
by -X- _ O
the -X- _ O
decoder -X- _ O
and -X- _ O
the -X- _ O
alignment -X- _ O
model -X- _ O
later -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
context -X- _ O
vector -X- _ O
(Eqs. -X- _ O
( -X- _ O
5)-( -X- _ O
6)). -X- _ O
See -X- _ O
Fig. -X- _ O
1 -X- _ O
for -X- _ O
the -X- _ O
graphical -X- _ O
illustration -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
model. -X- _ O

DESCRIPTION -X- _ O
x -X- _ O
1 -X- _ O
x -X- _ O
2 -X- _ O
x -X- _ O
3 -X- _ O
x -X- _ O
T -X- _ O
+ -X- _ O
α -X- _ O
t,1 -X- _ O
α -X- _ O
t,2 -X- _ O
α -X- _ O
t,3 -X- _ O
α -X- _ O
t,T -X- _ O
y -X- _ O
t-1 -X- _ O
y -X- _ O
t -X- _ O
h -X- _ O
1 -X- _ O
h -X- _ O
2 -X- _ O
h -X- _ O
3 -X- _ O
h -X- _ O
T -X- _ O
h -X- _ O
1 -X- _ O
h -X- _ O
2 -X- _ O
h -X- _ O
3 -X- _ O
h -X- _ O
T -X- _ O
s -X- _ O
t-1 -X- _ O
s -X- _ O
t -X- _ O
Figure -X- _ O
1: -X- _ O
The -X- _ O
graphical -X- _ O
illustration -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
trying -X- _ O
to -X- _ O
generate -X- _ O
the -X- _ O
t-th -X- _ O
target -X- _ O
word -X- _ O
y -X- _ O
t -X- _ O
given -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
(x -X- _ O
1 -X- _ O
, -X- _ O
x -X- _ O
2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
x -X- _ O
T -X- _ O
). -X- _ O
In -X- _ O
a -X- _ O
new -X- _ O
model -X- _ O
architecture, -X- _ O
we -X- _ O
define -X- _ O
each -X- _ O
conditional -X- _ O
probability -X- _ O
in -X- _ O
Eq. -X- _ O
(2) -X- _ O
as: -X- _ O
p(y -X- _ O
i -X- _ O
|y -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
y -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
x) -X- _ O
= -X- _ O
g(y -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
i -X- _ O
, -X- _ O
c -X- _ O
i -X- _ O
),(4) -X- _ O
where -X- _ O
s -X- _ O
i -X- _ O
is -X- _ O
an -X- _ O
RNN -X- _ B-MethodName
hidden -X- _ O
state -X- _ O
for -X- _ O
time -X- _ O
i, -X- _ O
computed -X- _ O
by -X- _ O
s -X- _ O
i -X- _ O
= -X- _ O
f -X- _ O
(s -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
y -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
c -X- _ O
i -X- _ O
). -X- _ O
It -X- _ O
should -X- _ O
be -X- _ O
noted -X- _ O
that -X- _ O
unlike -X- _ O
the -X- _ O
existing -X- _ O
encoder-decoder -X- _ B-MethodName
approach -X- _ O
(see -X- _ O
Eq. -X- _ O
(2)), -X- _ O
here -X- _ O
the -X- _ O
probability -X- _ O
is -X- _ O
conditioned -X- _ O
on -X- _ O
a -X- _ O
distinct -X- _ O
context -X- _ O
vector -X- _ O
c -X- _ O
i -X- _ O
for -X- _ O
each -X- _ O
target -X- _ O
word -X- _ O
y -X- _ O
i -X- _ O
. -X- _ O
The -X- _ O
context -X- _ O
vector -X- _ O
c -X- _ O
i -X- _ O
depends -X- _ O
on -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
annotations -X- _ O
(h -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
h -X- _ O
Tx -X- _ O
) -X- _ O
to -X- _ O
which -X- _ O
an -X- _ O
encoder -X- _ O
maps -X- _ O
the -X- _ O
input -X- _ O
sentence. -X- _ O
Each -X- _ O
annotation -X- _ O
h -X- _ O
i -X- _ O
contains -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
whole -X- _ O
input -X- _ O
sequence -X- _ O
with -X- _ O
a -X- _ O
strong -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
parts -X- _ O
surrounding -X- _ O
the -X- _ O
i-th -X- _ O
word -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sequence. -X- _ O
We -X- _ O
explain -X- _ O
in -X- _ O
detail -X- _ O
how -X- _ O
the -X- _ O
annotations -X- _ O
are -X- _ O
computed -X- _ O
in -X- _ O
the -X- _ O
next -X- _ O
section. -X- _ O
The -X- _ O
context -X- _ O
vector -X- _ O
c -X- _ O
i -X- _ O
is, -X- _ O
then, -X- _ O
computed -X- _ O
as -X- _ O
a -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
these -X- _ O
annotations -X- _ O
h -X- _ O
i -X- _ O
: -X- _ O
c -X- _ O
i -X- _ O
= -X- _ O
Tx -X- _ O
X -X- _ O
j=1 -X- _ O
↵ -X- _ O
ij -X- _ O
h -X- _ O
j -X- _ O
.(5) -X- _ O
The -X- _ O
weight -X- _ O
↵ -X- _ O
ij -X- _ O
of -X- _ O
each -X- _ O
annotation -X- _ O
h -X- _ O
j -X- _ O
is -X- _ O
computed -X- _ O
by -X- _ O
↵ -X- _ O
ij -X- _ O
= -X- _ O
exp -X- _ O
(e -X- _ O
ij -X- _ O
) -X- _ O
P -X- _ O
Tx -X- _ O
k=1 -X- _ O
exp -X- _ O
(e -X- _ O
ik -X- _ O
) -X- _ O
,(6) -X- _ O
where -X- _ O
e -X- _ O
ij -X- _ O
= -X- _ O
a(s -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
h -X- _ O
j -X- _ O
) -X- _ O
is -X- _ O
an -X- _ O
alignment -X- _ O
model -X- _ O
which -X- _ O
scores -X- _ O
how -X- _ O
well -X- _ O
the -X- _ O
inputs -X- _ O
around -X- _ O
position -X- _ O
j -X- _ O
and -X- _ O
the -X- _ O
output -X- _ O
at -X- _ O
position -X- _ O
i -X- _ O
match. -X- _ O
The -X- _ O
score -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
hidden -X- _ O
state -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
(just -X- _ O
before -X- _ O
emitting -X- _ O
y -X- _ O
i -X- _ O
, -X- _ O
Eq. -X- _ O
(4)) -X- _ O
and -X- _ O
the -X- _ O
j-th -X- _ O
annotation -X- _ O
h -X- _ O
j -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sentence. -X- _ O
We -X- _ O
parametrize -X- _ O
the -X- _ O
alignment -X- _ O
model -X- _ O
a -X- _ O
as -X- _ O
a -X- _ O
feedforward -X- _ O
neural -X- _ O
network -X- _ O
which -X- _ O
is -X- _ O
jointly -X- _ O
trained -X- _ O
with -X- _ O
all -X- _ O
the -X- _ O
other -X- _ O
components -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
system. -X- _ O
Note -X- _ O
that -X- _ O
unlike -X- _ O
in -X- _ O
traditional -X- _ O
machine -X- _ O
translation, -X- _ O
the -X- _ O
alignment -X- _ O
is -X- _ O
not -X- _ O
considered -X- _ O
to -X- _ O
be -X- _ O
a -X- _ O
latent -X- _ O
variable. -X- _ O
Instead, -X- _ O
the -X- _ O
alignment -X- _ O
model -X- _ O
directly -X- _ O
computes -X- _ O
a -X- _ O
soft -X- _ O
alignment, -X- _ O
which -X- _ O
allows -X- _ O
the -X- _ O
gradient -X- _ O
of -X- _ O
the -X- _ O
cost -X- _ O
function -X- _ O
to -X- _ O
be -X- _ O
backpropagated -X- _ O
through. -X- _ O
This -X- _ O
gradient -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
alignment -X- _ O
model -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
whole -X- _ O
translation -X- _ O
model -X- _ O
jointly. -X- _ O
We -X- _ O
can -X- _ O
understand -X- _ O
the -X- _ O
approach -X- _ O
of -X- _ O
taking -X- _ O
a -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
all -X- _ O
the -X- _ O
annotations -X- _ O
as -X- _ O
computing -X- _ O
an -X- _ O
expected -X- _ O
annotation, -X- _ O
where -X- _ O
the -X- _ O
expectation -X- _ O
is -X- _ O
over -X- _ O
possible -X- _ O
alignments. -X- _ O
Let -X- _ O
↵ -X- _ O
ij -X- _ O
be -X- _ O
a -X- _ O
probability -X- _ O
that -X- _ O
the -X- _ O
target -X- _ O
word -X- _ O
y -X- _ O
i -X- _ O
is -X- _ O
aligned -X- _ O
to, -X- _ O
or -X- _ O
translated -X- _ O
from, -X- _ O
a -X- _ O
source -X- _ O
word -X- _ O
x -X- _ O
j -X- _ O
. -X- _ O
Then, -X- _ O
the -X- _ O
i-th -X- _ O
context -X- _ O
vector -X- _ O
c -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
expected -X- _ O
annotation -X- _ O
over -X- _ O
all -X- _ O
the -X- _ O
annotations -X- _ O
with -X- _ O
probabilities -X- _ O
↵ -X- _ O
ij -X- _ O
. -X- _ O
The -X- _ O
probability -X- _ O
↵ -X- _ O
ij -X- _ O
, -X- _ O
or -X- _ O
its -X- _ O
associated -X- _ O
energy -X- _ O
e -X- _ O
ij -X- _ O
, -X- _ O
reflects -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
the -X- _ O
annotation -X- _ O
h -X- _ O
j -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
the -X- _ O
previous -X- _ O
hidden -X- _ O
state -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
in -X- _ O
deciding -X- _ O
the -X- _ O
next -X- _ O
state -X- _ O
s -X- _ O
i -X- _ O
and -X- _ O
generating -X- _ O
y -X- _ O
i -X- _ O
. -X- _ O
Intuitively, -X- _ O
this -X- _ O
implements -X- _ O
a -X- _ O
mechanism -X- _ O
of -X- _ O
attention -X- _ B-MethodName
in -X- _ I-MethodName
the -X- _ I-MethodName
decoder. -X- _ I-MethodName
The -X- _ O
decoder -X- _ O
decides -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
to -X- _ O
pay -X- _ O
attention -X- _ O
to. -X- _ O
By -X- _ O
letting -X- _ O
the -X- _ O
decoder -X- _ O
have -X- _ O
an -X- _ O
attention -X- _ O
mechanism, -X- _ O
we -X- _ O
relieve -X- _ O
the -X- _ O
encoder -X- _ O
from -X- _ O
the -X- _ O
burden -X- _ O
of -X- _ O
having -X- _ O
to -X- _ O
encode -X- _ O
all -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixedlength -X- _ O
vector. -X- _ O
With -X- _ O
this -X- _ O
new -X- _ O
approach -X- _ O
the -X- _ O
information -X- _ O
can -X- _ O
be -X- _ O
spread -X- _ O
throughout -X- _ O
the -X- _ O
sequence -X- _ O
of -X- _ O
annotations, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
selectively -X- _ O
retrieved -X- _ O
by -X- _ O
the -X- _ O
decoder -X- _ O
accordingly. -X- _ O

In -X- _ O
this -X- _ O
section, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
architecture -X- _ O
for -X- _ O
neural -X- _ O
machine -X- _ O
translation. -X- _ O
The -X- _ O
new -X- _ O
architecture -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
bidirectional -X- _ B-MethodName
RNN -X- _ I-MethodName
as -X- _ I-MethodName
an -X- _ I-MethodName
encoder -X- _ I-MethodName
(Sec. -X- _ O
3.2) -X- _ O
and -X- _ O
a -X- _ O
decoder -X- _ O
that -X- _ O
emulates -X- _ O
searching -X- _ O
through -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
during -X- _ O
decoding -X- _ O
a -X- _ O
translation -X- _ O
(Sec. -X- _ O
3.1). -X- _ O

Neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
is -X- _ O
a -X- _ O
recently -X- _ O
proposed -X- _ O
approach -X- _ O
to -X- _ O
machine -X- _ O
translation. -X- _ O
Unlike -X- _ O
the -X- _ O
traditional -X- _ O
statistical -X- _ O
machine -X- _ O
translation, -X- _ O
the -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
aims -X- _ O
at -X- _ O
building -X- _ O
a -X- _ O
single -X- _ O
neural -X- _ O
network -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
jointly -X- _ O
tuned -X- _ O
to -X- _ O
maximize -X- _ O
the -X- _ O
translation -X- _ O
performance. -X- _ O
The -X- _ O
models -X- _ O
proposed -X- _ O
recently -X- _ O
for -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
often -X- _ O
belong -X- _ O
to -X- _ O
a -X- _ O
family -X- _ O
of -X- _ O
encoder-decoders -X- _ B-MethodName
and -X- _ O
encode -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector -X- _ O
from -X- _ O
which -X- _ O
a -X- _ O
decoder -X- _ O
generates -X- _ O
a -X- _ O
translation. -X- _ O
In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
conjecture -X- _ O
that -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
fixed-length -X- _ O
vector -X- _ O
is -X- _ O
a -X- _ O
bottleneck -X- _ O
in -X- _ O
improving -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
this -X- _ O
basic -X- _ B-MethodName
encoder-decoder -X- _ I-MethodName
architecture, -X- _ O
and -X- _ O
propose -X- _ O
to -X- _ O
extend -X- _ O
this -X- _ O
by -X- _ O
allowing -X- _ O
a -X- _ O
model -X- _ O
to -X- _ O
automatically -X- _ O
(soft-)search -X- _ O
for -X- _ O
parts -X- _ O
of -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
that -X- _ O
are -X- _ O
relevant -X- _ O
to -X- _ O
predicting -X- _ O
a -X- _ O
target -X- _ O
word, -X- _ O
without -X- _ O
having -X- _ O
to -X- _ O
form -X- _ O
these -X- _ O
parts -X- _ O
as -X- _ O
a -X- _ O
hard -X- _ O
segment -X- _ O
explicitly. -X- _ O
With -X- _ O
this -X- _ O
new -X- _ O
approach, -X- _ O
we -X- _ O
achieve -X- _ O
a -X- _ O
translation -X- _ O
performance -X- _ O
comparable -X- _ O
to -X- _ O
the -X- _ O
existing -X- _ O
state-of-the-art -X- _ O
phrase-based -X- _ O
system -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
English-to-French -X- _ B-TaskName
translation. -X- _ I-TaskName
Furthermore, -X- _ O
qualitative -X- _ O
analysis -X- _ O
reveals -X- _ O
that -X- _ O
the -X- _ O
(soft-)alignments -X- _ O
found -X- _ O
by -X- _ O
the -X- _ O
model -X- _ O
agree -X- _ O
well -X- _ O
with -X- _ O
our -X- _ O
intuition. -X- _ O

Neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
is -X- _ O
a -X- _ O
newly -X- _ O
emerging -X- _ O
approach -X- _ O
to -X- _ O
machine -X- _ O
translation, -X- _ O
recently -X- _ O
proposed -X- _ O
by -X- _ O
Kalchbrenner -X- _ O
and -X- _ O
Blunsom -X- _ O
(2013), -X- _ O
Sutskever -X- _ O
et -X- _ O
al. -X- _ O
(2014) -X- _ O
and -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
(2014b). -X- _ O
Unlike -X- _ O
the -X- _ O
traditional -X- _ O
phrase-based -X- _ O
translation -X- _ O
system -X- _ O
(see, -X- _ O
e.g., -X- _ O
Koehn -X- _ O
et -X- _ O
al., -X- _ O
2003) -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
many -X- _ O
small -X- _ O
sub-components -X- _ O
that -X- _ O
are -X- _ O
tuned -X- _ O
separately, -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
attempts -X- _ O
to -X- _ O
build -X- _ O
and -X- _ O
train -X- _ O
a -X- _ O
single, -X- _ O
large -X- _ O
neural -X- _ O
network -X- _ O
that -X- _ O
reads -X- _ O
a -X- _ O
sentence -X- _ O
and -X- _ O
outputs -X- _ O
a -X- _ O
correct -X- _ O
translation. -X- _ O
Most -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
models -X- _ O
belong -X- _ O
to -X- _ O
a -X- _ O
family -X- _ O
of -X- _ O
encoderdecoders -X- _ O
(Sutskever -X- _ O
et -X- _ O
al., -X- _ O
2014;Cho -X- _ O
et -X- _ O
al., -X- _ O
2014a), -X- _ O
with -X- _ O
an -X- _ O
encoder -X- _ B-MethodName
and -X- _ I-MethodName
a -X- _ I-MethodName
decoder -X- _ I-MethodName
for -X- _ O
each -X- _ O
language, -X- _ O
or -X- _ O
involve -X- _ O
a -X- _ O
language-specific -X- _ O
encoder -X- _ O
applied -X- _ O
to -X- _ O
each -X- _ O
sentence -X- _ O
whose -X- _ O
outputs -X- _ O
are -X- _ O
then -X- _ O
compared -X- _ O
(Hermann -X- _ O
and -X- _ O
Blunsom, -X- _ O
2014). -X- _ O
An -X- _ O
encoder -X- _ O
neural -X- _ O
network -X- _ O
reads -X- _ O
and -X- _ O
encodes -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector. -X- _ O
A -X- _ O
decoder -X- _ O
then -X- _ O
outputs -X- _ O
a -X- _ O
translation -X- _ O
from -X- _ O
the -X- _ O
encoded -X- _ O
vector. -X- _ O
The -X- _ O
whole -X- _ O
encoder-decoder -X- _ O
system, -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
the -X- _ O
decoder -X- _ O
for -X- _ O
a -X- _ O
language -X- _ O
pair, -X- _ O
is -X- _ O
jointly -X- _ O
trained -X- _ O
to -X- _ O
maximize -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
a -X- _ O
correct -X- _ O
translation -X- _ O
given -X- _ O
a -X- _ O
source -X- _ O
sentence. -X- _ O
A -X- _ O
potential -X- _ O
issue -X- _ O
with -X- _ O
this -X- _ O
encoder-decoder -X- _ B-MethodName
approach -X- _ O
is -X- _ O
that -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
needs -X- _ O
to -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
compress -X- _ O
all -X- _ O
the -X- _ O
necessary -X- _ O
information -X- _ O
of -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector. -X- _ O
This -X- _ O
may -X- _ O
make -X- _ O
it -X- _ O
difficult -X- _ O
for -X- _ O
the -X- _ O
neural -X- _ O
network -X- _ O
to -X- _ O
cope -X- _ O
with -X- _ O
long -X- _ O
sentences, -X- _ O
especially -X- _ O
those -X- _ O
that -X- _ O
are -X- _ O
longer -X- _ O
than -X- _ O
the -X- _ O
sentences -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
corpus. -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
(2014b) -X- _ O
showed -X- _ O
that -X- _ O
indeed -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
a -X- _ O
basic -X- _ O
encoder-decoder -X- _ O
deteriorates -X- _ O
rapidly -X- _ O
as -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
an -X- _ O
input -X- _ O
sentence -X- _ O
increases. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
address -X- _ O
this -X- _ O
issue, -X- _ O
we -X- _ O
introduce -X- _ O
an -X- _ O
extension -X- _ O
to -X- _ O
the -X- _ O
encoder-decoder -X- _ O
model -X- _ O
which -X- _ O
learns -X- _ O
to -X- _ O
align -X- _ O
and -X- _ O
translate -X- _ O
jointly. -X- _ O
Each -X- _ O
time -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
generates -X- _ O
a -X- _ O
word -X- _ O
in -X- _ O
a -X- _ O
translation, -X- _ O
it -X- _ O
(soft-)searches -X- _ O
for -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
positions -X- _ O
in -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
where -X- _ O
the -X- _ O
most -X- _ O
relevant -X- _ O
information -X- _ O
is -X- _ O
concentrated. -X- _ O
The -X- _ O
model -X- _ O
then -X- _ O
predicts -X- _ O
a -X- _ O
target -X- _ O
word -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
context -X- _ O
vectors -X- _ O
associated -X- _ O
with -X- _ O
these -X- _ O
source -X- _ O
positions -X- _ O
and -X- _ O
all -X- _ O
the -X- _ O
previous -X- _ O
generated -X- _ O
target -X- _ O
words. -X- _ O

The -X- _ O
most -X- _ O
important -X- _ O
distinguishing -X- _ O
feature -X- _ O
of -X- _ O
this -X- _ O
approach -X- _ O
from -X- _ O
the -X- _ O
basic -X- _ B-MethodName
encoder-decoder -X- _ I-MethodName
is -X- _ O
that -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
attempt -X- _ O
to -X- _ O
encode -X- _ O
a -X- _ O
whole -X- _ O
input -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
fixed-length -X- _ O
vector. -X- _ O
Instead, -X- _ O
it -X- _ O
encodes -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
vectors -X- _ O
and -X- _ O
chooses -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
these -X- _ O
vectors -X- _ O
adaptively -X- _ O
while -X- _ O
decoding -X- _ O
the -X- _ O
translation. -X- _ O
This -X- _ O
frees -X- _ O
a -X- _ O
neural -X- _ O
translation -X- _ O
model -X- _ O
from -X- _ O
having -X- _ O
to -X- _ O
squash -X- _ O
all -X- _ O
the -X- _ O
information -X- _ O
of -X- _ O
a -X- _ O
source -X- _ O
sentence, -X- _ O
regardless -X- _ O
of -X- _ O
its -X- _ O
length, -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector. -X- _ O
We -X- _ O
show -X- _ O
this -X- _ O
allows -X- _ O
a -X- _ O
model -X- _ O
to -X- _ O
cope -X- _ O
better -X- _ O
with -X- _ O
long -X- _ O
sentences. -X- _ O
In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
proposed -X- _ O
approach -X- _ O
of -X- _ O
jointly -X- _ B-MethodName
learning -X- _ I-MethodName
to -X- _ I-MethodName
align -X- _ I-MethodName
and -X- _ I-MethodName
translate -X- _ I-MethodName
achieves -X- _ O
significantly -X- _ O
improved -X- _ O
translation -X- _ O
performance -X- _ O
over -X- _ O
the -X- _ O
basic -X- _ B-MethodName
encoder-decoder -X- _ I-MethodName
approach. -X- _ O
The -X- _ O
improvement -X- _ O
is -X- _ O
more -X- _ O
apparent -X- _ O
with -X- _ O
longer -X- _ O
sentences, -X- _ O
but -X- _ O
can -X- _ O
be -X- _ O
observed -X- _ O
with -X- _ O
sentences -X- _ O
of -X- _ O
any -X- _ O
length. -X- _ O
On -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
English-to-French -X- _ B-TaskName
translation, -X- _ I-TaskName
the -X- _ O
proposed -X- _ O
approach -X- _ O
achieves, -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
model, -X- _ O
a -X- _ O
translation -X- _ O
performance -X- _ O
comparable, -X- _ O
or -X- _ O
close, -X- _ O
to -X- _ O
the -X- _ O
conventional -X- _ O
phrase-based -X- _ O
system. -X- _ O
Furthermore, -X- _ O
qualitative -X- _ O
analysis -X- _ O
reveals -X- _ O
that -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
finds -X- _ O
a -X- _ O
linguistically -X- _ O
plausible -X- _ O
(soft-)alignment -X- _ O
between -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
and -X- _ O
the -X- _ O
corresponding -X- _ O
target -X- _ O
sentence. -X- _ O

From -X- _ O
a -X- _ O
probabilistic -X- _ O
perspective, -X- _ O
translation -X- _ O
is -X- _ O
equivalent -X- _ O
to -X- _ O
finding -X- _ O
a -X- _ O
target -X- _ O
sentence -X- _ O
y -X- _ O
that -X- _ O
maximizes -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
y -X- _ O
given -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
x, -X- _ O
i.e., -X- _ O
arg -X- _ O
max -X- _ O
y -X- _ O
p(y -X- _ O
| -X- _ O
x). -X- _ O
In -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation, -X- _ I-TaskName
we -X- _ O
fit -X- _ O
a -X- _ O
parameterized -X- _ O
model -X- _ O
to -X- _ O
maximize -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
sentence -X- _ O
pairs -X- _ O
using -X- _ O
a -X- _ O
parallel -X- _ O
training -X- _ O
corpus. -X- _ O
Once -X- _ O
the -X- _ O
conditional -X- _ O
distribution -X- _ O
is -X- _ O
learned -X- _ O
by -X- _ O
a -X- _ O
translation -X- _ O
model, -X- _ O
given -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
a -X- _ O
corresponding -X- _ O
translation -X- _ O
can -X- _ O
be -X- _ O
generated -X- _ O
by -X- _ O
searching -X- _ O
for -X- _ O
the -X- _ O
sentence -X- _ O
that -X- _ O
maximizes -X- _ O
the -X- _ O
conditional -X- _ O
probability. -X- _ O
Recently, -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
papers -X- _ O
have -X- _ O
proposed -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
neural -X- _ O
networks -X- _ O
to -X- _ O
directly -X- _ O
learn -X- _ O
this -X- _ O
conditional -X- _ O
distribution -X- _ O
(see, -X- _ O
e.g., -X- _ O
Kalchbrenner -X- _ O
and -X- _ O
Blunsom, -X- _ O
2013;Cho -X- _ O
et -X- _ O
al., -X- _ O
2014a;Sutskever -X- _ O
et -X- _ O
al., -X- _ O
2014;Cho -X- _ O
et -X- _ O
al., -X- _ O
2014b;Forcada -X- _ O
andÑeco, -X- _ O
1997). -X- _ O
This -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
approach -X- _ O
typically -X- _ O
consists -X- _ O
of -X- _ O
two -X- _ O
components, -X- _ O
the -X- _ O
first -X- _ O
of -X- _ O
which -X- _ O
encodes -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
x -X- _ O
and -X- _ O
the -X- _ O
second -X- _ O
decodes -X- _ O
to -X- _ O
a -X- _ O
target -X- _ O
sentence -X- _ O
y. -X- _ O
For -X- _ O
instance, -X- _ O
two -X- _ O
recurrent -X- _ O
neural -X- _ O
networks -X- _ O
(RNN) -X- _ O
were -X- _ O
used -X- _ O
by -X- _ O
(Cho -X- _ O
et -X- _ O
al., -X- _ O
2014a) -X- _ O
and -X- _ O
(Sutskever -X- _ O
et -X- _ O
al., -X- _ O
2014) -X- _ O
to -X- _ O
encode -X- _ O
a -X- _ O
variable-length -X- _ O
source -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector -X- _ O
and -X- _ O
to -X- _ O
decode -X- _ O
the -X- _ O
vector -X- _ O
into -X- _ O
a -X- _ O
variable-length -X- _ O
target -X- _ O
sentence. -X- _ O
Despite -X- _ O
being -X- _ O
a -X- _ O
quite -X- _ O
new -X- _ O
approach, -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
has -X- _ O
already -X- _ O
shown -X- _ O
promising -X- _ O
results. -X- _ O
Sutskever -X- _ O
et -X- _ O
al. -X- _ O
(2014) -X- _ O
reported -X- _ O
that -X- _ O
the -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
based -X- _ O
on -X- _ O
RNNs -X- _ O
with -X- _ O
long -X- _ O
shortterm -X- _ O
memory -X- _ O
(LSTM) -X- _ O
units -X- _ O
achieves -X- _ O
close -X- _ O
to -X- _ O
the -X- _ O
state-of-the-art -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
conventional -X- _ O
phrase-based -X- _ O
machine -X- _ O
translation -X- _ O
system -X- _ O
on -X- _ O
an -X- _ O
English-to-French -X- _ B-TaskName
translation -X- _ I-TaskName
task. -X- _ O
1 -X- _ O
Adding -X- _ O
neural -X- _ O
components -X- _ O
to -X- _ O
existing -X- _ O
translation -X- _ O
systems, -X- _ O
for -X- _ O
instance, -X- _ O
to -X- _ O
score -X- _ O
the -X- _ O
phrase -X- _ O
pairs -X- _ O
in -X- _ O
the -X- _ O
phrase -X- _ O
table -X- _ O
(Cho -X- _ O
et -X- _ O
al., -X- _ O
2014a) -X- _ O
or -X- _ O
to -X- _ O
re-rank -X- _ O
candidate -X- _ O
translations -X- _ O
(Sutskever -X- _ O
et -X- _ O
al., -X- _ O
2014), -X- _ O
has -X- _ O
allowed -X- _ O
to -X- _ O
surpass -X- _ O
the -X- _ O
previous -X- _ O
state-of-the-art -X- _ O
performance -X- _ O
level. -X- _ O

Ce -X- _ O
type -X- _ O
d'expérience -X- _ O
entre -X- _ O
dans -X- _ O
le -X- _ O
cadre -X- _ O
des -X- _ O
efforts -X- _ O
de -X- _ O
Disney -X- _ O
pour -X- _ O
"étendre -X- _ O
la -X- _ O
durée -X- _ O
de -X- _ O
vie -X- _ O
de -X- _ O
ses -X- _ O
séries -X- _ O
et -X- _ O
construire -X- _ O
de -X- _ O
nouvelles -X- _ O
relations -X- _ O
avec -X- _ O
son -X- _ O
public -X- _ O
grâceà -X- _ O
des -X- _ O
plateformes -X- _ O
numériques -X- _ O
qui -X- _ O
sont -X- _ O
de -X- _ O
plus -X- _ O
en -X- _ O
plus -X- _ O
importantes", -X- _ O
a-t-il -X- _ O
ajouté. -X- _ O

Ce -X- _ O
type -X- _ O
d'expérience -X- _ O
fait -X- _ O
partie -X- _ O
des -X- _ O
initiatives -X- _ O
du -X- _ O
Disney -X- _ O
pour -X- _ O
"prolonger -X- _ O
la -X- _ O
durée -X- _ O
de -X- _ O
vie -X- _ O
de -X- _ O
ses -X- _ O
nouvelles -X- _ O
et -X- _ O
de -X- _ O
développer -X- _ O
des -X- _ O
liens -X- _ O
avec -X- _ O
les -X- _ O
lecteurs -X- _ O
numériques -X- _ O
qui -X- _ O
deviennent -X- _ O
plus -X- _ O
complexes. -X- _ O
RNNsearch-50 -X- _ B-MethodName
Ce -X- _ O
genre -X- _ O
d'expérience -X- _ O
fait -X- _ O
partie -X- _ O
des -X- _ O
efforts -X- _ O
de -X- _ O
Disney -X- _ O
pour -X- _ O
"prolonger -X- _ O
la -X- _ O
durée -X- _ O
de -X- _ O
vie -X- _ O
de -X- _ O
ses -X- _ O
séries -X- _ O
et -X- _ O
créer -X- _ O
de -X- _ O
nouvelles -X- _ O
relations -X- _ O
avec -X- _ O
des -X- _ O
publics -X- _ O
via -X- _ O
des -X- _ O
plateformes -X- _ O
numériques -X- _ O
de -X- _ O
plus -X- _ O
en -X- _ O
plus -X- _ O
importantes", -X- _ O
a-t-il -X- _ O
ajouté. -X- _ O
Google -X- _ B-MethodName
Translate -X- _ I-MethodName
Ce -X- _ O
genre -X- _ O
d'expérience -X- _ O
fait -X- _ O
partie -X- _ O
des -X- _ O
efforts -X- _ O
de -X- _ O
Disneyà -X- _ O
"étendre -X- _ O
la -X- _ O
durée -X- _ O
de -X- _ O
vie -X- _ O
de -X- _ O
sa -X- _ O
série -X- _ O
et -X- _ O
construire -X- _ O
de -X- _ O
nouvelles -X- _ O
relations -X- _ O
avec -X- _ O
le -X- _ O
public -X- _ O
par -X- _ O
le -X- _ O
biais -X- _ O
des -X- _ O
plates-formes -X- _ O
numériques -X- _ O
qui -X- _ O
deviennent -X- _ O
de -X- _ O
plus -X- _ O
en -X- _ O
plus -X- _ O
important", -X- _ O
at-il -X- _ O
ajouté. -X- _ O

In -X- _ O
a -X- _ O
press -X- _ O
conference -X- _ O
on -X- _ O
Thursday, -X- _ O
Mr -X- _ O
Blair -X- _ O
stated -X- _ O
that -X- _ O
there -X- _ O
was -X- _ O
nothing -X- _ O
in -X- _ O
this -X- _ O
video -X- _ O
that -X- _ O
might -X- _ O
constitute -X- _ O
a -X- _ O
"reasonable -X- _ O
motive" -X- _ O
that -X- _ O
could -X- _ O
lead -X- _ O
to -X- _ O
criminal -X- _ O
charges -X- _ O
being -X- _ O
brought -X- _ O
against -X- _ O
the -X- _ O
mayor. -X- _ O
Reference -X- _ O
En -X- _ O
conférence -X- _ O
de -X- _ O
presse, -X- _ O
jeudi, -X- _ O
M. -X- _ O
Blair -X- _ O
a -X- _ O
affirmé -X- _ O
qu'il -X- _ O
n'y -X- _ O
avait -X- _ O
rien -X- _ O
dans -X- _ O
cette -X- _ O
vidéo -X- _ O
qui -X- _ O
puisse -X- _ O
constituer -X- _ O
des -X- _ O
"motifs -X- _ O
raisonnables" -X- _ O
pouvant -X- _ O
mener -X- _ O
au -X- _ O
dépôt -X- _ O
d'une -X- _ O
accusation -X- _ O
criminelle -X- _ O
contre -X- _ O
le -X- _ O
maire. -X- _ O
RNNenc-50 -X- _ B-MethodName
Lors -X- _ O
de -X- _ O
la -X- _ O
conférence -X- _ O
de -X- _ O
presse -X- _ O
de -X- _ O
jeudi, -X- _ O
M. -X- _ O
Blair -X- _ O
a -X- _ O
dit -X- _ O
qu'il -X- _ O
n'y -X- _ O
avait -X- _ O
rien -X- _ O
dans -X- _ O
cette -X- _ O
vidéo -X- _ O
qui -X- _ O
pourrait -X- _ O
constituer -X- _ O
une -X- _ O
"motivation -X- _ O
raisonnable" -X- _ O
pouvant -X- _ O
entraîner -X- _ O
des -X- _ O
accusations -X- _ O
criminelles -X- _ O
portées -X- _ O
contre -X- _ O
le -X- _ O
maire. -X- _ O
RNNsearch-50 -X- _ B-MethodName
Lors -X- _ O
d'une -X- _ O
conférence -X- _ O
de -X- _ O
presse -X- _ O
jeudi, -X- _ O
M. -X- _ O
Blair -X- _ O
a -X- _ O
déclaré -X- _ O
qu'il -X- _ O
n'y -X- _ O
avait -X- _ O
rien -X- _ O
dans -X- _ O
cette -X- _ O
vidéo -X- _ O
qui -X- _ O
pourrait -X- _ O
constituer -X- _ O
un -X- _ O
"motif -X- _ O
raisonnable" -X- _ O
qui -X- _ O
pourrait -X- _ O
conduireà -X- _ O
des -X- _ O
accusations -X- _ O
criminelles -X- _ O
contre -X- _ O
le -X- _ O
maire. -X- _ O
Google -X- _ B-MethodName
Translate -X- _ I-MethodName
Lors -X- _ O
d'une -X- _ O
conférence -X- _ O
de -X- _ O
presse -X- _ O
jeudi, -X- _ O
M. -X- _ O
Blair -X- _ O
a -X- _ O
déclaré -X- _ O
qu'il -X- _ O
n'y -X- _ O
avait -X- _ O
rien -X- _ O
dans -X- _ O
cette -X- _ O
vido -X- _ O
qui -X- _ O
pourrait -X- _ O
constituer -X- _ O
un -X- _ O
"motif -X- _ O
raisonnable" -X- _ O
qui -X- _ O
pourrait -X- _ O
menerà -X- _ O
des -X- _ O
accusations -X- _ O
criminelles -X- _ O
portes -X- _ O
contre -X- _ O
le -X- _ O
maire. -X- _ O

Un -X- _ O
privilège -X- _ O
d'admission -X- _ O
est -X- _ O
le -X- _ O
droit -X- _ O
d'un -X- _ O
médecin -X- _ O
de -X- _ O
reconnaître -X- _ O
un -X- _ O
patientà -X- _ O
l'hôpital -X- _ O
ou -X- _ O
un -X- _ O
centre -X- _ O
médical -X- _ O
d'un -X- _ O
diagnostic -X- _ O
ou -X- _ O
de -X- _ O
prendre -X- _ O
un -X- _ O
diagnostic -X- _ O
en -X- _ O
fonction -X- _ O
de -X- _ O
sonétat -X- _ O
de -X- _ O
santé. -X- _ O
RNNsearch-50 -X- _ B-MethodName
Un -X- _ O
privilège -X- _ O
d'admission -X- _ O
est -X- _ O
le -X- _ O
droit -X- _ O
d'un -X- _ O
médecin -X- _ O
d'admettre -X- _ O
un -X- _ O
patientà -X- _ O
un -X- _ O
hôpital -X- _ O
ou -X- _ O
un -X- _ O
centre -X- _ O
médical -X- _ O
pour -X- _ O
effectuer -X- _ O
un -X- _ O
diagnostic -X- _ O
ou -X- _ O
une -X- _ O
procédure, -X- _ O
selon -X- _ O
son -X- _ O
statut -X- _ O
de -X- _ O
travailleur -X- _ O
des -X- _ O
soins -X- _ O
de -X- _ O
santéà -X- _ O
l'hôpital. -X- _ O
Google -X- _ B-MethodName
Translate -X- _ I-MethodName
Un -X- _ O
privilège -X- _ O
admettre -X- _ O
est -X- _ O
le -X- _ O
droit -X- _ O
d'un -X- _ O
médecin -X- _ O
d'admettre -X- _ O
un -X- _ O
patient -X- _ O
dans -X- _ O
un -X- _ O
hôpital -X- _ O
ou -X- _ O
un -X- _ O
centre -X- _ O
médical -X- _ O
pour -X- _ O
effectuer -X- _ O
un -X- _ O
diagnostic -X- _ O
ou -X- _ O
une -X- _ O
procédure, -X- _ O
fondée -X- _ O
sur -X- _ O
sa -X- _ O
situation -X- _ O
en -X- _ O
tant -X- _ O
que -X- _ O
travailleur -X- _ O
de -X- _ O
soins -X- _ O
de -X- _ O
santé -X- _ O
dans -X- _ O
un -X- _ O
hôpital. -X- _ O

This -X- _ O
kind -X- _ O
of -X- _ O
experience -X- _ O
is -X- _ O
part -X- _ O
of -X- _ O
Disney's -X- _ O
efforts -X- _ O
to -X- _ O
"extend -X- _ O
the -X- _ O
lifetime -X- _ O
of -X- _ O
its -X- _ O
series -X- _ O
and -X- _ O
build -X- _ O
new -X- _ O
relationships -X- _ O
with -X- _ O
audiences -X- _ O
via -X- _ O
digital -X- _ O
platforms -X- _ O
that -X- _ O
are -X- _ O
becoming -X- _ O
ever -X- _ O
more -X- _ O
important," -X- _ O
he -X- _ O
added. -X- _ O

Le -X- _ O
privilège -X- _ O
d'admission -X- _ O
est -X- _ O
le -X- _ O
droit -X- _ O
d'un -X- _ O
médecin, -X- _ O
en -X- _ O
vertu -X- _ O
de -X- _ O
son -X- _ O
statut -X- _ O
de -X- _ O
membre -X- _ O
soignant -X- _ O
d'un -X- _ O
hôpital, -X- _ O
d'admettre -X- _ O
un -X- _ O
patient -X- _ O
dans -X- _ O
un -X- _ O
hôpital -X- _ O
ou -X- _ O
un -X- _ O
centre -X- _ O
médical -X- _ O
afin -X- _ O
d'y -X- _ O
délivrer -X- _ O
un -X- _ O
diagnostic -X- _ O
ou -X- _ O
un -X- _ O
traitement. -X- _ O

An -X- _ O
admitting -X- _ O
privilege -X- _ O
is -X- _ O
the -X- _ O
right -X- _ O
of -X- _ O
a -X- _ O
doctor -X- _ O
to -X- _ O
admit -X- _ O
a -X- _ O
patient -X- _ O
to -X- _ O
a -X- _ O
hospital -X- _ O
or -X- _ O
a -X- _ O
medical -X- _ O
centre -X- _ O
to -X- _ O
carry -X- _ O
out -X- _ O
a -X- _ O
diagnosis -X- _ O
or -X- _ O
a -X- _ O
procedure, -X- _ O
based -X- _ O
on -X- _ O
his -X- _ O
status -X- _ O
as -X- _ O
a -X- _ O
health -X- _ O
care -X- _ O
worker -X- _ O
at -X- _ O
a -X- _ O
hospital. -X- _ O

We -X- _ O
used -X- _ O
the -X- _ O
stochastic -X- _ O
gradient -X- _ O
descent -X- _ O
(SGD) -X- _ O
algorithm. -X- _ O
Adadelta -X- _ O
(Zeiler, -X- _ O
2012) -X- _ O
was -X- _ O
used -X- _ O
to -X- _ O
automatically -X- _ O
adapt -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
each -X- _ O
parameter -X- _ O
(✏ -X- _ O
= -X- _ O
10 -X- _ B-HyperparameterValue
6 -X- _ I-HyperparameterValue
and -X- _ O
⇢ -X- _ O
= -X- _ O
0.95). -X- _ B-HyperparameterValue
We -X- _ O
explicitly -X- _ O
normalized -X- _ O
the -X- _ O
L -X- _ O
2 -X- _ O
-norm -X- _ O
of -X- _ O
the -X- _ O
gradient -X- _ O
of -X- _ O
the -X- _ O
cost -X- _ O
function -X- _ O
each -X- _ O
time -X- _ O
to -X- _ O
be -X- _ O
at -X- _ O
most -X- _ O
a -X- _ O
predefined -X- _ O
threshold -X- _ O
of -X- _ O
1, -X- _ O
when -X- _ O
the -X- _ O
norm -X- _ O
was -X- _ O
larger -X- _ O
than -X- _ O
the -X- _ O
threshold -X- _ O
(Pascanu -X- _ O
et -X- _ O
al., -X- _ O
2013b). -X- _ O
Each -X- _ O
SGD -X- _ O
update -X- _ O
direction -X- _ O
was -X- _ O
computed -X- _ O
with -X- _ O
a -X- _ O
minibatch -X- _ B-HyperparameterName
of -X- _ O
80 -X- _ B-HyperparameterValue
sentences. -X- _ I-HyperparameterValue
At -X- _ O
each -X- _ O
update -X- _ O
our -X- _ O
implementation -X- _ O
requires -X- _ O
time -X- _ O
proportional -X- _ O
to -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
longest -X- _ O
sentence -X- _ O
in -X- _ O
a -X- _ O
minibatch. -X- _ B-HyperparameterName
Hence, -X- _ O
to -X- _ O
minimize -X- _ O
the -X- _ O
waste -X- _ O
of -X- _ O
computation, -X- _ O
before -X- _ O
every -X- _ O
20-th -X- _ O
update, -X- _ O
we -X- _ O
retrieved -X- _ O
1600 -X- _ O
sentence -X- _ O
pairs, -X- _ O
sorted -X- _ O
them -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
lengths -X- _ O
and -X- _ O
split -X- _ O
them -X- _ O
into -X- _ O
20 -X- _ O
minibatches. -X- _ B-HyperparameterName
The -X- _ O
training -X- _ O
data -X- _ O
was -X- _ O
shuffled -X- _ O
once -X- _ O
before -X- _ O
training -X- _ O
and -X- _ O
was -X- _ O
traversed -X- _ O
sequentially -X- _ O
in -X- _ O
this -X- _ O
manner. -X- _ O
In -X- _ O
Tables -X- _ O
2 -X- _ O
we -X- _ O
present -X- _ O
the -X- _ O
statistics -X- _ O
related -X- _ O
to -X- _ O
training -X- _ O
all -X- _ O
the -X- _ O
models -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
experiments. -X- _ O

For -X- _ O
all -X- _ O
the -X- _ O
models -X- _ O
used -X- _ O
in -X- _ O
this -X- _ O
paper, -X- _ O
the -X- _ O
size -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
a -X- _ I-HyperparameterName
hidden -X- _ I-HyperparameterName
layer -X- _ I-HyperparameterName
n -X- _ I-HyperparameterName
is -X- _ O
1000, -X- _ B-HyperparameterValue
the -X- _ O
word -X- _ B-HyperparameterName
embedding -X- _ I-HyperparameterName
dimensionality -X- _ I-HyperparameterName
m -X- _ B-HyperparameterName
is -X- _ O
620 -X- _ B-HyperparameterValue
and -X- _ O
the -X- _ O
size -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
maxout -X- _ I-HyperparameterName
hidden -X- _ I-HyperparameterName
layer -X- _ I-HyperparameterName
in -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
deep -X- _ I-HyperparameterName
output -X- _ I-HyperparameterName
l -X- _ B-HyperparameterName
is -X- _ O
500. -X- _ B-HyperparameterValue
The -X- _ O
number -X- _ O
of -X- _ O
hidden -X- _ O
units -X- _ O
in -X- _ O
the -X- _ O
alignment -X- _ O
model -X- _ O
n -X- _ O
0 -X- _ O
is -X- _ O
1000. -X- _ B-HyperparameterValue
U -X- _ O
r -X- _ O
as -X- _ O
random -X- _ O
orthogonal -X- _ O
matrices. -X- _ O
For -X- _ O
W -X- _ O
a -X- _ O
and -X- _ O
U -X- _ O
a -X- _ O
, -X- _ O
we -X- _ O
initialized -X- _ O
them -X- _ O
by -X- _ O
sampling -X- _ O
each -X- _ O
element -X- _ O
from -X- _ O
the -X- _ O
Gaussian -X- _ O
distribution -X- _ O
of -X- _ O
mean -X- _ O
0 -X- _ O
and -X- _ O
variance -X- _ O
0.001 -X- _ O
2 -X- _ O
. -X- _ O
All -X- _ O
the -X- _ O
elements -X- _ O
of -X- _ O
V -X- _ O
a -X- _ O
and -X- _ O
all -X- _ O
the -X- _ O
bias -X- _ O
vectors -X- _ O
were -X- _ O
initialized -X- _ O
to -X- _ O
zero. -X- _ O
Any -X- _ O
other -X- _ O
weight -X- _ O
matrix -X- _ O
was -X- _ O
initialized -X- _ O
by -X- _ O
sampling -X- _ O
from -X- _ O
the -X- _ O
Gaussian -X- _ O
distribution -X- _ O
of -X- _ O
mean -X- _ O
0 -X- _ O
and -X- _ O
variance -X- _ O
0.01 -X- _ O
2 -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section, -X- _ O
we -X- _ O
describe -X- _ O
in -X- _ O
detail -X- _ O
the -X- _ O
architecture -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
(RNNsearch) -X- _ B-MethodName
used -X- _ O
in -X- _ O
the -X- _ O
experiments -X- _ O
(see -X- _ O
. -X- _ O
From -X- _ O
here -X- _ O
on, -X- _ O
we -X- _ O
omit -X- _ O
all -X- _ O
bias -X- _ O
terms -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
increase -X- _ O
readability. -X- _ O
The -X- _ O
model -X- _ O
takes -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
of -X- _ O
1-of-K -X- _ O
coded -X- _ O
word -X- _ O
vectors -X- _ O
as -X- _ O
input -X- _ O
x -X- _ O
= -X- _ O
(x -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
x -X- _ O
Tx -X- _ O
), -X- _ O
x -X- _ O
i -X- _ O
2 -X- _ O
R -X- _ O
Kx -X- _ O
and -X- _ O
outputs -X- _ O
a -X- _ O
translated -X- _ O
sentence -X- _ O
of -X- _ O
1-of-K -X- _ O
coded -X- _ O
word -X- _ O
vectors -X- _ O
y -X- _ O
= -X- _ O
(y -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
y -X- _ O
Ty -X- _ O
), -X- _ O
y -X- _ O
i -X- _ O
2 -X- _ O
R -X- _ O
Ky -X- _ O
, -X- _ O
where -X- _ O
K -X- _ B-HyperparameterName
x -X- _ I-HyperparameterName
and -X- _ O
K -X- _ B-HyperparameterName
y -X- _ I-HyperparameterName
are -X- _ O
the -X- _ O
vocabulary -X- _ B-HyperparameterName
sizes -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
source -X- _ I-HyperparameterName
and -X- _ I-HyperparameterName
target -X- _ I-HyperparameterName
languages, -X- _ I-HyperparameterName
respectively. -X- _ B-HyperparameterName
T -X- _ I-HyperparameterName
x -X- _ I-HyperparameterName
and -X- _ O
T -X- _ B-HyperparameterName
y -X- _ I-HyperparameterName
respectively -X- _ O
denote -X- _ O
the -X- _ O
lengths -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
source -X- _ I-HyperparameterName
and -X- _ I-HyperparameterName
target -X- _ I-HyperparameterName
sentences. -X- _ I-HyperparameterName
First, -X- _ O
the -X- _ O
forward -X- _ O
states -X- _ O
of -X- _ O
the -X- _ O
bidirectional -X- _ O
recurrent -X- _ O
neural -X- _ O
network -X- _ O
(BiRNN) -X- _ B-MethodName
are -X- _ O
computed: -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
= -X- _ O
( -X- _ O
(1 -X- _ O
! -X- _ O
z -X- _ O
i -X- _ O
) -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
! -X- _ O
z -X- _ O
i -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
, -X- _ O
if -X- _ O
i -X- _ O
> -X- _ O
0 -X- _ O
0 -X- _ O
, -X- _ O
if -X- _ O
i -X- _ O
= -X- _ O
0 -X- _ O
where -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
= -X- _ O
tanh -X- _ O
⇣ -X- _ O
! -X- _ O
W -X- _ O
Ex -X- _ O
i -X- _ O
+ -X- _ O
! -X- _ O
U -X- _ O
h -X- _ O
! -X- _ O
r -X- _ O
i -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
1 -X- _ O
i⌘ -X- _ O
! -X- _ O
z -X- _ O
i -X- _ O
= -X- _ O
⇣ -X- _ O
! -X- _ O
W -X- _ O
z -X- _ O
Ex -X- _ O
i -X- _ O
+ -X- _ O
! -X- _ O
U -X- _ O
z -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
1 -X- _ O
⌘ -X- _ O
! -X- _ O
r -X- _ O
i -X- _ O
= -X- _ O
⇣ -X- _ O
! -X- _ O
W -X- _ O
r -X- _ O
Ex -X- _ O
i -X- _ O
+ -X- _ O
! -X- _ O
U -X- _ O
r -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
1 -X- _ O
⌘ -X- _ O
. -X- _ O
E -X- _ O
2 -X- _ O
R -X- _ O
m⇥Kx -X- _ O
is -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
matrix. -X- _ O
! -X- _ O
W -X- _ O
, -X- _ O
! -X- _ O
W -X- _ O
z -X- _ O
, -X- _ O
! -X- _ O
W -X- _ O
r -X- _ O
2 -X- _ O
R -X- _ O
n⇥m -X- _ O
, -X- _ O
! -X- _ O
U -X- _ O
, -X- _ O
! -X- _ O
U -X- _ O
z -X- _ O
, -X- _ O
! -X- _ O
U -X- _ O
r -X- _ O
2 -X- _ O
R -X- _ O
n⇥n -X- _ O
are -X- _ O
weight -X- _ O
matrices. -X- _ O
m -X- _ O
and -X- _ O
n -X- _ O
are -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
dimensionality -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
hidden -X- _ O
units, -X- _ O
respectively. -X- _ O
(•) -X- _ O
is -X- _ O
as -X- _ O
usual -X- _ O
a -X- _ O
logistic -X- _ O
sigmoid -X- _ O
function. -X- _ O
The -X- _ O
backward -X- _ O
states -X- _ O
( -X- _ O
h -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
h -X- _ O
Tx -X- _ O
) -X- _ O
are -X- _ O
computed -X- _ O
similarly. -X- _ O
We -X- _ O
share -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
matrix -X- _ O
E -X- _ O
between -X- _ O
the -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
RNNs, -X- _ B-MethodName
unlike -X- _ O
the -X- _ O
weight -X- _ O
matrices. -X- _ O
We -X- _ O
concatenate -X- _ O
the -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
states -X- _ O
to -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
annotations -X- _ O
(h -X- _ O
1 -X- _ O
, -X- _ O
h -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
h -X- _ O
Tx -X- _ O
), -X- _ O
where -X- _ O
h -X- _ O
i -X- _ O
= -X- _ O
" -X- _ O
! -X- _ O
h -X- _ O
i -X- _ O
h -X- _ O
i -X- _ O
# -X- _ O
(7) -X- _ O
A.2.2 -X- _ O
DECODER -X- _ O
The -X- _ O
hidden -X- _ O
state -X- _ O
s -X- _ O
i -X- _ O
of -X- _ O
the -X- _ O
decoder -X- _ O
given -X- _ O
the -X- _ O
annotations -X- _ O
from -X- _ O
the -X- _ O
encoder -X- _ O
is -X- _ O
computed -X- _ O
by -X- _ O
s -X- _ O
i -X- _ O
=(1 -X- _ O
z -X- _ O
i -X- _ O
) -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
z -X- _ O
i -X- _ O
s -X- _ O
i -X- _ O
, -X- _ O
wheres -X- _ O
i -X- _ O
= -X- _ O
tanh -X- _ O
(W -X- _ O
Ey -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
U -X- _ O
[r -X- _ O
i -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
] -X- _ O
+ -X- _ O
Cc -X- _ O
i -X- _ O
) -X- _ O
z -X- _ O
i -X- _ O
= -X- _ O
(W -X- _ O
z -X- _ O
Ey -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
U -X- _ O
z -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
C -X- _ O
z -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
r -X- _ O
i -X- _ O
= -X- _ O
(W -X- _ O
r -X- _ O
Ey -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
U -X- _ O
r -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
C -X- _ O
r -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
E -X- _ O
is -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
matrix -X- _ O
for -X- _ O
the -X- _ O
target -X- _ O
language. -X- _ O
W, -X- _ O
W -X- _ O
z -X- _ O
, -X- _ O
W -X- _ O
r -X- _ O
2 -X- _ O
R -X- _ O
n⇥m -X- _ O
, -X- _ O
U, -X- _ O
U -X- _ O
z -X- _ O
, -X- _ O
U -X- _ O
r -X- _ O
2 -X- _ O
R -X- _ O
n⇥n -X- _ O
, -X- _ O
and -X- _ O
C, -X- _ O
C -X- _ O
z -X- _ O
, -X- _ O
C -X- _ O
r -X- _ O
2 -X- _ O
R -X- _ O
n⇥2n -X- _ O
are -X- _ O
weights. -X- _ O
Again, -X- _ O
m -X- _ O
and -X- _ O
n -X- _ O
are -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
dimensionality -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
hidden -X- _ O
units, -X- _ O
respectively. -X- _ O
The -X- _ O
initial -X- _ O
hidden -X- _ O
state -X- _ O
s -X- _ O
0 -X- _ O
is -X- _ O
computed -X- _ O
by -X- _ O
s -X- _ O
0 -X- _ O
= -X- _ O
tanh -X- _ O
⇣ -X- _ O
W -X- _ O
s -X- _ O
h -X- _ O
1 -X- _ O
⌘ -X- _ O
, -X- _ O
where -X- _ O
W -X- _ O
s -X- _ O
2 -X- _ O
R -X- _ O
n⇥n -X- _ O
. -X- _ O
The -X- _ O
context -X- _ O
vector -X- _ O
c -X- _ O
i -X- _ O
are -X- _ O
recomputed -X- _ O
at -X- _ O
each -X- _ O
step -X- _ O
by -X- _ O
the -X- _ O
alignment -X- _ O
model: -X- _ O
c -X- _ O
i -X- _ O
= -X- _ O
Tx -X- _ O
X -X- _ O
j=1 -X- _ O
↵ -X- _ O
ij -X- _ O
h -X- _ O
j -X- _ O
, -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
U -X- _ O
a -X- _ O
h -X- _ O
j -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
h -X- _ O
j -X- _ O
is -X- _ O
the -X- _ O
j-th -X- _ O
annotation -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
(see -X- _ O
Eq. -X- _ O
( -X- _ O
7)). -X- _ O
v -X- _ O
a -X- _ O
2 -X- _ O
R -X- _ O
n -X- _ O
0 -X- _ O
, -X- _ O
W -X- _ O
a -X- _ O
2 -X- _ O
R -X- _ O
n -X- _ O
0 -X- _ O
⇥n -X- _ O
and -X- _ O
U -X- _ O
a -X- _ O
2 -X- _ O
R -X- _ O
n -X- _ O
0 -X- _ O
⇥2n -X- _ O
are -X- _ O
weight -X- _ O
matrices. -X- _ O
Note -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
becomes -X- _ O
RNN -X- _ O
Encoder-Decoder -X- _ O
(Cho -X- _ O
et -X- _ O
al., -X- _ O
2014a), -X- _ O
if -X- _ O
we -X- _ O
fix -X- _ O
c -X- _ O
i -X- _ O
to -X- _ O
! -X- _ O
h -X- _ O
Tx -X- _ O
. -X- _ O
With -X- _ O
the -X- _ O
decoder -X- _ O
state -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
context -X- _ O
c -X- _ O
i -X- _ O
and -X- _ O
the -X- _ O
last -X- _ O
generated -X- _ O
word -X- _ O
y -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
a -X- _ O
target -X- _ O
word -X- _ O
y -X- _ O
i -X- _ O
as -X- _ O
p(y -X- _ O
i -X- _ O
|s -X- _ O
i -X- _ O
, -X- _ O
y -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
/ -X- _ O
exp -X- _ O
y -X- _ O
> -X- _ O
i -X- _ O
W -X- _ O
o -X- _ O
t -X- _ O
i -X- _ O
, -X- _ O
where -X- _ O
t -X- _ O
i -X- _ O
= -X- _ O
⇥ -X- _ O
max -X- _ O
t -X- _ O
i,2j -X- _ O
1 -X- _ O
,t -X- _ O
i,2j -X- _ O
⇤ -X- _ O
> -X- _ O
j=1,...,l -X- _ O
andt -X- _ O
i,k -X- _ O
is -X- _ O
the -X- _ O
k-th -X- _ O
element -X- _ O
of -X- _ O
a -X- _ O
vectort -X- _ O
i -X- _ O
which -X- _ O
is -X- _ O
computed -X- _ O
bỹ -X- _ O
t -X- _ O
i -X- _ O
=U -X- _ O
o -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
V -X- _ O
o -X- _ O
Ey -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
C -X- _ O
o -X- _ O
c -X- _ O
i -X- _ O
. -X- _ O
W -X- _ O
o -X- _ O
2 -X- _ O
R -X- _ O
Ky⇥l -X- _ O
, -X- _ O
U -X- _ O
o -X- _ O
2 -X- _ O
R -X- _ O
2l⇥n -X- _ O
, -X- _ O
V -X- _ O
o -X- _ O
2 -X- _ O
R -X- _ O
2l⇥m -X- _ O
and -X- _ O
C -X- _ O
o -X- _ O
2 -X- _ O
R -X- _ O
2l⇥2n -X- _ O
are -X- _ O
weight -X- _ O
matrices. -X- _ O
This -X- _ O
can -X- _ O
be -X- _ O
understood -X- _ O
as -X- _ O
having -X- _ O
a -X- _ O
deep -X- _ O
output -X- _ O
(Pascanu -X- _ O
et -X- _ O
al., -X- _ O
2014) -X- _ O
with -X- _ O
a -X- _ O
single -X- _ B-HyperparameterValue
maxout -X- _ B-HyperparameterName
hidden -X- _ I-HyperparameterName
layer -X- _ I-HyperparameterName
(Goodfellow -X- _ O
et -X- _ O
al., -X- _ O
2013). -X- _ O

The -X- _ O
alignment -X- _ O
model -X- _ O
should -X- _ O
be -X- _ O
designed -X- _ O
considering -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
needs -X- _ O
to -X- _ O
be -X- _ O
evaluated -X- _ O
T -X- _ O
x -X- _ O
⇥ -X- _ O
T -X- _ O
y -X- _ O
times -X- _ O
for -X- _ O
each -X- _ O
sentence -X- _ O
pair -X- _ O
of -X- _ O
lengths -X- _ O
T -X- _ O
x -X- _ O
and -X- _ O
T -X- _ O
y -X- _ O
. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
reduce -X- _ O
computation, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
singlelayer -X- _ B-HyperparameterValue
multilayer -X- _ O
perceptron -X- _ O
such -X- _ O
that -X- _ O
a(s -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
h -X- _ O
j -X- _ O
) -X- _ O
= -X- _ O
v -X- _ O
> -X- _ O
a -X- _ O
tanh -X- _ O
(W -X- _ O
a -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
U -X- _ O
a -X- _ O
h -X- _ O
j -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
W -X- _ O
a -X- _ O
2 -X- _ O
R -X- _ O
n⇥n -X- _ O
, -X- _ O
U -X- _ O
a -X- _ O
2 -X- _ O
R -X- _ O
n⇥2n -X- _ O
and -X- _ O
v -X- _ O
a -X- _ O
2 -X- _ O
R -X- _ O
n -X- _ O
are -X- _ O
the -X- _ O
weight -X- _ O
matrices. -X- _ O
Since -X- _ O
U -X- _ O
a -X- _ O
h -X- _ O
j -X- _ O
does -X- _ O
not -X- _ O
depend -X- _ O
on -X- _ O
i, -X- _ O
we -X- _ O
can -X- _ O
pre-compute -X- _ O
it -X- _ O
in -X- _ O
advance -X- _ O
to -X- _ O
minimize -X- _ O
the -X- _ O
computational -X- _ O
cost. -X- _ O

For -X- _ O
the -X- _ O
activation -X- _ O
function -X- _ O
f -X- _ O
of -X- _ O
an -X- _ O
RNN, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
gated -X- _ O
hidden -X- _ O
unit -X- _ O
recently -X- _ O
proposed -X- _ O
by -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
(2014a). -X- _ O
The -X- _ O
gated -X- _ O
hidden -X- _ O
unit -X- _ O
is -X- _ O
an -X- _ O
alternative -X- _ O
to -X- _ O
the -X- _ O
conventional -X- _ O
simple -X- _ O
units -X- _ O
such -X- _ O
as -X- _ O
an -X- _ O
element-wise -X- _ O
tanh. -X- _ O
This -X- _ O
gated -X- _ O
unit -X- _ O
is -X- _ O
similar -X- _ O
to -X- _ O
a -X- _ O
long -X- _ O
short-term -X- _ O
memory -X- _ O
(LSTM) -X- _ O
unit -X- _ O
proposed -X- _ O
earlier -X- _ O
by -X- _ O
Hochreiter -X- _ O
and -X- _ O
Schmidhuber -X- _ O
(1997), -X- _ O
sharing -X- _ O
with -X- _ O
it -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
better -X- _ O
model -X- _ O
and -X- _ O
learn -X- _ O
long-term -X- _ O
dependencies. -X- _ O
This -X- _ O
is -X- _ O
made -X- _ O
possible -X- _ O
by -X- _ O
having -X- _ O
computation -X- _ O
paths -X- _ O
in -X- _ O
the -X- _ O
unfolded -X- _ O
RNN -X- _ O
for -X- _ O
which -X- _ O
the -X- _ O
product -X- _ O
of -X- _ O
derivatives -X- _ O
is -X- _ O
close -X- _ O
to -X- _ O
1. -X- _ O
These -X- _ O
paths -X- _ O
allow -X- _ O
gradients -X- _ O
to -X- _ O
flow -X- _ O
backward -X- _ O
easily -X- _ O
without -X- _ O
suffering -X- _ O
too -X- _ O
much -X- _ O
from -X- _ O
the -X- _ O
vanishing -X- _ O
effect -X- _ O
(Hochreiter, -X- _ O
1991;Bengio -X- _ O
et -X- _ O
al., -X- _ O
1994;Pascanu -X- _ O
et -X- _ O
al., -X- _ O
2013a). -X- _ O
It -X- _ O
is -X- _ O
therefore -X- _ O
possible -X- _ O
to -X- _ O
use -X- _ O
LSTM -X- _ O
units -X- _ O
instead -X- _ O
of -X- _ O
the -X- _ O
gated -X- _ O
hidden -X- _ O
unit -X- _ O
described -X- _ O
here, -X- _ O
as -X- _ O
was -X- _ O
done -X- _ O
in -X- _ O
a -X- _ O
similar -X- _ O
context -X- _ O
by -X- _ O
Sutskever -X- _ O
et -X- _ O
al. -X- _ O
(2014). -X- _ O
The -X- _ O
new -X- _ O
state -X- _ O
s -X- _ O
i -X- _ O
of -X- _ O
the -X- _ O
RNN -X- _ O
employing -X- _ O
n -X- _ B-HyperparameterName
gated -X- _ I-HyperparameterName
hidden -X- _ I-HyperparameterName
units -X- _ I-HyperparameterName
8 -X- _ B-HyperparameterValue
is -X- _ O
computed -X- _ O
by -X- _ O
s -X- _ O
i -X- _ O
= -X- _ O
f -X- _ O
(s -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
y -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
= -X- _ O
(1 -X- _ O
z -X- _ O
i -X- _ O
) -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
z -X- _ O
i -X- _ O
s -X- _ O
i -X- _ O
, -X- _ O
where -X- _ O
is -X- _ O
an -X- _ O
element-wise -X- _ O
multiplication, -X- _ O
and -X- _ O
z -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
update -X- _ O
gates -X- _ O
(see -X- _ O
below). -X- _ O
The -X- _ O
proposed -X- _ O
updated -X- _ O
states -X- _ O
i -X- _ O
is -X- _ O
computed -X- _ O
bỹ -X- _ O
s -X- _ O
i -X- _ O
= -X- _ O
tanh -X- _ O
(W -X- _ O
e(y -X- _ O
i -X- _ O
1 -X- _ O
) -X- _ O
+ -X- _ O
U -X- _ O
[r -X- _ O
i -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
] -X- _ O
+ -X- _ O
Cc -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
e(y -X- _ O
i -X- _ O
1 -X- _ O
) -X- _ O
2 -X- _ O
R -X- _ O
m -X- _ O
is -X- _ O
an -X- _ O
m-dimensional -X- _ O
embedding -X- _ O
of -X- _ O
a -X- _ O
word -X- _ O
y -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
and -X- _ O
r -X- _ O
i -X- _ O
is -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
reset -X- _ O
gates -X- _ O
(see -X- _ O
below). -X- _ O
When -X- _ O
y -X- _ O
i -X- _ O
is -X- _ O
represented -X- _ O
as -X- _ O
a -X- _ O
1-of-K -X- _ O
vector, -X- _ O
e(y -X- _ O
i -X- _ O
) -X- _ O
is -X- _ O
simply -X- _ O
a -X- _ O
column -X- _ O
of -X- _ O
an -X- _ O
embedding -X- _ O
matrix -X- _ O
E -X- _ O
2 -X- _ O
R -X- _ O
m⇥K -X- _ O
. -X- _ O
Whenever -X- _ O
possible, -X- _ O
we -X- _ O
omit -X- _ O
bias -X- _ O
terms -X- _ O
to -X- _ O
make -X- _ O
the -X- _ O
equations -X- _ O
less -X- _ O
cluttered. -X- _ O
The -X- _ O
update -X- _ O
gates -X- _ O
z -X- _ O
i -X- _ O
allow -X- _ O
each -X- _ O
hidden -X- _ O
unit -X- _ O
to -X- _ O
maintain -X- _ O
its -X- _ O
previous -X- _ O
activation, -X- _ O
and -X- _ O
the -X- _ O
reset -X- _ O
gates -X- _ O
r -X- _ O
i -X- _ O
control -X- _ O
how -X- _ O
much -X- _ O
and -X- _ O
what -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
previous -X- _ O
state -X- _ O
should -X- _ O
be -X- _ O
reset. -X- _ O
We -X- _ O
compute -X- _ O
them -X- _ O
by -X- _ O
z -X- _ O
i -X- _ O
= -X- _ O
(W -X- _ O
z -X- _ O
e(y -X- _ O
i -X- _ O
1 -X- _ O
) -X- _ O
+ -X- _ O
U -X- _ O
z -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
C -X- _ O
z -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O
r -X- _ O
i -X- _ O
= -X- _ O
(W -X- _ O
r -X- _ O
e(y -X- _ O
i -X- _ O
1 -X- _ O
) -X- _ O
+ -X- _ O
U -X- _ O
r -X- _ O
s -X- _ O
i -X- _ O
1 -X- _ O
+ -X- _ O
C -X- _ O
r -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
(•) -X- _ O
is -X- _ O
a -X- _ O
logistic -X- _ O
sigmoid -X- _ O
function. -X- _ O
At -X- _ O
each -X- _ O
step -X- _ O
of -X- _ O
the -X- _ O
decoder, -X- _ O
we -X- _ O
compute -X- _ O
the -X- _ O
output -X- _ O
probability -X- _ O
(Eq. -X- _ O
( -X- _ O
4)) -X- _ O
as -X- _ O
a -X- _ O
multi-layered -X- _ O
function -X- _ O
(Pascanu -X- _ O
et -X- _ O
al., -X- _ O
2014). -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
single -X- _ B-HyperparameterValue
hidden -X- _ B-HyperparameterName
layer -X- _ I-HyperparameterName
of -X- _ O
maxout -X- _ O
units -X- _ O
(Goodfellow -X- _ O
et -X- _ O
al., -X- _ O
2013) -X- _ O
and -X- _ O
normalize -X- _ O
the -X- _ O
output -X- _ O
probabilities -X- _ O
(one -X- _ O
for -X- _ O
each -X- _ O
word) -X- _ O
with -X- _ O
a -X- _ O
softmax -X- _ O
function -X- _ O
(see -X- _ O
Eq. -X- _ O
( -X- _ O
6)). -X- _ O

A.1 -X- _ O
ARCHITECTURAL -X- _ O
CHOICES -X- _ O
The -X- _ O
proposed -X- _ O
scheme -X- _ O
in -X- _ O
Section -X- _ O
3 -X- _ O
is -X- _ O
a -X- _ O
general -X- _ O
framework -X- _ O
where -X- _ O
one -X- _ O
can -X- _ O
freely -X- _ O
define, -X- _ O
for -X- _ O
instance, -X- _ O
the -X- _ O
activation -X- _ O
functions -X- _ O
f -X- _ O
of -X- _ O
recurrent -X- _ O
neural -X- _ O
networks -X- _ O
(RNN) -X- _ B-MethodName
and -X- _ O
the -X- _ O
alignment -X- _ O
model -X- _ O
a. -X- _ O
Here, -X- _ O
we -X- _ O
describe -X- _ O
the -X- _ O
choices -X- _ O
we -X- _ O
made -X- _ O
for -X- _ O
the -X- _ O
experiments -X- _ O
in -X- _ O
this -X- _ O
paper. -X- _ O

The -X- _ O
conventional -X- _ O
approach -X- _ O
to -X- _ O
neural -X- _ O
machine -X- _ O
translation, -X- _ O
called -X- _ O
an -X- _ O
encoder-decoder -X- _ B-MethodName
approach, -X- _ O
encodes -X- _ O
a -X- _ O
whole -X- _ O
input -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector -X- _ O
from -X- _ O
which -X- _ O
a -X- _ O
translation -X- _ O
will -X- _ O
be -X- _ O
decoded. -X- _ O
We -X- _ O
conjectured -X- _ O
that -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
a -X- _ O
fixed-length -X- _ O
context -X- _ O
vector -X- _ O
is -X- _ O
problematic -X- _ O
for -X- _ O
translating -X- _ O
long -X- _ O
sentences, -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
recent -X- _ O
empirical -X- _ O
study -X- _ O
reported -X- _ O
by -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
(2014b) -X- _ O
and -X- _ O
Pouget-Abadie -X- _ O
et -X- _ O
al. -X- _ O
(2014). -X- _ O
In -X- _ O
this -X- _ O
paper, -X- _ O
we -X- _ O
proposed -X- _ O
a -X- _ O
novel -X- _ O
architecture -X- _ O
that -X- _ O
addresses -X- _ O
this -X- _ O
issue. -X- _ O
We -X- _ O
extended -X- _ O
the -X- _ O
basic -X- _ O
encoder-decoder -X- _ O
by -X- _ O
letting -X- _ O
a -X- _ O
model -X- _ O
(soft-)search -X- _ O
for -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
input -X- _ O
words, -X- _ O
or -X- _ O
their -X- _ O
annotations -X- _ O
computed -X- _ O
by -X- _ O
an -X- _ O
encoder, -X- _ O
when -X- _ O
generating -X- _ O
each -X- _ O
target -X- _ O
word. -X- _ O
This -X- _ O
frees -X- _ O
the -X- _ O
model -X- _ O
from -X- _ O
having -X- _ O
to -X- _ O
encode -X- _ O
a -X- _ O
whole -X- _ O
source -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector, -X- _ O
and -X- _ O
also -X- _ O
lets -X- _ O
the -X- _ O
model -X- _ O
focus -X- _ O
only -X- _ O
on -X- _ O
information -X- _ O
relevant -X- _ O
to -X- _ O
the -X- _ O
generation -X- _ O
of -X- _ O
the -X- _ O
next -X- _ O
target -X- _ O
word. -X- _ O
This -X- _ O
has -X- _ O
a -X- _ O
major -X- _ O
positive -X- _ O
impact -X- _ O
on -X- _ O
the -X- _ O
ability -X- _ O
of -X- _ O
the -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
system -X- _ O
to -X- _ O
yield -X- _ O
good -X- _ O
results -X- _ O
on -X- _ O
longer -X- _ O
sentences. -X- _ O
Unlike -X- _ O
with -X- _ O
the -X- _ O
traditional -X- _ O
machine -X- _ O
translation -X- _ O
systems, -X- _ O
all -X- _ O
of -X- _ O
the -X- _ O
pieces -X- _ O
of -X- _ O
the -X- _ O
translation -X- _ O
system, -X- _ O
including -X- _ O
the -X- _ O
alignment -X- _ O
mechanism, -X- _ O
are -X- _ O
jointly -X- _ O
trained -X- _ O
towards -X- _ O
a -X- _ O
better -X- _ O
log-probability -X- _ O
of -X- _ O
producing -X- _ O
correct -X- _ O
translations. -X- _ O
We -X- _ O
tested -X- _ O
the -X- _ O
proposed -X- _ O
model, -X- _ O
called -X- _ O
RNNsearch, -X- _ B-MethodName
on -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
English-to-French -X- _ B-TaskName
translation. -X- _ I-TaskName
The -X- _ O
experiment -X- _ O
revealed -X- _ O
that -X- _ O
the -X- _ O
proposed -X- _ O
RNNsearch -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
conventional -X- _ O
encoder-decoder -X- _ O
model -X- _ O
(RNNencdec) -X- _ B-MethodName
significantly, -X- _ O
regardless -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
length -X- _ O
and -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
much -X- _ O
more -X- _ O
robust -X- _ O
to -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
a -X- _ O
source -X- _ O
sentence. -X- _ O
From -X- _ O
the -X- _ O
qualitative -X- _ O
analysis -X- _ O
where -X- _ O
we -X- _ O
investigated -X- _ O
the -X- _ O
(soft-)alignment -X- _ O
generated -X- _ O
by -X- _ O
the -X- _ O
RNNsearch, -X- _ B-MethodName
we -X- _ O
were -X- _ O
able -X- _ O
to -X- _ O
conclude -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
can -X- _ O
correctly -X- _ O
align -X- _ O
each -X- _ O
target -X- _ O
word -X- _ O
with -X- _ O
the -X- _ O
relevant -X- _ O
words, -X- _ O
or -X- _ O
their -X- _ O
annotations, -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
as -X- _ O
it -X- _ O
generated -X- _ O
a -X- _ O
correct -X- _ O
translation. -X- _ O
Perhaps -X- _ O
more -X- _ O
importantly, -X- _ O
the -X- _ O
proposed -X- _ O
approach -X- _ O
achieved -X- _ O
a -X- _ O
translation -X- _ O
performance -X- _ O
comparable -X- _ O
to -X- _ O
the -X- _ O
existing -X- _ O
phrase-based -X- _ O
statistical -X- _ O
machine -X- _ O
translation. -X- _ O
It -X- _ O
is -X- _ O
a -X- _ O
striking -X- _ O
result, -X- _ O
considering -X- _ O
that -X- _ O
the -X- _ O
proposed -X- _ O
architecture, -X- _ O
or -X- _ O
the -X- _ O
whole -X- _ O
family -X- _ O
of -X- _ O
neural -X- _ O
machine -X- _ O
translation, -X- _ O
has -X- _ O
only -X- _ O
been -X- _ O
proposed -X- _ O
as -X- _ O
recently -X- _ O
as -X- _ O
this -X- _ O
year. -X- _ O
We -X- _ O
believe -X- _ O
the -X- _ O
architecture -X- _ O
proposed -X- _ O
here -X- _ O
is -X- _ O
a -X- _ O
promising -X- _ O
step -X- _ O
toward -X- _ O
better -X- _ O
machine -X- _ O
translation -X- _ O
and -X- _ O
a -X- _ O
better -X- _ O
understanding -X- _ O
of -X- _ O
natural -X- _ O
languages -X- _ O
in -X- _ O
general. -X- _ O
One -X- _ O
of -X- _ O
challenges -X- _ O
left -X- _ O
for -X- _ O
the -X- _ O
future -X- _ O
is -X- _ O
to -X- _ O
better -X- _ O
handle -X- _ O
unknown, -X- _ O
or -X- _ O
rare -X- _ O
words. -X- _ O
This -X- _ O
will -X- _ O
be -X- _ O
required -X- _ O
for -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
widely -X- _ O
used -X- _ O
and -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
current -X- _ O
state-of-the-art -X- _ O
machine -X- _ O
translation -X- _ O
systems -X- _ O
in -X- _ O
all -X- _ O
contexts. -X- _ O

Since -X- _ O
Bengio -X- _ O
et -X- _ O
al. -X- _ O
(2003) -X- _ O
introduced -X- _ O
a -X- _ O
neural -X- _ O
probabilistic -X- _ O
language -X- _ O
model -X- _ O
which -X- _ O
uses -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
conditional -X- _ O
probability -X- _ O
of -X- _ O
a -X- _ O
word -X- _ O
given -X- _ O
a -X- _ O
fixed -X- _ O
number -X- _ O
of -X- _ O
the -X- _ O
preceding -X- _ O
words, -X- _ O
neural -X- _ O
networks -X- _ O
have -X- _ O
widely -X- _ O
been -X- _ O
used -X- _ O
in -X- _ O
machine -X- _ O
translation. -X- _ O
However, -X- _ O
the -X- _ O
role -X- _ O
of -X- _ O
neural -X- _ O
networks -X- _ O
has -X- _ O
been -X- _ O
largely -X- _ O
limited -X- _ O
to -X- _ O
simply -X- _ O
providing -X- _ O
a -X- _ O
single -X- _ O
feature -X- _ O
to -X- _ O
an -X- _ O
existing -X- _ O
statistical -X- _ O
machine -X- _ O
translation -X- _ O
system -X- _ O
or -X- _ O
to -X- _ O
re-rank -X- _ O
a -X- _ O
list -X- _ O
of -X- _ O
candidate -X- _ O
translations -X- _ O
provided -X- _ O
by -X- _ O
an -X- _ O
existing -X- _ O
system. -X- _ O
For -X- _ O
instance, -X- _ O
Schwenk -X- _ O
(2012) -X- _ O
proposed -X- _ O
using -X- _ O
a -X- _ O
feedforward -X- _ O
neural -X- _ O
network -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
score -X- _ O
of -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
phrases -X- _ O
and -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
score -X- _ O
as -X- _ O
an -X- _ O
additional -X- _ O
feature -X- _ O
in -X- _ O
the -X- _ O
phrase-based -X- _ O
statistical -X- _ O
machine -X- _ O
translation -X- _ O
system. -X- _ O
More -X- _ O
recently, -X- _ O
Kalchbrenner -X- _ O
and -X- _ O
Blunsom -X- _ O
(2013) -X- _ O
and -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
(2014) -X- _ O
reported -X- _ O
the -X- _ O
successful -X- _ O
use -X- _ O
of -X- _ O
the -X- _ O
neural -X- _ O
networks -X- _ O
as -X- _ O
a -X- _ O
sub-component -X- _ O
of -X- _ O
the -X- _ O
existing -X- _ O
translation -X- _ O
system. -X- _ O
Traditionally, -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
trained -X- _ O
as -X- _ O
a -X- _ O
target-side -X- _ O
language -X- _ O
model -X- _ O
has -X- _ O
been -X- _ O
used -X- _ O
to -X- _ O
rescore -X- _ O
or -X- _ O
rerank -X- _ O
a -X- _ O
list -X- _ O
of -X- _ O
candidate -X- _ O
translations -X- _ O
(see, -X- _ O
e.g., -X- _ O
Schwenk -X- _ O
et -X- _ O
al., -X- _ O
2006). -X- _ O
Although -X- _ O
the -X- _ O
above -X- _ O
approaches -X- _ O
were -X- _ O
shown -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
translation -X- _ O
performance -X- _ O
over -X- _ O
the -X- _ O
stateof-the-art -X- _ O
machine -X- _ O
translation -X- _ O
systems, -X- _ O
we -X- _ O
are -X- _ O
more -X- _ O
interested -X- _ O
in -X- _ O
a -X- _ O
more -X- _ O
ambitious -X- _ O
objective -X- _ O
of -X- _ O
designing -X- _ O
a -X- _ O
completely -X- _ O
new -X- _ O
translation -X- _ O
system -X- _ O
based -X- _ O
on -X- _ O
neural -X- _ O
networks. -X- _ O
The -X- _ O
neural -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
approach -X- _ O
we -X- _ O
consider -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
is -X- _ O
therefore -X- _ O
a -X- _ O
radical -X- _ O
departure -X- _ O
from -X- _ O
these -X- _ O
earlier -X- _ O
works. -X- _ O
Rather -X- _ O
than -X- _ O
using -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
as -X- _ O
a -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
existing -X- _ O
system, -X- _ O
our -X- _ O
model -X- _ O
works -X- _ O
on -X- _ O
its -X- _ O
own -X- _ O
and -X- _ O
generates -X- _ O
a -X- _ O
translation -X- _ O
from -X- _ O
a -X- _ O
source -X- _ O
sentence -X- _ O
directly. -X- _ O

A -X- _ O
similar -X- _ O
approach -X- _ O
of -X- _ O
aligning -X- _ O
an -X- _ O
output -X- _ O
symbol -X- _ O
with -X- _ O
an -X- _ O
input -X- _ O
symbol -X- _ O
was -X- _ O
proposed -X- _ O
recently -X- _ O
by -X- _ O
Graves -X- _ O
(2013) -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
handwriting -X- _ O
synthesis. -X- _ O
Handwriting -X- _ O
synthesis -X- _ O
is -X- _ O
a -X- _ O
task -X- _ O
where -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
asked -X- _ O
to -X- _ O
generate -X- _ O
handwriting -X- _ O
of -X- _ O
a -X- _ O
given -X- _ O
sequence -X- _ O
of -X- _ O
characters. -X- _ O
In -X- _ O
his -X- _ O
work, -X- _ O
he -X- _ O
used -X- _ O
a -X- _ O
mixture -X- _ O
of -X- _ O
Gaussian -X- _ O
kernels -X- _ O
to -X- _ O
compute -X- _ O
the -X- _ O
weights -X- _ O
of -X- _ O
the -X- _ O
annotations, -X- _ O
where -X- _ O
the -X- _ O
location, -X- _ O
width -X- _ O
and -X- _ O
mixture -X- _ O
coefficient -X- _ O
of -X- _ O
each -X- _ O
kernel -X- _ O
was -X- _ O
predicted -X- _ O
from -X- _ O
an -X- _ O
alignment -X- _ O
model. -X- _ O
More -X- _ O
specifically, -X- _ O
his -X- _ O
alignment -X- _ O
was -X- _ O
restricted -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
location -X- _ O
such -X- _ O
that -X- _ O
the -X- _ O
location -X- _ O
increases -X- _ O
monotonically. -X- _ O
The -X- _ O
main -X- _ O
difference -X- _ O
from -X- _ O
our -X- _ O
approach -X- _ O
is -X- _ O
that, -X- _ O
in -X- _ O
(Graves, -X- _ O
2013), -X- _ O
the -X- _ O
modes -X- _ O
of -X- _ O
the -X- _ O
weights -X- _ O
of -X- _ O
the -X- _ O
annotations -X- _ O
only -X- _ O
move -X- _ O
in -X- _ O
one -X- _ O
direction. -X- _ O
In -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
machine -X- _ O
translation, -X- _ O
this -X- _ O
is -X- _ O
a -X- _ O
severe -X- _ O
limitation, -X- _ O
as -X- _ O
(long-distance) -X- _ O
reordering -X- _ O
is -X- _ O
often -X- _ O
needed -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
grammatically -X- _ O
correct -X- _ O
translation -X- _ O
(for -X- _ O
instance, -X- _ O
English-to-German). -X- _ O
Our -X- _ O
approach, -X- _ O
on -X- _ O
the -X- _ O
other -X- _ O
hand, -X- _ O
requires -X- _ O
computing -X- _ O
the -X- _ O
annotation -X- _ O
weight -X- _ O
of -X- _ O
every -X- _ O
word -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
for -X- _ O
each -X- _ O
word -X- _ O
in -X- _ O
the -X- _ O
translation. -X- _ O
This -X- _ O
drawback -X- _ O
is -X- _ O
not -X- _ O
severe -X- _ O
with -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
translation -X- _ O
in -X- _ O
which -X- _ O
most -X- _ O
of -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
sentences -X- _ O
are -X- _ O
only -X- _ O
15-40 -X- _ O
words. -X- _ O
However, -X- _ O
this -X- _ O
may -X- _ O
limit -X- _ O
the -X- _ O
applicability -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
scheme -X- _ O
to -X- _ O
other -X- _ O
tasks. -X- _ O

Ce -X- _ O
type -X- _ O
d'expérience -X- _ O
fait -X- _ O
partie -X- _ O
des -X- _ O
initiatives -X- _ O
du -X- _ O
Disney -X- _ O
pour -X- _ O
"prolonger -X- _ O
la -X- _ O
durée -X- _ O
de -X- _ O
vie -X- _ O
de -X- _ O
ses -X- _ O
nouvelles -X- _ O
et -X- _ O
de -X- _ O
développer -X- _ O
des -X- _ O
liens -X- _ O
avec -X- _ O
les -X- _ O
lecteurs -X- _ O
numériques -X- _ O
qui -X- _ O
deviennent -X- _ O
plus -X- _ O
complexes. -X- _ O
As -X- _ O
with -X- _ O
the -X- _ O
previous -X- _ O
example, -X- _ O
the -X- _ O
RNNencdec -X- _ B-MethodName
began -X- _ O
deviating -X- _ O
from -X- _ O
the -X- _ O
actual -X- _ O
meaning -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
after -X- _ O
generating -X- _ O
approximately -X- _ O
30 -X- _ O
words -X- _ O
(see -X- _ O
the -X- _ O
underlined -X- _ O
phrase). -X- _ O
After -X- _ O
that -X- _ O
point, -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
the -X- _ O
translation -X- _ O
deteriorates, -X- _ O
with -X- _ O
basic -X- _ O
mistakes -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
a -X- _ O
closing -X- _ O
quotation -X- _ O
mark. -X- _ O
Again, -X- _ O
the -X- _ O
RNNsearch-50 -X- _ B-MethodName
was -X- _ O
able -X- _ O
to -X- _ O
translate -X- _ O
this -X- _ O
long -X- _ O
sentence -X- _ O
correctly: -X- _ O
Ce -X- _ O
genre -X- _ O
d'expérience -X- _ O
fait -X- _ O
partie -X- _ O
des -X- _ O
efforts -X- _ O
de -X- _ O
Disney -X- _ O
pour -X- _ O
"prolonger -X- _ O
la -X- _ O
durée -X- _ O
de -X- _ O
vie -X- _ O
de -X- _ O
ses -X- _ O
séries -X- _ O
et -X- _ O
créer -X- _ O
de -X- _ O
nouvelles -X- _ O
relations -X- _ O
avec -X- _ O
des -X- _ O
publics -X- _ O
via -X- _ O
des -X- _ O
plateformes -X- _ O
numériques -X- _ O
de -X- _ O
plus -X- _ O
en -X- _ O
plus -X- _ O
importantes", -X- _ O
a-t-il -X- _ O
ajouté. -X- _ O
In -X- _ O
conjunction -X- _ O
with -X- _ O
the -X- _ O
quantitative -X- _ O
results -X- _ O
presented -X- _ O
already, -X- _ O
these -X- _ O
qualitative -X- _ O
observations -X- _ O
confirm -X- _ O
our -X- _ O
hypotheses -X- _ O
that -X- _ O
the -X- _ O
RNNsearch -X- _ B-MethodName
architecture -X- _ O
enables -X- _ O
far -X- _ O
more -X- _ O
reliable -X- _ O
translation -X- _ O
of -X- _ O
long -X- _ O
sentences -X- _ O
than -X- _ O
the -X- _ O
standard -X- _ O
RNNencdec -X- _ B-MethodName
model. -X- _ O
In -X- _ O
Appendix -X- _ O
C, -X- _ O
we -X- _ O
provide -X- _ O
a -X- _ O
few -X- _ O
more -X- _ O
sample -X- _ O
translations -X- _ O
of -X- _ O
long -X- _ O
source -X- _ O
sentences -X- _ O
generated -X- _ O
by -X- _ O
the -X- _ O
RNNencdec-50, -X- _ B-MethodName
RNNsearch-50 -X- _ B-MethodName
and -X- _ O
Google -X- _ O
Translate -X- _ O
along -X- _ O
with -X- _ O
the -X- _ O
reference -X- _ O
translations. -X- _ O
6 -X- _ O
RELATED -X- _ O
WORK -X- _ O

As -X- _ O
clearly -X- _ O
visible -X- _ O
from -X- _ O
Fig. -X- _ O
2 -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
(RNNsearch) -X- _ B-MethodName
is -X- _ O
much -X- _ O
better -X- _ O
than -X- _ O
the -X- _ O
conventional -X- _ O
model -X- _ O
(RNNencdec) -X- _ B-MethodName
at -X- _ O
translating -X- _ B-TaskName
long -X- _ I-TaskName
sentences. -X- _ I-TaskName
This -X- _ O
is -X- _ O
likely -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
the -X- _ O
RNNsearch -X- _ B-MethodName
does -X- _ O
not -X- _ O
require -X- _ O
encoding -X- _ O
a -X- _ O
long -X- _ O
sentence -X- _ O
into -X- _ O
a -X- _ O
fixed-length -X- _ O
vector -X- _ O
perfectly, -X- _ O
but -X- _ O
only -X- _ O
accurately -X- _ O
encoding -X- _ O
the -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
that -X- _ O
surround -X- _ O
a -X- _ O
particular -X- _ O
word. -X- _ O
As -X- _ O
an -X- _ O
example, -X- _ O
consider -X- _ O
this -X- _ O
source -X- _ O
sentence -X- _ O
from -X- _ O
the -X- _ O
test -X- _ O
set: -X- _ O
An -X- _ O
admitting -X- _ O
privilege -X- _ O
is -X- _ O
the -X- _ O
right -X- _ O
of -X- _ O
a -X- _ O
doctor -X- _ O
to -X- _ O
admit -X- _ O
a -X- _ O
patient -X- _ O
to -X- _ O
a -X- _ O
hospital -X- _ O
or -X- _ O
a -X- _ O
medical -X- _ O
centre -X- _ O
to -X- _ O
carry -X- _ O
out -X- _ O
a -X- _ O
diagnosis -X- _ O
or -X- _ O
a -X- _ O
procedure, -X- _ O
based -X- _ O
on -X- _ O
his -X- _ O
status -X- _ O
as -X- _ O
a -X- _ O
health -X- _ O
care -X- _ O
worker -X- _ O
at -X- _ O
a -X- _ O
hospital. -X- _ O
The -X- _ O
RNNencdec-50 -X- _ B-MethodName
translated -X- _ O
this -X- _ O
sentence -X- _ O
into: -X- _ O
Un -X- _ O
privilège -X- _ O
d'admission -X- _ O
est -X- _ O
le -X- _ O
droit -X- _ O
d'un -X- _ O
médecin -X- _ O
de -X- _ O
reconnaître -X- _ O
un -X- _ O
patientà -X- _ O
l'hôpital -X- _ O
ou -X- _ O
un -X- _ O
centre -X- _ O
médical -X- _ O
d'un -X- _ O
diagnostic -X- _ O
ou -X- _ O
de -X- _ O
prendre -X- _ O
un -X- _ O
diagnostic -X- _ O
en -X- _ O
fonction -X- _ O
de -X- _ O
sonétat -X- _ O
de -X- _ O
santé. -X- _ O
The -X- _ O
RNNencdec-50 -X- _ B-MethodName
correctly -X- _ O
translated -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
until -X- _ O
[a -X- _ O
medical -X- _ O
center]. -X- _ O
However, -X- _ O
from -X- _ O
there -X- _ O
on -X- _ O
(underlined), -X- _ O
it -X- _ O
deviated -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
meaning -X- _ O
of -X- _ O
the -X- _ O
source -X- _ O
sentence. -X- _ O
For -X- _ O
instance, -X- _ O
it -X- _ O
replaced -X- _ O
[based -X- _ O
on -X- _ O
his -X- _ O
status -X- _ O
as -X- _ O
a -X- _ O
health -X- _ O
care -X- _ O
worker -X- _ O
at -X- _ O
a -X- _ O
hospital] -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
with -X- _ O
[en -X- _ O
fonction -X- _ O
de -X- _ O
sonétat -X- _ O
de -X- _ O
santé] -X- _ O
("based -X- _ O
on -X- _ O
his -X- _ O
state -X- _ O
of -X- _ O
health"). -X- _ O
On -X- _ O
the -X- _ O
other -X- _ O
hand, -X- _ O
the -X- _ O
RNNsearch-50 -X- _ B-MethodName
generated -X- _ O
the -X- _ O
following -X- _ O
correct -X- _ O
translation, -X- _ O
preserving -X- _ O
the -X- _ O
whole -X- _ O
meaning -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
sentence -X- _ O
without -X- _ O
omitting -X- _ O
any -X- _ O
details: -X- _ O
Un -X- _ O
privilège -X- _ O
d'admission -X- _ O
est -X- _ O
le -X- _ O
droit -X- _ O
d'un -X- _ O
médecin -X- _ O
d'admettre -X- _ O
un -X- _ O
patientà -X- _ O
un -X- _ O
hôpital -X- _ O
ou -X- _ O
un -X- _ O
centre -X- _ O
médical -X- _ O
pour -X- _ O
effectuer -X- _ O
un -X- _ O
diagnostic -X- _ O
ou -X- _ O
une -X- _ O
procédure, -X- _ O
selon -X- _ O
son -X- _ O
statut -X- _ O
de -X- _ O
travailleur -X- _ O
des -X- _ O
soins -X- _ O
de -X- _ O
santéà -X- _ O
l'hôpital. -X- _ O
Let -X- _ O
us -X- _ O
consider -X- _ O
another -X- _ O
sentence -X- _ O
from -X- _ O
the -X- _ O
test -X- _ O
set: -X- _ O
This -X- _ O
kind -X- _ O
of -X- _ O
experience -X- _ O
is -X- _ O
part -X- _ O
of -X- _ O
Disney's -X- _ O
efforts -X- _ O
to -X- _ O
"extend -X- _ O
the -X- _ O
lifetime -X- _ O
of -X- _ O
its -X- _ O
series -X- _ O
and -X- _ O
build -X- _ O
new -X- _ O
relationships -X- _ O
with -X- _ O
audiences -X- _ O
via -X- _ O
digital -X- _ O
platforms -X- _ O
that -X- _ O
are -X- _ O
becoming -X- _ O
ever -X- _ O
more -X- _ O
important," -X- _ O
he -X- _ O
added. -X- _ O

The -X- _ O
proposed -X- _ O
approach -X- _ O
provides -X- _ O
an -X- _ O
intuitive -X- _ O
way -X- _ O
to -X- _ O
inspect -X- _ O
the -X- _ O
(soft-)alignment -X- _ O
between -X- _ O
the -X- _ O
words -X- _ O
in -X- _ O
a -X- _ O
generated -X- _ O
translation -X- _ O
and -X- _ O
those -X- _ O
in -X- _ O
a -X- _ O
source -X- _ O
sentence. -X- _ O
This -X- _ O
is -X- _ O
done -X- _ O
by -X- _ O
visualizing -X- _ O
the -X- _ O
annotation -X- _ O
weights -X- _ O
↵ -X- _ O
ij -X- _ O
from -X- _ O
Eq. -X- _ O
(6), -X- _ O
as -X- _ O
in -X- _ O
Fig. -X- _ O
3. -X- _ O
Each -X- _ O
row -X- _ O
of -X- _ O
a -X- _ O
matrix -X- _ O
in -X- _ O
each -X- _ O
plot -X- _ O
indicates -X- _ O
the -X- _ O
weights -X- _ O
associated -X- _ O
with -X- _ O
the -X- _ O
annotations. -X- _ O
From -X- _ O
this -X- _ O
we -X- _ O
see -X- _ O
which -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
source -X- _ O
sentence -X- _ O
were -X- _ O
considered -X- _ O
more -X- _ O
important -X- _ O
when -X- _ O
generating -X- _ O
the -X- _ O
target -X- _ O
word. -X- _ O
We -X- _ O
can -X- _ O
see -X- _ O
from -X- _ O
the -X- _ O
alignments -X- _ O
in -X- _ O
Fig. -X- _ O
3 -X- _ O
that -X- _ O
the -X- _ O
alignment -X- _ O
of -X- _ O
words -X- _ O
between -X- _ O
English -X- _ O
and -X- _ O
French -X- _ O
is -X- _ O
largely -X- _ O
monotonic. -X- _ O
We -X- _ O
see -X- _ O
strong -X- _ O
weights -X- _ O
along -X- _ O
the -X- _ O
diagonal -X- _ O
of -X- _ O
each -X- _ O
matrix. -X- _ O
However, -X- _ O
we -X- _ O
also -X- _ O
observe -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
non-trivial, -X- _ O
non-monotonic -X- _ O
alignments. -X- _ O
Adjectives -X- _ O
and -X- _ O
nouns -X- _ O
are -X- _ O
typically -X- _ O
ordered -X- _ O
differently -X- _ O
between -X- _ O
French -X- _ O
and -X- _ O
English, -X- _ O
and -X- _ O
we -X- _ O
see -X- _ O
an -X- _ O
example -X- _ O
in -X- _ O
Fig. -X- _ O
3 -X- _ O
We -X- _ O
observe -X- _ O
similar -X- _ O
behaviors -X- _ O
in -X- _ O
all -X- _ O
the -X- _ O
presented -X- _ O
cases -X- _ O
in -X- _ O
Fig. -X- _ O
3. -X- _ O
An -X- _ O
additional -X- _ O
benefit -X- _ O
of -X- _ O
the -X- _ O
soft -X- _ O
alignment -X- _ O
is -X- _ O
that -X- _ O
it -X- _ O
naturally -X- _ O
deals -X- _ O
with -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
phrases -X- _ O
of -X- _ O
different -X- _ O
lengths, -X- _ O
without -X- _ O
requiring -X- _ O
a -X- _ O
counter-intuitive -X- _ O
way -X- _ O
of -X- _ O
mapping -X- _ O
some -X- _ O
words -X- _ O
to -X- _ O
or -X- _ O
from -X- _ O
nowhere -X- _ O
([NULL]) -X- _ O
(see, -X- _ O
e.g., -X- _ O
Chapters -X- _ O
4 -X- _ O
and -X- _ O
5 -X- _ O
of -X- _ O
Koehn, -X- _ O
2010). -X- _ O

Here, -X- _ O
we -X- _ O
describe -X- _ O
briefly -X- _ O
the -X- _ O
underlying -X- _ O
framework, -X- _ O
called -X- _ O
RNN -X- _ B-MethodName
Encoder-Decoder, -X- _ I-MethodName
proposed -X- _ O
by -X- _ O
Cho -X- _ O
et -X- _ O
al. -X- _ O
(2014a) -X- _ O
and -X- _ O
Sutskever -X- _ O
et -X- _ O
al. -X- _ O
(2014) -X- _ O
upon -X- _ O
which -X- _ O
we -X- _ O
build -X- _ O
a -X- _ O
novel -X- _ O
architecture -X- _ O
that -X- _ O
learns -X- _ O
to -X- _ O
align -X- _ O
and -X- _ O
translate -X- _ O
simultaneously. -X- _ O
In -X- _ O
the -X- _ O
Encoder-Decoder -X- _ B-MethodName
framework, -X- _ O
an -X- _ O
encoder -X- _ O
reads -X- _ O
the -X- _ O
input -X- _ O
sentence, -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
vectors -X- _ O
x -X- _ O
= -X- _ O
(x -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
x -X- _ O
Tx -X- _ O
), -X- _ O
into -X- _ O
a -X- _ O
vector -X- _ O
c. -X- _ O
2 -X- _ O
The -X- _ O
most -X- _ O
common -X- _ O
approach -X- _ O
is -X- _ O
to -X- _ O
use -X- _ O
an -X- _ O
RNN -X- _ B-MethodName
such -X- _ O
that -X- _ O
h -X- _ O
t -X- _ O
= -X- _ O
f -X- _ O
(x -X- _ O
t -X- _ O
, -X- _ O
h -X- _ O
t -X- _ O
1 -X- _ O
) -X- _ O
(1) -X- _ O
and -X- _ O
c -X- _ O
= -X- _ O
q -X- _ O
({h -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
h -X- _ O
Tx -X- _ O
}) -X- _ O
, -X- _ O
where -X- _ O
h -X- _ O
t -X- _ O
2 -X- _ O
R -X- _ O
n -X- _ O
is -X- _ O
a -X- _ O
hidden -X- _ O
state -X- _ O
at -X- _ O
time -X- _ O
t, -X- _ O
and -X- _ O
c -X- _ O
is -X- _ O
a -X- _ O
vector -X- _ O
generated -X- _ O
from -X- _ O
the -X- _ O
sequence -X- _ O
of -X- _ O
the -X- _ O
hidden -X- _ O
states. -X- _ O
f -X- _ O
and -X- _ O
q -X- _ O
are -X- _ O
some -X- _ O
nonlinear -X- _ O
functions. -X- _ O
Sutskever -X- _ O
et -X- _ O
al. -X- _ O
(2014) -X- _ O
used -X- _ O
an -X- _ O
LSTM -X- _ O
as -X- _ O
f -X- _ O
and -X- _ O
q -X- _ O
({h -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
h -X- _ O
T -X- _ O
}) -X- _ O
= -X- _ O
h -X- _ O
T -X- _ O
, -X- _ O
for -X- _ O
instance. -X- _ O
The -X- _ O
decoder -X- _ O
is -X- _ O
often -X- _ O
trained -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
next -X- _ O
word -X- _ O
y -X- _ O
t -X- _ O
0 -X- _ O
given -X- _ O
the -X- _ O
context -X- _ O
vector -X- _ O
c -X- _ O
and -X- _ O
all -X- _ O
the -X- _ O
previously -X- _ O
predicted -X- _ O
words -X- _ O
{y -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
y -X- _ O
t -X- _ O
0 -X- _ O
1 -X- _ O
}. -X- _ O
In -X- _ O
other -X- _ O
words, -X- _ O
the -X- _ O
decoder -X- _ O
defines -X- _ O
a -X- _ O
probability -X- _ O
over -X- _ O
the -X- _ O
translation -X- _ O
y -X- _ O
by -X- _ O
decomposing -X- _ O
the -X- _ O
joint -X- _ O
probability -X- _ O
into -X- _ O
the -X- _ O
ordered -X- _ O
conditionals: -X- _ O
p(y) -X- _ O
= -X- _ O
T -X- _ O
Y -X- _ O
t=1 -X- _ O
p(y -X- _ O
t -X- _ O
| -X- _ O
{y -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
y -X- _ O
t -X- _ O
1 -X- _ O
} -X- _ O
, -X- _ O
c),(2) -X- _ O
where -X- _ O
y -X- _ O
= -X- _ O
y -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
y -X- _ O
Ty -X- _ O
. -X- _ O
With -X- _ O
an -X- _ O
RNN, -X- _ B-MethodName
each -X- _ O
conditional -X- _ O
probability -X- _ O
is -X- _ O
modeled -X- _ O
as -X- _ O
p(y -X- _ O
t -X- _ O
| -X- _ O
{y -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
y -X- _ O
t -X- _ O
1 -X- _ O
} -X- _ O
, -X- _ O
c) -X- _ O
= -X- _ O
g(y -X- _ O
t -X- _ O
1 -X- _ O
, -X- _ O
s -X- _ O
t -X- _ O
, -X- _ O
c), -X- _ O
(3 -X- _ O
) -X- _ O
where -X- _ O
g -X- _ O
is -X- _ O
a -X- _ O
nonlinear, -X- _ O
potentially -X- _ O
multi-layered, -X- _ O
function -X- _ O
that -X- _ O
outputs -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
y -X- _ O
t -X- _ O
, -X- _ O
and -X- _ O
s -X- _ O
t -X- _ O
is -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
RNN. -X- _ B-MethodName
It -X- _ O
should -X- _ O
be -X- _ O
noted -X- _ O
that -X- _ O
other -X- _ O
architectures -X- _ O
such -X- _ O
as -X- _ O
a -X- _ O
hybrid -X- _ O
of -X- _ O
an -X- _ O
RNN -X- _ B-MethodName
and -X- _ O
a -X- _ O
de-convolutional -X- _ O
neural -X- _ O
network -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
(Kalchbrenner -X- _ O
and -X- _ O
Blunsom, -X- _ O
2013). -X- _ O

