With recent developments in new architectures like Transformer and pretraining techniques, significant progress has been made in applications of natural language processing (NLP). However, the high energy cost and long inference delay of Transformer is preventing NLP from entering broader scenarios including edge and mobile computing. Efficient NLP research aims to comprehensively consider computation, time and carbon emission for the entire life-cycle of NLP, including data preparation, model training and inference. In this survey, we focus on the inference stage and review the current state of model compression for NLP, including the benchmarks, metrics and methodology. We outline the current obstacles and future research directions. Recent success on applying deep Transformer [Vaswani et al., 2017] on different NLP tasks [Devlin et al., 2019;Raffel et al., 2020] has raised concerns about its efficiency. The high computational cost also prevents these models from being deployed in production . To address this problem, efficient inference refers to techniques that aim to make inference of an ML model faster (time-efficient), consume fewer computational resources (computation-efficient), less memory (memory-efficient) and less disk space (storageefficient). One popular class of techniques is model compression, where a large and slow model is compressed to a lightweight model that, for example, can be stored with limited disk space on a mobile device, or run on a mobile chip with low latency. Also, training a large model and then compressing it to a small one can be efficient for training and good for generalization [Li et al., 2020]. In addition to technical considerations, large models also raise environmental and ethical concerns [Bender et al., 2021]. Large models have high carbon emission which a compressed model can reduce, potentially with little sacrifice in performance. Meanwhile, large models set obstacles for engineers and researchers from developing countries who cannot afford the necessary hardware for running the model [Bender et al., 2021]. Thus, model compression can be critical to make state-of-the-art NLP techniques more accessible and facilitate inclusiveness for NLP. There have been surveys that cover some aspects of the topic of efficient inference for NLP. Qiu et al. [2020] and Han et al. [2021] review current research directions in pretrained language models, including model acceleration. Xu et al. [2021b] give a high-level review of green (i.e., energy-efficient) deep learning with their applications in computer vision and NLP. Different from these works, we focus on the latest progress on model compression for NLP, highlighting the intersection between language technology and efficient ML, in the era of pretrained language models. In this survey, we aim to highlight the most important works in the field of model compression for NLP with an emphasis on the latest progress in compressing pretrained language models. We review the metrics, benchmarks, and methods in model compression for NLP, organizing these papers in a new taxonomy, as shown in Figure 1. Widely-used techniques, including weight sharing, low-rank factorization, pruning, quantization and knowledge distillation, are covered in this survey with comparative analysis. We also highlight current challenges and future research directions in the field, calling for community efforts to build an environment-friendly, inclusive and sustainable future of NLP.  There are various metrics to depict inference efficiency in different dimensions. These metrics are often reported together with accuracy to evaluate an NLP model. Floating point operations (FLOPs) directly measures the number of floating points operations needed for executing an instance. FLOPs can serve as a metric for computational efficiency and is somewhat hardware-agnostic. However, FLOPs cannot accurately reflect the real runtime since the degree of parallelism (DOP) varies for different algorithms. Inference time (i.e., delay) is used to measure the runtime of an algorithm in its inference stage. Inference time can vary on different infrastructures. When testing on the same architecture, compared to FLOPs, inference time can better approximate the real-world performance of a system by taking parallelism into consideration. Speed-up Ratio is the ratio of inference time of the baseline model to the accelerated model. Compared to inference time, speed-up ratio draws a relative comparison which can be roughly compared across different hardware. In some works [Zhou et al., 2020;, speed-up ratio is approximated by the ratio of the number of Transformer layers in the baseline model to those used in calculation of an acceleration method. Size are often reported in NLP studies as metrics that directly reflect the storage cost of a model. This can be important for mobile deployment of an NLP model due to limited storage on a mobile device. It can also be an indicator for the memory footprint and computational cost for training and inference. An exception is models with weight sharing. For example, the FLOPs and memory use of ALBERT [Lan et al., 2020] is slightly higher than a BERT model [Devlin et al., 2019] with the same number of layers. However, since all Transformer layers in ALBERT share the same weights, the model size of n-layer ALBERT is only 1/n of n-layer BERT. Carbon Footprint measures the environmental impact. Lacoste et al. [2019] provide a calculator for CO 2 by querying a database of carbon emission of mainstream cloud computing providers. Alternatively, Experiment Impact Tracker [Henderson et al., 2020] and CodeCarbon 1 are two plugins that can record the energy use of hardware and estimate the carbon emission based on the geolocation. Loyalty/Fidelity Recent works Xu et al. [2021a] and Stanton et al. [2021] propose loyalty and fidelity, respectively. Both are similarity metrics calculated between the predicted distributions of the teacher and the student, in a teacher-student distillation or compression setting. Loyalty and fidelity can reflect how successful the knowledge transfer is from the teacher 1  to the student, providing interpretability and reliability [Xu et al., 2021a;Stanton et al., 2021], and can be an indicator of better generalization in distilling large teacher models and ensembles [Stanton et al., 2021]. Robustness Su et al. [2018] find that smaller neural networks are more vulnerable to adversarial attacks. Xu et al. [2021a] suggest reporting adversarial robustness in addition to accuracy. In addition to adversarial robustness, Du et al. [2021] find compressed pretrained language models are significantly less robust on out-of-distribution data. Standard Benchmarks Most studies evaluate on common NLP benchmarks. For example, GLUE [Wang et al., 2019b] and SuperGLUE [Wang et al., 2019a] are used for natural language understanding. SQuAD [Rajpurkar et al., 2016] is used for machine reading comprehension. EfficientQA EfficientQA [Min et al., 2020] is an opendomain question answering benchmark encouraging solutions that efficiently store and access knowledge with the smallest number of bytes. EfficientQA has three resource-restricted tracks, including two tracks with a 6 GB and 500 MB cutoff for system size and one track that ranks the systems that achieves 25% accuracy with the smallest size. SustaiNLP The shared task of SustaiNLP 2020 [Wang and Wolf, 2020] uses SuperGLUE [Wang et al., 2019a] to evaluate the performance of submissions. There are three tracks that target different accuracy levels and hardware (2 GPU tracks and 1 CPU track). Within each track, submissions are ranked by lowest energy consumption, measured by Experiment Impact Tracker [Henderson et al., 2020]. ELUE Efficient Language Understanding Evaluation [Liu et al., 2021] is proposed as an attempt to clearly depict the Pareto Front of FLOPs versus performance. ELUE consists of six datasets of three tasks (sentiment analysis, natural language inference, and paraphrasing). ELUE has four tracks with a parameter number cut-off of 40M, 55M, 70M and 110M. The metric used in ELUE is ELEU Score, which describes the progress on the Pareto Front.  Weight sharing is based on the assumption that large-scale models, like Transformer [Vaswani et al., 2017] are overparameterized [Li et al., 2020]. Weight sharing provides a way to decouple computation and parameters by reusing the same parameters for multiple computations. Weight sharing can reduce inference memory footprint and number of parameters and thus is memory-and storage-efficient. In the vanilla Transformer model [Vaswani et al., 2017] for neural machine translation (NMT), there is one encoder for encoding the input into hidden representations, and one decoder for decoding it to the target language. Tied Transformer [Xia et al., 2019] shares the weights of the encoder and decoder of Transformer. The results of Tied Transformer are comparable to the vanilla Transformer. Rothe et al. [2020] leverage pretrained language model checkpoints to initialize a sequence-to-sequence model. They experiment with a shared encoder and decoder to reduce memory footprint. Layer Sharing In Transformer [Vaswani et al., 2017], both the encoder and decoder are stacks of Transformer layers. Thus, a simple and straightforward way to share the weights in a Transformer is to share them across all Transformer layers. Dabre and Fujita [2019] share the weights across all Transformer layers for NMT with minimal performance drop. Universal Transformer [Dehghani et al., 2019] shares the weights across all layers, allowing for recurrent computation with a dynamic halting mechanism and achieves better performance than the vanilla Transformer. ALBERT [Lan et al., 2020] introduces the idea into pretrained language models for natural language understanding (NLU). Although it cannot reduce the computational overheads and has an inevitable negative effect on performance, this design saves up to 95% of disk space for storing the model, which can be critical for deployment on mobile devices with limited storage. Takase and Kiyono [2021] systematically study the strategies for sharing weights across layers. Instead of using the weights of one Transformer layer for all layers, they aim to explore the best way to use the parameters of M layers for N layers (M < N ). Reid et al. [2021] introduce a strategy named "sandwich-style" parameter sharing, which shares the weights for central layers while leaving the first and last layers independent. The weight matrices in a neural network are often low-rank, indicating redundancy in model weights [Sainath et al., 2013]. Thus, a natural idea is to factorize the weight matrices into two or more smaller matrices to save parameters. A common technique for low-rank factorization is singular value decomposition (SVD). For a matrix A ∈ R m×n , there exists A = U ΣV T , where r ≤ min {m, n} is the rank of A; U ∈ R m×r , V ∈ R n×r are two orthogonal matrices; Σ ∈ R r×r is a diagonal matrix with only the non-zero singular values of A. Thus, the space complexity can be effectively reduced from O(mn) to O(mr + rn). Low-Rank factorization can be applied to any linear layer. Grachev et al. [2017] factorize the weights of an LSTM language model. Following that, Winata et al. [2019] exploit SVD for both the LSTM cell in a language modeling task and a pretrained LSTM language model, ELMo [Peters et al., 2018]. This is one of the earliest attempts to compress a pretrained language model. Ma et al. [2019] propose a new self-attention module, namely multi-linear attention, as a substitute for the standard multi-head attention module in a Transformer. They use block-term tensor decomposition (BTD, [Lathauwer, 2008]) to factorize multi-head attention. Their results demonstrate comparable performance to the vanilla Transformer while being parameter-efficient. Noach and Goldberg [2020] propose a two-stage approach to compress a pretrained language model. In the first stage, they decompose each weight matrix in the pretrained language model with SVD. Then, for the second stage, they fine-tune or use knowledge distillation to refine the weights.  propose data-aware low-rank compression (DRONE) by exploiting the prior knowledge of the data distribution. Instead of minimizing the reconstruction error of the the weight matrix, they minimize the approximation error of the outputs. DRONE achieves better performance than SVD. ALBERT [Lan et al., 2020] use factorization only for the embedding layer. Both the embedding and hidden states of Transformer are in high dimensions, bringing too many parameters in the embedding layer. Since the power of Transformer mainly comes from its contextual learning ability, the parameters in the token embedding layer are not efficient. It intuitively makes sense to reduce them by factorizing the embedding matrix. Reid et al. [2021] propose self-attentive factorized embeddings (SAFE) by adding a small self-attention layer on the basis of linear projection to achieve better performance. Pruning [LeCun et al., 1989] aims to remove unimportant weights from a neural network to achieve parameter-efficiency while preserving model performance. There are two key elements in a pruning method: (1) A pruning unit is the atomic unit to be removed from a model; it can be a single weight (unstructured pruning), an attention head or even a Transformer layer (structured pruning). (2) A saliency score is the criterion for making pruning decisions. Based on whether it uses a gradient and which order of gradient it uses, we can categorize pruning methods to zeroth-order (only considering weight magnitude), first-order and second-order approaches. We summarize some pruning methods in Table 1. Unstructured Pruning Unstructured pruning removes "unimportant" connections in a neural network by setting the corresponding weights to 0. After pruning, the weight matrix often becomes sparse. To exploit the characteristics of a sparse matrix to achieve computation-and memory-efficiency, a specialized hardware (e.g., sparse tensor cores in Nvidia A100 [Mishra et al., 2021]) and software (e.g., PyTorch Sparse API 2 ) are necessary. See et al. [2016] uses magnitude-based pruning with retraining to compress RNN models for NMT. Magnitude-based pruning [Han et al., 2016] simply prunes weights with smallest magnitude (i.e., absolute values). After pruning, See et al. [2016] continue to fine-tune the pruned network to obtain better performance. Narang et al. [2017a] prune an RNN model gradually during training. The magnitude threshold for pruning is gradually increased with increasing training steps. Wang et al. [2020b] first prunes and retrains NMT models with magnitude pruning and then restores the pruned parameters to train the entire network again, in order to achieve better performance than the original model. Zhang and Stadie [2020] propose a one-shot pruning technique based on the Jacobian spectrum. Different from iterative pruning methods, one-shot pruning techniques only prune a network once and then use standard training to train the sparse network. Some recent works target transfer learning as it has become a new paradigm in NLP. Gordon et al. [2020] aim to reveal how pruning affects transfer learning. They find that low levels of pruning (30%-40%) do not affect pretraining loss or transfer to downstream tasks at all. However, further pruning has a Hessian-based pruning Magnitude pruning L 0 regularization Movement pruning [LeCun et al., 1989] [ Han et al., 2016] [Louizos et al., 2018] [Sanh et al., 2020 Pruning Decision 2nd order 0th order 1st order 1st order Learning Objective  ., 2020] and find this approach can automatically learn to prune out full components in Transformer, e.g., an attention head. L L L + λ l0 E(L 0 ) L Scores S − t ( ∂ 2 L ∂W 2 i,j ) (t) W 2(t) i,j |W i,j | − t ( ∂L ∂Wi,j ) (t) W (t) i,j f (S (t) i,j ) − t ( ∂L ∂Wi,j ) (t) W (t) i,j propose the "lottery ticket hypothesis": dense, randomlyinitialized, feed-forward networks contain subnetworks (winning tickets) that -when trained in isolation -reach test accuracy comparable to the original network in a similar number of iterations. It reveals that retraining from remaining weights [Han et al., 2016] in a pruned network is not necessary for the pruned network. In contrary, a "winning ticket" can always learn better, even when training from scratch (as long as it is initialized with the same random weights). Following this,  verify the lottery ticket hypothesis on BERT with iterative magnitude pruning. They find that subnetworks found on the pretraining task (i.e., masked language modeling, MLM) transfer universally to downstream tasks whereas those found on downstream tasks do not. Prasanna et al. [2020] also verify the lottery ticket hypothesis with BERT, for both magnitude and structured pruning. They find that even the worst subnetwork in BERT remains highly trainable, suggesting that the weights of BERT may have relatively low redundancy. This seems to be consistent with previous finding on over-parameterized models [Nakkiran et al., 2020].  [Jacob et al., 2018] to quantize both weights and activations to INT8 dynamically. They also explore quantization-aware training (QAT) for BERT. They use fake quantization [Jacob et al., 2018] to introduce quantization error into the model during training phase to simulate the rounding effect. They use the Straight-Through Estimator (STE) [Bengio et al., 2013] to estimate the gradient of the non-differentiable fake quantization. They find that dynamic post-training quantization hurts the downstream performance slightly while QAT achieves comparable performance to the original model. Similarly, Prato et al. [2020] apply QAT with STE for Transformers on neural machine translation and achieve results that are similar to the original model. Previous works still need floating point calculation for activation during inference. I-BERT [Kim et al., 2021] eliminates floating point calculation in BERT by exploiting lightweight integer-only approximation methods for non-linear operations (e.g., GELU, Softmax and LayerNorm). The resulted I-BERT model is capable of doing pure INT8 inference thus has a better acceleration ratio. Lower-Bit Quantization Recent works aim to push quantization to even lower precision. Lower-bit quantization faces more challenges, including difficulty to optimize, and lack of model expressivity. Shen et al. [2020] propose a group-wise quantization scheme and a use second-order Hessian-based mixed-precision method  to quantize BERT down to 2 bits. They claim that weights corresponding to each neuron could lie in different range of real numbers. For example, for a multi-head self-attention module, they split the weight matrix to 12 groups, with respect to each attention head. Then they further split each group and have a total number of 128 subgroups, each of which has its own quantization range. GOBO [Zadeh et al., 2020] separates the weights into two groups -"Gaussian" and "Outliers" where the former is quantized to 3 bits and the latter remains a full-precision float (FP32). TernaryBERT [Zhang et al., 2020] combines approximation-aware and loss-aware ternarization (i.e., using only {−1, 0, 1} for weights) methods with different granularity for different components in BERT. They further add knowledge distillation to improve the performance of QAT. Bai et al. [2021] observe a large performance drop from a ternary network to a binary network when trained directly, due to its loss landscape. They propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network. The initialized network inherits good performance and can be further fine-tuned without optimization difficulty. Knowledge Distillation [Hinton et al., 2015] is a widely used technique to transfer knowledge from a large model (teacher) to a smaller one (student). It usually designs a loss function to minimize the distance of the output or intermediate features between the student and the teacher. As illustrated in Figure 2, we summarize the designs of loss functions used in recent works distilling NLP models. Based on the loss function designs, we can further categorize the methods into logitbased KD, feature-based KD, KD with a dynamic target, and module replacing. Logit-based KD Following Hinton et al. [2015], logit-based KD methods are the first attempts to distill a large pretrained language model into a smaller one to improve its efficiency. Logit-based KD uses the KL divergence or mean squared error (MSE) to minimize the logits between the student and the teacher. Tang et al. [2019] distills fine-tuned BERT into a BiLSTM model in a task-specific setting. The resulted BiLSTM outperforms its counterpart trained without KD by a large margin. DistilBERT [Sanh et al., 2019] distills BERT in a pretraining setting on the task of masked language modeling (MLM). The loss is a combination of three components: the original MLM loss, cosine distance, and KL divergence. After distillation, the model can be finetuned and perform downstream tasks. Turc et al. [2019] explore the effect of initialization for the student. They find that a student BERT pretrained with MLM outperforms random initialization and truncated teacher [Sanh et al., 2019; when used to initialize the student model. Liang et al. [2021] use MixUp [Zhang et al., 2018] for data augmentation to distill BERT.   TinyBERT [Jiao et al., 2020] further introduces an attention loss that aims to align the attention matrices in layers between the teacher and the student, as illustrated in Figure 2(e). TinyBERT also demonstrates that performing KD in both pretraining and fine-tuning stages can improve the performance of KD. Similarly, MiniLM [Wang et al., 2020a;Wang et al., 2021] aligns the attention matrix and valuesvalues scaled dot-product (formula shown in Figure 2(f)). The added feature complements the attention matrix (i.e., querieskeys scaled dot-product) and allows the complete transfer of multi-head self-attention. Wu et al. [2021] propose a multiteacher distillation framework that use both intermediate features and soft labels from multiple teachers to distill a student and achieve better performance. MobileBERT  redesigns a BERT architecture that is suitable for mobile devices. In addition to the layer-wise feature distillation  and attention distillation [Jiao et al., 2020], they introduce a progressive knowledge transfer mechanism by distilling the model layer by layer, instead of altogether. Module Replacing A special case of KD is BERT-of-Theseus . As shown in Figure 2(d), BERTof-Theseus does not apply any knowledge transfer loss to minimize the distance between the student and the teacher. Instead, they freeze the teacher modules and train a hybrid model by randomly replacing some modules in the teacher model with student modules. They further design a linear scheduler to increase the probability of replacement to bridge the gap between training and inference. KD with Dynamic Targets In traditional KD, the teacher serves as a static target for the student to match, without any update during distillation. However, this can be suboptimal since the teacher is unaware of the student or its goal to transfer the knowledge to the student. ProKT [Shi et al., 2020] projects the supervision signals of a teacher model into the student's parameter space by decomposing the training objective into local intermediate targets with approximate mirror descent [Beck and Teboulle, 2003]. Zhou et al. [2021] propose a simpler framework with meta learning to allow the teacher to adapt itself for better knowledge transfer. The student is evaluated on a "quiz" set after a few training steps and provides feedback to the teacher. Evaluation Although there have been benchmarks proposed for evaluating model compression as introduced in Section 2.2, there are several drawbacks in current evaluation of model compression. First, there is no generally recognized setting for evaluation of model compression. Different studies often yield models with different speed-up ratio, number of parameters and accuracy. Thus, it is often difficult to directly compare them, not to mention the difference in hardware. Second, general NLU benchmarks like GLUE [Wang et al., 2019b] or SuperGLUE [Wang et al., 2019a] may not be the best to represent more common tasks on a mobile device. Tasks like intention detection, dense retrieval, and spam classification could be more representative. Combining Compression Techniques Although there have been attempts combining multiple model compression techniques [Kim and Awadalla, 2020;Sanh et al., 2020;Xu et al., 2021a], there is a lack of comprehensive and systematic study for combining compression techniques for better performance and efficiency. Constructing a best practice to compress a large model can be useful for practitioners and engineers. Explainability and Robustness Recent works [Stanton et al., 2021;Xu et al., 2021a] cast doubt on the explainability of model compression. Meanwhile, recent works [Du et al., 2021;Xu et al., 2021a] report negative effects of model compression on robustness. Explainable and robust compression methods can be important for applications of model compression. Also, explainable and robust compression minimizes effort to re-evaluate the compressed model, and thus can be reliable and predictable in production [Stanton et al., 2021;Xu et al., 2021a]. Getting Rid of Human Efforts Current compression approaches still largely rely on human heuristics to achieve good performance. For example, KD often requires an elaborately designed loss function; pruning relies on the saliency score; weight sharing and low-rank factorization involve expertise to appoint modules for sharing or factorization. One promising direction could be applying Meta Learning [Finn et al., 2017] or Neural Architecture Search [Liu et al., 2019] to model compression, to minimize the need for hyperparameters and human design.