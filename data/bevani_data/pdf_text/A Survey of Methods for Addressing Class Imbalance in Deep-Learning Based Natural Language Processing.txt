Many natural language processing (NLP) tasks are naturally imbalanced, as some target categories occur much more frequently than others in the real world. In such scenarios, current NLP models still tend to perform poorly on less frequent classes. Addressing class imbalance in NLP is an active research topic, yet, finding a good approach for a particular task and imbalance scenario is difficult. With this survey, the first overview on class imbalance in deep-learning based NLP, we provide guidance for NLP researchers and practitioners dealing with imbalanced data. We first discuss various types of controlled and realworld class imbalance. Our survey then covers approaches that have been explicitly proposed for class-imbalanced NLP tasks or, originating in the computer vision community, have been evaluated on them. We organize the methods by whether they are based on sampling, data augmentation, choice of loss function, staged learning, or model design. Finally, we discuss open problems such as dealing with multilabel scenarios, and propose systematic benchmarking and reporting in order to move forward on this problem as a community. Class imbalance is a major problem in natural language processing (NLP), because target category distributions are almost always skewed in NLP tasks. As illustrated by Figure 1, this often leads to poor performance on minority classes. Which categories matter is highly task-specific and may even depend on the intended downstream use. Developing methods that improve model performance in imbalanced data settings has been an active area for decades (e.g., Bruzzone and Serpico, 1997;Japkowicz et al., 2000;Estabrooks and Japkowicz, 2001;Park and Zhang, 2002;Tan, 2005), and is recently gaining momentum in the context of maturing neural approaches (e.g., Buda et al., 2018;  2020; Yang et al., 2020;Jiang et al., 2021;Spangher et al., 2021). The problem aggravates when classes overlap in the feature space (Lin et al., 2019;Tian et al., 2020). For example, in patent classification, technical categories differ largely in frequency, and the concepts mentioned in the different categories can be very similar. On a large variety of NLP tasks, transformer models (Vaswani et al., 2017;Devlin et al., 2019) exhibit a stronger performance compared to traditional models and even compared to their neural predecessors (Liu et al., 2019;Mathew et al., 2021). Performance for minority classes is also often higher when using selfsupervised pre-trained models compared to earlier approaches (e.g., Li and Scarton, 2020;Niklaus et al., 2021), which parallels findings from computer vision . However, the advent of BERT has not solved the class imbalance problem in NLP, as illustrated by Figure 1. Tänzer et al. (2022) find that despite BERT's strong generalization abilities, on some named entity datasets, the model requires at least 25 instances of a minority class in order to predict it at all, and 100 examples to learn to predict it with some accuracy. Despite the relevance of methods improving model performance under class imbalance to NLP, related surveys only exist in the computer vision domain (Johnson and Khoshgoftaar, 2019b;Zhang et al., 2021b). Our contribution is to draw a clear landscape of approaches applicable to deeplearning (DL) based NLP, providing an entry point for researchers and developers who struggle with the class imbalance in their dataset(s). We set out with a problem definition (Sec. 2), and then organize approaches by whether they are based on sampling, data augmentation, choice of loss function, staged learning, or model design (Sec. 3). Moreover, we discuss particular challenges of nonstandard classification settings, e.g., imbalanced multi-label classification and catch-all classes, and make connections to computer vision research explicit where applicable. Finally, we outline promising directions for future research (Sec. 4). Scope of this survey. We focus on approaches evaluated on or developed for neural methods. In "traditional" NLP, a number of works exist as well (e.g., Tomanek and Hahn, 2009;Li et al., 2011;Li and Nenkova, 2014;Kunchukuttan and Bhattacharyya, 2015). Further related work exists in the areas of Natural Language Generation (e.g., Nishino et al., 2020) and Automatic Speech Recognition (e.g., Winata et al., 2020;Deng et al., 2022). Other types of imbalances such as differently sized data sets of subtasks in continual learning (Ahrens et al., 2021) or imbalanced regression  are beyond the scope of this survey. In Sec. 3.5, we briefly touch upon the related area of few-shot learning (Wang et al., 2020c). Related surveys. Class imbalance settings often entail the availability of only very little data especially for the minority classes. We review imbalance-specific data augmentation approaches in Sec. 3.2. Feng et al. (2021) give a broader overview of data augmentation in NLP, and Hedderich et al. (2021) provide an overview of lowresource NLP. Class imbalance refers to a classification setting in which one or multiple classes (minority classes) are considerably less frequent than others (majority classes). Much research focuses thus on improving all minority classes equally while maintaining or at least monitoring majority class performance (e.g., Huang et al., 2021;Yang et al., 2020;Spangher et al., 2021). We next discuss prototypical types of imbalance (Sec. 2.1) and then compare controlled and real-world settings (Sec. 2.2). To systematically investigate the effect of imbalance, Buda et al. (2018) define two types of label distributions, which we explain next. Step imbalance is characterized by the fraction of minority classes, µ, and the size ratio between majority and minority classes, ρ. Larger ρ values indicate more imbalanced data sets. In prototypical step imbalance, if there are multiple minority classes, all of them are equally sized; if there are several majority classes, they also have equal size. Figure 2a shows a step-imbalanced distribution with 40% of the classes being minority classes and an imbalance ratio of ρ = 10. The ρ ratio has also been reported in NLP, e.g., by , although more task-specific imbalance measures have been proposed, e.g., for single-label text classification (Tian et al., 2020). In linear imbalance, class size grows linearly with imbalance ratio ρ (see Figure 2b). Longtailed label distributions as shown in Figure 2c contain many data points for a small number of classes (head classes), but only very few for the rest of the classes (tail classes). These distributions are common in computer vision tasks like instance segmentation (e.g., Gupta et al., 2019a), but also in multi-label text classification, for example with the goal of assigning clinical codes (Mullenbach et al., 2018), patent categories (Pujari et al., 2021), or news and research topics (Huang et al., 2021). Most real-world label distributions in NLP tasks do not perfectly match the prototypical distributions proposed by Buda et al. (2018). Yet, awareness of these settings helps practitioners to select appropriate methods for their data set or problem by comparing distribution plots. Using synthetically imbalanced data sets, researchers can control for more experimental factors and investigate several scenarios at once. Evaluating on naturally imbalanced data provides evidence of a method's realworld effectiveness. Some recent studies combine both types of evaluation (e.g., Tian et al., 2021;Subramanian et al., 2021;Jang et al., 2021). Many NLP tasks require treating a large often heterogenous catch-all class, while the remaining (minority) classes are approximately same-sized. Examples include the "Outside" label in IOB sequence tagging, or tweets that mention products in contexts that are irrelevant to the annotated categories (Adel et al., 2017). Such real-world settings often roughly follow a step imbalance distribution, with the additional difficulty of the catch-all class. As accuracy and micro-averages mostly reflect majority class performance, choosing a good evaluation setting and metric is non-trivial. It is also highly task-dependent: in many NLP tasks, recognizing one or all minority classes well is at least equally important as majority class performance. For instance, when analyzing Twitter data with regard to hatefulness, non-hateful tweets are much more frequent (Waseem and Hovy, 2016), but recognizing hateful content is the key motivation of hate speech detection. Which classes matter may even depend on downstream considerations, i.e., the same named entity tagger might be used in one application where the majority class is important, and another where some minority classes are much more important. Several evaluation metrics exist that have been designed to account for class-imbalanced settings, but no de facto standard exists. For example, balanced accuracy (Brodersen et al., 2010), corresponds to the average of per-class recall scores. It is often useful to record performance on all classes and to report macro averages, which treat all classes equally. Imbalance in NLP In this section, we survey methods that either have been explicitly proposed to address classimbalance issues in NLP or that have been empirically shown to be applicable for NLP problems. We provide an overview of which methods are applicable to a selection of NLP tasks in Appendix A. To increase the importance of minority instances in training, the label distribution can be changed by various sampling strategies. Sampling can either be executed once or repeatedly during training (Pouyanfar et al., 2018). In random oversampling (ROS), a random choice of minority instances are duplicated, whereas in random undersampling (RUS), a random choice of majority instances are removed from the dataset. ROS can lead to overfitting, e.g., the learned function might change to cover a single duplicated instance, affecting generalization. RUS, however, discards potentially valuable data. When applied to DL models, ROS has been found to outperform RUS both in synthetic settings (Buda et al., 2018) and in binary and multiclass English and Korean text classification (Juuti et al., 2020;Akhbardeh et al., 2021;Jang et al., 2021). More flexible variants, e.g., re-sampling only a tunable share of classes, have been shown to lead to some further improvements (Tepper et al., 2020). Class-aware sampling (CAS, Shen et al., 2016), also referred to as class-balanced sampling, first chooses a class and then an instance from this class. Performance-based re-sampling during training, following the idea of Pouyanfar et al. (2018), has been shown to work well in multi-class text classification (Akhbardeh et al., 2021). Issues in multi-label classification. In multilabel classification, label dependencies between majority and minority classes complicate sampling approaches, as over-sampling an instance with a minority label may simultaneously amplify the majority class count (Charte et al., 2015;Huang et al., 2021). CAS also suffers from this issue and additionally introduces within-class imbalance, as instances of one class are selected with different probabilities depending on the co-assigned labels . Effective sampling in such settings is still an open issue. Existing approaches monitor the class distributions during sampling (Charte et al., 2015) or assign instance-based sampling probabilities (Gupta et al., 2019b). Class imbalance settings typically entail insufficient amounts of data for the minority classes. Increasing the amount of minority class data already during corpus construction, e.g., by writing additional examples or selecting examples to be labeled using Active Learning, can mitigate this problem to some extent (Cho et al., 2020;Ein-Dor et al., 2020). However, this is particularly laborious in naturally imbalanced settings as it may require finding "the needle in the haystack," or may lead to biased minority class examples, e.g., due to collection via keyword queries. Synthetically generating additional minority instances thus is a promising direction. In this section, we survey data augmentation methods that have been explicitly proposed to mitigate class imbalance and that have been evaluated in combination with DL models; Feng et al. (2021) describe data augmentation for NLP in general. Text augmentation generates new natural language instances of minority classes, with methods ranging from simple string-based manipulations such as synonym replacements to Transformerbased generation. Easy Data Augmentation (EDA, Wei and Zou, 2019), while not explicitly targeting class imbalance, has been shown to work well in class-imbalanced settings (Jiang et al., 2021;Juuti et al., 2020). EDA covers dictionary-based synonym replacements, random insertion, random swap, and random deletion. Juuti et al. (2020), generate new minority class instances for English binary text classification using EDA and embeddingbased synonym replacements, and by adding a random majority class sentence to a minority class document. They also prompt the pre-trained language model GPT-2 (Radford et al., 2019) with a minority class instance to generate new minority class samples. Tepper et al. (2020) evaluate generation with GPT-2 on English multiclass text classification datasets, coupled with a flexible balancing policy (see Sec. 3.1). Hidden space augmentation generates new instance vectors that are not directly associated with a particular natural language string, leveraging the representations of real examples. Using representation-based augmentations to tackle class imbalance is not tied to DL. SMOTE (Chawla et al., 2002), which interpolates minority instances with randomly chosen examples from their K-nearest neighbours, is popular in traditional machine learning (Fernández et al., 2018), but leads to mixed results in DL based NLP (Ek and Ghanimifard, 2019;Tran and Litman, 2021;Wei et al., 2022). Inspired by CutMix (Yun et al., 2019), which cuts and pastes a single pixel region in an image, TextCut (Jiang et al., 2021) mixes BERT representations of two instances to synthesize a new sample, randomly replacing small parts of the representation of one instance with those of the other. In binary and multi-class text classification experiments, using TextCut improves over non-augmented BERT and EDA. Good-enough example extrapolation (GE3, Wei, 2021) and REPRINT (Wei et al., 2022) also operate in the original representation space. To synthesize a new minority instance, GE adds the vector representing the difference between a majority class instance and the centroid of the respective majority class to the mean of a minority class. Evaluations on synthetically step-imbalanced English multi-class text classification datasets show improvements over oversampling and hidden space augmentation baselines which use minority class data only. GE3 assumes that the distribution of data points of a class around its mean can be extrapolated to other classes, an assumption potentially hurting performance if the minority class distribution differs. To account for this when subtracting out majority characteristics, REPRINT performs a principal component analysis (PCA) for each class, leveraging the information on relevant dimensions during sample generation, and also makes use of a random minority class instance. This method usually leads to better performance than GE3, coming at the cost of an additional hyperparameter (subspace dimensionality). MISO (Tian et al., 2021) generates new instances by transforming the representations of minority class instances whose representations are located closely to those of majority class instances. To this end, they learn a mapping from minority instance vectors to "disentangled" representations, making use of mutual information loss neural estimators (Belghazi et al., 2018) to push these representations further away from the majority class and closer to the minority class. An adversarially trained generator then generates minority instances using these disentangled representations. Tian et al. apply MISO in naturally and synthetically imbalanced English and Chinese binary and multi-class text classification with a single minority class. ECRT  learns to map encoder representations (feature space) to a new space (source space) whose components are independent of each other given the class, assuming an invariant causal mechanism from source to feature space. The class-conditional independence of components enables them to generate new meaningful minority examples by permuting or sampling components in the source space. While the authors show improvements of ECRT on a large multi-label text classification dataset with many labels, ablations on an image classification dataset suggest that this performance gain may mainly come from classifying based on the disentangled source space, with augmentations still having a smaller effect. Transfer learning. Further related work exists in the area of transfer learning (Ruder et al., 2019), e.g., by selecting additional suitable datasets that provide complementary information on minority classes. For instance, Spangher et al. (2021) demonstrate moderate gains by manually selecting auxiliary datasets to improve imbalanced sentencebased discourse classification. However, for each application, potentially complementary existing datasets have to be retrieved and task loss coefficients have to be tuned. Adapting methods to predict useful transfer sources  might help alleviate these problems. In standard cross-entropy loss (CE), the loss is composed from the predictions for instances that carry the label in the gold standard, resulting in the classifier fitting the majority classes well, but minority classes less so. In this section, we summarize loss functions designed to improve classification results in the presence of class imbalance. They either aim at re-weighting instances by class membership or prediction difficulty, or explicitly model class margins to change the decision boundary. For an overview, see Table 1. Losses for Single-Label Scenarios. Weighted cross-entropy (WCE) uses class-specific weights that are tuned as hyperparameters or set to the inverse class frequency (e.g., Adel et al., 2017;Tayyar Madabushi et al., 2019;Li and Xiao, 2020). While WCE treats all instances of one class in the same way, focal loss (FL, Lin et al., 2017) downweights instances for which the model is already confident. 1 Instead of mimicking accuracy like CE, dice loss (Dice, Milletari et al., 2016) tries to capture class-wise F1 score, with predicted probabilty p j proxying precision and ground truth indicator y j proxying recall.  propose to combine confidence-based down-weighting with Dice loss, and call this self-adjusting dice loss (ADL). For sequence labeling, QA and matching on English and Chinese datasets, Dice performs better than FL and ADL. Rather than re-weighting instances, labeldistribution-aware margin loss (LDAM, Cao et al., 2019), essentially a smooth hinge loss with labeldependent margins, aims to increase the distance of the minority class instances to the decision boundary with the aim of better generalization for these classes. Cao et al.'s evaluation largely focuses on computer vision, but they also report results for LDAM on an artificially imbalanced version of the IMDB review dataset (Maas et al., 2011), achieving a much lower error on the minority class than vanilla CE or CE with re-sampling or re-weighting. Subramanian et al. (2021) propose LDAM variants that also consider bias related to socially salient groups (e.g., gender-based bias) in addition to class imbalance. In settings with a large artificial and potentially heterogeneous catch-all class (see Sec. 2.2), many areas of the space contain representations of the catch-all class. Here, vanilla LDAM might be an appropriate loss function as it encourages larger margins for minority classes. In such cases, ranking losses (RL) can also be effective to incentivize the model to only pay attention to "real" classes. On an imbalanced English multi-class dataset with a large catch-all class, Adel et al. (2017) find an RL introduced by dos Santos et al. (2015) to improve over CE and WCE. For minority classes, this loss function maximizes the score of the correct label while at the same time minimizing the score of the highest-scoring incorrect label. For the catch-all  class, only the score of the best competitive (minority) label is minimized; the score for the catch-all class is ignored. Similarly, Hu et al. (2022) apply class weights only to non-catch-all classes. Single-label CE − C j=1 yj log pj WCE − C j=1 αjyj log pj FL − C j=1 yj(1 − pj) β log pj Dice C j=1 1 − 2pjyj + γ p 2 j + y 2 j + γ ADL C j=1 1 − 2(1 − pj)pjyj + γ (1 − pj)pj + yj + γ LDAM − C j=1 yj log exp(zj − ∆j) exp(zj − ∆j) + l =j exp(z l ) with ∆j = K/n 1/4 j RL 1(gt = A) log(1 + exp(ρ(m + − zgt))) + log(1 + exp(ρ(m − − z c − ))) Multi-label BCE − C j=1 [yj log pj + (1 − yj) log(1 − pj)] WBCE − C j=1 αj[yj log pj + (1 − yj) log(1 − pj)] FL − C j=1 [yj(1 − pj) β log pj + (1 − yj)p β j log(1 − pj)] DB − C j=1 [yjαj(1 − qj) β log qj + (1 − yj)αj 1 λ q β j log(1 − qj)] with qj = yjσ(zj − vj) + (1 − yj)σ(λ(zj − vj)) Losses for Multi-Label Scenarios. In multilabel classification, each label assignment can be viewed as a binary decision, hence binary crossentropy (BCE) is often used here. In imbalanced multi-label classification, two issues arise. First, although class-specific weights have been used with BCE (e.g., Yang et al., 2020), their effect on minority classes is less clear than in single-label classification. For each instance, all classes contribute to BCE, including the labels not assigned to the instance (called negative classes). Thus, if WBCE uses a high weight for a class, it also increases the importance of negative instances for a minority class, which may further encourage the model not to predict this minority class. To leverage class weights more effectively in BCE, one option is to only apply them to the loss of positive instances as proposed for multi-label image classification (Kumar et al., 2018). Related work includes uniformly upweighting positive instances of all classes in hierarchical multi-label text classification (e.g., Rathnayaka et al., 2019), and an approach to multi-label emotion classification by Yilmaz et al. (2021), which performs training time balancing by adapting FL such that for a given mini-batch the loss over all instances in this minibatch has exactly the same value for every class. Second, if a classifier already correctly predicts a negative class for an instance, the loss can be further decreased by reducing the respective label's logits. In CE, due to the softmax that uses the logits of all classes, the impact of this effect becomes minor once the logit for the correct class is much larger than those of the other classes. However, the problem is more severe in BCE , as logits are treated independently. As minority labels mostly occur as negative classes, this logit suppression leads to a bias in the decision boundary, making it less likely for minority classes to be predicted. To tackle this issue and based on a multi-label version of FL,  propose distribution-balanced loss (DB) for object detection, adding Negative Tolerant Regularization for the loss for negative classes by transforming the logits of positive and negative classes differently (see q j in Table 1). This regularization imposes a sharp drop in the loss function for negative classes once the respective logit is below a threshold. DB introduces instance-specific class weights (α in Table 1) to account for imbalances caused by classaware sampling (see Sec. 3.1) in multi-label scenarios. These weights reflect the frequency of a class and the quantity and frequency of the positive labels of the instance. Huang et al. (2021) have shown large improvements of DB over BCE even when using uniform sampling on two long-tailed multi-label English text classification datasets. While only indirectly targeting class imbalance Ferreira and Vlachos (2019) show that applying a cross-label dependency loss (Yeh et al., 2017;Zhang and Zhou, 2006) can be helpful for multilabel stance classification. Similarly, Lin et al. (2019) introduce a label-confusion aware cost factor into their loss function. Combining such losses with class weighting techniques could be one fruitful direction for future research. For example, Suresh and Ong (2021) integrate inter-label relationships into a contrastive loss (Khosla et al., 2020), which compares the score of a positive ex-ample with the distance to that of other positive and negative examples in order to push its representation closer to the correct class and further away from the wrong class(es). The adaptive loss of Suresh and Ong (2021) learns how to increase the weight of confusable negative labels relative to other negative labels. One approach to finding a good trade-off between learning features that are representative of the underlying data distribution and reducing the classifier's bias towards the majority class(es) is to perform the training in several stages. Two-staged training is common in imbalanced or data-scarce computer vision tasks (e.g., Wang et al., 2020b,a;Zhang et al., 2021a). The first stage usually performs standard training in order to train or fine-tune the feature extraction network. Later stages may freeze the feature extractor and re-train the classifier layers using special methods to address class imbalance, e.g., using more balanced data distributions or specific losses. For example, Cao et al. (2019) find their LDAM loss to be most effective when the training happens in two stages. In NLP, deep-learning models are usually based on neural text encoders or word embeddings that are pre-trained on large amounts of unlabeled text in a self-supervised fashion. Further domainspecific pre-training before starting the fine-tuning stage(s) can also lead to even more effective representations (Gururangan et al., 2020). Several NLP approaches that fall into the staged learning category are directly inspired by results from computer vision research. In the context of long-tailed image classification, Kang et al. (2020) find that class-balanced sampling helps when performing single-stage training, but that in their twostage classifier re-training (cRT) method, using the original distribution in the first stage is more effective than class-balanced sampling. cRT employs the latter only in the second stage after freezing the representation weights.  perform a similar decoupling analysis on long-tailed relation classification, essentially confirming Kang et al. (2020)'s results on this NLP task with respect to the re-sampling strategies. Additionally, they find that loss re-weighting under this analysis behaves similar to re-sampling, i.e., it leads to worse performance when applied during representation learning, but boosts performance when re-training the classifier. Hu et al. (2022)  Active Learning (AL), which contains several stages by definition, has also been shown to boost performance of BERT models for minority classes (Ein-Dor et al., 2020). For a discussion about AL and DL, see Schröder and Niekler (2020). The methods described so far are largely independent of model architecture. This section describes model modifications that aim at improving performance in imbalanced settings, ranging from parameter normalization over using class representatives to problem-specific architectures. Observing that the weight vectors for smaller classes have smaller norms in standard joint training compared to staged-learning based cRT (see Sec. 3.4), Kang et al. (2020) normalize the classifier weights directly in one-staged training using a hyperparameter τ to control the normalization "temperature" (τ -norm). τ -norm achieves similar or better performance than cRT in long-tailed image classification and outperforms cRT in relation extraction, but cRT works better for named entity recognition and event detection (Nan et al., 2021). SetConv (Gao et al., 2020) and ProtoBERT (Tänzer et al., 2022) learn representatives for each class using support sets and classify an input (the query) based on its similarity to these representatives. SetConv applies convolution kernels that capture intra-and inter-class correlations to extract class representatives. ProtoBERT uses class centroids in a learned BERT-based feature space, treating the distance of any instance to the catch-all class as just another learnable parameter. At each training step, SetConv uses only one instance per class in the query set, but preserves the original class imbalance in the support set, whereas Proto-BERT uses fixed ratios. In the respective experimental studies, ProtoBERT performs better than using a standard classification layer on top of BERT for minority classes in NER if less than 100 examples are seen by the model, while SetConv excels in binary text classification with higher degrees of imbalance and in multi-class text classification. The HSCNN model (Yang et al., 2020) uses class representatives only for the classification of tail classes, while head classes are assigned using a standard text CNN (Kim, 2014). HSCNN learns label-specific similarity functions, extracting instance representations from the pre-final layers of two copies of the original CNN, and assigns a tail class if the similarity to the class representative (computed as the mean of 5 random support instances) exceeds 0.5. On tail classes, HSCNN consistently improves over the vanilla CNN. In addition, there are a number of approaches that propose problem-specific solutions. Prange et al. (2021) propose to construct CCG supertags from predicted tree structures rather than treating the problem as a standard classification task. In order to recognize implicit positive interpretations in negated statements in a class-imbalanced dataset, van Son et al. (2018) argue that leveraging information structure could be one way to improve inference. Structural causal models (SCMs) have been applied to imbalanced NLP tasks, encoding task-specific causal graphs (e.g., Nan et al., 2021). Similarly,  causally model how bias in long-tailed corpora affects topic modeling (Blei et al., 2003) and use this to improve training of a variational autoencoder. A research area closely related to class imbalance is that of few-shot learning (FSL, Wang et al., 2020c), which aims to learn classes based on only very few training examples. Leveraging model ideas from FSL for long-tail settings is an active research area, often interacting with data augmentation methods. In long-tailed settings, tail classes can be learned based on information transferred from head classes, e.g., leveraging relational information about class labels in the form of knowledge graph embeddings or other forms of embedding hierarchical relationships between labels (Han et al., 2018;Zhang et al., 2019), or computing labelspecific representations (Mullenbach et al., 2018). We have provided a comprehensive, concise and structured overview of current approaches to dealing with class imbalance in DL-based NLP, intended to serve as a practical reference. We have also highlighted numerous opportunities for future research aiming at more robust DL models. We thereby add to the ongoing discussion in the NLP community about achieving robustness Omar et al., 2022), which can be defined as keeping performance high when the test data is from a different distribution than the training data. Hence, in the possibly simplest dimension of robustness, models should perform well when the label distribution changes. Most work on class-imbalanced NLP has focused on single-label text classification, but as discussed throughout our survey, it is often difficult to transfer these approaches to multi-label settings. Moreover, class imbalance also poses problems in NLP tasks such as sequence labeling or parsing, and we believe that the interaction of structured prediction models with methods to address class imbalance is a promising area for future research. A main hindrance to making progress on class imbalance in computer vision and NLP alike is that experimental results are often hard to compare Khoshgoftaar, 2019a, 2020). A first important step would be to not restrict baselines to methods of same type, e.g., a new data augmentation approach for class-imbalanced settings should not only compare to other data augmentation methods, but also to using loss functions for class imbalance. Establishing a shared and systematic benchmark of a diverse set of class-imbalanced NLP tasks would be highly beneficial for both researchers and practitioners. Much NLP work only reports aggregate statistics (Harbecke et al., 2022), making it hard to judge the impact on improvements for the minority vs. majority classes. An exception is FSL, where it is common practice to report performance by class frequency. We argue that NLP researchers should always report per-class statistics, which would enable the community to subsequently re-use their data to answer research questions on class imbalance. Reviewers should also value works that analyze performance for relevant minority classes rather than focusing largely only on accuracy improvements, as this is a critical challenge for the entire research field. This paper is a survey, structuring, organizing and describing works and concepts to class imbalance including long-tailed learning. While we touch upon data augmentation and few-shot learning, we do not comprehensively review those areas. Details on the scope of this review have also been described in Sec. 1. The search process for the survey included searching for the keywords class imbalance and long tail in Google Scholar and the ACL Anthology, as well as carefully checking the papers that cite relevant papers. Finally, the paper only constitutes a literature review, it does not yet provide a comprehensive empirical study which is much needed in this research area, but it will be of use in carrying out such a study. We here provide details on a selection of methods surveyed in this paper. Table 3 shows whether they have been applied respectively whether they are applicable in binary, multi-class, and multi-label classification. Moreover, it contains information on whether authors open-sourced their implementation. For links to open-sourced code, see Table 2. Data Augmentation EDA (Wei and Zou, 2019) GitHub GE3 (Wei, 2021) ACL Anthology ECRT  GitHub Loss Functions FL (Lin et al., 2017) GitHub ADL  GitHub LDAM (Cao et al., 2019) GitHub DB  GitHub  