{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing necessary packages to perform certain preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import scipdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url: str, title: str, storage_path=os.path.join(os.getcwd(), \"data\", \"pdfs\")):\n",
    "    \"\"\"\n",
    "    This function is used to download research paper's pdf and store it in the storage path\n",
    "    passed in as an argument\n",
    "\n",
    "    Args:\n",
    "        url (str): Research paper PDF file\n",
    "        \n",
    "        title (str): This is the file name which will be used while storing\n",
    "        \n",
    "        storage_path (str, optional): The path where the pdf file will be stored. \n",
    "        Defaults to os.path.join(os.getcwd(), \"data\", \"pdfs\").\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    file_path = os.path.join(storage_path, f\"{title}.pdf\")\n",
    "    with open(file_path, 'wb') as fd:\n",
    "        fd.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pdf_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Survey on Model Compression for Natural Lang...</td>\n",
       "      <td>http://arxiv.org/pdf/2202.07105v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Noisy Text Data: Achilles' Heel of popular tra...</td>\n",
       "      <td>http://arxiv.org/pdf/2110.03353v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improving the robustness and accuracy of biome...</td>\n",
       "      <td>http://arxiv.org/pdf/2111.08529v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automated essay scoring using efficient transf...</td>\n",
       "      <td>http://arxiv.org/pdf/2102.13136v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annotating the Tweebank Corpus on Named Entity...</td>\n",
       "      <td>http://arxiv.org/pdf/2201.07281v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A Survey on Model Compression for Natural Lang...   \n",
       "1  Noisy Text Data: Achilles' Heel of popular tra...   \n",
       "2  Improving the robustness and accuracy of biome...   \n",
       "3  Automated essay scoring using efficient transf...   \n",
       "4  Annotating the Tweebank Corpus on Named Entity...   \n",
       "\n",
       "                             pdf_url  \n",
       "0  http://arxiv.org/pdf/2202.07105v1  \n",
       "1  http://arxiv.org/pdf/2110.03353v1  \n",
       "2  http://arxiv.org/pdf/2111.08529v1  \n",
       "3  http://arxiv.org/pdf/2102.13136v1  \n",
       "4  http://arxiv.org/pdf/2201.07281v2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the csv file which was generated by pdf_data_collect.py\n",
    "csv_path = \"./ner_task_pdf_links.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "# filtering out the columns which are important\n",
    "df = df.loc[:, [\"title\", \"pdf_url\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over all rows and downloading the research paper PDF's\n",
    "for _, row in df.iterrows():\n",
    "    download_pdf(url=row[\"pdf_url\"], title=row[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Data Prepation - Parsing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_regex():\n",
    "    \"\"\"\n",
    "    Regex to remove url\n",
    "\n",
    "    Returns:\n",
    "        str: regex which can be used to remove urls from string content\n",
    "    \"\"\"\n",
    "    regex = r\"https?:\\/\\/[^\\s]+\"\n",
    "    return regex\n",
    "\n",
    "def remove_reference_num_regex():\n",
    "    \"\"\"\n",
    "    Regex to remove paper references in a research paper\n",
    "\n",
    "    Returns:\n",
    "        str: regex which can be used to remove references eg. [1,2] or [1]\n",
    "    \"\"\"\n",
    "    regex = r\"\\[[\\d,]+\\]\"\n",
    "    return regex\n",
    "\n",
    "def parse_and_clean_pdf(file):\n",
    "    \"\"\"\n",
    "    Accepts research paper pdf file path and performs parsing and cleaning\n",
    "    In order for this function to work, the prerequisite is to make sure the\n",
    "    grobid server is running (bash serve_grobid.sh)\n",
    "\n",
    "    Once the server is running the function called the scipdf parser to extract\n",
    "    string content from each section in the research paper which further undergoes\n",
    "    certain cleaning steps - removing urls, removing reference nums, stripping \n",
    "    unwanted white spaces\n",
    "\n",
    "    Args:\n",
    "        file (str): research pdf file path\n",
    "\n",
    "    Returns:\n",
    "        str: parsed and cleaned pdf string content\n",
    "    \"\"\"\n",
    "    file_content = scipdf.parse_pdf_to_dict(file)\n",
    "    res = []\n",
    "    res.append(file_content[\"abstract\"])\n",
    "    for section in file_content[\"sections\"]:\n",
    "        text = re.sub(remove_url_regex(), \"\", section[\"text\"])\n",
    "        text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "        text = re.sub(remove_reference_num_regex(), \"\", text)\n",
    "        res.append(text.strip())\n",
    "    res = \"\\n\".join(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_data(directory):\n",
    "    \"\"\"\n",
    "    This function accepts a valid directory path which contains all the pdf files\n",
    "    It then iterates over the research pdf files in the directory and calls the\n",
    "    \"parse_and_clean_pdf\" function to extract paper contents as singular file where\n",
    "    paragraphs are separated by a new line\n",
    "\n",
    "    Args:\n",
    "        directory (str): research pdfs directory path\n",
    "    \"\"\"\n",
    "    pdf_text_path = os.path.join(directory, \"pdf_text\")\n",
    "    \n",
    "    def write_content(title, content):\n",
    "        \"\"\"\n",
    "        Store a research paper's content after parsing in the path specified by\n",
    "        \"pdf_text_path\"\n",
    "\n",
    "        Args:\n",
    "            title (str): research paper name\n",
    "            content (str): research paper content\n",
    "        \"\"\"\n",
    "        txt_file_path = os.path.join(pdf_text_path, f\"{title}.txt\")\n",
    "        with open(txt_file_path, 'w') as fd:\n",
    "            fd.write(content)\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files = [file for file in files if file.endswith(\".pdf\")]\n",
    "    files = sorted(files)\n",
    "    for file in files:\n",
    "        content = parse_and_clean_pdf(os.path.join(directory, file))\n",
    "        filename, _ = os.path.splitext(os.path.basename(file))\n",
    "        write_content(filename, content)\n",
    "\n",
    "# calling process_pdf_data with the directory that contains all pdfs\n",
    "process_pdf_data(os.path.join(os.getcwd(), \"data\", \"pdfs\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
